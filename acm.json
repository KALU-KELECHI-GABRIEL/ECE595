[
    {
      "Key": "V68QAANU",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Jacobs, Arthur S.; Beltiukov, Roman; Willinger, Walter; Ferreira, Ronaldo A.; Gupta, Arpit; Granville, Lisandro Z.",
      "Title": "AI/ML for Network Security: The Emperor has no Clothes",
      "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9450-5",
      "ISSN": "",
      "DOI": "10.1145/3548606.3560609",
      "Url": "https://doi.org/10.1145/3548606.3560609",
      "Abstract": "Several recent research efforts have proposed Machine Learning (ML)-based solutions that can detect complex patterns in network traffic for a wide range of network security problems. However, without understanding how these black-box models are making their decisions, network operators are reluctant to trust and deploy them in their production settings. One key reason for this reluctance is that these models are prone to the problem of underspecification, defined here as the failure to specify a model in adequate detail. Not unique to the network security domain, this problem manifests itself in ML models that exhibit unexpectedly poor behavior when deployed in real-world settings and has prompted growing interest in developing interpretable ML solutions (e.g., decision trees) for \"explaining” to humans how a given black-box model makes its decisions. However, synthesizing such explainable models that capture a given black-box model's decisions with high fidelity while also being practical (i.e., small enough in size for humans to comprehend) is challenging.In this paper, we focus on synthesizing high-fidelity and low-complexity decision trees to help network operators determine if their ML models suffer from the problem of underspecification. To this end, we present Trustee, a framework that takes an existing ML model and training dataset as input and generates a high-fidelity, easy-to-interpret decision tree and associated trust report as output. Using published ML models that are fully reproducible, we show how practitioners can use Trustee to identify three common instances of model underspecification; i.e., evidence of shortcut learning, presence of spurious correlations, and vulnerability to out-of-distribution samples.",
      "Date": "2022",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "1537–1551",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Los Angeles, CA, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "artificial intelligence; explainability; interpretability; machine learning; network security; trust",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "ZCC6XACX",
      "Item Type": "conferencePaper",
      "Publication Year": 2023,
      "Author": "Arazzi, Marco; Conti, Mauro; Nocera, Antonino; Picek, Stjepan",
      "Title": "Turning Privacy-preserving Mechanisms against Federated Learning",
      "Publication Title": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "9.80E+12",
      "ISSN": "",
      "DOI": "10.1145/3576915.3623114",
      "Url": "https://doi.org/10.1145/3576915.3623114",
      "Abstract": "Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, researchers proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data.In this paper, we identify a crucial security flaw in such a configuration and design an attack capable of deceiving state-of-the-art defenses for federated learning. The proposed attack includes two operating modes, the first one focusing on convergence inhibition (Adversarial Mode), and the second one aiming at building a deceptive rating injection on the global federated model (Backdoor Mode). The experimental results show the effectiveness of our attack in both its modes, returning on average 60% performance detriment in all the tests on Adversarial Mode and fully effective backdoors in 93% of cases for the tests performed on Backdoor Mode.",
      "Date": "2023",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "1482–1495",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '23",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "federated learning; graph neural network; model poisoning; privacy; recommender systems",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "CGVE9NWW",
      "Item Type": "conferencePaper",
      "Publication Year": 2021,
      "Author": "He, Chaoxiang; Zhu, Bin Benjamin; Ma, Xiaojing; Jin, Hai; Hu, Shengshan",
      "Title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "Publication Title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8454-4",
      "ISSN": "",
      "DOI": "10.1145/3460120.3485378",
      "Url": "https://doi.org/10.1145/3460120.3485378",
      "Abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
      "Date": "2021",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "3159–3176",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '21",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "adversarial attacks; adversarial examples; feature-indistinguishable attack; neural networks; trapdoor enabled defense",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "CI2F564T",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Pang, Ren; Shen, Hua; Zhang, Xinyang; Ji, Shouling; Vorobeychik, Yevgeniy; Luo, Xiapu; Liu, Alex; Wang, Ting",
      "Title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-7089-9",
      "ISSN": "",
      "DOI": "10.1145/3372297.3417253",
      "Url": "https://doi.org/10.1145/3372297.3417253",
      "Abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs – maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models – adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors – leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "Date": "2020",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "85–99",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '20",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual Event, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "adversarial attack; backdoor attack; trojaning attack",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "T2WNT89D",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Romanelli, Marco; Chatzikokolakis, Konstantinos; Palamidessi, Catuscia; Piantanida, Pablo",
      "Title": "Estimating g-Leakage via Machine Learning",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-7089-9",
      "ISSN": "",
      "DOI": "10.1145/3372297.3423363",
      "Url": "https://doi.org/10.1145/3372297.3423363",
      "Abstract": "This paper considers the problem of estimating the information leakage of a system in the black-box scenario, i.e. when the system's internals are unknown to the learner, or too complicated to analyze, and the only available information are pairs of input-output data samples, obtained by submitting queries to the system or provided by a third party. The frequentist approach relies on counting the frequencies to estimate the input-output conditional probabilities, however this method is not accurate when the domain of possible outputs is large. To overcome this difficulty, the estimation of the Bayes error of the ideal classifier was recently investigated using Machine Learning (ML) models, and it has been shown to be more accurate thanks to the ability of those models to learn the input-output correspondence. However, the Bayes vulnerability is only suitable to describe one-try attacks. A more general and flexible measure of leakage is the g-vulnerability, which encompasses several different types of adversaries, with different goals and capabilities. We propose a novel approach to perform black-box estimation of the g-vulnerability using ML which does not require to estimate the conditional probabilities and is suitable for a large class of ML algorithms. First, we formally show the learnability for all data distributions. Then, we evaluate the performance via various experiments using k-Nearest Neighbors and Neural Networks. Our approach outperform the frequentist one when the observables domain is large.",
      "Date": "2020",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "697–716",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '20",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual Event, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "g-vulnerability estimation; machine learning; neural networks",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "3JZ3KK5L",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Li, Zheng; Liu, Yiyong; He, Xinlei; Yu, Ning; Backes, Michael; Zhang, Yang",
      "Title": "Auditing Membership Leakages of Multi-Exit Networks",
      "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9450-5",
      "ISSN": "",
      "DOI": "10.1145/3548606.3559359",
      "Url": "https://doi.org/10.1145/3548606.3559359",
      "Abstract": "Relying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
      "Date": "2022",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "1917–1931",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCS '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Los Angeles, CA, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "machine learning; membership leakages; multi-exit networks",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "6G8E8DUX",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Zhao, Benjamin Zi Hao; Kaafar, Mohamed Ali; Kourtellis, Nicolas",
      "Title": "Not one but many Tradeoffs: Privacy Vs. Utility in Differentially Private Machine Learning",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Cloud Computing Security Workshop",
      "ISBN": "978-1-4503-8084-3",
      "ISSN": "",
      "DOI": "10.1145/3411495.3421352",
      "Url": "https://doi.org/10.1145/3411495.3421352",
      "Abstract": "Data holders are increasingly seeking to protect their user's privacy, whilst still maximizing their ability to produce machine learning (ML) models with high quality predictions. In this work, we empirically evaluate various implementations of differential privacy (DP), and measure their ability to fend off real-world privacy attacks, in addition to measuring their core goal of providing accurate classifications. We establish an evaluation framework to ensure each of these implementations are fairly evaluated. Our selection of DP implementations add DP noise at different positions within the framework, either at the point of data collection/release, during updates while training of the model, or after training by perturbing learned model parameters. We evaluate each implementation across a range of privacy budgets and datasets, each implementation providing the same mathematical privacy guarantees. By measuring the models' resistance to real world attacks of membership and attribute inference, and their classification accuracy. we determine which implementations provide the most desirable tradeoff between privacy and utility. We found that the number of classes of a given dataset is unlikely to influence where the privacy and utility tradeoff occurs, a counter-intuitive inference in contrast to the known relationship of increased privacy vulnerability in datasets with more classes. Additionally, in the scenario that high privacy constraints are required, perturbing input training data before applying ML modeling does not trade off as much utility, as compared to noise added later in the ML process.",
      "Date": "2020",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "15–26",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "CCSW'20",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual Event, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "attribute inference attack; differential privacy attack; machine learning; membership inference attack; privacy; privacy attack; tradeoff; utility",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "3P7NCBYB",
      "Item Type": "conferencePaper",
      "Publication Year": 2023,
      "Author": "Greshake, Kai; Abdelnabi, Sahar; Mishra, Shailesh; Endres, Christoph; Holz, Thorsten; Fritz, Mario",
      "Title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.80E+12",
      "ISSN": "",
      "DOI": "10.1145/3605764.3623985",
      "Url": "https://doi.org/10.1145/3605764.3623985",
      "Abstract": "Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",
      "Date": "2023",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "79–90",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "AISec '23",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "indirect prompt injection; large language models",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "SWCK2LLK",
      "Item Type": "conferencePaper",
      "Publication Year": 2023,
      "Author": "Imgrund, Erik; Ganz, Tom; Härterich, Martin; Pirch, Lukas; Risse, Niklas; Rieck, Konrad",
      "Title": "Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.80E+12",
      "ISSN": "",
      "DOI": "10.1145/3605764.3623915",
      "Url": "https://doi.org/10.1145/3605764.3623915",
      "Abstract": "Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery.",
      "Date": "2023",
      "Date Added": "4/16/24 18:26",
      "Date Modified": "4/16/24 18:26",
      "Access Date": "",
      "Pages": "149–160",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "AISec '23",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "causal learning; confounding effect; large language models; overfitting; vulnerability discovery",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "ICWV8FEK",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Villalobos-Arias, Leonardo; Quesada-López, Christian; Guevara-Coto, Jose; Martínez, Alexandra; Jenkins, Marcelo",
      "Title": "Evaluating hyper-parameter tuning using random search in support vector machines for software effort estimation",
      "Publication Title": "Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering",
      "ISBN": "978-1-4503-8127-7",
      "ISSN": "",
      "DOI": "10.1145/3416508.3417121",
      "Url": "https://doi.org/10.1145/3416508.3417121",
      "Abstract": "Studies in software effort estimation&nbsp;(SEE) have explored the use of hyper-parameter tuning for machine learning algorithms&nbsp;(MLA) to improve the accuracy of effort estimates. In other contexts random search&nbsp;(RS) has shown similar results to grid search, while being less computationally-expensive. In this paper, we investigate to what extent the random search hyper-parameter tuning approach affects the accuracy and stability of support vector regression&nbsp;(SVR) in SEE. Results were compared to those obtained from ridge regression models and grid search-tuned models. A case study with four data sets extracted from the ISBSG 2018 repository shows that random search exhibits similar performance to grid search, rendering it an attractive alternative technique for hyper-parameter tuning. RS-tuned SVR achieved an increase of 0.227 standardized accuracy&nbsp;(SA) with respect to default hyper-parameters. In addition, random search improved prediction stability of SVR models to a minimum ratio of 0.840. The analysis showed that RS-tuned SVR attained performance equivalent to GS-tuned SVR. Future work includes extending this research to cover other hyper-parameter tuning approaches and machine learning algorithms, as well as using additional data sets.",
      "Date": "2020",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "31–40",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "PROMISE 2020",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "empirical study; grid search; hyper-parameter tuning; random search; Software effort estimation; support vector machines",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "W9LYSPZA",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Aljamaan, Hamoud; Alazba, Amal",
      "Title": "Software defect prediction using tree-based ensembles",
      "Publication Title": "Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering",
      "ISBN": "978-1-4503-8127-7",
      "ISSN": "",
      "DOI": "10.1145/3416508.3417114",
      "Url": "https://doi.org/10.1145/3416508.3417114",
      "Abstract": "Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.",
      "Date": "2020",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "1–10",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "PROMISE 2020",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual, USA",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "Bagging; Boosting; Classification; Ensemble Learning; Machine Learning; Prediction; Software Defect",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "ELRRTHQ7",
      "Item Type": "conferencePaper",
      "Publication Year": 2024,
      "Author": "Arteaga Garcia, Emily Judith; Nicolaci Pimentel, João Felipe; Feng, Zixuan; Gerosa, Marco; Steinmacher, Igor; Sarma, Anita",
      "Title": "How to Support ML End-User Programmers through a Conversational Agent",
      "Publication Title": "Proceedings of the IEEE/ACM 46th International Conference on Software Engineering",
      "ISBN": "9.80E+12",
      "ISSN": "",
      "DOI": "10.1145/3597503.3608130",
      "Url": "https://doi.org/10.1145/3597503.3608130",
      "Abstract": "Machine Learning (ML) is increasingly gaining significance for enduser programmer (EUP) applications. However, machine learning end-user programmers (ML-EUPs) without the right background face a daunting learning curve and a heightened risk of mistakes and flaws in their models. In this work, we designed a conversational agent named \"Newton\" as an expert to support ML-EUPs. Newton's design was shaped by a comprehensive review of existing literature, from which we identified six primary challenges faced by ML-EUPs and five strategies to assist them. To evaluate the efficacy of Newton's design, we conducted a Wizard of Oz within-subjects study with 12 ML-EUPs. Our findings indicate that Newton effectively assisted ML-EUPs, addressing the challenges highlighted in the literature. We also proposed six design guidelines for future conversational agents, which can help other EUP applications and software engineering activities.",
      "Date": "2024",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICSE '24",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: <conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "conversational agent; end-user programming; wizard of Oz",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "HCMB48VA",
      "Item Type": "conferencePaper",
      "Publication Year": 2020,
      "Author": "Islam, Md Johirul; Pan, Rangeet; Nguyen, Giang; Rajan, Hridesh",
      "Title": "Repairing deep neural networks: fix patterns and challenges",
      "Publication Title": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
      "ISBN": "978-1-4503-7121-6",
      "ISSN": "",
      "DOI": "10.1145/3377811.3380378",
      "Url": "https://doi.org/10.1145/3377811.3380378",
      "Abstract": "Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.",
      "Date": "2020",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "1135–1146",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICSE '20",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Seoul, South Korea",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "bug fix; bug fix patterns; bugs; deep neural networks",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "YQANKF8S",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Pan, Rangeet; Rajan, Hridesh",
      "Title": "Decomposing convolutional neural networks into reusable and replaceable modules",
      "Publication Title": "Proceedings of the 44th International Conference on Software Engineering",
      "ISBN": "978-1-4503-9221-1",
      "ISSN": "",
      "DOI": "10.1145/3510003.3510051",
      "Url": "https://doi.org/10.1145/3510003.3510051",
      "Abstract": "Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replaceability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3% and 0.5% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by 37 times compared to training the model from scratch.",
      "Date": "2022",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "524–535",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICSE '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Pittsburgh, Pennsylvania",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "cnn; decomposition; deep learning; deep neural network; modularity",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "8Z8WMHCU",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Nguyen, Giang; Islam, Md Johirul; Pan, Rangeet; Rajan, Hridesh",
      "Title": "Manas: mining software repositories to assist AutoML",
      "Publication Title": "Proceedings of the 44th International Conference on Software Engineering",
      "ISBN": "978-1-4503-9221-1",
      "ISSN": "",
      "DOI": "10.1145/3510003.3510052",
      "Url": "https://doi.org/10.1145/3510003.3510052",
      "Abstract": "Today deep learning is widely used for building software. A software engineering problem with deep learning is that finding an appropriate convolutional neural network (CNN) model for the task can be a challenge for developers. Recent work on AutoML, more precisely neural architecture search (NAS), embodied by tools like Auto-Keras aims to solve this problem by essentially viewing it as a search problem where the starting point is a default CNN model, and mutation of this CNN model allows exploration of the space of CNN models to find a CNN model that will work best for the problem. These works have had significant success in producing high-accuracy CNN models. There are two problems, however. First, NAS can be very costly, often taking several hours to complete. Second, CNN models produced by NAS can be very complex that makes it harder to understand them and costlier to train them. We propose a novel approach for NAS, where instead of starting from a default CNN model, the initial model is selected from a repository of models extracted from GitHub. The intuition being that developers solving a similar problem may have developed a better starting point compared to the default model. We also analyze common layer patterns of CNN models in the wild to understand changes that the developers make to improve their models. Our approach uses commonly occurring changes as mutation operators in NAS. We have extended Auto-Keras to implement our approach. Our evaluation using 8 top voted problems from Kaggle for tasks including image classification and image regression shows that given the same search time, without loss of accuracy, Manas produces models with 42.9% to 99.6% fewer number of parameters than Auto-Keras' models. Benchmarked on GPU, Manas' models train 30.3% to 641.6% faster than Auto-Keras' models.",
      "Date": "2022",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "1368–1380",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICSE '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Pittsburgh, Pennsylvania",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "AutoML; deep learning; mining software repositories; MSR",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "3XSYZKDY",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Wan, Chengcheng; Liu, Shicheng; Xie, Sophie; Liu, Yifan; Hoffmann, Henry; Maire, Michael; Lu, Shan",
      "Title": "Automated testing of software that uses machine learning APIs",
      "Publication Title": "Proceedings of the 44th International Conference on Software Engineering",
      "ISBN": "978-1-4503-9221-1",
      "ISSN": "",
      "DOI": "10.1145/3510003.3510068",
      "Url": "https://doi.org/10.1145/3510003.3510068",
      "Abstract": "An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.",
      "Date": "2022",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "212–224",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICSE '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Pittsburgh, Pennsylvania",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "machine learning; machine learning API; software testing",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "DPCPYVHA",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Vélez, Tatiana Castro; Khatchadourian, Raffi; Bagherzadeh, Mehdi; Raja, Anita",
      "Title": "Challenges in migrating imperative deep learning programs to graph execution: an empirical study",
      "Publication Title": "Proceedings of the 19th International Conference on Mining Software Repositories",
      "ISBN": "978-1-4503-9303-4",
      "ISSN": "",
      "DOI": "10.1145/3524842.3528455",
      "Url": "https://doi.org/10.1145/3524842.3528455",
      "Abstract": "Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the \"best of both worlds,\" the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges—and resultant bugs—involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation—the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.",
      "Date": "2022",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "469–481",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "MSR '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Pittsburgh, Pennsylvania",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "deep learning; empirical studies; graph-based execution; hybrid programming paradigms; imperative programs; software evolution",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "RUYRDE69",
      "Item Type": "conferencePaper",
      "Publication Year": 2022,
      "Author": "Haryono, Stefanus A.; Kang, Hong Jin; Sharma, Abhishek; Sharma, Asankhaya; Santosa, Andrew; Yi, Ang Ming; Lo, David",
      "Title": "Automated identification of libraries from vulnerability data: can we do better?",
      "Publication Title": "Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension",
      "ISBN": "978-1-4503-9298-3",
      "ISSN": "",
      "DOI": "10.1145/3524610.3527893",
      "Url": "https://doi.org/10.1145/3524610.3527893",
      "Abstract": "Software engineers depend heavily on software libraries and have to update their dependencies once vulnerabilities are found in them. Software Composition Analysis (SCA) helps developers identify vulnerable libraries used by an application. A key challenge is the identification of libraries related to a given reported vulnerability in the National Vulnerability Database (NVD), which may not explicitly indicate the affected libraries. Recently, researchers have tried to address the problem of identifying the libraries from an NVD report by treating it as an extreme multi-label learning (XML) problem, characterized by its large number of possible labels and severe data sparsity. As input, the NVD report is provided, and as output, a set of relevant libraries is returned.In this work, we evaluated multiple XML techniques. While previous work only evaluated a traditional XML technique, FastXML, we trained four other traditional XML models (DiSMEC, Parabel, Bonsai, ExtremeText) as well as two deep learning-based models (XML-CNN and LightXML). We compared both their effectiveness and the time cost of training and using the models for predictions. We find that other than DiSMEC and XML-CNN, recent XML models outperform the FastXML model by 3%–10% in terms of F1-scores on Top-k (k=1,2,3) predictions. Furthermore, we observe significant improvements in both the training and prediction time of these XML models, with Bonsai and Parabel model achieving 627x and 589x faster training time and 12x faster prediction time from the FastXML baseline. We discuss the implications of our experimental results and highlight limitations for future work to address.",
      "Date": "2022",
      "Date Added": "4/16/24 18:27",
      "Date Modified": "4/16/24 18:27",
      "Access Date": "",
      "Pages": "178–189",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "ICPC '22",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "event-place: Virtual Event",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "machine learning; multi-label classification; vulnerability report",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "CTM5I3BF",
      "Item Type": "journalArticle",
      "Publication Year": 2023,
      "Author": "Suneja, Sahil; Zhuang, Yufan; Zheng, Yunhui; Laredo, Jim; Morari, Alessandro; Khurana, Udayan",
      "Title": "Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3597202",
      "Url": "https://doi.org/10.1145/3597202",
      "Abstract": "AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.",
      "Date": "2023-09",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": 6,
      "Volume": 32,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "curriculum learning; data augmentation; explainability; Machine learning; neural networks; reliability; signal awareness",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "74MYC3ZW",
      "Item Type": "journalArticle",
      "Publication Year": 2023,
      "Author": "Li, Siyuan; Wang, Yongpan; Dong, Chaopeng; Yang, Shouguo; Li, Hong; Sun, Hao; Lang, Zhe; Chen, Zuxin; Wang, Weijie; Zhu, Hongsong; Sun, Limin",
      "Title": "LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3625294",
      "Url": "https://doi.org/10.1145/3625294",
      "Abstract": "Third-party libraries (TPLs) are extensively utilized by developers to expedite the software development process and incorporate external functionalities. Nevertheless, insecure TPL reuse can lead to significant security risks. Existing methods, which involve extracting strings or conducting function matching, are employed to determine the presence of TPL code in the target binary. However, these methods often yield unsatisfactory results due to the recurrence of strings and the presence of numerous similar non-homologous functions. Furthermore, the variation in C/C++ binaries across different optimization options and architectures exacerbates the problem. Additionally, existing approaches struggle to identify specific pieces of reused code in the target binary, complicating the detection of complex reuse relationships and impeding downstream tasks. And, we call this issue the poor interpretability of TPL detection results.In this article, we observe that TPL reuse typically involves not just isolated functions but also areas encompassing several adjacent functions on the Function Call Graph (FCG). We introduce LibAM, a novel Area Matching framework that connects isolated functions into function areas on FCG and detects TPLs by comparing the similarity of these function areas, significantly mitigating the impact of different optimization options and architectures. Furthermore, LibAM is the first approach capable of detecting the exact reuse areas on FCG and offering substantial benefits for downstream tasks. To validate our approach, we compile the first TPL detection dataset for C/C++ binaries across various optimization options and architectures. Experimental results demonstrate that LibAM outperforms all existing TPL detection methods and provides interpretable evidence for TPL detection results by identifying exact reuse areas. We also evaluate LibAM’s scalability on large-scale, real-world binaries in IoT firmware and generate a list of potential vulnerabilities for these devices. Our experiments indicate that the Area Matching framework performs exceptionally well in the TPL detection task and holds promise for other binary similarity analysis tasks. Last but not least, by analyzing the detection results of IoT firmware, we make several interesting findings, for instance, different target binaries always tend to reuse the same code area of TPL. The datasets and source code used in this article are available at .",
      "Date": "2023-12",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": 2,
      "Volume": 33,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "software component analysis; Static binary analysis; third-party library detection",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "M56W2ALI",
      "Item Type": "journalArticle",
      "Publication Year": 2024,
      "Author": "Wan, Xiaohui; Zheng, Zheng; Qin, Fangyun; Lu, Xuhui",
      "Title": "Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3649596",
      "Url": "https://doi.org/10.1145/3649596",
      "Abstract": "Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this paper, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4) integrating data complexity information into the learning process can enhance an algorithm’s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.",
      "Date": "2024-02",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "<p>Just Accepted</p>",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "data complexity; Defect prediction; instance hardness.; machine learning",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "7KZKAIU6",
      "Item Type": "journalArticle",
      "Publication Year": 2023,
      "Author": "Attaoui, Mohammed; Fahmy, Hazem; Pastore, Fabrizio; Briand, Lionel",
      "Title": "Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3550271",
      "Url": "https://doi.org/10.1145/3550271",
      "Abstract": "Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. Experimental results show the superior ability of SAFE in identifying different root causes of DNN errors based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining, while saving significant execution time and memory when compared to alternatives.",
      "Date": "2023-04",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": 3,
      "Volume": 32,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "clustering; DNN debugging; DNN explanation; DNN functional safety analysis; transfer learning",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "PZN99XB3",
      "Item Type": "journalArticle",
      "Publication Year": 2024,
      "Author": "Aghababaeyan, Zohreh; Abdellatif, Manel; Dadkhah, Mahboubeh; Briand, Lionel",
      "Title": "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3644388",
      "Url": "https://doi.org/10.1145/3644388",
      "Abstract": "Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability: (1) White-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.",
      "Date": "2024-02",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "<p>Just Accepted</p>",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "Deep Learning Model Evaluation; Deep Neural Network; Diversity; DNN Fault Detection; Model Retraining Guidance; Multi-Objective Optimization; Test Case Selection; Uncertainty Metrics",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "5T39SIRU",
      "Item Type": "journalArticle",
      "Publication Year": 2024,
      "Author": "Attaoui, Mohammed Oualid; Fahmy, Hazem; Pastore, Fabrizio; Briand, Lionel",
      "Title": "Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3643671",
      "Url": "https://doi.org/10.1145/3643671",
      "Abstract": "The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.",
      "Date": "2024-02",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": null,
      "Volume": null,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "<p>Just Accepted</p>",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "Clustering; DNN Debugging; DNN Explanation; DNN Functional Safety Analysis; Transfer Learning",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "3G9X6GVL",
      "Item Type": "journalArticle",
      "Publication Year": 2023,
      "Author": "Huang, Wei; Zhao, Xingyu; Banks, Alec; Cox, Victoria; Huang, Xiaowei",
      "Title": "Hierarchical Distribution-aware Testing of Deep Learning",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3625290",
      "Url": "https://doi.org/10.1145/3625290",
      "Abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution–agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model’s dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm–based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
      "Date": "2023-12",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": 2,
      "Volume": 33,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "adversarial examples detection; Deep learning robustness; distribution-aware testing; natural perturbations; robustness growth; safe AI",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    },
    {
      "Key": "3BUM8496",
      "Item Type": "journalArticle",
      "Publication Year": 2023,
      "Author": "Ding, Zishuo; Li, Heng; Shang, Weiyi; Chen, Tse-Hsun (Peter)",
      "Title": "Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks",
      "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
      "ISBN": "",
      "ISSN": "1049-331X",
      "DOI": "10.1145/3542944",
      "Url": "https://doi.org/10.1145/3542944",
      "Abstract": "Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.",
      "Date": "2023-03",
      "Date Added": "4/16/24 18:25",
      "Date Modified": "4/16/24 18:25",
      "Access Date": "",
      "Pages": "",
      "Num Pages": "",
      "Issue": 2,
      "Volume": 32,
      "Number Of Volumes": "",
      "Journal Abbreviation": "",
      "Short Title": "",
      "Series": "",
      "Series Number": "",
      "Series Text": "",
      "Series Title": "",
      "Publisher": "",
      "Place": "",
      "Language": "",
      "Rights": "",
      "Type": "",
      "Archive": "",
      "Archive Location": "",
      "Library Catalog": "",
      "Call Number": "",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Notes": "",
      "File Attachments": "",
      "Link Attachments": "",
      "Manual Tags": "code embeddings; Machine learning; neural network; source code representation",
      "Automatic Tags": "",
      "Editor": "",
      "Series Editor": "",
      "Translator": "",
      "Contributor": "",
      "Attorney Agent": "",
      "Book Author": "",
      "Cast Member": "",
      "Commenter": "",
      "Composer": "",
      "Cosponsor": "",
      "Counsel": "",
      "Interviewer": "",
      "Producer": "",
      "Recipient": "",
      "Reviewed Author": "",
      "Scriptwriter": "",
      "Words By": "",
      "Guest": "",
      "Number": "",
      "Edition": "",
      "Running Time": "",
      "Scale": "",
      "Medium": "",
      "Artwork Size": "",
      "Filing Date": "",
      "Application Number": "",
      "Assignee": "",
      "Issuing Authority": "",
      "Country": "",
      "Meeting Name": "",
      "Conference Name": "",
      "Court": "",
      "References": "",
      "Reporter": "",
      "Legal Status": "",
      "Priority Numbers": "",
      "Programming Language": "",
      "Version": "",
      "System": "",
      "Code": "",
      "Code Number": "",
      "Section": "",
      "Session": "",
      "Committee": "",
      "History": "",
      "Legislative Body": ""
    }
   ]