[
    {
      "Key": "5JIVD6ZQ",
      "Item Type": "conferencePaper",
      "Publication Year": "2014",
      "Author": "Ye, Xin; Bunescu, Razvan; Liu, Chang",
      "Title": "Learning to rank relevant files for bug reports using domain knowledge",
      "Publication Title": "Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering",
      "ISBN": "978-1-4503-3056-5",
      "DOI": "10.1145/2635868.2635874",
      "Url": "https://doi.org/10.1145/2635868.2635874",
      "Abstract": "When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects.",
      "Date": "2014",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "689–699",
      "Series": "FSE 2014",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Hong Kong, China",
      "Manual_Tags": "bug reports; learning to rank; software maintenance"
    },
    {
      "Key": "YCGUWCPP",
      "Item Type": "conferencePaper",
      "Publication Year": "2016",
      "Author": "Tan, Shin Hwei; Yoshida, Hiroaki; Prasad, Mukul R.; Roychoudhury, Abhik",
      "Title": "Anti-patterns in search-based program repair",
      "Publication Title": "Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
      "ISBN": "978-1-4503-4218-6",
      "DOI": "10.1145/2950290.2950295",
      "Url": "https://doi.org/10.1145/2950290.2950295",
      "Abstract": "Search-based program repair automatically searches for a program fix within a given repair space. This may be accomplished by retrofitting a generic search algorithm for program repair as evidenced by the GenProg tool, or by building a customized search algorithm for program repair as in SPR. Unfortunately, automated program repair approaches may produce patches that may be rejected by programmers, because of which past works have suggested using human-written patches to produce templates to guide program repair. In this work, we take the position that we will not provide templates to guide the repair search because that may unduly restrict the repair space and attempt to overfit the repairs into one of the provided templates. Instead, we suggest the use of a set of anti-patterns — a set of generic forbidden transformations that can be enforced on top of any search-based repair tool. We show that by enforcing our anti-patterns, we obtain repairs that localize the correct lines or functions, involve less deletion of program functionality, and are mostly obtained more efficiently. Since our set of anti-patterns are generic, we have integrated them into existing search based repair tools, including GenProg and SPR, thereby allowing us to obtain higher quality program patches with minimal effort.",
      "Date": "2016",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "727–738",
      "Series": "FSE 2016",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Seattle, WA, USA",
      "Manual_Tags": "and repair; Debugging; fault localization"
    },
    {
      "Key": "ZFBVWNHI",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Li, Jiangnan; Yang, Yingyuan; Sun, Jinyuan Stella; Tomsovic, Kevin; Qi, Hairong",
      "Title": "ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems",
      "Publication Title": "Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8287-8",
      "DOI": "10.1145/3433210.3437513",
      "Url": "https://doi.org/10.1145/3433210.3437513",
      "Abstract": "Recent research demonstrated that the superficially well-trained machine learning (ML) models are highly vulnerable to adversarial examples. As ML techniques are becoming a popular solution for cyber-physical systems (CPSs) applications in research literatures, the security of these applications is of concern. However, current studies on adversarial machine learning (AML) mainly focus on pure cyberspace domains. The risks the adversarial examples can bring to the CPS applications have not been well investigated. In particular, due to the distributed property of data sources and the inherent physical constraints imposed by CPSs, the widely-used threat models and the state-of-the-art AML algorithms in previous cyberspace research become infeasible.We study the potential vulnerabilities of ML applied in CPSs by proposing Constrained Adversarial Machine Learning (ConAML), which generates adversarial examples that satisfy the intrinsic constraints of the physical systems. We first summarize the difference between AML in CPSs and AML in existing cyberspace systems and propose a general threat model for ConAML. We then design a best-effort search algorithm to iteratively generate adversarial examples with linear physical constraints. We evaluate our algorithms with simulations of two typical CPSs, the power grids and the water treatment system. The results show that our ConAML algorithms can effectively generate adversarial examples which significantly decrease the performance of the ML models even under practical constraints.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "52–66",
      "Series": "ASIA CCS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Hong Kong",
      "Manual_Tags": "adversarial machine learning; cyber-physical system; intrusion detection"
    },
    {
      "Key": "YWFZ6VEW",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Wurzenberger, Markus; Höld, Georg; Landauer, Max; Skopik, Florian; Kastner, Wolfgang",
      "Title": "Creating Character-based Templates for Log Data to Enable Security Event Classification",
      "Publication Title": "Proceedings of the 15th ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6750-9",
      "DOI": "10.1145/3320269.3384722",
      "Url": "https://doi.org/10.1145/3320269.3384722",
      "Abstract": "Log data analysis is an essential task when it comes to understanding a computer's or a network's system behavior, and enables security analysis, fault diagnosis, performance analysis, or intrusion detection. An established technique for log analysis is log line clustering, which allows to group similar events and to detect outliers, malicious clusters or changes in system behavior. However, log line clusters usually lack meaningful descriptions that are required to understand the information provided by log lines within a cluster. Template generators allow to produce such descriptions in form of patterns that match all log lines within a cluster and therefore describe the common features of the lines. Current approaches only allow generation of token-based (e.g., space-separated words) templates, which are often inaccurate, because they do not recognize words that can be spelled differently as similar and require further information on the structure and syntax of the data, such as predefined delimiters. Consequently, novel character-based template generators are required that provide robust templates for any type of computer log data, which can be applied in security information and event management (SIEM) solutions, for continuous auditing, quality inspection and control. In this paper, we propose a novel approach for computing character-based templates, which combines comparison-based methods and heuristics. To achieve this goal, we solve the problem of efficiently calculating a multi-line alignment for a group of log lines and compute an accurate approximation of the optimal character-based template, while reducing the runtime from O(n^m) to O(mn^2). We demonstrate the accuracy of our approach in a detailed evaluation, applying a newly introduced measure for accuracy, the Sim-Score, which can be computed independently from a ground truth, and the established F-Score. Furthermore, we assess the robustness of the algorithm and the influence of different log data properties on the quality of the resulting templates.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "141–152",
      "Series": "ASIA CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Taipei, Taiwan",
      "Manual_Tags": "character-based templates; log analysis; multi-line alignment; template generation"
    },
    {
      "Key": "6H9JVFQN",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Namba, Ryota; Sakuma, Jun",
      "Title": "Robust Watermarking of Neural Network with Exponential Weighting",
      "Publication Title": "Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6752-3",
      "DOI": "10.1145/3321705.3329808",
      "Url": "https://doi.org/10.1145/3321705.3329808",
      "Abstract": "Deep learning has been achieving top levels of performance in many tasks. However, since it is costly to train a deep learning model, neural network models must be treated as valuable intellectual properties. One concern arising from our current situation is that malicious users might redistribute proprietary models or provide prediction services using such models without permission. One promising solution to this problem is digital watermarking, which works by embedding a mechanism into the model so that the model owners can verify their ownership of the model externally. In this study, we present a novel attack method against such watermarks known as query modification and demonstrate that all currently existing watermarking methods are vulnerable to either query modification or other existing attack methods (such as model modification). To overcome these vulnerabilities, we then present a novel watermarking method that we have named exponential weighting and experimentally show that our watermarking method achieves high watermark verification performance even under malicious invalidation processing attempts by unauthorized service providers (such as model modification and query modification) without sacrificing the predictive performance of the neural network model itself.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "228–240",
      "Series": "Asia CCS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Auckland, New Zealand",
      "Manual_Tags": "deep neural network; watermark"
    },
    {
      "Key": "SIZWH4H4",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Sarkar, Esha; Benkraouda, Hadjer; Maniatakos, Michail",
      "Title": "I came, I saw, I hacked: Automated Generation of Process-independent Attacks for Industrial Control Systems",
      "Publication Title": "Proceedings of the 15th ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6750-9",
      "DOI": "10.1145/3320269.3384730",
      "Url": "https://doi.org/10.1145/3320269.3384730",
      "Abstract": "Malicious manipulations on Industrial Control Systems (ICSs) endanger critical infrastructures, causing unprecedented losses. State-of-the-art research in the discovery and exploitation of vulnerability typically assumes full visibility and control of the industrial process, which in real-world scenarios is unrealistic. In this work, we investigate the possibility of an automated end-to-end attack for an unknown control process in the constrained scenario of infecting just one industrial computer. We create databases of human-machine interface images, and Programmable Logic Controller (PLC) binaries using publicly available resources to train machine-learning models for modular and granular fingerprinting of the ICS sectors and the processes, respectively. We then explore control-theoretic attacks on the process leveraging common/ubiquitous control algorithm modules like Proportional Integral Derivative blocks using a PLC binary reverse-engineering tool, causing stable or oscillatory deviations within the operational limits of the plant. We package the automated attack and evaluate it against a benchmark chemical process, demonstrating the feasibility of advanced attacks even in constrained scenarios.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "744–758",
      "Series": "ASIA CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Taipei, Taiwan",
      "Manual_Tags": "fingerprinting; industrial control systems security; machine learning; process-aware attacks"
    },
    {
      "Key": "38IG4FDB",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Ji, Yuede; Cui, Lei; Huang, H. Howie",
      "Title": "BugGraph: Differentiating Source-Binary Code Similarity with Graph Triplet-Loss Network",
      "Publication Title": "Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8287-8",
      "DOI": "10.1145/3433210.3437533",
      "Url": "https://doi.org/10.1145/3433210.3437533",
      "Abstract": "Binary code similarity detection, which answers whether two pieces of binary code are similar, has been used in a number of applications,such as vulnerability detection and automatic patching. Existing approaches face two hurdles in their efforts to achieve high accuracy and coverage: (1) the problem of source-binary code similarity detection, where the target code to be analyzed is in the binary format while the comparing code (with ground truth) is in source code format. Meanwhile, the source code is compiled to the comparing binary code with either a random or fixed configuration (e.g.,architecture, compiler family, compiler version, and optimization level), which significantly increases the difficulty of code similarity detection; and (2) the existence of different degrees of code similarity. Less similar code is known to be more, if not equally, important in various applications such as binary vulnerability study. To address these challenges, we design BugGraph, which performs source-binary code similarity detection in two steps. First, BugGraph identifies the compilation provenance of the target binary and compiles the comparing source code to a binary with the same provenance.Second, BugGraph utilizes a new graph triplet-loss network on the attributed control flow graph to produce a similarity ranking. The experiments on four real-world datasets show that BugGraph achieves 90% and 75% true positive rate for syntax equivalent and similar code, respectively, an improvement of 16% and 24% overstate-of-the-art methods. Moreover, BugGraph is able to identify 140 vulnerabilities in six commercial firmware.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "702–715",
      "Series": "ASIA CCS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Hong Kong",
      "Manual_Tags": "binary code; code similarity; graph embedding; vulnerability"
    },
    {
      "Key": "K8K6825D",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Andrade, Daniel; Takahashi, Yusuke; Hasumi, Daichi",
      "Title": "POSTER: Detecting Suspicious Processes from Log-Data via a Bayesian Block Model",
      "Publication Title": "Proceedings of the 15th ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6750-9",
      "DOI": "10.1145/3320269.3405445",
      "Url": "https://doi.org/10.1145/3320269.3405445",
      "Abstract": "Analyzing the behavior of an attacker is critical for determining the scope of damage of a cyber attack, recovering, and fixing system vulnerabilities. However, finding all attacker's traces from log data is a laborsome task, where the performance of existing machine learning methods is still insufficient. In this work, we focus on the task of detecting all processes that were executed by the attacker. For this task, standard anomaly detection methods like Isolation Forest, perform poorly, due to many processes that are used by both the attacker and the client user. Therefore, we propose to incorporate prior knowledge about the temporal concentration of the attacker's activity. In general, we expect that an attacker is active only during a relatively small time window (block assumption), rather than being active at completely random time points. We propose a generative model that allows us to incorporate such prior knowledge effectively. Experiments on intrusion log data, shows that the proposed method achieves considerably better detection performance than a strong baseline method which also incorporates the block assumption.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "922–924",
      "Series": "ASIA CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Taipei, Taiwan",
      "Manual_Tags": "block anomaly detection; dataset creation; forensic data analysis; generative model"
    },
    {
      "Key": "N5T24YAN",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Ding, Fei; Li, Hongda; Luo, Feng; Hu, Hongxin; Cheng, Long; Xiao, Hai; Ge, Rong",
      "Title": "DeepPower: Non-intrusive and Deep Learning-based Detection of IoT Malware Using Power Side Channels",
      "Publication Title": "Proceedings of the 15th ACM Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6750-9",
      "DOI": "10.1145/3320269.3384727",
      "Url": "https://doi.org/10.1145/3320269.3384727",
      "Abstract": "The vulnerability of Internet of Things (IoT) devices to malware attacks poses huge challenges to current Internet security. The IoT malware attacks are usually composed of three stages: intrusion, infection and monetization. Existing approaches for IoT malware detection cannot effectively identify the executed malicious activities at intrusion and infection stages, and thus cannot help stop potential attacks timely. In this paper, we present DeepPower, a non-intrusive approach to infer malicious activities of IoT malware via analyzing power side-channel signals using deep learning. DeepPower first filters raw power signals of IoT devices to obtain suspicious signals, and then performs a fine-grained analysis on these signals to infer corresponding executed activities inside the devices. DeepPower determines whether there exists an ongoing malware infection by conducting a correlation analysis on these identified activities. We implement a prototype of DeepPower leveraging low-cost sensors and devices and evaluate the effectiveness of DeepPower against real-world IoT malware using commodity IoT devices. Our experimental results demonstrate that DeepPower is able to detect infection activities of different IoT malware with a high accuracy without any changes to the monitored devices.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "33–46",
      "Series": "ASIA CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Taipei, Taiwan",
      "Manual_Tags": "deep learning; IoT; malware detection; non-intrusive; power side channels"
    },
    {
      "Key": "D5E6JFFA",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Lin, Yuhang; Tunde-Onadele, Olufogorehan; Gu, Xiaohui",
      "Title": "CDL: Classified Distributed Learning for Detecting Security Attacks in Containerized Applications",
      "Publication Title": "Proceedings of the 36th Annual Computer Security Applications Conference",
      "ISBN": "978-1-4503-8858-0",
      "DOI": "10.1145/3427228.3427236",
      "Url": "https://doi.org/10.1145/3427228.3427236",
      "Abstract": "Containers have been widely adopted in production computing environments for its efficiency and low overhead of isolation. However, recent studies have shown that containerized applications are prone to various security attacks. Moreover, containerized applications are often highly dynamic and short-lived, which further exacerbates the problem. In this paper, we present CDL, a classified distributed learning framework to achieve efficient security attack detection for containerized applications. CDL integrates online application classification and anomaly detection to overcome the challenge of lacking sufficient training data for dynamic short-lived containers while considering diversified normal behaviors in different applications. We have implemented a prototype of CDL and evaluated it over 33 real world vulnerability attacks in 24 commonly used server applications. Our experimental results show that CDL can reduce the false positive rate from over 12% to 0.24% compared to traditional anomaly detection schemes without aggregating training data. By introducing application classification into container behavior learning, CDL can improve the detection rate from catching 20 attacks to 31 attacks before those attacks succeed. CDL is light-weight, which can complete application classification and anomaly detection for each data sample within a few milliseconds.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "179–188",
      "Series": "ACSAC '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Anomaly Detection; Container Security; Machine Learning"
    },
    {
      "Key": "FYPGCNFR",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Ganz, Tom; Imgrund, Erik; Härterich, Martin; Rieck, Konrad",
      "Title": "PAVUDI: Patch-based Vulnerability Discovery using Machine Learning",
      "Publication Title": "Proceedings of the 39th Annual Computer Security Applications Conference",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3627106.3627188",
      "Url": "https://doi.org/10.1145/3627106.3627188",
      "Abstract": "Machine learning has been increasingly adopted for automatic security vulnerability discovery in research and industry. The ability to automatically identify and prioritize bugs in patches is crucial to organizations seeking to defend against potential threats. Previous works, however only consider bug discovery on statement, function or file level. How one would apply them to patches in realistic scenarios remains unclear. This paper presents a novel deep learning-based approach leveraging an interprocedural patch graph representation and graph neural networks to analyze software patches for identifying and locating potential security vulnerabilities. We modify current state-of-the-art learning-based static analyzers to be applicable to patches and show that our patch-based vulnerability discovery method, a context and flow-sensitive learning-based model, has a more than increased detection performance, is twice as robust against concept drift after model deployment and is particularly better suited for analyzing large patches. In comparison, other methods already lose their efficiency when a patch touches more than five methods.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "704–717",
      "Series": "ACSAC '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Patches; Program Representations; Vulnerability Discovery"
    },
    {
      "Key": "SVXM6IYL",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Tan, Mingtian; Xie, Xiaofei; Sun, Jun; Wang, Tianhao",
      "Title": "Mitigating Membership Inference Attacks via Weighted Smoothing",
      "Publication Title": "Proceedings of the 39th Annual Computer Security Applications Conference",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3627106.3627189",
      "Url": "https://doi.org/10.1145/3627106.3627189",
      "Abstract": "Recent advancements in deep learning have spotlighted a crucial privacy vulnerability to membership inference attack (MIA), where adversaries can determine if specific data was present in a training set, thus potentially revealing sensitive information. In this paper, we introduce a technique, weighted smoothing (WS), to mitigate MIA risks. Our approach is anchored on the observation that training samples differ in their vulnerability to MIA, primarily based on their distance to clusters of similar samples. The intuition is clusters will make model predictions more confident and increase MIA risks. Thus WS strategically introduces noise to training samples, depending on whether they are near a cluster or isolated. We evaluate WS against MIAs on multiple benchmark datasets and model architectures, demonstrating its effectiveness. We publish code at https://github.com/BennyTMT/weighted-smoothing.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "787–798",
      "Series": "ACSAC '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>"
    },
    {
      "Key": "4EXYAHRW",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Patrick-Evans, James; Cavallaro, Lorenzo; Kinder, Johannes",
      "Title": "Probabilistic Naming of Functions in Stripped Binaries",
      "Publication Title": "Proceedings of the 36th Annual Computer Security Applications Conference",
      "ISBN": "978-1-4503-8858-0",
      "DOI": "10.1145/3427228.3427265",
      "Url": "https://doi.org/10.1145/3427228.3427265",
      "Abstract": "Debugging symbols in binary executables carry the names of functions and global variables. When present, they greatly simplify the process of reverse engineering, but they are almost always removed (stripped) for deployment. We present the design and implementation of punstrip, a tool which combines a probabilistic fingerprint of binary code based on high-level features with a probabilistic graphical model to learn the relationship between function names and program structure. As there are many naming conventions and developer styles, functions from different applications do not necessarily have the exact same name, even if they implement the exact same functionality. We therefore evaluate punstrip across three levels of name matching: exact; an approach based on natural language processing of name components; and using Symbol2Vec, a new embedding of function names based on random walks of function call graphs. We show that our approach is able to recognize functions compiled across different compilers and optimization levels and then demonstrate that punstrip can predict semantically similar function names based on code structure. We evaluate our approach over open source C binaries from the Debian Linux distribution and compare against the state of the art.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "373–385",
      "Series": "ACSAC '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "binaries; function names; machine learning"
    },
    {
      "Key": "8U9LCJPP",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Li, Yu; Liu, Yannan; Li, Min; Tian, Ye; Luo, Bo; Xu, Qiang",
      "Title": "D2NN: a fine-grained dual modular redundancy framework for deep neural networks",
      "Publication Title": "Proceedings of the 35th Annual Computer Security Applications Conference",
      "ISBN": "978-1-4503-7628-0",
      "DOI": "10.1145/3359789.3359831",
      "Url": "https://doi.org/10.1145/3359789.3359831",
      "Abstract": "Deep Neural Networks (DNNs) have attracted mainstream adoption in various application domains. Their reliability and security are therefore serious concerns in those safety-critical applications such as surveillance and medical systems. In this paper, we propose a novel dual modular redundancy framework for DNNs, namely D2NN, which is able to tradeoff the system robustness with overhead in a fine-grained manner. We evaluate D2NN framework with DNN models trained on MNIST and CIFAR10 datasets under fault injection attacks, and experimental results demonstrate the efficacy of our proposed solution.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "138–147",
      "Series": "ACSAC '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: San Juan, Puerto Rico, USA",
      "Manual_Tags": "DNN; dual modular redundancy; fault injection attack; security"
    },
    {
      "Key": "Y7IYHK4J",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Yan, Hua; Sui, Yulei; Chen, Shiping; Xue, Jingling",
      "Title": "Machine-Learning-Guided Typestate Analysis for Static Use-After-Free Detection",
      "Publication Title": "Proceedings of the 33rd Annual Computer Security Applications Conference",
      "ISBN": "978-1-4503-5345-8",
      "DOI": "10.1145/3134600.3134620",
      "Url": "https://doi.org/10.1145/3134600.3134620",
      "Abstract": "Typestate analysis relies on pointer analysis for detecting temporal memory safety errors, such as use-after-free (UAF). For large programs, scalable pointer analysis is usually imprecise in analyzing their hard \"corner cases\", such as infeasible paths, recursion cycles, loops, arrays, and linked lists. Due to a sound over-approximation of the points-to information, a large number of spurious aliases will be reported conservatively, causing the corresponding typestate analysis to report a large number of false alarms. Thus, the usefulness of typestate analysis for heap-intensive clients, like UAF detection, becomes rather limited, in practice.We introduce Tac, a static UAF detector that bridges the gap between typestate and pointer analyses by machine learning. Tac learns the correlations between program features and UAF-related aliases by using a Support Vector Machine (SVM) and applies this knowledge to further disambiguate the UAF-related aliases reported imprecisely by the pointer analysis so that only the ones validated by its SVM classifier are further investigated by the typestate analysis. Despite its unsoundness, Tac represents a practical typestate analysis approach for UAF detection. We have implemented Tac in LLVM-3.8.0 and evaluated it using a set of eight open-source C/C++ programs. The results show that Tac is effective (in terms of finding 5 known CVE vulnerabilities, 1 known bug, and 8 new bugs with a low false alarm rate) and scalable (in terms of analyzing a large codebase with 2,098 KLOC in just over 4 hours).",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "42–54",
      "Series": "ACSAC '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Orlando, FL, USA",
      "Manual_Tags": "machine learning; static analysis; use-after-free; vulnerability detection"
    },
    {
      "Key": "U8PYHNPQ",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Weeks, Connor; Cheruvu, Aravind; Abdullah, Sifat Muhammad; Kanchi, Shravya; Yao, Daphne; Viswanath, Bimal",
      "Title": "A First Look at Toxicity Injection Attacks on Open-domain Chatbots",
      "Publication Title": "Proceedings of the 39th Annual Computer Security Applications Conference",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3627106.3627122",
      "Url": "https://doi.org/10.1145/3627106.3627122",
      "Abstract": "Chatbot systems have improved significantly because of the advances made in language modeling. These machine learning systems follow an end-to-end data-driven learning paradigm and are trained on large conversational datasets. Imperfections or harmful biases in the training datasets can cause the models to learn toxic behavior, and thereby expose their users to harmful responses. Prior work has focused on measuring the inherent toxicity of such chatbots, by devising queries that are more likely to produce toxic responses. In this work, we ask the question: How easy or hard is it to inject toxicity into a chatbot after deployment? We study this in a practical scenario known as Dialog-based Learning (DBL), where a chatbot is periodically trained on recent conversations with its users after deployment. A DBL setting can be exploited to poison the training dataset for each training cycle. Our attacks would allow an adversary to manipulate the degree of toxicity in a model and also enable control over what type of queries can trigger a toxic response. Our fully automated attacks only require LLM-based software agents masquerading as (malicious) users to inject high levels of toxicity. We systematically explore the vulnerability of popular chatbot pipelines to this threat. Lastly, we show that several existing toxicity mitigation strategies (designed for chatbots) can be significantly weakened by adaptive attackers.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "521–534",
      "Series": "ACSAC '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "adversarial inputs; Chatbots; data poisoning; toxicity injection and detection"
    },
    {
      "Key": "VEQX73FD",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Cao, Ying; Liang, Ruigang; Chen, Kai; Hu, Peiwei",
      "Title": "Boosting Neural Networks to Decompile Optimized Binaries",
      "Publication Title": "Proceedings of the 38th Annual Computer Security Applications Conference",
      "ISBN": "978-1-4503-9759-9",
      "DOI": "10.1145/3564625.3567998",
      "Url": "https://doi.org/10.1145/3564625.3567998",
      "Abstract": "Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "508–518",
      "Series": "ACSAC '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>"
    },
    {
      "Key": "HY5IPL8I",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Liu, Xin; Wu, Yixiong; Yu, Qingchen; Song, Shangru; Liu, Yue; Zhou, Qingguo; Zhuge, Jianwei",
      "Title": "PG-VulNet: Detect Supply Chain Vulnerabilities in IoT Devices using Pseudo-code and Graphs",
      "Publication Title": "Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement",
      "ISBN": "978-1-4503-9427-7",
      "DOI": "10.1145/3544902.3546240",
      "Url": "https://doi.org/10.1145/3544902.3546240",
      "Abstract": "Background: With the boosting development of IoT technology, the supply chains of IoT devices become more powerful and sophisticated, and the security issues introduced by code reuse are becoming more prominent. Therefore, the detection and management of vulnerabilities through code similarity detection technology is of great significance for protecting the security of IoT devices. Aim: We aim to propose a more accurate, parallel-friendly, and realistic software supply chain vulnerability detection solution for IoT devices. Method: This paper presents PG-VulNet, standing for Vulnerability-detection Network based on Pseudo-code Graphs. It is a ”multi-model” cross-architecture vulnerability detection solution based on pseudo-code and Graph Matching Network (GMN). PG-VulNet extracts both behavioral and structural features of pseudo-code to build customized feature graphs and then uses GMN to detect supply chain vulnerabilities based on these graphs. Results: The experiments show that PG-VulNet achieves an average detection accuracy of 99.14%, significantly higher than existing approaches like Gemini, VulSeeker, FIT, and Asteria. In addition to this, PG-VulNet also excels in detection overhead and false alarms. In the real-world evaluation, PG-VulNet detected 690 known vulnerabilities in 1,611 firmwares. Conclusions: PG-VulNet can effectively detect the vulnerabilities introduced by software supply chain in IoT firmwares and is well suited for large-scale detection. Compared with existing approaches, PG-VulNet has significant advantages.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "205–215",
      "Series": "ESEM '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Helsinki, Finland",
      "Manual_Tags": "Binary Code Similarity; Graph Neural Network; IoT Software Supply Chain; Vulnerability Detection"
    },
    {
      "Key": "SYALXN8F",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Falessi, Davide; Russo, Barbara; Mullen, Kathleen",
      "Title": "What if i had no smells?",
      "Publication Title": "Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
      "ISBN": "978-1-5090-4039-1",
      "DOI": "10.1109/ESEM.2017.14",
      "Url": "https://doi.org/10.1109/ESEM.2017.14",
      "Abstract": "What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "78–84",
      "Series": "ESEM '17",
      "Publisher": "IEEE Press",
      "Extra": "Place: Markham, Ontario, Canada",
      "Manual_Tags": "code smells; machine learning; software estimation; technical debt"
    },
    {
      "Key": "R3XFHP7U",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Moussa, Rebecca; Guizzo, Giovani; Sarro, Federica",
      "Title": "MEG: Multi-objective Ensemble Generation for Software Defect Prediction",
      "Publication Title": "Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement",
      "ISBN": "978-1-4503-9427-7",
      "DOI": "10.1145/3544902.3546255",
      "Url": "https://doi.org/10.1145/3544902.3546255",
      "Abstract": "Background: Defect Prediction research aims at assisting software engineers in the early identification of software defect during the development process. A variety of automated approaches, ranging from traditional classification models to more sophisticated learning approaches, have been explored to this end. Among these, recent studies have proposed the use of ensemble prediction models (i.e., aggregation of multiple base classifiers) to build more robust defect prediction models. Aims: In this paper, we introduce a novel approach based on multi-objective evolutionary search to automatically generate defect prediction ensembles. Our proposal is not only novel with respect to the more general area of evolutionary generation of ensembles, but it also advances the state-of-the-art in the use of ensemble in defect prediction. Method: We assess the effectiveness of our approach, dubbed as Multi-objectiveEnsembleGeneration (MEG), by empirically benchmarking it with respect to the most related proposals we found in the literature on defect prediction ensembles and on multi-objective evolutionary ensembles (which, to the best of our knowledge, had never been previously applied to tackle defect prediction). Result: Our results show that MEG is able to generate ensembles which produce similar or more accurate predictions than those achieved by all the other approaches considered in 73% of the cases (with favourable large effect sizes in 80% of them). Conclusions: MEG is not only able to generate ensembles that yield more accurate defect predictions with respect to the benchmarks considered, but it also does it automatically, thus relieving the engineers from the burden of manual design and experimentation.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "159–170",
      "Series": "ESEM '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Helsinki, Finland",
      "Manual_Tags": "Defect Prediction; Empirical Study; Hyper-Heuristic; Multi-Objective Optimisation; Search-Based Software Engineering"
    },
    {
      "Key": "6XWXKYE6",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Fan, Qiang; Yu, Yue; Yin, Gang; Wang, Tao; Wang, Huaimin",
      "Title": "Where is the road for issue reports classification based on text mining?",
      "Publication Title": "Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
      "ISBN": "978-1-5090-4039-1",
      "DOI": "10.1109/ESEM.2017.19",
      "Url": "https://doi.org/10.1109/ESEM.2017.19",
      "Abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt.In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "121–130",
      "Series": "ESEM '17",
      "Publisher": "IEEE Press",
      "Extra": "Place: Markham, Ontario, Canada",
      "Manual_Tags": "issue tracking system; machine learning technique; mining software repositories"
    },
    {
      "Key": "VRZCZKVR",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Gesi, Jiri; Li, Jiawei; Ahmed, Iftekhar",
      "Title": "An Empirical Examination of the Impact of Bias on Just-in-time Defect Prediction",
      "Publication Title": "Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
      "ISBN": "978-1-4503-8665-4",
      "DOI": "10.1145/3475716.3475791",
      "Url": "https://doi.org/10.1145/3475716.3475791",
      "Abstract": "Background: Just-In-Time (JIT) defect prediction models predict if a commit will introduce defects in the future. DeepJIT and CC2Vec are two state-of-the-art JIT Deep Learning (DL) techniques. Usually, defect prediction techniques are evaluated, treating all training data equally. However, data is usually imbalanced not only in terms of the overall class label (e.g., defect and non-defect) but also in terms of characteristics such as File Count, Edit Count, Multiline Comments, Inward Dependency Sum etc. Prior research has investigated the impact of class imbalance on prediction technique's performance but not the impact of imbalance of other characteristics. Aims: We aim to explore the impact of different commit related characteristic's imbalance on DL defect prediction. Method: We investigated different characteristic's impact on the overall performance of DeepJIT and CC2Vec. We also propose a Siamese network based few-shot learning framework for JIT defect prediction (SifterJIT) combining Siamese network and DeepJIT. Results: Our results show that DeepJIT and CC2Vec lose out on the performance by around 20% when trained and tested on imbalanced data. However, SifterJIT can outperform state-of-the-art DL techniques with an average of 8.65% AUC score, 11% precision, and 6% F1-score improvement. Conclusions: Our results highlight that dataset imbalanced in terms of commit characteristics can significantly impact prediction performance, and few-shot learning based techniques can help alleviate the situation.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ESEM '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Bari, Italy",
      "Manual_Tags": "Deep learning; defect prediction; few-shot learning; software engineering"
    },
    {
      "Key": "C8FFHALB",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Shcherban, Sergei; Liang, Peng; Tahir, Amjed; Li, Xueying",
      "Title": "Automatic Identification of Code Smell Discussions on Stack Overflow: A Preliminary Investigation",
      "Publication Title": "Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
      "ISBN": "978-1-4503-7580-1",
      "DOI": "10.1145/3382494.3422161",
      "Url": "https://doi.org/10.1145/3382494.3422161",
      "Abstract": "Background: Code smells indicate potential design or implementation problems that may have a negative impact on programs. Similar to other software artefacts, developers use Stack Overflow (SO) to ask questions about code smells. However, given the high number of questions asked on the platform, and the limitations of the default tagging system, it takes significant effort to extract knowledge about code smells by means of manual approaches. Aim: We utilized supervised machine learning techniques to automatically identify code-smell discussions from SO posts. Method: We conducted an experiment using a manually labeled dataset that contains 3000 code-smell and 3000 non-code-smell posts to evaluate the performance of different classifiers when automatically identifying code smell discussions. Results: Our results show that Logistic Regression (LR) with parameter C=20 (inverse of regularization strength) and Bag of Words (BoW) feature extraction technique achieved the best performance amongst the algorithms we evaluated with a precision of 0.978, a recall of 0.965, and an F1-score of 0.971. Conclusion: Our results show that machine learning approach can effectively locate code-smell posts even if posts' title and/or tags cannot be of help. The technique can be used to extract code smell discussions from other textual artefacts (e.g., code reviews), and promisingly to extract SO discussions of other topics.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ESEM '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Bari, Italy",
      "Manual_Tags": "Automatic Classification; Code Smell; Discussion; Stack Overflow"
    },
    {
      "Key": "MVBBZZDP",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Croft, Roland; Newlands, Dominic; Chen, Ziyu; Babar, M. Ali",
      "Title": "An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing",
      "Publication Title": "Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
      "ISBN": "978-1-4503-8665-4",
      "DOI": "10.1145/3475716.3475781",
      "Url": "https://doi.org/10.1145/3475716.3475781",
      "Abstract": "Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ESEM '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Bari, Italy",
      "Manual_Tags": "Machine Learning; Security; Static Application Security Testing"
    },
    {
      "Key": "EE56BGHR",
      "Item Type": "journalArticle",
      "Publication Year": "2023",
      "Author": "Wang, Yu; Wang, Ke; Wang, Linzhang",
      "Title": "An Explanation Method for Models of Code",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3622826",
      "Url": "https://doi.org/10.1145/3622826",
      "Abstract": "This paper introduces a novel method, called WheaCha, for explaining the predictions of code models. Similar to attribution methods, WheaCha seeks to identify input features that are responsible for a particular prediction that models make. On the other hand, it differs from attribution methods in crucial ways. Specifically, WheaCha separates an input program into \"wheat\" (i.e., defining features that are the reason for which models predict the label that they predict) and the rest \"chaff\" for any given prediction. We realize WheaCha in a tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT. Results show that (1) HuoYan is efficient — taking on average under twenty seconds to compute wheat for an input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models use to make predictions is predominantly comprised of simple syntactic or even lexical properties (i.e., identifier names); (3) neither the latest explainability methods for code models (i.e., SIVAND and CounterFactual Explanations) nor the most noteworthy attribution methods (i.e., Integrated Gradients and SHAP) can precisely capture wheat. Finally, we set out to demonstrate the usefulness of WheaCha, in particular, we assess if WheaCha’s explanations can help end users to identify defective code models (e.g., trained on mislabeled data or learned spurious correlations from biased data). We find that, with WheaCha, users achieve far higher accuracy in identifying faulty models than SIVAND, CounterFactual Explanations, Integrated Gradients and SHAP.",
      "Date": "2023-10",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA2",
      "Volume": "7",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Defining Features; Explainability Method; Models of Code"
    },
    {
      "Key": "HB66SALV",
      "Item Type": "journalArticle",
      "Publication Year": "2020",
      "Author": "Mukherjee, Suvam; Deligiannis, Pantazis; Biswas, Arpita; Lal, Akash",
      "Title": "Learning-based controlled concurrency testing",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3428298",
      "Url": "https://doi.org/10.1145/3428298",
      "Abstract": "Concurrency bugs are notoriously hard to detect and reproduce. Controlled concurrency testing (CCT) techniques aim to offer a solution, where a scheduler explores the space of possible interleavings of a concurrent program looking for bugs. Since the set of possible interleavings is typically very large, these schedulers employ heuristics that prioritize the search to “interesting” subspaces. However, current heuristics are typically tuned to specific bug patterns, which limits their effectiveness in practice. In this paper, we present QL, a learning-based CCT framework where the likelihood of an action being selected by the scheduler is influenced by earlier explorations. We leverage the classical Q-learning algorithm to explore the space of possible interleavings, allowing the exploration to adapt to the program under test, unlike previous techniques. We have implemented and evaluated QL on a set of microbenchmarks, complex protocols, as well as production cloud services. In our experiments, we found QL to consistently outperform the state-of-the-art in CCT.",
      "Date": "2020-11",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "4",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Concurrency; Reinforcement Learning; Systematic Testing"
    },
    {
      "Key": "8QNNYFNX",
      "Item Type": "journalArticle",
      "Publication Year": "2020",
      "Author": "David, Yaniv; Alon, Uri; Yahav, Eran",
      "Title": "Neural reverse engineering of stripped binaries using augmented control flow graphs",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3428293",
      "Url": "https://doi.org/10.1145/3428293",
      "Abstract": "We address the problem of reverse engineering of stripped executables, which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and the diverse assembly code patterns arising from compiler optimizations. We present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with neural models. The main idea is to use static analysis to obtain augmented representations of call sites; encode the structure of these call sites using the control-flow graph (CFG) and finally, generate a target name while attending to these call sites. We use our representation to drive graph-based, LSTM-based and Transformer-based architectures. Our evaluation shows that our models produce predictions that are difficult and time consuming for humans, while improving on existing methods by 28% and by 100% over state-of-the-art neural textual models that do not use any static analysis. Code and data for this evaluation are available at https://github.com/tech-srl/Nero.",
      "Date": "2020-11",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "4",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Neural Reverse Engineering; Static Binary Analysis"
    },
    {
      "Key": "9RP9EMJE",
      "Item Type": "journalArticle",
      "Publication Year": "2019",
      "Author": "Lee, Wonyeol; Yu, Hangyeol; Rival, Xavier; Yang, Hongseok",
      "Title": "Towards verified stochastic variational inference for probabilistic programs",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3371084",
      "Url": "https://doi.org/10.1145/3371084",
      "Abstract": "Probabilistic programming is the idea of writing models from statistics and machine learning using program notations and reasoning about these models using generic inference engines. Recently its combination with deep learning has been explored intensely, which led to the development of so called deep probabilistic programming languages, such as Pyro, Edward and ProbTorch. At the core of this development lie inference engines based on stochastic variational inference algorithms. When asked to find information about the posterior distribution of a model written in such a language, these algorithms convert this posterior-inference query into an optimisation problem and solve it approximately by a form of gradient ascent or descent. In this paper, we analyse one of the most fundamental and versatile variational inference algorithms, called score estimator or REINFORCE, using tools from denotational semantics and program analysis. We formally express what this algorithm does on models denoted by programs, and expose implicit assumptions made by the algorithm on the models. The violation of these assumptions may lead to an undefined optimisation objective or the loss of convergence guarantee of the optimisation process. We then describe rules for proving these assumptions, which can be automated by static program analyses. Some of our rules use nontrivial facts from continuous mathematics, and let us replace requirements about integrals in the assumptions, such as integrability of functions defined in terms of programs' denotations, by conditions involving differentiation or boundedness, which are much easier to prove automatically (and manually). Following our general methodology, we have developed a static program analysis for the Pyro programming language that aims at discharging the assumption about what we call model-guide support match. Our analysis is applied to the eight representative model-guide pairs from the Pyro webpage, which include sophisticated neural network models such as AIR. It finds a bug in one of these cases, reveals a non-standard use of an inference engine in another, and shows that the assumptions are met in the remaining six cases.",
      "Date": "2019-12",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "POPL",
      "Volume": "4",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "correctness; Probabilistic programming; semantics; static analysis"
    },
    {
      "Key": "AFA4ZYFH",
      "Item Type": "journalArticle",
      "Publication Year": "2020",
      "Author": "Yefet, Noam; Alon, Uri; Yahav, Eran",
      "Title": "Adversarial examples for models of code",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3428230",
      "Url": "https://doi.org/10.1145/3428230",
      "Abstract": "Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program’s semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model’s inputs, while holding the model weights constant, and following the gradients to slightly modify the input code. We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. Our evaluations demonstrate that DAMP has up to 89% success rate in changing a prediction to the adversary’s choice (a targeted attack) and a success rate of up to 94% in changing a given prediction to any incorrect prediction (a non-targeted attack). To defend a model against such attacks, we empirically examine a variety of possible defenses and discuss their trade-offs. We show that some of these defenses can dramatically drop the success rate of the attacker, with a minor penalty of 2% relative degradation in accuracy when they are not performing under attack. Our code, data, and trained models are available at &lt;a&gt;https://github.com/tech-srl/adversarial-examples&lt;/a&gt; .",
      "Date": "2020-11",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "4",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Adversarial Attacks; Neural Models of Code; Targeted Attacks"
    },
    {
      "Key": "248LDXA6",
      "Item Type": "journalArticle",
      "Publication Year": "2023",
      "Author": "Tao, Zhe; Nawas, Stephanie; Mitchell, Jacqueline; Thakur, Aditya V.",
      "Title": "Architecture-Preserving Provable Repair of Deep Neural Networks",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3591238",
      "Url": "https://doi.org/10.1145/3591238",
      "Abstract": "Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our knowledge, this is the first provable repair approach that has all of these features. We implement our approach in a tool called APRNN. Using MNIST, ImageNet, and ACAS Xu DNNs, we show that it has better efficiency, scalability, and generalization compared to PRDNN and REASSURE, prior provable repair methods that are not architecture preserving.",
      "Date": "2023-06",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "PLDI",
      "Volume": "7",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Bug fixing; Deep Neural Networks; Repair; Synthesis"
    },
    {
      "Key": "PFQ8D4HJ",
      "Item Type": "journalArticle",
      "Publication Year": "2018",
      "Author": "Pradel, Michael; Sen, Koushik",
      "Title": "DeepBugs: a learning approach to name-based bug detection",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3276517",
      "Url": "https://doi.org/10.1145/3276517",
      "Abstract": "Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89% and 95%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68% true positive rate) in real-world code.",
      "Date": "2018-10",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "2",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Bug detection; JavaScript; Machine learning; Name-based program analysis; Natural language"
    },
    {
      "Key": "58D8RGTI",
      "Item Type": "journalArticle",
      "Publication Year": "2020",
      "Author": "Wang, Yu; Wang, Ke; Gao, Fengjuan; Wang, Linzhang",
      "Title": "Learning semantic program embeddings with graph interval neural network",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3428205",
      "Url": "https://doi.org/10.1145/3428205",
      "Abstract": "Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed (fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic program embeddings.",
      "Date": "2020-11",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "4",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "Control-flow graphs; Graph neural networks; Intervals; Null pointer dereference detection; Program embeddings"
    },
    {
      "Key": "4B6JB8FB",
      "Item Type": "journalArticle",
      "Publication Year": "2017",
      "Author": "Seidel, Eric L.; Sibghat, Huma; Chaudhuri, Kamalika; Weimer, Westley; Jhala, Ranjit",
      "Title": "Learning to blame: localizing novice type errors with data-driven diagnosis",
      "Publication Title": "Proc. ACM Program. Lang.",
      "DOI": "10.1145/3138818",
      "Url": "https://doi.org/10.1145/3138818",
      "Abstract": "Localizing type errors is challenging in languages with global type inference, as the type checker must make assumptions about what the programmer intended to do. We introduce Nate, a data-driven approach to error localization based on supervised learning. Nate analyzes a large corpus of training data – pairs of ill-typed programs and their \"fixed\" versions – to automatically learn a model of where the error is most likely to be found. Given a new ill-typed program, Nate executes the model to generate a list of potential blame assignments ranked by likelihood. We evaluate Nate by comparing its precision to the state of the art on a set of over 5,000 ill-typed OCaml programs drawn from two instances of an introductory programming course. We show that when the top-ranked blame assignment is considered, Nate's data-driven model is able to correctly predict the exact sub-expression that should be changed 72% of the time, 28 points higher than OCaml and 16 points higher than the state-of-the-art SHErrLoc tool. Furthermore, Nate's accuracy surpasses 85% when we consider the top two locations and reaches 91% if we consider the top three.",
      "Date": "2017-10",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Issue": "OOPSLA",
      "Volume": "1",
      "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
      "Manual_Tags": "fault localization; type errors"
    },
    {
      "Key": "VU2ZZ6PZ",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Senanayake, Janaka; Kalutarage, Harsha; Al-Kadri, Mhd Omar; Petrovski, Andrei; Piras, Luca",
      "Title": "Developing Secured Android Applications by Mitigating Code Vulnerabilities with Machine Learning",
      "Publication Title": "Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9140-5",
      "DOI": "10.1145/3488932.3527290",
      "Url": "https://doi.org/10.1145/3488932.3527290",
      "Abstract": "Mobile application developers sometimes might not be serious about source code security and publish apps to the marketplaces. Therefore, it is essential to have a fully automated security solutions generator to integrate security-by-design into the development practices, especially for the Android platform. This research proposes a Machine Learning (ML) based highly accurate method to detect Android source code vulnerabilities. A new labelled dataset containing Android source code vulnerability samples was generated initially. The dataset was used to train binary and multi-class classification based ML models, to identify code issues by following a static analysis approach. The proposed model can detect code vulnerabilities with a 0.90 F1-Score and vulnerability categories (CWE) with a 0.96 F1-Score. By integrating this with the Android development environment, app developers can analyse source code and identify security vulnerabilities in real-time. The proposed framework can be extended to suggest suitable patches to overcome the source code issues by providing real-time fixes in future.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1255–1257",
      "Series": "ASIA CCS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Nagasaki, Japan",
      "Manual_Tags": "android; code vulnerability detection; machine learning; secure mobile apps; static analysis; vulnerability dataset"
    },
    {
      "Key": "24Q54J79",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Pei, Weiping; Yue, Chuan",
      "Title": "Generating Content-Preserving and Semantics-Flipping Adversarial Text",
      "Publication Title": "Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9140-5",
      "DOI": "10.1145/3488932.3517397",
      "Url": "https://doi.org/10.1145/3488932.3517397",
      "Abstract": "Natural Language Processing (NLP) models are often vulnerable to semantics-preserving adversarial attacks. That is, they make different semantic predictions on input instances with similar content and semantics. However, it remains unclear to which extent modern NLP models are vulnerable to content-preserving and semantics-flipping (CPSF) adversarial attacks. That is, they would make the same semantic prediction on input instances with similar content but flipped semantics. Attackers can use either semantics-preserving or CPSF adversarial examples to create misunderstanding between humans and models, and incur severe consequences in real-world applications. However, this equally important problem on CPSF adversarial examples has not been studied by researchers yet. In this paper, we perform the first study to investigate CPSF adversarial examples and propose CPSF adversarial attacks to reveal this new type of vulnerability of NLP models. We develop a two-stage approach to generate CPSF adversarial examples. Our experiments on two types of NLP tasks, sentiment analysis and textual entailment, demonstrate that CPSF adversarial examples can successfully fool victim models while preserving the same content with flipped semantics to humans. We further validate the good transferability of CPSF adversarial examples on NLP services of Microsoft and Google. Moreover, we demonstrate that adversarial training can to a meaningful extent mitigate CPSF adversarial attacks. Overall, our work implies that researchers need to improve NLP models' robustness against CPSF adversarial attacks that uniquely exploit the blind spots where NLP models are too insensitive to even big changes in semantics.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "975–989",
      "Series": "ASIA CCS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Nagasaki, Japan",
      "Manual_Tags": "adversarial examples; sentiment analysis; textual entailment"
    },
    {
      "Key": "Q377M2GS",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Kolak, Sophia",
      "Title": "Detecting performance patterns with deep learning",
      "Publication Title": "Companion Proceedings of the 2020 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity",
      "ISBN": "978-1-4503-8179-6",
      "DOI": "10.1145/3426430.3428132",
      "Url": "https://doi.org/10.1145/3426430.3428132",
      "Abstract": "Performance has a major impact on the overall quality of software projects. Performance bugs—bugs that substantially decrease run-time—have long been studied in software engineering, and yet they remain incredibly difficult for developers to handle. Because these bugs do not cause fail-stop errors, they are both harder to discover and to fix. As a result, techniques to help programmers detect and reason about performance are needed for managing performance bugs. Here we propose a static, probabilistic embedding technique to provide developers with useful information about potential performance bugs at the statement level. Using Leetcode samples scraped from real algorithms challenges, we use DeepWalk to embed data dependency graphs in Euclidean space. We then describe how these graph embeddings can be used to detect which statements in code are likely to contribute to performance bugs.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "19–21",
      "Series": "SPLASH Companion 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual, USA",
      "Manual_Tags": "deep learning; DeepWalk; performance; Python"
    },
    {
      "Key": "523FDHSZ",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Mosaner, Raphael; Leopoldseder, David; Stadler, Lukas; Mössenböck, Hanspeter",
      "Title": "Using machine learning to predict the code size impact of duplication heuristics in a dynamic compiler",
      "Publication Title": "Proceedings of the 18th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes",
      "ISBN": "978-1-4503-8675-3",
      "DOI": "10.1145/3475738.3480943",
      "Url": "https://doi.org/10.1145/3475738.3480943",
      "Abstract": "Code duplication is a major opportunity to enable optimizations in subsequent compiler phases. However, duplicating code prematurely or too liberally can result in tremendous code size increases. Thus, modern compilers use trade-offs between estimated costs in terms of code size increase and benefits in terms of performance increase. In the context of this ongoing research project, we propose the use of machine learning to provide trade-off functions with accurate predictions for code size impact. To evaluate our approach, we implemented a neural network predictor in the GraalVM compiler and compared its performance against a human-crafted, highly tuned heuristic. First results show promising performance improvements, leading to code size reductions of more than 10% for several benchmarks. Additionally, we present an assistance mode for finding flaws in the human-crafted heuristic, leading to improvements for the duplication optimization itself.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "127–135",
      "Series": "MPLR 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Münster, Germany",
      "Manual_Tags": "Code Duplication; Dynamic Compiler; Heuristics; Machine Learning; Neural Networks; Optimization; Regression"
    },
    {
      "Key": "RXKS9JVN",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Gan, Yu; Liang, Mingyu; Dev, Sundar; Lo, David; Delimitrou, Christina",
      "Title": "Sage: practical and scalable ML-driven performance debugging in microservices",
      "Publication Title": "Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
      "ISBN": "978-1-4503-8317-2",
      "DOI": "10.1145/3445814.3446700",
      "Url": "https://doi.org/10.1145/3445814.3446700",
      "Abstract": "Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely-coupled microservices. Despite the advantages of modularity and elasticity microservices offer, they also complicate cluster management and performance debugging, as dependencies between tiers introduce backpressure and cascading QoS violations. Prior work on performance debugging for cloud services either relies on empirical techniques, or uses supervised learning to diagnose the root causes of performance issues, which requires significant application instrumentation, and is difficult to deploy in practice. We present Sage, a machine learning-driven root cause analysis system for interactive cloud microservices that focuses on practicality and scalability. Sage leverages unsupervised ML models to circumvent the overhead of trace labeling, captures the impact of dependencies between microservices to determine the root cause of unpredictable performance online, and applies corrective actions to recover a cloud service’s QoS. In experiments on both dedicated local clusters and large clusters on Google Compute Engine we show that Sage consistently achieves over 93% accuracy in correctly identifying the root cause of QoS violations, and improves performance predictability.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "135–151",
      "Series": "ASPLOS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual, USA",
      "Manual_Tags": "Bayesian network; cloud computing; counterfactual; microservices; performance debugging; QoS; variational autoencoder"
    },
    {
      "Key": "WPHPKCAB",
      "Item Type": "conferencePaper",
      "Publication Year": "2024",
      "Author": "Gan, Yu; Liu, Guiyang; Zhang, Xin; Zhou, Qi; Wu, Jiesheng; Jiang, Jiangwei",
      "Title": "Sleuth: A Trace-Based Root Cause Analysis System for Large-Scale Microservices with Graph Neural Networks",
      "Publication Title": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3623278.3624758",
      "Url": "https://doi.org/10.1145/3623278.3624758",
      "Abstract": "Cloud microservices are being scaled up due to the rising demand for new features and the convenience of cloud-native technologies. However, the growing scale of microservices complicates the remote procedure call (RPC) dependency graph, exacerbates the tail-of-scale effect, and makes many of the empirical rules for detecting the root cause of end-to-end performance issues unreliable. Additionally, existing open-source microservice benchmarks are too small to evaluate performance debugging algorithms at a production-scale with hundreds or even thousands of services and RPCs.To address these challenges, we present Sleuth, a trace-based root cause analysis (RCA) system for large-scale microservices using un-supervised graph learning. Sleuth leverages a graph neural network to capture the causal impact of each span in a trace, and trace clustering using a trace distance metric to reduce the amount of traces required for root cause localization. A pre-trained Sleuth model can be transferred to different microservice applications without any retraining or with few-shot fine-tuning. To quantitatively evaluate the performance and scalability of Sleuth, we propose a method to generate microservice benchmarks comparable to a production-scale. The experiments on the existing benchmark suites and synthetic large-scale microservices indicate that Sleuth has significantly outperformed the prior work in detection accuracy, performance, and adaptability on a large-scale deployment.",
      "Date": "2024",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "324–337",
      "Series": "ASPLOS '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>"
    },
    {
      "Key": "U6C6KZFH",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Liu, Jiawei; Lin, Jinkun; Ruffy, Fabian; Tan, Cheng; Li, Jinyang; Panda, Aurojit; Zhang, Lingming",
      "Title": "NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers",
      "Publication Title": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",
      "ISBN": "978-1-4503-9916-6",
      "DOI": "10.1145/3575693.3575707",
      "Url": "https://doi.org/10.1145/3575693.3575707",
      "Abstract": "Deep-learning (DL) compilers such as TVM and TensorRT are increasingly being used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can result in models whose semantics differ from the original ones, producing incorrect results that corrupt the correctness of downstream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach consists of (i) generating diverse yet valid DNN test models that can exercise a large part of the compiler's transformation logic using light-weight operator specifications; (ii) performing gradient-based search to find model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) using differential testing to identify bugs. We implemented this approach in NNSmith which has found 72 new bugs for TVM, TensorRT, ONNXRuntime, and PyTorch to date. Of these 58 have been confirmed and 51 have been fixed by their respective project maintainers.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "530–543",
      "Series": "ASPLOS 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vancouver, BC, Canada",
      "Manual_Tags": "Compiler Testing; Deep Learning Compilers; Fuzzing"
    },
    {
      "Key": "KIMSENQ3",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Jung, Jaehoon; Kim, Jinpyo; Lee, Jaejin",
      "Title": "DeepUM: Tensor Migration and Prefetching in Unified Memory",
      "Publication Title": "Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2",
      "ISBN": "978-1-4503-9916-6",
      "DOI": "10.1145/3575693.3575736",
      "Url": "https://doi.org/10.1145/3575693.3575736",
      "Abstract": "Deep neural networks (DNNs) are continuing to get wider and deeper. As a result, it requires a tremendous amount of GPU memory and computing power. In this paper, we propose a framework called DeepUM that exploits CUDA Unified Memory (UM) to allow GPU memory oversubscription for DNNs. While UM allows memory oversubscription using a page fault mechanism, page migration introduces enormous overhead. DeepUM uses a new correlation prefetching technique to hide the page migration overhead. It is fully automatic and transparent to users. We also propose two optimization techniques to minimize the GPU fault handling time. We evaluate the performance of DeepUM using nine large-scale DNNs from MLPerf, PyTorch examples, and Hugging Face and compare its performance with six state-of-the-art GPU memory swapping approaches. The evaluation result indicates that DeepUM is very effective for GPU memory oversubscription and can handle larger models that other approaches fail to handle.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "207–221",
      "Series": "ASPLOS 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vancouver, BC, Canada",
      "Manual_Tags": "CUDA; data prefetching; deep learning; device driver; neural networks; runtime system; unified memory"
    },
    {
      "Key": "FHETJ3AI",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Li, Zheng; Liu, Yiyong; He, Xinlei; Yu, Ning; Backes, Michael; Zhang, Yang",
      "Title": "Auditing Membership Leakages of Multi-Exit Networks",
      "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9450-5",
      "DOI": "10.1145/3548606.3559359",
      "Url": "https://doi.org/10.1145/3548606.3559359",
      "Abstract": "Relying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1917–1931",
      "Series": "CCS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Los Angeles, CA, USA",
      "Manual_Tags": "machine learning; membership leakages; multi-exit networks"
    },
    {
      "Key": "8SPEPKK3",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Biggio, Battista; Roli, Fabio",
      "Title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
      "Publication Title": "Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-5693-0",
      "DOI": "10.1145/3243734.3264418",
      "Url": "https://doi.org/10.1145/3243734.3264418",
      "Abstract": "Deep neural networks and machine-learning algorithms are pervasively used in several applications, ranging from computer vision to computer security. In most of these applications, the learning algorithm has to face intelligent and adaptive attackers who can carefully manipulate data to purposely subvert the learning process. As these algorithms have not been originally designed under such premises, they have been shown to be vulnerable to well-crafted, sophisticated attacks, including training-time poisoning and test-time evasion attacks (also known as adversarial examples). The problem of countering these threats and learning secure classifiers in adversarial settings has thus become the subject of an emerging, relevant research field known as adversarial machine learning. The purposes of this tutorial are: (a) to introduce the fundamentals of adversarial machine learning to the security community; (b) to illustrate the design cycle of a learning-based pattern recognition system for adversarial tasks; (c) to present novel techniques that have been recently proposed to assess performance of pattern classifiers and deep learning algorithms under attack, evaluate their vulnerabilities, and implement defense strategies that make learning algorithms more robust to attacks; and (d) to show some applications of adversarial machine learning to pattern recognition tasks like object recognition in images, biometric identity recognition, spam and malware detection.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2154–2156",
      "Series": "CCS '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Toronto, Canada",
      "Manual_Tags": "adversarial examples; adversarial machine learning; deep learning; evasion attacks; training data poisoning"
    },
    {
      "Key": "E9XQGYNI",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Nadeem, Azqa; Verwer, Sicco; Moskal, Stephen; Yang, Shanchieh Jay",
      "Title": "Enabling Visual Analytics via Alert-driven Attack Graphs",
      "Publication Title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8454-4",
      "DOI": "10.1145/3460120.3485361",
      "Url": "https://doi.org/10.1145/3460120.3485361",
      "Abstract": "Attack graphs (AG) are a popular area of research that display all the paths an attacker can exploit to penetrate a network. Existing techniques for AG generation rely heavily on expert input regarding vulnerabilities and network topology. In this work, we advocate the use of AGs that are built directly using the actions observed through intrusion alerts, without prior expert input. We have developed an unsupervised visual analytics system, called SAGE, to learn alert-driven attack graphs. We show how these AGs (i) enable forensic analysis of prior attacks, and (ii) enable proactive defense by providing relevant threat intelligence regarding attacker strategies. We believe that alert-driven AGs can play a key role in AI-enabled cyber threat intelligence as they open up new avenues for attacker strategy analysis whilst reducing analyst workload.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2420–2422",
      "Series": "CCS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "attack graphs; finite state automaton; intrusion alerts"
    },
    {
      "Key": "UHMZCW97",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Romanelli, Marco; Chatzikokolakis, Konstantinos; Palamidessi, Catuscia; Piantanida, Pablo",
      "Title": "Estimating g-Leakage via Machine Learning",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-7089-9",
      "DOI": "10.1145/3372297.3423363",
      "Url": "https://doi.org/10.1145/3372297.3423363",
      "Abstract": "This paper considers the problem of estimating the information leakage of a system in the black-box scenario, i.e. when the system's internals are unknown to the learner, or too complicated to analyze, and the only available information are pairs of input-output data samples, obtained by submitting queries to the system or provided by a third party. The frequentist approach relies on counting the frequencies to estimate the input-output conditional probabilities, however this method is not accurate when the domain of possible outputs is large. To overcome this difficulty, the estimation of the Bayes error of the ideal classifier was recently investigated using Machine Learning (ML) models, and it has been shown to be more accurate thanks to the ability of those models to learn the input-output correspondence. However, the Bayes vulnerability is only suitable to describe one-try attacks. A more general and flexible measure of leakage is the g-vulnerability, which encompasses several different types of adversaries, with different goals and capabilities. We propose a novel approach to perform black-box estimation of the g-vulnerability using ML which does not require to estimate the conditional probabilities and is suitable for a large class of ML algorithms. First, we formally show the learnability for all data distributions. Then, we evaluate the performance via various experiments using k-Nearest Neighbors and Neural Networks. Our approach outperform the frequentist one when the observables domain is large.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "697–716",
      "Series": "CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "g-vulnerability estimation; machine learning; neural networks"
    },
    {
      "Key": "QVLE69A9",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Ji, Yujie; Wang, Ting",
      "Title": "Towards Understanding the Dynamics of Adversarial Attacks",
      "Publication Title": "Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-5693-0",
      "DOI": "10.1145/3243734.3278528",
      "Url": "https://doi.org/10.1145/3243734.3278528",
      "Abstract": "An intriguing property of deep neural networks (DNNs) is their inherent vulnerability to adversarial inputs, which significantly hinder the application of DNNs in security-critical domains. Despite the plethora of work on adversarial attacks and defenses, many important questions regarding the inference behaviors of adversarial inputs remain mysterious. This work represents a solid step towards answering those questions by investigating the information flows of normal and adversarial inputs within various DNN models and conducting in-depth comparative analysis of their discriminative patterns. Our work points to several promising directions for designing more effective defense mechanisms.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2228–2230",
      "Series": "CCS '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Toronto, Canada",
      "Manual_Tags": "adversarial sample; deep neural network; mutual information"
    },
    {
      "Key": "JJTN74PB",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Lin, Guanjun; Zhang, Jun; Luo, Wei; Pan, Lei; Xiang, Yang",
      "Title": "POSTER: Vulnerability Discovery with Function Representation Learning from Unlabeled Projects",
      "Publication Title": "Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-4946-8",
      "DOI": "10.1145/3133956.3138840",
      "Url": "https://doi.org/10.1145/3133956.3138840",
      "Abstract": "In cybersecurity, vulnerability discovery in source code is a fundamental problem. To automate vulnerability discovery, Machine learning (ML) based techniques has attracted tremendous attention. However, existing ML-based techniques focus on the component or file level detection, and thus considerable human effort is still required to pinpoint the vulnerable code fragments. Using source code files also limit the generalisability of the ML models across projects. To address such challenges, this paper targets at the function-level vulnerability discovery in the cross-project scenario. A function representation learning method is proposed to obtain the high-level and generalizable function representations from the abstract syntax tree (AST). First, the serialized ASTs are used to learn project independence features. Then, a customized bi-directional LSTM neural network is devised to learn the sequential AST representations from the large number of raw features. The new function-level representation demonstrated promising performance gain, using a unique dataset where we manually labeled 6000+ functions from three open-source projects. The results confirm that the huge potential of the new AST-based function representation learning.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2539–2541",
      "Series": "CCS '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Dallas, Texas, USA",
      "Manual_Tags": "AST; cross-project; representation learning; vulnerability detection"
    },
    {
      "Key": "WEI3JU6N",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "He, Ping; Xia, Yifan; Zhang, Xuhong; Ji, Shouling",
      "Title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "Publication Title": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3576915.3623117",
      "Url": "https://doi.org/10.1145/3576915.3623117",
      "Abstract": "The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "90–104",
      "Series": "CCS '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "adversarial android malware; machine learning security; malware"
    },
    {
      "Key": "ZFIU4SV7",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "He, Chaoxiang; Zhu, Bin Benjamin; Ma, Xiaojing; Jin, Hai; Hu, Shengshan",
      "Title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "Publication Title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8454-4",
      "DOI": "10.1145/3460120.3485378",
      "Url": "https://doi.org/10.1145/3460120.3485378",
      "Abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "3159–3176",
      "Series": "CCS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "adversarial attacks; adversarial examples; feature-indistinguishable attack; neural networks; trapdoor enabled defense"
    },
    {
      "Key": "VBXDD28Q",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Pang, Ren; Shen, Hua; Zhang, Xinyang; Ji, Shouling; Vorobeychik, Yevgeniy; Luo, Xiapu; Liu, Alex; Wang, Ting",
      "Title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-7089-9",
      "DOI": "10.1145/3372297.3417253",
      "Url": "https://doi.org/10.1145/3372297.3417253",
      "Abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs – maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models – adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors – leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "85–99",
      "Series": "CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "adversarial attack; backdoor attack; trojaning attack"
    },
    {
      "Key": "8B5JSMV7",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Jacobs, Arthur S.; Beltiukov, Roman; Willinger, Walter; Ferreira, Ronaldo A.; Gupta, Arpit; Granville, Lisandro Z.",
      "Title": "AI/ML for Network Security: The Emperor has no Clothes",
      "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-9450-5",
      "DOI": "10.1145/3548606.3560609",
      "Url": "https://doi.org/10.1145/3548606.3560609",
      "Abstract": "Several recent research efforts have proposed Machine Learning (ML)-based solutions that can detect complex patterns in network traffic for a wide range of network security problems. However, without understanding how these black-box models are making their decisions, network operators are reluctant to trust and deploy them in their production settings. One key reason for this reluctance is that these models are prone to the problem of underspecification, defined here as the failure to specify a model in adequate detail. Not unique to the network security domain, this problem manifests itself in ML models that exhibit unexpectedly poor behavior when deployed in real-world settings and has prompted growing interest in developing interpretable ML solutions (e.g., decision trees) for \"explaining” to humans how a given black-box model makes its decisions. However, synthesizing such explainable models that capture a given black-box model's decisions with high fidelity while also being practical (i.e., small enough in size for humans to comprehend) is challenging.In this paper, we focus on synthesizing high-fidelity and low-complexity decision trees to help network operators determine if their ML models suffer from the problem of underspecification. To this end, we present Trustee, a framework that takes an existing ML model and training dataset as input and generates a high-fidelity, easy-to-interpret decision tree and associated trust report as output. Using published ML models that are fully reproducible, we show how practitioners can use Trustee to identify three common instances of model underspecification; i.e., evidence of shortcut learning, presence of spurious correlations, and vulnerability to out-of-distribution samples.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1537–1551",
      "Series": "CCS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Los Angeles, CA, USA",
      "Manual_Tags": "artificial intelligence; explainability; interpretability; machine learning; network security; trust"
    },
    {
      "Key": "62JH7BLI",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Tramèr, Florian; Dupré, Pascal; Rusak, Gili; Pellegrino, Giancarlo; Boneh, Dan",
      "Title": "AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning",
      "Publication Title": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6747-9",
      "DOI": "10.1145/3319535.3354222",
      "Url": "https://doi.org/10.1145/3319535.3354222",
      "Abstract": "Perceptual ad-blocking is a novel approach that detects online advertisements based on their visual content. Compared to traditional filter lists, the use of perceptual signals is believed to be less prone to an arms race with web publishers and ad networks. We demonstrate that this may not be the case. We describe attacks on multiple perceptual ad-blocking techniques, and unveil a new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual ad-blocking can also introduce new vulnerabilities that let an attacker bypass web security boundaries and mount DDoS attacks. We first analyze the design space of perceptual ad-blockers and present a unified architecture that incorporates prior academic and commercial work. We then explore a variety of attacks on the ad-blocker's detection pipeline, that enable publishers or ad networks to evade or detect ad-blocking, and at times even abuse its high privilege level to bypass web security boundaries. On one hand, we show that perceptual ad-blocking must visually classify rendered web content to escape an arms race centered on obfuscation of page markup. On the other, we present a concrete set of attacks on visual ad-blockers by constructing adversarial examples in a real web page context. For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and native web content that misleads perceptual ad-blocking with 100% success rates. In one of our attacks, we demonstrate how a malicious user can upload adversarial content, such as a perturbed image in a Facebook post, that fools the ad-blocker into removing another users' non-ad content. Moving beyond the Web and visual domain, we also build adversarial examples for AdblockRadio, an open source radio client that uses machine learning to detects ads in raw audio streams.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2005–2021",
      "Series": "CCS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: London, United Kingdom",
      "Manual_Tags": "ad blocking; adversarial example; machine learning"
    },
    {
      "Key": "W5D86EB6",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Kannan, Tejas; Feamster, Nick; Hoffmann, Henry",
      "Title": "Prediction Privacy in Distributed Multi-Exit Neural Networks: Vulnerabilities and Solutions",
      "Publication Title": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3576915.3623069",
      "Url": "https://doi.org/10.1145/3576915.3623069",
      "Abstract": "Distributed Multi-exit Neural Networks (MeNNs) use partitioning and early exits to reduce the cost of neural network inference on low-power sensing systems. Existing MeNNs exhibit high inference accuracy using policies that select when to exit based on data-dependent prediction confidence. This paper presents a side-channel attack against distributed MeNNs employing data-dependent early exit policies. We find that an adversary can observe when a distributed MeNN exits early using encrypted communication patterns. An adversary can then use these observations to discover the MeNN's predictions with over 1.85× the accuracy of random guessing. In some cases, the side-channel leaks over 80% of the model's predictions. This leakage occurs because prior policies make decisions using a single threshold on varying prediction confidence distributions. We address this problem through two new exit policies. The first method, Per-Class Exiting (PCE), uses multiple thresholds to balance exit rates across predicted classes. This policy retains high accuracy and lowers prediction leakage, but we prove it has no privacy guarantees. We obtain these guarantees with a second policy, Confidence-Guided Randomness (CGR), which randomly selects when to exit using probabilities biased toward PCE's decisions. CGR provides statistically equivalent privacy with consistently higher inference accuracy than exiting early uniformly at random. Both PCE and CGR have low overhead, making them viable security solutions in resource-constrained settings.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1123–1137",
      "Series": "CCS '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "neural networks; sensor network security; side channels"
    },
    {
      "Key": "NBMLGADM",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Li, Yu; Li, Min; Luo, Bo; Tian, Ye; Xu, Qiang",
      "Title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-7089-9",
      "DOI": "10.1145/3372297.3423338",
      "Url": "https://doi.org/10.1145/3372297.3423338",
      "Abstract": "Deep neural networks (DNNs) have become one of the enabling technologies in many safety-critical applications, e.g., autonomous driving and medical image analysis. DNN systems, however, suffer from various kinds of threats, such as adversarial example attacks and fault injection attacks. While there are many defense methods proposed against maliciously crafted inputs, solutions against faults presented in the DNN system itself (e.g., parameters and calculations) are far less explored. In this paper, we develop a novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification. The key to enabling such lightweight checking is that the smaller neural network only needs to produce approximate results for the initial task without sacrificing fault coverage much. We develop efficient and effective architecture and task exploration techniques to achieve optimized risk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve can reduce 90% of the risks at around 10% overhead.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "101–112",
      "Series": "CCS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "deep learning; dynamic verification; fault injection attack"
    },
    {
      "Key": "AN8QHVJ8",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Siadati, Hossein; Memon, Nasir",
      "Title": "Detecting Structurally Anomalous Logins Within Enterprise Networks",
      "Publication Title": "Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-4946-8",
      "DOI": "10.1145/3133956.3134003",
      "Url": "https://doi.org/10.1145/3133956.3134003",
      "Abstract": "Many network intrusion detection systems use byte sequences to detect lateral movements that exploit remote vulnerabilities. Attackers bypass such detection by stealing valid credentials and using them to transmit from one computer to another without creating abnormal network traffic. We call this method Credential-based Lateral Movement. To detect this type of lateral movement, we develop the concept of a Network Login Structure that specifies normal logins within a given network. Our method models a network login structure by automatically extracting a collection of login patterns by using a variation of the market-basket algorithm. We then employ an anomaly detection approach to detect malicious logins that are inconsistent with the enterprise network's login structure. Evaluations show that the proposed method is able to detect malicious logins in a real setting. In a simulated attack, our system was able to detect 82% of malicious logins, with a 0.3% false positive rate. We used a real dataset of millions of logins over the course of five months within a global financial company for evaluation of this work.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1273–1284",
      "Series": "CCS '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Dallas, Texas, USA",
      "Manual_Tags": "anomaly detection; authentication; data-driven security; network security"
    },
    {
      "Key": "WF6DFPK6",
      "Item Type": "conferencePaper",
      "Publication Year": "2016",
      "Author": "Sachidananda, Vinay; Toh, Jinghui; Siboni, Shachar; Shabtai, Asaf; Elovici, Yuval",
      "Title": "POSTER: Towards Exposing Internet of Things: A Roadmap",
      "Publication Title": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-4139-4",
      "DOI": "10.1145/2976749.2989046",
      "Url": "https://doi.org/10.1145/2976749.2989046",
      "Abstract": "Considering the exponential increase of Internet of Things (IoT) devices there is also unforeseen vulnerabilities associated with these IoT devices. One of the major problems in the IoT is the security testing and analysis due to the heterogeneous nature of deployments. Currently, there is no mechanism that performs security testing for IoT devices in different contexts. In addition, there is a missing framework to be able to adapt and tune accordingly with various security testing perspectives. In this paper, we propose an innovative security testbed targeted at IoT devices and also briefly introduce Adaptable and Tunable Framework (ATF) for testing IoT devices.",
      "Date": "2016",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1820–1822",
      "Series": "CCS '16",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vienna, Austria",
      "Manual_Tags": "framework; internet of things (IoT); privacy; security; testbed"
    },
    {
      "Key": "F42IKAC3",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Arazzi, Marco; Conti, Mauro; Nocera, Antonino; Picek, Stjepan",
      "Title": "Turning Privacy-preserving Mechanisms against Federated Learning",
      "Publication Title": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3576915.3623114",
      "Url": "https://doi.org/10.1145/3576915.3623114",
      "Abstract": "Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, researchers proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data.In this paper, we identify a crucial security flaw in such a configuration and design an attack capable of deceiving state-of-the-art defenses for federated learning. The proposed attack includes two operating modes, the first one focusing on convergence inhibition (Adversarial Mode), and the second one aiming at building a deceptive rating injection on the global federated model (Backdoor Mode). The experimental results show the effectiveness of our attack in both its modes, returning on average 60% performance detriment in all the tests on Adversarial Mode and fully effective backdoors in 93% of cases for the tests performed on Backdoor Mode.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1482–1495",
      "Series": "CCS '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "privacy; federated learning; recommender systems; graph neural network; model poisoning"
    },
    {
      "Key": "QLKES3VM",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Ahmed, Muhammad Ejaz; Kim, Hyoungshick",
      "Title": "Poster: Adversarial Examples for Classifiers in High-Dimensional Network Data",
      "Publication Title": "Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-4946-8",
      "DOI": "10.1145/3133956.3138853",
      "Url": "https://doi.org/10.1145/3133956.3138853",
      "Abstract": "Many machine learning methods make assumptions about the data, such as, data stationarity and data independence, for an efficient learning process that requires less data. However, these assumptions may give rise to vulnerabilities if violated by smart adversaries. In this paper, we propose a novel algorithm to craft the input samples by modifying a certain fraction of input features as small as in order to bypass the decision boundary of widely used binary classifiers using Support Vector Machine (SVM). We show that our algorithm can reliably produce adversarial samples which are misclassified with 98% success rate while modifying 22% of the input features on average. Our goal is to evaluate the robustness of classification algorithms for high demensional network data by intentionally performing evasion attacks with carefully designed adversarial examples. The proposed algorithm is evaluated using real network traffic datasets (CAIDA 2007 and CAIDA 2016).",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2467–2469",
      "Series": "CCS '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Dallas, Texas, USA",
      "Manual_Tags": "adversarial machine learning; evasion attacks; high dimensional data; network attacks; network intrusion"
    },
    {
      "Key": "LFNV4WWP",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Zhao, Yue; Zhu, Hong; Chen, Kai; Zhang, Shengzhi",
      "Title": "AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks",
      "Publication Title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-8454-4",
      "DOI": "10.1145/3460120.3484818",
      "Url": "https://doi.org/10.1145/3460120.3484818",
      "Abstract": "Deep neural network (DNN) has been widely utilized in many areas due to its increasingly high accuracy. However, DNN models could also produce wrong outputs due to internal errors, which may lead to severe security issues. Unlike fixing bugs in traditional computer software, tracing the errors in DNN models and fixing them are much more difficult due to the uninterpretability of DNN. In this paper, we present a novel and systematic approach to trace and fix the errors in deep learning models. In particular, we locate the error-inducing neurons that play a leading role in the erroneous output. With the knowledge of error-inducing neurons, we propose two methods to fix the errors: the neuron-flip and the neuron-fine-tuning. We evaluate our approach using five different training datasets and seven different model architectures. The experimental results demonstrate its efficacy in different application scenarios, including backdoor removal and general defects fixing.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "141–158",
      "Series": "CCS '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "neural networks; model optimization; backdoor removal"
    },
    {
      "Key": "FG6PDL3U",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Oak, Rajvardhan",
      "Title": "Poster: Adversarial Examples for Hate Speech Classifiers",
      "Publication Title": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6747-9",
      "DOI": "10.1145/3319535.3363271",
      "Url": "https://doi.org/10.1145/3319535.3363271",
      "Abstract": "With the advent of the Internet, social media platforms have become an increasingly popular medium of communication for people. Platforms like Twitter and Quora allow people to express their opinions on a large scale. These platforms are, however, plagued by the problem of hate speech and toxic content. Such content is generally sexist, homophobic or racist. Automatic text classification can filter out toxic content so some extent. In this paper, we discuss the adversarial attacks on hate speech classifiers. We demonstrate that by changing the text slightly, a classifier can be fooled to misclassifying a toxic comment as acceptable. We attack hate speech classifiers with known attacks as well as introduce four new attacks. We find that our method can degrade the performance of a Random Forest classifier by 20%. We hope that our work sheds light on the vulnerabilities of text classifiers, and opens doors for further research on this topic.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2621–2623",
      "Series": "CCS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: London, United Kingdom",
      "Manual_Tags": "adversarial machine learning; hate speech"
    },
    {
      "Key": "RB2976AN",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Co, Kenneth T.; Muñoz-González, Luis; de Maupeou, Sixte; Lupu, Emil C.",
      "Title": "Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks",
      "Publication Title": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-6747-9",
      "DOI": "10.1145/3319535.3345660",
      "Url": "https://doi.org/10.1145/3319535.3345660",
      "Abstract": "Deep Convolutional Networks (DCNs) have been shown to be vulnerable to adversarial examples—perturbed inputs specifically designed to produce intentional errors in the learning algorithms at test time. Existing input-agnostic adversarial perturbations exhibit interesting visual patterns that are currently unexplained. In this paper, we introduce a structured approach for generating Universal Adversarial Perturbations (UAPs) with procedural noise functions. Our approach unveils the systemic vulnerability of popular DCN models like Inception v3 and YOLO v3, with single noise patterns able to fool a model on up to 90% of the dataset. Procedural noise allows us to generate a distribution of UAPs with high universal evasion rates using only a few parameters. Additionally, we propose Bayesian optimization to efficiently learn procedural noise parameters to construct inexpensive untargeted black-box attacks. We demonstrate that it can achieve an average of less than 10 queries per successful attack, a 100-fold improvement on existing methods. We further motivate the use of input-agnostic defences to increase the stability of models to adversarial perturbations. The universality of our attacks suggests that DCN models may be sensitive to aggregations of low-level class-agnostic features. These findings give insight on the nature of some universal adversarial perturbations and how they could be generated in other applications.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "275–289",
      "Series": "CCS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: London, United Kingdom",
      "Manual_Tags": "deep neural networks; adversarial machine learning; black-box attacks; bayesian optimization; procedural noise; universal adversarial perturbations"
    },
    {
      "Key": "GXBMC5NH",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Breier, Jakub; Hou, Xiaolu; Jap, Dirmanto; Ma, Lei; Bhasin, Shivam; Liu, Yang",
      "Title": "Practical Fault Attack on Deep Neural Networks",
      "Publication Title": "Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-5693-0",
      "DOI": "10.1145/3243734.3278519",
      "Url": "https://doi.org/10.1145/3243734.3278519",
      "Abstract": "As deep learning systems are widely adopted in safety- and security-critical applications, such as autonomous vehicles, banking systems, etc., malicious faults and attacks become a tremendous concern, which potentially could lead to catastrophic consequences. In this paper, we initiate the first study of leveraging physical fault injection attacks on Deep Neural Networks (DNNs), by using laser injection technique on embedded systems. In particular, our exploratory study targets four widely used activation functions in DNNs development, that are the general main building block of DNNs that creates non-linear behaviors – ReLu, softmax, sigmoid, and tanh. Our results show that by targeting these functions, it is possible to achieve a misclassification by injecting faults into the hidden layer of the network. Such result can have practical implications for real-world applications, where faults can be introduced by simpler means (such as altering the supply voltage).",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2204–2206",
      "Series": "CCS '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Toronto, Canada",
      "Manual_Tags": "adversarial attacks; deep learning security; fault attacks"
    },
    {
      "Key": "2C6J242X",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Mayerhofer, Robin; Mayer, Rudolf",
      "Title": "Poisoning Attacks against Feature-Based Image Classification",
      "Publication Title": "Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy",
      "ISBN": "978-1-4503-9220-4",
      "DOI": "10.1145/3508398.3519363",
      "Url": "https://doi.org/10.1145/3508398.3519363",
      "Abstract": "Adversarial machine learning and the robustness of machine learning is gaining attention, especially in image classification. Attacks based on data poisoning, with the aim to lower the integrity or availability of a model, showed high success rates, while barely reducing the classifiers accuracy - particularly against Deep Learning approaches such as Convolutional Neural Networks (CNNs). While Deep Learning has become the most prominent technique for many pattern recognition tasks, feature-extraction based systems still have their applications - and there is surprisingly little research dedicated to the vulnerability of those approaches. We address this gap and show preliminary results in evaluating poisoning attacks against feature-extraction based systems, and compare them to CNNs, on a traffic sign classification dataset. Our findings show that feature-extraction based ML systems require higher poisoning percentages to achieve similar backdoor success, and also need a consistent (static) backdoor position to work.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "358–360",
      "Series": "CODASPY '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Baltimore, MD, USA",
      "Manual_Tags": "adversarial machine learning; feature-based image classification; poisoning attacks"
    },
    {
      "Key": "98HWBYMZ",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Kairajärvi, Sami; Costin, Andrei; Hämäläinen, Timo",
      "Title": "ISAdetect: Usable Automated Detection of CPU Architecture and Endianness for Executable Binary Files and Object Code",
      "Publication Title": "Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy",
      "ISBN": "978-1-4503-7107-0",
      "DOI": "10.1145/3374664.3375742",
      "Url": "https://doi.org/10.1145/3374664.3375742",
      "Abstract": "Static and dynamic binary analysis techniques are actively used to reverse engineer software's behavior and to detect its vulnerabilities, even when only the binary code is available for analysis. To avoid analysis errors due to misreading op-codes for a wrong CPU architecture, these analysis tools must precisely identify the Instruction Set Architecture (ISA) of the object code under analysis. The variety of CPU architectures that modern security and reverse engineering tools must support is ever increasing due to massive proliferation of IoT devices and the diversity of firmware and malware targeting those devices. Recent studies concluded that falsely identifying the binary code's ISA caused alone about 10% of failures of IoT firmware analysis. The state of the art approaches detecting ISA for executable object code look promising, and their results demonstrate effectiveness and high-performance. However, they lack the support of publicly available datasets and toolsets, which makes the evaluation, comparison, and improvement of those techniques, datasets, and machine learning models quite challenging (if not impossible). This paper bridges multiple gaps in the field of automated and precise identification of architecture and endianness of binary files and object code. We develop from scratch the toolset and datasets that are lacking in this research space. As such, we contribute a comprehensive collection of open data, open source, and open API web-services. We also attempt experiment reconstruction and cross-validation of effectiveness, efficiency, and results of the state of the art methods. When training and testing classifiers using solely code-sections from executable binary files, all our classifiers performed equally well achieving over 98% accuracy. The results are consistent and comparable with the current state of the art, hence supports the general validity of the algorithms, features, and approaches suggested in those works.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "376–380",
      "Series": "CODASPY '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: New Orleans, LA, USA",
      "Manual_Tags": "reverse engineering; digital forensics; malware analysis; binary code analysis; firmware analysis; instruction set architecture (isa); supervised machine learning"
    },
    {
      "Key": "AAQCCF68",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Zhong, Haoti; Liao, Cong; Squicciarini, Anna Cinzia; Zhu, Sencun; Miller, David",
      "Title": "Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation",
      "Publication Title": "Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy",
      "ISBN": "978-1-4503-7107-0",
      "DOI": "10.1145/3374664.3375751",
      "Url": "https://doi.org/10.1145/3374664.3375751",
      "Abstract": "Deep learning models have consistently outperformed traditional machine learning models in various classification tasks, including image classification. As such, they have become increasingly prevalent in many real world applications including those where security is of great concern. Such popularity, however, may attract attackers to exploit the vulnerabilities of the deployed deep learning models and launch attacks against security-sensitive applications. In this paper, we focus on a specific type of data poisoning attack, which we refer to as a em backdoor injection attack. The main goal of the adversary performing such attack is to generate and inject a backdoor into a deep learning model that can be triggered to recognize certain embedded patterns with a target label of the attacker's choice. Additionally, a backdoor injection attack should occur in a stealthy manner, without undermining the efficacy of the victim model. Specifically, we propose two approaches for generating a backdoor that is hardly perceptible yet effective in poisoning the model. We consider two attack settings, with backdoor injection carried out either before model training or during model updating. We carry out extensive experimental evaluations under various assumptions on the adversary model, and demonstrate that such attacks can be effective and achieve a high attack success rate (above 90%) at a small cost of model accuracy loss with a small injection rate, even under the weakest assumption wherein the adversary has no knowledge either of the original training data or the classifier model.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "97–108",
      "Series": "CODASPY '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: New Orleans, LA, USA",
      "Manual_Tags": "adversarial machine learning; stealthy attacks"
    },
    {
      "Key": "LSGKKE4B",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Li, Jiacheng; Li, Ninghui; Ribeiro, Bruno",
      "Title": "Membership Inference Attacks and Defenses in Classification Models",
      "Publication Title": "Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy",
      "ISBN": "978-1-4503-8143-7",
      "DOI": "10.1145/3422337.3447836",
      "Url": "https://doi.org/10.1145/3422337.3447836",
      "Abstract": "We study the membership inference (MI) attack against classifiers, where the attacker's goal is to determine whether a data instance was used for training the classifier. Through systematic cataloging of existing MI attacks and extensive experimental evaluations of them, we find that a model's vulnerability to MI attacks is tightly related to the generalization gap—the difference between training accuracy and test accuracy. We then propose a defense against MI attacks that aims to close the gap by intentionally reduces the training accuracy. More specifically, the training process attempts to match the training and validation accuracies, by means of a new set regularizer using the Maximum Mean Discrepancy between the softmax output empirical distributions of the training and validation sets. Our experimental results show that combining this approach with another simple defense (mix-up training) significantly improves state-of-the-art defense against MI attacks, with minimal impact on testing accuracy.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "5–16",
      "Series": "CODASPY '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "neural networks; membership inference; image classification"
    },
    {
      "Key": "GQR67RET",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Elsabagh, Mohamed; Barbara, Daniel; Fleck, Dan; Stavrou, Angelos",
      "Title": "Detecting ROP with Statistical Learning of Program Characteristics",
      "Publication Title": "Proceedings of the Seventh ACM on Conference on Data and Application Security and Privacy",
      "ISBN": "978-1-4503-4523-1",
      "DOI": "10.1145/3029806.3029812",
      "Url": "https://doi.org/10.1145/3029806.3029812",
      "Abstract": "Return-Oriented Programming (ROP) has emerged as one of the most widely used techniques to exploit software vulnerabilities. Unfortunately, existing ROP protections suffer from a number of shortcomings: they require access to source code and compiler support, focus on specific types of gadgets, depend on accurate disassembly and construction of Control Flow Graphs, or use hardware-dependent (microarchitectural) characteristics. In this paper, we propose EigenROP, a novel system to detect ROP payloads based on unsupervised statistical learning of program characteristics. We study, for the first time, the feasibility and effectiveness of using microarchitecture-independent program characteristics – namely, memory locality, register traffic, and memory reuse distance – for detecting ROP. We propose a novel directional statistics based algorithm to identify deviations from the expected program characteristics during execution. EigenROP works transparently to the protected program, without requiring debug information, source code or disassembly. We implemented a dynamic instrumentation prototype of EigenROP using Intel Pin and measured it against in-the-wild ROP exploits and on payloads generated by the ROP compiler ROPC. Overall, EigenROP achieved significantly higher accuracy than prior anomaly-based solutions. It detected the execution of the ROP gadget chains with 81% accuracy, 80% true positive rate, only 0.8% false positive rate, and incurred comparable overhead to similar Pin-based solutions.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "219–226",
      "Series": "CODASPY '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Scottsdale, Arizona, USA",
      "Manual_Tags": "anomaly detection; directional statistics; program characteristics; return oriented programming"
    },
    {
      "Key": "3YHLHLZC",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Hendler, Danny; Kels, Shay; Rubin, Amir",
      "Title": "Detecting Malicious PowerShell Commands using Deep Neural Networks",
      "Publication Title": "Proceedings of the 2018 on Asia Conference on Computer and Communications Security",
      "ISBN": "978-1-4503-5576-6",
      "DOI": "10.1145/3196494.3196511",
      "Url": "https://doi.org/10.1145/3196494.3196511",
      "Abstract": "Microsoft's PowerShell is a command-line shell and scripting language that is installed by default on Windows machines. Based on Microsoft's .NET framework, it includes an interface that allows programmers to access operating system services. While PowerShell can be configured by administrators for restricting access and reducing vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be easily generated dynamically, executed from memory, encoded and obfuscated, thus making the logging and forensic analysis of code executed by PowerShell challenging. For all these reasons, PowerShell is increasingly used by cybercriminals as part of their attacks' tool chain, mainly for downloading malicious contents and for lateral movement. Indeed, a recent comprehensive technical report by Symantec dedicated to PowerShell's abuse by cybercrimials [52] reported on a sharp increase in the number of malicious PowerShell samples they received and in the number of penetration tools and frameworks that use PowerShell. This highlights the urgent need of developing effective methods for detecting malicious PowerShell commands. In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both \"traditional\" natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset. Our evaluation results show that, although our detectors (and especially the traditional NLP-based ones) individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, since the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied. Our detectors provide high recall values while maintaining a very low false positive rate, making us cautiously optimistic that they can be of practical value.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "187–197",
      "Series": "ASIACCS '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Incheon, Republic of Korea",
      "Manual_Tags": "natural language processing; deep learning; neural networks; malware detection; powershell"
    },
    {
      "Key": "FWADPSRY",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Tounsi, Youssef; Anoun, Houda; Hassouni, Larbi",
      "Title": "CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the new generation of Gradient Boosting Algorithms",
      "Publication Title": "Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security",
      "ISBN": "978-1-4503-7634-1",
      "DOI": "10.1145/3386723.3387851",
      "Url": "https://doi.org/10.1145/3386723.3387851",
      "Abstract": "Credit risk is one of the main risks facing banks and credit institutions, with the current progress in machine learning, artificial intelligence and big data. Recent research has proposed several systems for improving credit rating. In this paper, a new scalable credit scoring multi-agent system called \"CSMAS\" is introduced for the prediction of problems in data mining of credit scoring domain. This engine is built using a seven-layer multi-agent system architecture to generate a data mining process based on the coordination of intelligent agents. CSMAS performance is based on preprocessing and data forecasting. The first layer is designed to retrieve any data from various core banking systems, payment systems, credit Bureaus and external databases and data sources and to store it in big data platform. The second layer is devoted to three different subtasks; feature engineering, pre-processing data and integrating diverse datasets. While the third layer is dedicated to dealing with missing Values and treating outliers. In the fourth layer, the techniques of dimensionality reduction are used to reduce the number of features in the original set of features. The fifth layer is dedicated to build a model using the new generation of Gradient Boosting Algorithms (XGBoost, LightGBM and CatBoost) and make predictions. The sixth layer is designed for the model's evaluation. The seventh layer is made to perform the rating of new credit applicants. The performance of CSMAS is assessed using a large dataset of Home Credit Default Risk from Kaggle Challenge (307511 records) to evaluate the risk of a loan applicant as a major problem for banks. The results show that the CSMAS give relevant results. Therefore, the results indicated that the CSMAS can be further employed as a reliable tool to predict more complicated case in credit scoring.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "NISS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Marrakech, Morocco",
      "Manual_Tags": "Big Data; CatBoost; Credit Scoring; LightGBM; Multi-Agent System; XgBoost"
    },
    {
      "Key": "ZF22D4YL",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Ftaimi, Sara; Mazri, Tomader",
      "Title": "A comparative study of Machine learning algorithms for VANET networks",
      "Publication Title": "Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security",
      "ISBN": "978-1-4503-7634-1",
      "DOI": "10.1145/3386723.3387829",
      "Url": "https://doi.org/10.1145/3386723.3387829",
      "Abstract": "Vehicular Ad Hoc Networks (VANET) had incredible potential in improving road security, diminishing mishap rates and making the travel experience valuable for passengers. Like any other network, VANET too has problems and vulnerabilities that threaten its inherent nodes, and by implication, its reliability. In order to solve the security problems involving the VANET network, a lot of studies in machine learning algorithms have been undergone to improve the reliability of VANET by means of detecting intrusions and making prediction, ultimately, they have achieved satisfactory results. Therefore, the establishment of powerful VANET networks is significantly dependent on their security and protection alternatives, which is the subject of the present paper. This article starts with an overview of VANET Networks, then it proceeds to highlighting a variety of attacks that can threaten them. Next, we introduce the major concepts of machine learning before we conclude with the most frequently adopted artificial intelligence algorithms in the VANET networks",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "NISS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Marrakech, Morocco",
      "Manual_Tags": "Machine learning; 5G; Attacks; smart city; VANET"
    },
    {
      "Key": "AJV6N3X2",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "El Mrabet, Zakaria; Ezzari, Mehdi; Elghazi, Hassan; El Majd, Badr Abou",
      "Title": "Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure",
      "Publication Title": "Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security",
      "ISBN": "978-1-4503-6645-8",
      "DOI": "10.1145/3320326.3320391",
      "Url": "https://doi.org/10.1145/3320326.3320391",
      "Abstract": "Smart grid is an alternative solution of the conventional power grid which harnesses the power of the information technology to save the energy and meet todays' environment requirements. Due to the inherent vulnerabilities in the information technology, the smart grid is exposed to wide variety of threats that could be translated into cyber-attacks. In this paper, we develop a deep learning-based intrusion detection system to defend against cyber-attacks in the advanced metering infrastructure network. The proposed machine learning approach is trained and tested extensively on an empirical industrial dataset which is composed of several attack' categories including the scanning, buffer overflow, and denial of service attacks. Then, an experimental comparison in terms of detection accuracy is conducted to evaluate the performance of the proposed approach with Naïve Bayes, Support Vector Machine, and Random Forest. The obtained results suggest that the proposed approaches produce optimal results comparing to the other algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based intrusion detection system across the Advanced metering infrastructure network. In addition, we propose a network security architecture composed of two types of Intrusion detection system types, Host and Network based, deployed across the Advanced Metering Infrastructure network to inspect the traffic and detect the malicious one at all the levels.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "NISS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Rabat, Morocco",
      "Manual_Tags": "Deep learning; Advanced Metering Infrastructure; cross entropy loss; detection accuracy; Intrusion detection system"
    },
    {
      "Key": "CIL3V9CK",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Dekhane, Azzeddine; Djellal, Adel; Boutebbakh, Fouaz; Lakel, Rabah",
      "Title": "Cooling Fan Combined Fault Vibration Analysis Using Convolutional Neural Network Classifier",
      "Publication Title": "Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security",
      "ISBN": "978-1-4503-7634-1",
      "DOI": "10.1145/3386723.3387898",
      "Url": "https://doi.org/10.1145/3386723.3387898",
      "Abstract": "In this paper, an application of Convolutional Neural Network (CNN) to detect a predefined fault in vibration signal without any feature extraction. The vibration signal, after being normalized, is converted into a 2-D data called vibration image, and these images are passed in the CNN as input to detect whether there is a fault or not. Experiments are carried out with bearing data from the cooling Fan of a cement oven in CILAS-Biskra. Tests are done using different image sizes, and different training/testing data sets.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "NISS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Marrakech, Morocco",
      "Manual_Tags": "Deep learning; Convolutional Neural Network; Bearing fault diagnosis"
    },
    {
      "Key": "64HL3PWQ",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Whelan, Jason; Sangarapillai, Thanigajan; Minawi, Omar; Almehmadi, Abdulaziz; El-Khatib, Khalil",
      "Title": "Novelty-based Intrusion Detection of Sensor Attacks on Unmanned Aerial Vehicles",
      "Publication Title": "Proceedings of the 16th ACM Symposium on QoS and Security for Wireless and Mobile Networks",
      "ISBN": "978-1-4503-8120-8",
      "DOI": "10.1145/3416013.3426446",
      "Url": "https://doi.org/10.1145/3416013.3426446",
      "Abstract": "Unmanned Aerial Vehicles (UAVs) have proven to be a useful technology in numerous industries including industrial control systems surveillance, law enforcement, and military operations. Due to their heavy reliance on wireless protocols and hostile operating environments, UAVs face a large threat landscape. As attacks against UAVs increase, an intelligent intrusion detection system (IDS) is needed to aid the UAV in identifying attacks. The UAV domain presents unique challenges for intelligent IDS development, such as the variety of sensors, communication protocols, UAV platforms, control configurations, and dataset availability. In this paper, we propose a novelty-based approach to intrusion detection in UAVs by using one-class classifiers. One-class classifiers require only non-anomalous data to exist in the training set. This allows for the use of flight logs as training data, which are created by most UAVs during flight by default. Principal Component Analysis is applied to sensor logs for dimensionality reduction, and one-class classifier models are generated per sensor. A number of one-class classifiers are selected: One-Class Support Vector Machine, Autoencoder Neural Network, and Local Outlier Factor. The pre-processing, feature selection, training, and tuning of the selected algorithms is discussed. GPS spoofing is used throughout the paper as a common example of an external sensor-based attack. This approach shows to be effective across multiple UAV platforms with platform-specific F1 scores up to 99.56% and 99.73% for benign and malicious sensor readings respectively.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "23–28",
      "Series": "Q2SWinet '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Alicante, Spain",
      "Manual_Tags": "machine learning; cyber-physical systems; intrusion detection; novelty detection; robotic vehicles; unmanned aerial vehicles"
    },
    {
      "Key": "96ZWD5P7",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Guan, Chongqi; Liu, Heting; Cao, Guohong; Zhu, Sencun; La Porta, Thomas",
      "Title": "HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning",
      "Publication Title": "Proceedings of the 16th ACM Conference on Security and Privacy in Wireless and Mobile Networks",
      "ISBN": "978-1-4503-9859-6",
      "DOI": "10.1145/3558482.3590195",
      "Url": "https://doi.org/10.1145/3558482.3590195",
      "Abstract": "As IoT devices are becoming widely deployed, there exist many threats to IoT-based systems due to their inherent vulnerabilities. One effective approach to improving IoT security is to deploy IoT honeypot systems, which can collect attack information and reveal the methods and strategies used by attackers. However, building high-interaction IoT honeypots is challenging due to the heterogeneity of IoT devices. Vulnerabilities in IoT devices typically depend on specific device types or firmware versions, which encourages attackers to perform pre-attack checks to gather device information before launching attacks. Moreover, conventional honeypots are easily detected because their replying logic differs from that of the IoT devices they try to mimic.To address these problems, we develop an adaptive high-interaction honeypot for IoT devices, called em HoneyIoT. We first build a real device based attack trace collection system to learn how attackers interact with IoT devices. We then model the attack behavior through markov decision process and leverage reinforcement learning techniques to learn the best responses to engage attackers based on the attack trace. We also use differential analysis techniques to mutate response values in some fields to generate high-fidelity responses.HoneyIoT has been deployed on the public Internet. Experimental results show that HoneyIoT can effectively bypass the pre-attack checks and mislead the attackers into uploading malware. Furthermore, HoneyIoT is covert against widely used reconnaissance and honeypot detection tools.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "49–59",
      "Series": "WiSec '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Guildford</city>, <country>United Kingdom</country>, </conf-loc>",
      "Manual_Tags": "security; internet of things; reinforcement learning; honeypot"
    },
    {
      "Key": "7GEGQ56E",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Majdani, Farzan; Batik, Lynne; Petrovski, Andrei; Petrovski, Sergei",
      "Title": "Detecting Malicious Signal Manipulation in Smart Grids Using Intelligent Analysis of Contextual Data",
      "Publication Title": "13th International Conference on Security of Information and Networks",
      "ISBN": "978-1-4503-8751-4",
      "DOI": "10.1145/3433174.3433613",
      "Url": "https://doi.org/10.1145/3433174.3433613",
      "Abstract": "This paper looks at potential vulnerabilities of the Smart Grid energy infrastructure to data injection cyber-attacks and the means of addressing these vulnerabilities through intelligent data analysis. Efforts are being made by multiple groups to provide to defence-in-depth to Smart Grid systems by developing attack detection algorithms utilising artificial neural networks that evaluate data communication between system components. The first priority of such algorithms is the detection of anomalous commands or data states; however, anomalous data states may also result from physical situations legitimately encountered by equipment. This work aims at not only detecting and alerting on anomalies, but at intelligent learning of the system behaviour to distinguish between malicious interference and anomalous system states occurring due to maintenance activity or natural phenomena, such as for instance a nearby lightning strike causing a short-circuit fault.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "SIN 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Merkez, Turkey",
      "Manual_Tags": "Machine Learning; Artificial Neural Networks; Contextual Data; Intelligent Analysis; Malicious Interference; SCADA Cybersecurity; Smart Grid"
    },
    {
      "Key": "LTEU863Q",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Zhao, Benjamin Zi Hao; Kaafar, Mohamed Ali; Kourtellis, Nicolas",
      "Title": "Not one but many Tradeoffs: Privacy Vs. Utility in Differentially Private Machine Learning",
      "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Cloud Computing Security Workshop",
      "ISBN": "978-1-4503-8084-3",
      "DOI": "10.1145/3411495.3421352",
      "Url": "https://doi.org/10.1145/3411495.3421352",
      "Abstract": "Data holders are increasingly seeking to protect their user's privacy, whilst still maximizing their ability to produce machine learning (ML) models with high quality predictions. In this work, we empirically evaluate various implementations of differential privacy (DP), and measure their ability to fend off real-world privacy attacks, in addition to measuring their core goal of providing accurate classifications. We establish an evaluation framework to ensure each of these implementations are fairly evaluated. Our selection of DP implementations add DP noise at different positions within the framework, either at the point of data collection/release, during updates while training of the model, or after training by perturbing learned model parameters. We evaluate each implementation across a range of privacy budgets and datasets, each implementation providing the same mathematical privacy guarantees. By measuring the models' resistance to real world attacks of membership and attribute inference, and their classification accuracy. we determine which implementations provide the most desirable tradeoff between privacy and utility. We found that the number of classes of a given dataset is unlikely to influence where the privacy and utility tradeoff occurs, a counter-intuitive inference in contrast to the known relationship of increased privacy vulnerability in datasets with more classes. Additionally, in the scenario that high privacy constraints are required, perturbing input training data before applying ML modeling does not trade off as much utility, as compared to noise added later in the ML process.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "15–26",
      "Series": "CCSW'20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "machine learning; privacy; membership inference attack; attribute inference attack; differential privacy attack; privacy attack; tradeoff; utility"
    },
    {
      "Key": "IW7BRD6I",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Li, Yi; Wang, Shaohua; Nguyen, Tien N.",
      "Title": "Fault localization to detect co-change fixing locations",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549137",
      "Url": "https://doi.org/10.1145/3540250.3549137",
      "Abstract": "Fault Localization (FL) is a precursor step to most Automated Program Repair (APR) approaches, which fix the faulty statements identified by the FL tools. We present FixLocator, a Deep Learning (DL)-based fault localization approach supporting the detection of faulty statements in one or multiple methods that need to be modified accordingly in the same fix. Let us call them co-change (CC) fixing locations for a fault. We treat this FL problem as dual-task learning with two models. The method-level FL model, MethFL, learns the methods to be fixed together. The statement-level FL model, StmtFL, learns the statements to be co-fixed. Correct learning in one model can benefit the other and vice versa. Thus, we simultaneously train them with soft-sharing the models' parameters via cross-stitch units to enable the propagation of the impact of MethFL and StmtFL onto each other. Moreover, we explore a novel feature for FL: the co-changed statements. We also use Graph-based Convolution Network to integrate different types of program dependencies. Our empirical results show that FixLocator relatively improves over the state-of-the-art statement-level FL baselines by locating 26.5%–155.6% more CC fixing statements. To evaluate its usefulness in APR, we used FixLocator in combination with the state-of-the-art APR tools. The results show that FixLocator+DEAR (the original FL in DEAR replaced by FixLocator) and FixLocator+CURE improve relatively over the original DEAR and Ochiai+CURE by 10.5% and 42.9% in terms of the number of fixed bugs.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "659–671",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Deep Learning; Co-Change Fixing Locations; Fault Localization"
    },
    {
      "Key": "QBD3BUYJ",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Chen, Zhenpeng; Zhang, Jie M.; Sarro, Federica; Harman, Mark",
      "Title": "MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549093",
      "Url": "https://doi.org/10.1145/3540250.3549093",
      "Abstract": "Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1122–1134",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "bias mitigation; ensemble learning; fairness-performance trade-off; machine learning software; Software fairness"
    },
    {
      "Key": "3792HI3N",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Wang, Chaozheng; Yang, Yuanhang; Gao, Cuiyun; Peng, Yun; Zhang, Hongyu; Lyu, Michael R.",
      "Title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549113",
      "Url": "https://doi.org/10.1145/3540250.3549113",
      "Abstract": "Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "382–394",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "empirical study; code intelligence; prompt tuning"
    },
    {
      "Key": "BF7RX9CX",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Ibrahimzada, Ali Reza; Varli, Yigit; Tekinoglu, Dilara; Jabbarvand, Reyhaneh",
      "Title": "Perfect is the enemy of test oracle",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549086",
      "Url": "https://doi.org/10.1145/3540250.3549086",
      "Abstract": "Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents , a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation of tests are similar to that of MUTs they pass on them, but dissimilar to MUTs they fail on them. The classifier built on top of this vector representation serves as the oracle to generate “fail” labels, when test inputs detect a bug in MUT or “pass” labels, otherwise. Our extensive experiments on applying to more than 5K unit tests from a diverse set of open-source Java projects show that the produced oracle is (1) effective in predicting the fail or pass labels, achieving an overall accuracy, precision, recall, and F1 measure of 93%, 86%, 94%, and 90%, (2) generalizable, predicting the labels for the unit test of projects that were not in training or validation set with negligible performance drop, and (3) efficient, detecting the existence of bugs in only 6.5 milliseconds on average. Moreover, by interpreting the neural model and looking at it beyond a closed-box solution, we confirm that the oracle is valid, i.e., it predicts the labels through learning relevant features.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "70–81",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Software Testing; Deep Learning; Test Automation; Test Oracle"
    },
    {
      "Key": "AUXDMP6J",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Zhang, Jiyang; Nie, Pengyu; Li, Junyi Jessy; Gligoric, Milos",
      "Title": "Multilingual Code Co-evolution using Large Language Models",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616350",
      "Url": "https://doi.org/10.1145/3611643.3616350",
      "Abstract": "Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "695–707",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "software evolution; code translation; Language model"
    },
    {
      "Key": "N7EJ9QZF",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Vegas, Sira; Elbaum, Sebastian",
      "Title": "Pitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616320",
      "Url": "https://doi.org/10.1145/3611643.3616320",
      "Abstract": "Software engineering (SE) techniques are increasingly relying on deep learning approaches to support many SE tasks, from bug triaging to code generation. To assess the efficacy of such techniques researchers typically perform controlled experiments. Conducting these experiments, however, is particularly challenging given the complexity of the space of variables involved, from specialized and intricate architectures and algorithms to a large number of training hyper-parameters and choices of evolving datasets, all compounded by how rapidly the machine learning technology is advancing, and the inherent sources of randomness in the training process. In this work we conduct a mapping study, examining 194 experiments with techniques that rely on deep neural networks (DNNs) appearing in 55 papers published in premier SE venues to provide a characterization of the state of the practice, pinpointing experiments’ common trends and pitfalls. Our study reveals that most of the experiments, including those that have received ACM artifact badges, have fundamental limitations that raise doubts about the reliability of their findings. More specifically, we find: 1) weak analyses to determine that there is a true relationship between independent and dependent variables (87% of the experiments), 2) limited control over the space of DNN relevant variables, which can render a relationship between dependent variables and treatments that may not be causal but rather correlational (100% of the experiments), and 3) lack of specificity in terms of what are the DNN variables and their values utilized in the experiments (86% of the experiments) to define the treatments being applied, which makes it unclear whether the techniques designed are the ones being assessed, or how the sources of extraneous variation are controlled. We provide some practical recommendations to address these limitations.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "528–540",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "deep learning; machine learning for software engineering; software engineering experimentation"
    },
    {
      "Key": "2IEF69QJ",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Song, Liyan; Minku, Leandro; Teng, Cong; Yao, Xin",
      "Title": "A Practical Human Labeling Method for Online Just-in-Time Software Defect Prediction",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616307",
      "Url": "https://doi.org/10.1145/3611643.3616307",
      "Abstract": "Just-in-Time Software Defect Prediction (JIT-SDP) can be seen as an online learning problem where additional software changes produced over time may be labeled and used to create training examples. These training examples form a data stream that can be used to update JIT-SDP models in an attempt to avoid models becoming obsolete and poorly performing. However, labeling procedures adopted in existing online JIT-SDP studies implicitly assume that practitioners would not inspect software changes upon a defect-inducing prediction, delaying the production of training examples. This is inconsistent with a real-world scenario where practitioners would adopt JIT-SDP models and inspect certain software changes predicted as defect-inducing to check whether they really induce defects. Such inspection means that some software changes would be labeled much earlier than assumed in existing work, potentially leading to different JIT-SDP models and performance results. This paper aims at formulating a more practical human labeling procedure that takes into account the adoption of JIT-SDP models during the software development process. It then analyses whether and to what extent it would impact the predictive performance of JIT-SDP models. We also propose a new method to target the labeling of software changes with the aim of saving human inspection effort. Experiments based on 14 GitHub projects revealed that adopting a more realistic labeling procedure led to significantly higher predictive performance than when delaying the labeling process, meaning that existing work may have been underestimating the performance of JIT-SDP. In addition, our proposed method to target the labeling process was able to reduce human effort while maintaining predictive performance by recommending practitioners to inspect software changes that are more likely to induce defects. We encourage the adoption of more realistic human labeling methods in research studies to obtain an evaluation of JIT-SDP predictive performance that is closer to reality.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "605–617",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "online learning; human inspection; human labeling; Just-in-time software defect prediction; verification latency; waiting time"
    },
    {
      "Key": "LHW5AEZ8",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Nguyen, Giang; Biswas, Sumon; Rajan, Hridesh",
      "Title": "Fix Fairness, Don’t Ruin Accuracy: Performance Aware Fairness Repair using AutoML",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616257",
      "Url": "https://doi.org/10.1145/3611643.3616257",
      "Abstract": "Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "502–514",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "automated machine learning; bias mitigation; machine learning software; Software fairness; fairness-accuracy trade-off"
    },
    {
      "Key": "BY8J2YXI",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Ahmed, Shibbir; Imtiaz, Sayem Mohammad; Khairunnesa, Samantha Syeda; Cruz, Breno Dantas; Rajan, Hridesh",
      "Title": "Design by Contract for Deep Learning APIs",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616247",
      "Url": "https://doi.org/10.1145/3611643.3616247",
      "Abstract": "Deep Learning (DL) techniques are increasingly being incorporated in critical software systems today. DL software is buggy too. Recent work in SE has characterized these bugs, studied fix patterns, and proposed detection and localization strategies. In this work, we introduce a preventative measure. We propose design by contract for DL libraries, DL Contract for short, to document the properties of DL libraries and provide developers with a mechanism to identify bugs during development. While DL Contract builds on the traditional design by contract techniques, we need to address unique challenges. In particular, we need to document properties of the training process that are not visible at the functional interface of the DL libraries. To solve these problems, we have introduced mechanisms that allow developers to specify properties of the model architecture, data, and training process. We have designed and implemented DL Contract for Python-based DL libraries and used it to document the properties of Keras, a well-known DL library. We evaluate DL Contract in terms of effectiveness, runtime overhead, and usability. To evaluate the utility of DL Contract, we have developed 15 sample contracts specifically for training problems and structural bugs. We have adopted four well-vetted benchmarks from prior works on DL bug detection and repair. For the effectiveness, DL Contract correctly detects 259 bugs in 272 real-world buggy programs, from well-vetted benchmarks provided in prior work on DL bug detection and repair. We found that the DL Contract overhead is fairly minimal for the used benchmarks. Lastly, to evaluate the usability, we conducted a survey of twenty participants who have used DL Contract to find and fix bugs. The results reveal that DL Contract can be very helpful to DL application developers when debugging their code.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "94–106",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Deep learning; API contracts; specification language"
    },
    {
      "Key": "YLQ9V3QH",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Li, Lingwei; Yang, Li; Jiang, Huaxi; Yan, Jun; Luo, Tiejian; Hua, Zihan; Liang, Geng; Zuo, Chun",
      "Title": "AUGER: automatically generating review comments with pre-training models",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549099",
      "Url": "https://doi.org/10.1145/3540250.3549099",
      "Abstract": "Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, consider- ing what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments. This paper explores a synergy of multiple practical review comments to enhance code review and proposes AUGER (AUtomatically GEnerating Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38% in ROUGE-L. 29% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1009–1021",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Code Review; Machine Learning; Review Comments; Text Generation"
    },
    {
      "Key": "QAH6UEXP",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Sokolov, Alexander N.; Ragozin, Andrey N.; Pyatnitsky, Ilya A.; Alabugin, Sergei K.",
      "Title": "Applying of digital signal processing techniques to improve the performance of machine learning-based cyber attack detection in industrial control system",
      "Publication Title": "Proceedings of the 12th International Conference on Security of Information and Networks",
      "ISBN": "978-1-4503-7242-8",
      "DOI": "10.1145/3357613.3357637",
      "Url": "https://doi.org/10.1145/3357613.3357637",
      "Abstract": "Traditional approaches to building attack detection systems, such as a signature-based approach, do not allow detecting zero-day attacks. To improve the performance of attack detection, one resort to use methods of machine learning, in particular, neural networks. This paper considers the problem of detecting cyber attacks in Industrial Control Systems (ICS) using digital signal processing (DSP) technology. By processing signals from sensors using a comb of digital low-pass filters (LPF), additional informative features, that describe the control system, have been created. Experimental studies were conducted on the Secure Water Treatment (SWaT) dataset, which showed that the use of additional features obtained using DSP technologies improves the accuracy of detecting cyberattacks on ACS by reducing errors of the second kind.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "SIN '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Sochi, Russia",
      "Manual_Tags": "neural network; cyber attacks detection; digital filter; digital signal processing (DSP); industrial control system (ICS)"
    },
    {
      "Key": "9ANASF9W",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Li, Yi; Yadavally, Aashish; Zhang, Jiaxing; Wang, Shaohua; Nguyen, Tien N.",
      "Title": "Commit-Level, Neural Vulnerability Detection and Assessment",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616346",
      "Url": "https://doi.org/10.1145/3611643.3616346",
      "Abstract": "Software Vulnerabilities (SVs) are security flaws that are exploitable in cyber-attacks. Delay in the detection and assessment of SVs might cause serious consequences due to the unknown impacts on the attacked systems. The state-of-the-art approaches have been proposed to work directly on the committed code changes for early detection. However, none of them could provide both commit-level vulnerability detection and assessment at once. Moreover, the assessment approaches still suffer low accuracy due to limited representations for code changes and surrounding contexts. We propose a Context-aware, Graph-based, Commit-level Vulnerability Detection and Assessment Model, VDA, that evaluates a code change, detects any vulnerability and provides the CVSS assessment grades. To build VDA, we have key novel components. First, we design a novel context-aware, graph-based, representation learning model to learn the contextualized embeddings for the code changes that integrate program dependencies and the surrounding contexts of code changes, facilitating the automated vulnerability detection and assessment. Second, VDA considers the mutual impact of learning to detect vulnerability and learning to assess each vulnerability assessment type. To do so, it leverages multi-task learning among the vulnerability detection and vulnerability assessment tasks, improving all the tasks at the same time. Our empirical evaluation shows that on a C vulnerability dataset, VDA achieves 25.5% and 26.9% relatively higher than the baselines in vulnerability assessment regarding F-score and MCC, respectively. In a Java dataset, it achieves 31% and 33.3% relatively higher than the baselines in F-score and MCC, respectively. VDA also relatively improves the vulnerability detection over the baselines from 13.4–322% in F-score.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1024–1036",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Software Security; Neural Networks; Vulnerability Detection; Deep Learning; Vulnerability Assessment"
    },
    {
      "Key": "VWLD424T",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Hossain, Soneya Binta; Filieri, Antonio; Dwyer, Matthew B.; Elbaum, Sebastian; Visser, Willem",
      "Title": "Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons Learned",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616265",
      "Url": "https://doi.org/10.1145/3611643.3616265",
      "Abstract": "Defining test oracles is crucial and central to test development, but manual construction of oracles is expensive. While recent neural-based automated test oracle generation techniques have shown promise, their real-world effectiveness remains a compelling question requiring further exploration and understanding. This paper investigates the effectiveness of TOGA, a recently developed neural-based method for automatic test oracle generation. TOGA utilizes EvoSuite-generated test inputs and generates both exception and assertion oracles. In a Defects4j study, TOGA outperformed specification, search, and neural-based techniques, detecting 57 bugs, including 30 unique bugs not detected by other methods. To gain a deeper understanding of its applicability in real-world settings, we conducted a series of external, extended, and conceptual replication studies of TOGA. In a large-scale study involving 25 real-world Java systems, 223.5K test cases, and 51K injected faults, we evaluate TOGA’s ability to improve fault-detection effectiveness relative to the state-of-the-practice and the state-of-the-art. We find that TOGA misclassifies the type of oracle needed 24% of the time and that when it classifies correctly around 62% of the time it is not confident enough to generate any assertion oracle. When it does generate an assertion oracle, more than 47% of them are false positives, and the true positive assertions only increase fault detection by 0.3% relative to prior work. These findings expose limitations of the state-of-the-art neural-based oracle generation technique, provide valuable insights for improvement, and offer lessons for evaluating future automated oracle generation methods.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "120–132",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "EvoSuite; Mutation Testing; Neural Test Oracle Generation; TOGA"
    },
    {
      "Key": "GHYRC52H",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Gulwani, Sumit",
      "Title": "AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3569444",
      "Url": "https://doi.org/10.1145/3540250.3569444",
      "Abstract": "AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly—as input-output examples or natural-language specification—or implicitly—where they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps). The task of synthesizing an intended program snippet from the user’s intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques. Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Machine Learning; Program Synthesis; Interactive Programming; Symbolic Reasoning"
    },
    {
      "Key": "YCMWDECF",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Liu, Jiawei; Peng, Jinjun; Wang, Yuyao; Zhang, Lingming",
      "Title": "NeuRI: Diversifying DNN Generation via Inductive Rule Inference",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616337",
      "Url": "https://doi.org/10.1145/3611643.3616337",
      "Abstract": "Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) using hybrid model generation which incorporates both symbolic and concrete operators. Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24% and 15% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10% of all high-priority bugs of the period. Open-source developers regard error-inducing tests reported by us as \"high-quality\" and \"common in practice\".",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "657–669",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Fuzzing; Compiler Testing; Deep Learning Compilers"
    },
    {
      "Key": "8A2P8RX5",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Shanbhag, Shriram; Chimalakonda, Sridhar",
      "Title": "Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3560883",
      "Url": "https://doi.org/10.1145/3540250.3560883",
      "Abstract": "The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1610–1614",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "federated learning; data privacy; software engineering research; non-open source data"
    },
    {
      "Key": "BXBTXY7B",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Orvalho, Pedro; Janota, Mikoláš; Manquinho, Vasco",
      "Title": "MultIPAs: applying program transformations to introductory programming assignments for data augmentation",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3558931",
      "Url": "https://doi.org/10.1145/3540250.3558931",
      "Abstract": "There has been a growing interest, over the last few years, in the topic of automated program repair applied to fixing introductory programming assignments (IPAs). However, the datasets of IPAs publicly available tend to be small and with no valuable annotations about the defects of each program. Small datasets are not very useful for program repair tools that rely on machine learning models. Furthermore, a large diversity of correct implementations allows computing a smaller set of repairs to fix a given incorrect program rather than always using the same set of correct implementations for a given IPA. For these reasons, there has been an increasing demand for the task of augmenting IPAs benchmarks. This paper presents MultIPAs, a program transformation tool that can augment IPAs benchmarks by: (1) applying six syntactic mutations that conserve the program's semantics and (2) applying three semantic mutilations that introduce faults in the IPAs. Moreover, we demonstrate the usefulness of MultIPAs by augmenting with millions of programs two publicly available benchmarks of programs written in the C language, and also by generating an extensive benchmark of semantically incorrect programs.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1657–1661",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Automated Program Repair; Data Augmentation; Introductory Programming Assignments; MOOCs; Program Transformation"
    },
    {
      "Key": "CK2HZPYD",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Nguyen, Hoang H.; Nguyen, Nhat-Minh; Doan, Hong-Phuc; Ahmadi, Zahra; Doan, Thanh-Nam; Jiang, Lingxiao",
      "Title": "MANDO-GURU: vulnerability detection for smart contract source code by heterogeneous graph embeddings",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3558927",
      "Url": "https://doi.org/10.1145/3540250.3558927",
      "Abstract": "Smart contracts are increasingly used with blockchain systems for high-value applications. It is highly desired to ensure the quality of smart contract source code before they are deployed. This paper proposes a new deep learning-based tool, MANDO-GURU, that aims to accurately detect vulnerabilities in smart contracts at both coarse-grained contract-level and fine-grained line-level. Using a combination of control-flow graphs and call graphs of Solidity code, we design new heterogeneous graph attention neural networks to encode more structural and potentially semantic relations among different types of nodes and edges of such graphs and use the encoded embeddings of the graphs and nodes to detect vulnerabilities. Our validation of real-world smart contract datasets shows that MANDO-GURU can significantly improve many other vulnerability detection techniques by up to 24% in terms of the F1-score at the contract level, depending on vulnerability types. It is the first learning-based tool for Ethereum smart contracts that identify vulnerabilities at the line level and significantly improves the traditional code analysis-based techniques by up to 63.4%. Our tool is publicly available at https://github.com/MANDO-Project/ge-sc-machine. A test version is currently deployed at http://mandoguru.com, and a demo video of our tool is available at http://mandoguru.com/demo-video.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1736–1740",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "vulnerability detection; smart contracts; graph neural networks; Ethereum blockchain; heterogeneous graphs"
    },
    {
      "Key": "8ZRSPBJW",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Grishina, Anastasiia; Hort, Max; Moonen, Leon",
      "Title": "The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616304",
      "Url": "https://doi.org/10.1145/3611643.3616304",
      "Abstract": "The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models. We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer. Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection accuracy on Devign with only 3 out of 12 layers of CodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early layers can be used to obtain better results using the same resources, as well as to reduce resource usage during fine-tuning and inference.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "895–907",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "vulnerability detection; transformer; sustainability; model optimization; AI4Code; AI4SE; code classification; ML4SE"
    },
    {
      "Key": "83LLSQBP",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Bibaev, Vitaliy; Kalina, Alexey; Lomshakov, Vadim; Golubev, Yaroslav; Bezzubov, Alexander; Povarov, Nikita; Bryksin, Timofey",
      "Title": "All you need is logs: improving code completion by learning from anonymous IDE usage logs",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3558968",
      "Url": "https://doi.org/10.1145/3540250.3558968",
      "Abstract": "In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832. The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1269–1279",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "machine learning; code completion; A/B-testing; anonymous usage logs; integrated development environment"
    },
    {
      "Key": "79NYLVYA",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Nguyen, Truong Giang; Le-Cong, Thanh; Kang, Hong Jin; Le, Xuan-Bach D.; Lo, David",
      "Title": "VulCurator: a vulnerability-fixing commit detector",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3558936",
      "Url": "https://doi.org/10.1145/3540250.3558936",
      "Abstract": "Open-source software (OSS) vulnerability management process is important nowadays, as the number of discovered OSS vulnerabilities is increasing over time. Monitoring vulnerability-fixing commits is a part of the standard process to prevent vulnerability exploitation. Manually detecting vulnerability-fixing commits is, however, time-consuming due to the possibly large number of commits to review. Recently, many techniques have been proposed to automatically detect vulnerability-fixing commits using machine learning. These solutions either: (1) did not use deep learning, or (2) use deep learning on only limited sources of information. This paper proposes VulCurator, a tool that leverages deep learning on richer sources of information, including commit messages, code changes and issue reports for vulnerability-fixing commit classification. Our experimental results show that VulCurator outperforms the state-of-the-art baselines up to 16.1% in terms of F1-score. VulCurator tool is publicly available at https://github.com/ntgiang71096/VFDetector and https://zenodo.org/record/7034132# .Yw3MN-xBzDI, with a demo video at https://youtu.be/uMlFmWSJYOE",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1726–1730",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Deep Learning; BERT; Vulnerability-Fixing Commits"
    },
    {
      "Key": "N3BK7H2D",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Zhang, Chenxi; Peng, Xin; Zhou, Tong; Sha, Chaofeng; Yan, Zhenghui; Chen, Yiru; Yang, Hong",
      "Title": "TraceCRL: contrastive representation learning for microservice trace analysis",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549146",
      "Url": "https://doi.org/10.1145/3540250.3549146",
      "Abstract": "Due to the large amount and high complexity of trace data, microservice trace analysis tasks such as anomaly detection, fault diagnosis, and tail-based sampling widely adopt machine learning technology. These trace analysis approaches usually use a preprocessing step to map structured features of traces to vector representations in an ad-hoc way. Therefore, they may lose important information such as topological dependencies between service operations. In this paper, we propose TraceCRL, a trace representation learning approach based on contrastive learning and graph neural network, which can incorporate graph structured information in the downstream trace analysis tasks. Given a trace, TraceCRL constructs an operation invocation graph where nodes represent service operations and edges represent operation invocations together with predefined features for invocation status and related metrics. Based on the operation invocation graphs of traces TraceCRL uses a contrastive learning method to train a graph neural network-based model for trace representation. In particular, TraceCRL employs six trace data augmentation strategies to alleviate the problems of class collision and uniformity of representation in contrastive learning. Our experimental studies show that TraceCRL can significantly improve the performance of trace anomaly detection and offline trace sampling. It also confirms the effectiveness of the trace augmentation strategies and the efficiency of TraceCRL.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1221–1232",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Deep Learning; Graph Neural Network; Contrastive Learning; Microservice; Tracing"
    },
    {
      "Key": "YX8RLHZN",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Garg, Spandan; Moghaddam, Roshanak Zilouchian; Clement, Colin B.; Sundaresan, Neel; Wu, Chen",
      "Title": "DeepDev-PERF: a deep learning-based approach for improving software performance",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549096",
      "Url": "https://doi.org/10.1145/3540250.3549096",
      "Abstract": "Improving software performance is an important yet challenging part of the software development cycle. Today, the majority of performance inefficiencies are identified and patched by performance experts. Recent advancements in deep learning approaches and the wide-spread availability of open-source data creates a great opportunity to automate the identification and patching of performance problems. In this paper, we present DeepDev-PERF, a transformer-based approach to suggest performance improvements for C# applications. We pretrain DeepDev-PERF on English and Source code corpora, followed by finetuning for the task of generating performance improvement patches for C# applications. Our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in ‍53",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "948–958",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Artificial Intelligence; Bug Fixing; Software Performance"
    },
    {
      "Key": "6NAPJ3VG",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Risse, Niklas",
      "Title": "Detecting Overfitting of Machine Learning Techniques for Automatic Vulnerability Detection",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3617845",
      "Url": "https://doi.org/10.1145/3611643.3617845",
      "Abstract": "Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function f, models trained by machine learning techniques can decide if f contains a security flaw with up to 70% accuracy. But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model’s accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels. In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose a cross validation algorithm, where a semantic preserving transformation is applied during the amplification of either the training set or the testing set. Using 11 transformations and 3 ML techniques, we find that the improved robustness only applies to the specific transformations used during training data amplification. In other words, the robustified models still rely on unrelated features for predicting the vulnerabilities in the testing data.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "2189–2191",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "machine learning; large language models; automatic vulnerability detection; semantic preserving transformations"
    },
    {
      "Key": "BSU3J3DU",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Nicolae, Maria-Irina; Eisele, Max; Zeller, Andreas",
      "Title": "Revisiting Neural Program Smoothing for Fuzzing",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3616308",
      "Url": "https://doi.org/10.1145/3611643.3616308",
      "Abstract": "Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing, a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation. In this paper, we conduct the most extensive evaluation of neural program smoothing (NPS) fuzzers against standard gray-box fuzzers (&gt;11 CPU years and &gt;5.5 GPU years), and make the following contributions: We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers. As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data are public.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "133–145",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "machine learning; fuzzing; neural networks; neural program smoothing"
    },
    {
      "Key": "BWPQP2YZ",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Kim, Misoo; Kim, Youngkyoung; Jeong, Hohyeon; Heo, Jinseok; Kim, Sungoh; Chung, Hyunhee; Lee, Eunseok",
      "Title": "An empirical study of deep transfer learning-based program repair for Kotlin projects",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3558967",
      "Url": "https://doi.org/10.1145/3540250.3558967",
      "Abstract": "Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning. This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307%. Furthermore, we confirmed that the performance was improved by 532% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1441–1452",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Empirical study; Transfer learning; Deep learning-based program repair; Industrial Kotlin project; SonarQube defects"
    },
    {
      "Key": "ARB5GDGF",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Si, Haotian; Pei, Changhua; Li, Zhihan; Zhao, Yadong; Li, Jingjing; Zhang, Haiming; Diao, Zulong; Li, Jianhui; Xie, Gaogang; Pei, Dan",
      "Title": "Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3613896",
      "Url": "https://doi.org/10.1145/3611643.3613896",
      "Abstract": "Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate potential conflicts while fostering inter-metric promotions. Upon thorough investigation, we find that the poor performance of vanilla MMoE mainly comes from the input-output misalignment settings of MTS formulation and convergence issues arising from expansive tasks. To address these challenges, we propose a straightforward yet effective task-oriented metric selection and p&amp;s (personalized and shared) gating mechanism, which establishes CAD as the first practicable multi-task learning (MTL) based MTS AD model. Evaluations on multiple public datasets reveal that CAD obtains an average F1-score of 0.943 across three public datasets, notably outperforming state-of-the-art methods. Our code is accessible at https://github.com/dawnvince/MTS_CAD.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1635–1645",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "Multivariate Time Series; Unsupervised Anomaly Detection"
    },
    {
      "Key": "8NBALENN",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Jin, Matthew; Shahriar, Syed; Tufano, Michele; Shi, Xin; Lu, Shuai; Sundaresan, Neel; Svyatkovskiy, Alexey",
      "Title": "InferFix: End-to-End Program Repair with LLMs",
      "Publication Title": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3611643.3613892",
      "Url": "https://doi.org/10.1145/3611643.3613892",
      "Abstract": "Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1646–1656",
      "Series": "ESEC/FSE 2023",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>",
      "Manual_Tags": "finetuning; Program repair; prompt augmentation; static analyses"
    },
    {
      "Key": "I8QXKBPC",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Pan, Shengyi; Zhou, Jiayuan; Cogo, Filipe Roseiro; Xia, Xin; Bao, Lingfeng; Hu, Xing; Li, Shanping; Hassan, Ahmed E.",
      "Title": "Automated unearthing of dangerous issue reports",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549156",
      "Url": "https://doi.org/10.1145/3540250.3549156",
      "Abstract": "The coordinated vulnerability disclosure (CVD) process is commonly adopted for open source software (OSS) vulnerability management, which suggests to privately report the discovered vulnerabilities and keep relevant information secret until the official disclosure. However, in practice, due to various reasons (e.g., lacking security domain expertise or the sense of security management), many vulnerabilities are first reported via public issue reports (IRs) before its official disclosure. Such IRs are dangerous IRs, since attackers can take advantages of the leaked vulnerability information to launch zero-day attacks. It is crucial to identify such dangerous IRs at an early stage, such that OSS users can start the vulnerability remediation process earlier and OSS maintainers can timely manage the dangerous IRs. In this paper, we propose and evaluate a deep learning based approach, namely MemVul, to automatically identify dangerous IRs at the time they are reported. MemVul augments the neural networks with a memory component, which stores the external vulnerability knowledge from Common Weakness Enumeration (CWE). We rely on publicly accessible CVE-referred IRs (CIRs) to operationalize the concept of dangerous IR. We mine 3,937 CIRs distributed across 1,390 OSS projects hosted on GitHub. Evaluated under a practical scenario of high data imbalance, MemVul achieves the best trade-off between precision and recall among all baselines. In particular, the F1-score of MemVul (i.e., 0.49) improves the best performing baseline by 44%. For IRs that are predicted as CIRs but not reported to CVE, we conduct a user study to investigate their usefulness to OSS stakeholders. We observe that 82% (41 out of 50) of these IRs are security-related and 28 of them are suggested by security experts to be publicly disclosed, indicating MemVul is capable of identifying undisclosed dangerous IRs.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "834–846",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Software Security; Deep Learning; Vulnerability; Issue Report"
    },
    {
      "Key": "KAKXLWJR",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Wan, Yao; Zhang, Shijie; Zhang, Hongyu; Sui, Yulei; Xu, Guandong; Yao, Dezhong; Jin, Hai; Sun, Lichao",
      "Title": "You see what I want you to see: poisoning vulnerabilities in neural code search",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549153",
      "Url": "https://doi.org/10.1145/3540250.3549153",
      "Abstract": "Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset. Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50% to top 4.43%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1233–1245",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "backdoor attack; software vulnerability; deep learning; data poisoning; Code search"
    },
    {
      "Key": "TNYK74EK",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Alon, Yoav; David, Cristina",
      "Title": "Using graph neural networks for program termination",
      "Publication Title": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-9413-0",
      "DOI": "10.1145/3540250.3549095",
      "Url": "https://doi.org/10.1145/3540250.3549095",
      "Abstract": "Termination analyses investigate the termination behavior of programs, intending to detect nontermination, which is known to cause a variety of program bugs (e.g. hanging programs, denial-of-service vulnerabilities). Beyond formal approaches, various attempts have been made to estimate the termination behavior of programs using neural networks. However, the majority of these approaches continue to rely on formal methods to provide strong soundness guarantees and consequently suffer from similar limitations. In this paper, we move away from formal methods and embrace the stochastic nature of machine learning models. Instead of aiming for rigorous guarantees that can be interpreted by solvers, our objective is to provide an estimation of a program's termination behavior and of the likely reason for nontermination (when applicable) that a programmer can use for debugging purposes. Compared to previous approaches using neural networks for program termination, we also take advantage of the graph representation of programs by employing Graph Neural Networks. To further assist programmers in understanding and debugging nontermination bugs, we adapt the notions of attention and semantic segmentation, previously used for other application domains, to programs. Overall, we designed and implemented classifiers for program termination based on Graph Convolutional Networks and Graph Attention Networks, as well as a semantic segmentation Graph Neural Network that localizes AST nodes likely to cause nontermination. We also illustrated how the information provided by semantic segmentation can be combined with program slicing to further aid debugging.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "910–921",
      "Series": "ESEC/FSE 2022",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>",
      "Manual_Tags": "Graph Neural Networks; Graph Attention Networks; Program Nontermination; Program Termination"
    },
    {
      "Key": "EZWWQ92W",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Berta, Árpád; Danner, Gábor; Hegedus, István; Jelasity, Mark",
      "Title": "Hiding Needles in a Haystack: Towards Constructing Neural Networks that Evade Verification",
      "Publication Title": "Proceedings of the 2022 ACM Workshop on Information Hiding and Multimedia Security",
      "ISBN": "978-1-4503-9355-3",
      "DOI": "10.1145/3531536.3532966",
      "Url": "https://doi.org/10.1145/3531536.3532966",
      "Abstract": "Machine learning models are vulnerable to adversarial attacks, where a small, invisible, malicious perturbation of the input changes the predicted label. A large area of research is concerned with verification techniques that attempt to decide whether a given model has adversarial inputs close to a given benign input. Here, we show that current approaches to verification have a key vulnerability: we construct a model that is not robust but passes current verifiers. The idea is to insert artificial adversarial perturbations by adding a backdoor to a robust neural network model. In our construction, the adversarial input subspace that triggers the backdoor has a very small volume, and outside this subspace the gradient of the model is identical to that of the clean model. In other words, we seek to create a \"needle in a haystack\" search problem. For practical purposes, we also require that the adversarial samples be robust to JPEG compression. Large \"needle in the haystack\" problems are practically impossible to solve with any search algorithm. Formal verifiers can handle this in principle, but they do not scale up to real-world networks at the moment, and achieving this is a challenge because the verification problem is NP-complete. Our construction is based on training a hiding and a revealing network using deep steganography. Using the revealing network, we create a separate backdoor network and integrate it into the target network. We train our deep steganography networks over the CIFAR-10 dataset. We then evaluate our construction using state-of-the-art adversarial attacks and backdoor detectors over the CIFAR-10 and the ImageNet datasets. We made the code and models publicly available at https://github.com/szegedai/hiding-needles-in-a-haystack.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "51–62",
      "Series": "IH&amp;MMSec '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Santa Barbara, CA, USA",
      "Manual_Tags": "backdoor attack; neural networks; adversarial robustness; Trojan attack"
    },
    {
      "Key": "NU2FU37F",
      "Item Type": "conferencePaper",
      "Publication Year": "2016",
      "Author": "Biggio, Battista",
      "Title": "Machine Learning under Attack: Vulnerability Exploitation and Security Measures",
      "Publication Title": "Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security",
      "ISBN": "978-1-4503-4290-2",
      "DOI": "10.1145/2909827.2930784",
      "Url": "https://doi.org/10.1145/2909827.2930784",
      "Abstract": "Learning to discriminate between secure and hostile patterns is a crucial problem for species to survive in nature. Mimetism and camouflage are well-known examples of evolving weapons and defenses in the arms race between predators and preys. It is thus clear that all of the information acquired by our senses should not be considered necessarily secure or reliable. In machine learning and pattern recognition systems, however, we have started investigating these issues only recently. This phenomenon has been especially observed in the context of adversarial settings like malware detection and spam filtering, in which data can be purposely manipulated by humans to undermine the outcome of an automatic analysis. As current pattern recognition methods are not natively designed to deal with the intrinsic, adversarial nature of these problems, they exhibit specific vulnerabilities that an attacker may exploit either to mislead learning or to evade detection. Identifying these vulnerabilities and analyzing the impact of the corresponding attacks on learning algorithms has thus been one of the main open issues in the novel research field of adversarial machine learning, along with the design of more secure learning algorithms.In the first part of this talk, I introduce a general framework that encompasses and unifies previous work in the field, allowing one to systematically evaluate classifier security against different, potential attacks. As an example of application of this framework, in the second part of the talk, I discuss evasion attacks, where malicious samples are manipulated at test time to evade detection. I then show how carefully-designed poisoning attacks can mislead some learning algorithms by manipulating only a small fraction of their training data. In addition, I discuss some defense mechanisms against both attacks in the context of real-world applications, including biometric identity recognition and computer security. Finally, I briefly discuss our ongoing work on attacks against clustering algorithms, and sketch some promising future research directions.",
      "Date": "2016",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1–2",
      "Series": "IH&amp;MMSec '16",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vigo, Galicia, Spain",
      "Manual_Tags": "adversarial machine learning; evasion attacks; poisoning attacks; secure pattern recognition"
    },
    {
      "Key": "2EHQV6DE",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Bullough, Benjamin L.; Yanchenko, Anna K.; Smith, Christopher L.; Zipkin, Joseph R.",
      "Title": "Predicting Exploitation of Disclosed Software Vulnerabilities Using Open-source Data",
      "Publication Title": "Proceedings of the 3rd ACM on International Workshop on Security And Privacy Analytics",
      "ISBN": "978-1-4503-4909-3",
      "DOI": "10.1145/3041008.3041009",
      "Url": "https://doi.org/10.1145/3041008.3041009",
      "Abstract": "Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "45–53",
      "Series": "IWSPA '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Scottsdale, Arizona, USA",
      "Manual_Tags": "machine learning; exploit prediction; software vulnerabilities"
    },
    {
      "Key": "RW4QLBRN",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Qin, Su-Juan; Liu, Zhao; Ren, Feixiang; Tan, Chong",
      "Title": "Smart Contract Vulnerability Detection Based on Critical Combination Path and Deep Learning",
      "Publication Title": "Proceedings of the 2022 12th International Conference on Communication and Network Security",
      "ISBN": "978-1-4503-9752-0",
      "DOI": "10.1145/3586102.3586135",
      "Url": "https://doi.org/10.1145/3586102.3586135",
      "Abstract": "Ethereum is currently one of the most popular blockchain platforms. Smart contracts are an important part of blockchain. Because developers lack understanding of contract security and the huge value of contracts themselves, contracts are often attacked. Therefore, how to effectively detect smart contract vulnerabilities has become a crucial issue. This paper uses deep learning to detect vulnerabilities, which can get rid of dependence on expert experience. In order to solve the problem of poor detection effect caused by excessive noise, this paper proposes a vulnerability detection technology based on critical combination path and deep learning. The critical combination path only contains code related to vulnerabilities, eliminating many invalid codes, thus greatly reducing the impact of noise. At the same time, by analyzing the characteristics of assembly code, a normalization method is proposed to remove many homogeneous codes. The normalized critical combination paths are then vectorized using SimHash, and then converted to grayscale images for classification using a neural network. The experimental results show that the proposed scheme is effective.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "219–226",
      "Series": "ICCNS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Beijing</city>, <country>China</country>, </conf-loc>"
    },
    {
      "Key": "4XEATH7A",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Pârundefinedachi, Profir-Petru; Dash, Santanu Kumar; Allamanis, Miltiadis; Barr, Earl T.",
      "Title": "Flexeme: untangling commits using lexical flows",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409693",
      "Url": "https://doi.org/10.1145/3368089.3409693",
      "Abstract": "Today, most developers bundle changes into commits that they submit to a shared code repository. Tangled commits intermix distinct concerns, such as a bug fix and a new feature. They cause issues for developers, reviewers, and researchers alike: they restrict the usability of tools such as git bisect, make patch comprehension more difficult, and force researchers who mine software repositories to contend with noise. We present a novel data structure, the 𝛿-NFG, a multiversion Program Dependency Graph augmented with name flows. A 𝛿-NFG directly and simultaneously encodes different program versions, thereby capturing commits, and annotates data flow edges with the names/lexemes that flow across them. Our technique, Flexeme, builds a 𝛿-NFG from commits, then applies Agglomerative Clustering using Graph Similarity to that 𝛿-NFG to untangle its commits. At the untangling task on a C# corpus, our implementation, Heddle, improves the state-of-the-art on accuracy by 0.14, achieving 0.81, in a fraction of the time: Heddle is 32 times faster than the previous state-of-the-art.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "63–74",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "clustering; commint untangling; graph kernels"
    },
    {
      "Key": "66MJWLRQ",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Harel-Canada, Fabrice; Wang, Lingxiao; Gulzar, Muhammad Ali; Gu, Quanquan; Kim, Miryung",
      "Title": "Is neuron coverage a meaningful measure for testing deep neural networks?",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409754",
      "Url": "https://doi.org/10.1145/3368089.3409754",
      "Abstract": "Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "851–862",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "Software Engineering; Testing; Machine Learning; Adversarial Attack; Neuron Coverage"
    },
    {
      "Key": "ZUBD8THD",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Zhang, Yuhao; Ren, Luyao; Chen, Liqian; Xiong, Yingfei; Cheung, Shing-Chi; Xie, Tao",
      "Title": "Detecting numerical bugs in neural network architectures",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409720",
      "Url": "https://doi.org/10.1145/3368089.3409720",
      "Abstract": "Detecting bugs in deep learning software at the architecture level provides additional benefits that detecting bugs at the model level does not provide. This paper makes the first attempt to conduct static analysis for detecting numerical bugs at the architecture level. We propose a static analysis approach for detecting numerical bugs in neural architectures based on abstract interpretation. Our approach mainly comprises two kinds of abstraction techniques, i.e., one for tensors and one for numerical values. Moreover, to scale up while maintaining adequate detection precision, we propose two abstraction techniques: tensor partitioning and (elementwise) affine relation analysis to abstract tensors and numerical values, respectively. We realize the combination scheme of tensor partitioning and affine relation analysis (together with interval analysis) as DEBAR, and evaluate it on two datasets: neural architectures with known bugs (collected from existing studies) and real-world neural architectures. The evaluation results show that DEBAR outperforms other tensor and numerical abstraction techniques on accuracy without losing scalability. DEBAR successfully detects all known numerical bugs with no false positives within 1.7–2.3 seconds per architecture. On the real-world architectures, DEBAR reports 529 warnings within 2.6–135.4 seconds per architecture, where 299 warnings are true positives.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "826–837",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "Static Analysis; Neural Network; Numerical Bugs"
    },
    {
      "Key": "8UFDU8GU",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Suneja, Sahil; Zheng, Yunhui; Zhuang, Yufan; Laredo, Jim A.; Morari, Alessandro",
      "Title": "Probing model signal-awareness via prediction-preserving input minimization",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468545",
      "Url": "https://doi.org/10.1145/3468264.3468545",
      "Abstract": "This work explores the signal awareness of AI models for source code understanding. Using a software vulnerability detection use case, we evaluate the models' ability to capture the correct vulnerability signals to produce their predictions. Our prediction-preserving input minimization (P2IM) approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction. The model's reliance on incorrect signals is then uncovered when the vulnerability in the original code is missing in the minimal snippet, both of which the model however predicts as being vulnerable. We measure the signal awareness of models using a new metric we propose – Signal-aware Recall (SAR). We apply P2IM on three different neural network architectures across multiple datasets. The results show a sharp drop in the model's Recall from the high 90s to sub-60s with the new metric, highlighting that the models are presumably picking up a lot of noise or dataset nuances while learning their vulnerability detection logic. Although the drop in model performance may be perceived as an adversarial attack, but this isn't P2IM's objective. The idea is rather to uncover the signal-awareness of a black-box model in a data-driven manner via controlled queries. SAR's purpose is to measure the impact of task-agnostic model training, and not to suggest a shortcoming in the Recall metric. The expectation, in fact, is for SAR to match Recall in the ideal scenario where the model truly captures task-specific signals.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "945–955",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "machine learning; model signal-awareness; signal-aware recall"
    },
    {
      "Key": "7VW77NUN",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Rabin, Md Rafiqul Islam; Hellendoorn, Vincent J.; Alipour, Mohammad Amin",
      "Title": "Understanding neural code intelligence through program simplification",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468539",
      "Url": "https://doi.org/10.1145/3468264.3468539",
      "Abstract": "A wide range of code intelligence (CI) tools, powered by deep neural networks, have been developed recently to improve programming productivity and perform program analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to reduce this opacity in the vein of \"transparent/interpretable-AI\". However, these approaches are often specific to a particular set of network architectures, even requiring access to the network's parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on software debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that reduce the size of input programs of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input programs. We believe that SIVAND's extracted features may help understand neural CI systems' predictions and learned behavior.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "441–452",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Models of Code; Interpretable AI; Program Simplification"
    },
    {
      "Key": "CUKWFLXJ",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Chen, Simin; Bateni, Soroush; Grandhi, Sampath; Li, Xiaodi; Liu, Cong; Yang, Wei",
      "Title": "DENAS: automated rule generation by knowledge extraction from neural networks",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409733",
      "Url": "https://doi.org/10.1145/3368089.3409733",
      "Abstract": "Deep neural networks (DNNs) have been widely applied in the software development process to automatically learn patterns from massive data. However, many applications still make decisions based on rules that are manually crafted and verified by domain experts due to safety or security concerns. In this paper, we aim to close the gap between DNNs and rule-based systems by automating the rule generation process via extracting knowledge from well-trained DNNs. Existing techniques with similar purposes either rely on specific DNNs input instances or use inherently unstable random sampling of the input space. Therefore, these approaches either limit the exploration area to a local decision-space of the DNNs or fail to converge to a consistent set of rules. The resulting rules thus lack representativeness and stability. In this paper, we address the two aforementioned shortcomings by discovering a global property of the DNNs and use it to remodel the DNNs decision-boundary. We name this property as the activation probability, and show that this property is stable. With this insight, we propose an approach named DENAS including a novel rule-generation algorithm. Our proposed algorithm approximates the non-linear decision boundary of DNNs by iteratively superimposing a linearized optimization function. We evaluate the representativeness, stability, and accuracy of DENAS against five state-of-the-art techniques (LEMNA, Gradient, IG, DeepTaylor, and DTExtract) on three software engineering and security applications: Binary analysis, PDF malware detection, and Android malware detection. Our results show that DENAS can generate more representative rules consistently in a more stable manner over other approaches. We further offer case studies that demonstrate the applications of DENAS such as debugging faults in the DNNs and generating signatures that can detect zero-day malware.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "813–825",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "Machine Learning; Deep Neural Networks; Explainable AI"
    },
    {
      "Key": "FR9DUTYY",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Cito, Jürgen; Dillig, Isil; Kim, Seohyun; Murali, Vijayaraghavan; Chandra, Satish",
      "Title": "Explaining mispredictions of machine learning models using rule induction",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468614",
      "Url": "https://doi.org/10.1145/3468264.3468614",
      "Abstract": "While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "716–727",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "machine learning; explainability; rule induction"
    },
    {
      "Key": "Y6CMEQ5R",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Yan, Ming; Chen, Junjie; Zhang, Xiangyu; Tan, Lin; Wang, Gan; Wang, Zan",
      "Title": "Exposing numerical bugs in deep learning via gradient back-propagation",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468612",
      "Url": "https://doi.org/10.1145/3468264.3468612",
      "Abstract": "Numerical computation is dominant in deep learning (DL) programs. Consequently, numerical bugs are one of the most prominent kinds of defects in DL programs. Numerical bugs can lead to exceptional values such as NaN (Not-a-Number) and INF (Infinite), which can be propagated and eventually cause crashes or invalid outputs. They occur when special inputs cause invalid parameter values at internal mathematical operations such as log(). In this paper, we propose the first dynamic technique, called GRIST, which automatically generates a small input that can expose numerical bugs in DL programs. GRIST piggy-backs on the built-in gradient computation functionalities of DL infrastructures. Our evaluation on 63 real-world DL programs shows that GRIST detects 78 bugs including 56 unknown bugs. By submitting them to the corresponding issue repositories, eight bugs have been confirmed and three bugs have been fixed. Moreover, GRIST can save 8.79X execution time to expose numerical bugs compared to running original programs with its provided inputs. Compared to the state-of-the-art technique DEBAR (which is a static technique), DEBAR produces 12 false positives and misses 31 true bugs (of which 30 bugs can be found by GRIST), while GRIST only misses one known bug in those programs and no false positive. The results demonstrate the effectiveness of GRIST.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "627–638",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Numerical Bug; Deep Learning Testing; Gradient Back-propagation; Search-based Software Testing"
    },
    {
      "Key": "PSXHCMCQ",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Chakraborty, Mohna",
      "Title": "Does reusing pre-trained NLP model propagate bugs?",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3473494",
      "Url": "https://doi.org/10.1145/3468264.3473494",
      "Abstract": "In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients’ code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1686–1688",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Deep Learning; NLP; BERT; Bug; Reuse"
    },
    {
      "Key": "S48S9KBZ",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Patra, Jibesh; Pradel, Michael",
      "Title": "Semantic bug seeding: a learning-based approach for creating realistic bugs",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468623",
      "Url": "https://doi.org/10.1145/3468264.3468623",
      "Abstract": "When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens. This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "906–918",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "machine learning; bugs; dataset; bug injection; token embeddings"
    },
    {
      "Key": "ZPLYIWLD",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Suh, Alexander",
      "Title": "Adapting bug prediction models to predict reverted commits at Wayfair",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3417062",
      "Url": "https://doi.org/10.1145/3368089.3417062",
      "Abstract": "Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1251–1262",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "reverted commits; software defect prediction; software deployment"
    },
    {
      "Key": "NC7EL5DH",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Wang, Zan; Yan, Ming; Chen, Junjie; Liu, Shuang; Zhang, Dongdi",
      "Title": "Deep learning library testing via effective model generation",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409761",
      "Url": "https://doi.org/10.1145/3368089.3409761",
      "Abstract": "Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice. However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains. Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems. In this work, we propose a novel approach, LEMON, to testing DL libraries. In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries. We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet. The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers. Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "788–799",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "Deep Learning Testing; Search-based Software Testing; Library Testing; Model Generation; Mutation"
    },
    {
      "Key": "M26IITJG",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Shen, Qingchao; Ma, Haoyang; Chen, Junjie; Tian, Yongqiang; Cheung, Shing-Chi; Chen, Xiang",
      "Title": "A comprehensive study of deep learning compiler bugs",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468591",
      "Url": "https://doi.org/10.1145/3468264.3468591",
      "Abstract": "There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on specific hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in mission-critical systems. On the other hand, the DL models processed by DL compilers differ fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers. In this paper, we present the first systematic study of DL compiler bugs by analyzing 603 bugs arising in three popular DL compilers (i.e., TVM from Apache, Glow from Facebook, and nGraph from Intel). We analyzed these bugs according to their root causes, symptoms, and the stages where they occur during compilation. We obtain 12 findings, and provide a series of valuable guidelines for future work on DL compiler bug detection and debugging. For example, a large portion (nearly 20%) of DL compiler bugs are related to types, especially tensor types. The analysis of these bugs helps design new mutation operators (e.g., adding type cast for a tensor to promote implicit type conversion in subsequent tensor computations) to facilitate type-related bug detection. Further, we developed TVMfuzz as a proof-of-concept application of our findings to test the TVM DL compiler. It generates new tests based on TVM's original test suite. They expose 8 TVM bugs that are missed by the original test suite. The result demonstrates the usefulness of our findings.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "968–980",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Empirical Study; Deep Learning; Compiler Testing; Deep Learning Compiler Bug"
    },
    {
      "Key": "Y3KCC3WR",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Li, Zenan; Ma, Xiaoxing; Xu, Chang; Xu, Jingwei; Cao, Chun; Lü, Jian",
      "Title": "Operational calibration: debugging confidence errors for DNNs in the field",
      "Publication Title": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-7043-1",
      "DOI": "10.1145/3368089.3409696",
      "Url": "https://doi.org/10.1145/3368089.3409696",
      "Abstract": "Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system. Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (&gt;0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "901–913",
      "Series": "ESEC/FSE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, USA",
      "Manual_Tags": "Deep Neural Networks; Gaussian Process; Operational Calibration"
    },
    {
      "Key": "BLPRVDZD",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Chirkova, Nadezhda; Troshin, Sergey",
      "Title": "Empirical study of transformers for source code",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468611",
      "Url": "https://doi.org/10.1145/3468264.3468611",
      "Abstract": "Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "703–715",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "neural networks; code completion; function naming; transformer; variable misuse detection"
    },
    {
      "Key": "9BP5S6E3",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Pei, Kexin; Guan, Jonas; Broughton, Matthew; Chen, Zhongtian; Yao, Songchen; Williams-King, David; Ummadisetty, Vikas; Yang, Junfeng; Ray, Baishakhi; Jana, Suman",
      "Title": "StateFormer: fine-grained type recovery from binaries using generative state modeling",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468607",
      "Url": "https://doi.org/10.1145/3468264.3468607",
      "Abstract": "Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries. We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types. We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33%.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "690–702",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Transfer Learning; Machine Learning for Program Analysis; Reverse Engineering; Type Inference"
    },
    {
      "Key": "FKUVG2SU",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Chen, Ke; Li, Yufei; Chen, Yingfeng; Fan, Changjie; Hu, Zhipeng; Yang, Wei",
      "Title": "GLIB: towards automated test oracle for graphically-rich applications",
      "Publication Title": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-8562-6",
      "DOI": "10.1145/3468264.3468586",
      "Url": "https://doi.org/10.1145/3468264.3468586",
      "Abstract": "Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1093–1104",
      "Series": "ESEC/FSE 2021",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Athens, Greece",
      "Manual_Tags": "Deep Learning; Game Testing; Automated Test Oracle; GUI Testing"
    },
    {
      "Key": "PYT2ZMFH",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Roy, Sourav Dey; Das, Dipak Hrishi; Bhowmik, Mrinal Kanti",
      "Title": "Conventional and deep feature oriented quality inspection of internal defected eggs using infrared imaging",
      "Publication Title": "Proceedings of the 6th International Conference on Networking, Systems and Security",
      "ISBN": "978-1-4503-7699-0",
      "DOI": "10.1145/3362966.3362969",
      "Url": "https://doi.org/10.1145/3362966.3362969",
      "Abstract": "Automatic separation of defective eggs from qualified ones would lead to a great reduction on the graders visual stress as well as to an improvement on the quality control process. Due to the increasing incidence, Infrared Imaging Technology provides an important window for eggs sorting especially when the defects are not visible externally. In this paper, we have investigated the role of infrared imaging for classification of internal defective eggs from the fresh eggs. For our work, a new infrared image dataset of fresh and defective eggs (i.e. internal defective) has been designed by maintaining standard acquisition protocol. The study also includes investigation of conventional and deep features for accurate separation of defective eggs from qualified ones. Experimental results shows that DeepFS outer performs the remaining three feature sets (i.e. AComFS, SSigFS and ASigFS) with an average accuracy of 96.26% for all the used classifiers and hence able to effectively separate the fresh and defective eggs.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "12–20",
      "Series": "NSysS '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Dhaka, Bangladesh",
      "Manual_Tags": "classification; defect; feature selection; feature extraction; asymmetric analysis; egg; infrared imaging"
    },
    {
      "Key": "B44N9N93",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Bhuiyan, Md. Harunur Rashid; Islam, Muhammad Tafsirul; Islam, Nazmul; Islam, Mynul; Mondol, Anupom; Toha, Tarik Reza; Alam, Shaikh Md. Mominul",
      "Title": "Sound-Based Fault Detection For Textile Machinery",
      "Publication Title": "Proceedings of the 9th International Conference on Networking, Systems and Security",
      "ISBN": "978-1-4503-9903-6",
      "DOI": "10.1145/3569551.3569557",
      "Url": "https://doi.org/10.1145/3569551.3569557",
      "Abstract": "The textile sector is one of the vital driving forces in the economy of south Asian countries like Bangladesh, India, Pakistan, etc. However, most of the textile industries suffer from frequent machinery faults everyday which reduces their productivity, which in terms reduces their profit. Existing systems for detecting the faults in textile machinery fails to find a remedy to this problem due to several limitations. Among them, sound based and vibration based fault detection systems are based on prototype machinery and has smaller data set to detect machinery fault properly. The fabric defect based, machine learning based approaches only detect machinery fault after fabric has become already defected. To remedy these limitations, in this paper, we propose a sound based fault detection system consisting of trained machine learning model from large data set that can detect machinery fault in textile industry. We use a sound sensor to measure the sound signal of the machine. We artificially create three real faults in the experimented machine and measure the sound signal during the faults. Next, we conduct Fast Fourier Analysis derive sound frequency and statistical analysis to derive different statistical features from the prepared data set. From these two analysis, we determine if the sound frequency and amplitude changes during the fault. After that, we feed the data set to ten machine learning algorithms. Finally, we evaluate our trained machine leaning models through ten fold cross validation to determine the precision, recall, and F1 score. We find the highest F1 of 57.7% in Nearest Centroid Algorithm.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "53–62",
      "Series": "NSysS '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Cox's Bazar</city>, <country>Bangladesh</country>, </conf-loc>",
      "Manual_Tags": "Statistical Analysis; Fast Fourier Analysis; Machine-Learning; Sound"
    },
    {
      "Key": "WU5T78J9",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Ma, Shiqing; Liu, Yingqi; Lee, Wen-Chuan; Zhang, Xiangyu; Grama, Ananth",
      "Title": "MODE: automated neural network model debugging via state differential analysis and input selection",
      "Publication Title": "Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5573-5",
      "DOI": "10.1145/3236024.3236082",
      "Url": "https://doi.org/10.1145/3236024.3236082",
      "Abstract": "Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "175–186",
      "Series": "ESEC/FSE 2018",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Lake Buena Vista, FL, USA",
      "Manual_Tags": "Debugging; Deep Neural Network; Differential Analysis"
    },
    {
      "Key": "37D4J44X",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Sonnekalb, Tim",
      "Title": "Machine-learning supported vulnerability detection in source code",
      "Publication Title": "Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5572-8",
      "DOI": "10.1145/3338906.3341466",
      "Url": "https://doi.org/10.1145/3338906.3341466",
      "Abstract": "The awareness of writing secure code rises with the increasing number of attacks and their resultant damage. But often, software developers are no security experts and vulnerabilities arise unconsciously during the development process. They use static analysis tools for bug detection, which often come with a high false positive rate. The developers, therefore, need a lot of resources to mind about all alarms, if they want to consistently take care of the security of their software project. We want to investigate, if machine learning techniques could point the user to the position of a security weak point in the source code with a higher accuracy than ordinary methods with static analysis. For this purpose, we focus on current machine learning on code approaches for our initial studies to evolve an efficient way for finding security-related software bugs. We will create a configuration interface to discover certain vulnerabilities, categorized in CWEs. We want to create a benchmark tool to compare existing source code representations and machine learning architectures for vulnerability detection and develop a customizable feature model. At the end of this PhD project, we want to have an easy-to-use vulnerability detection tool based on machine learning on code.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1180–1183",
      "Series": "ESEC/FSE 2019",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Tallinn, Estonia",
      "Manual_Tags": "software security; vulnerability detection; vulnerabilities; machine learning on code; source code analysis"
    },
    {
      "Key": "JB7IJYBN",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "DeFreez, Daniel; Thakur, Aditya V.; Rubio-González, Cindy",
      "Title": "Path-based function embedding and its application to error-handling specification mining",
      "Publication Title": "Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5573-5",
      "DOI": "10.1145/3236024.3236059",
      "Url": "https://doi.org/10.1145/3236024.3236059",
      "Abstract": "Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2&lt;pre&gt;vec&lt;/pre&gt;, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2&lt;pre&gt;vec&lt;/pre&gt; at identifying function synonyms in the Linux kernel. Finally, we apply Func2&lt;pre&gt;vec&lt;/pre&gt; to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2&lt;pre&gt;vec&lt;/pre&gt; result in error-handling specifications with high support.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "423–433",
      "Series": "ESEC/FSE 2018",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Lake Buena Vista, FL, USA",
      "Manual_Tags": "program analysis; program comprehension; error handling; program embeddings; specification mining"
    },
    {
      "Key": "PEW3PCU8",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Islam, Md Johirul; Nguyen, Giang; Pan, Rangeet; Rajan, Hridesh",
      "Title": "A comprehensive study on deep learning bug characteristics",
      "Publication Title": "Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5572-8",
      "DOI": "10.1145/3338906.3338955",
      "Url": "https://doi.org/10.1145/3338906.3338955",
      "Abstract": "Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "510–520",
      "Series": "ESEC/FSE 2019",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Tallinn, Estonia",
      "Manual_Tags": "Bugs; A forums; Deep learning bugs; Deep learning software; Empirical Study of Bugs; Q&amp"
    },
    {
      "Key": "SACJR8CH",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Meijer, Erik",
      "Title": "Behind every great deep learning framework is an even greater programming languages concept (keynote)",
      "Publication Title": "Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5573-5",
      "DOI": "10.1145/3236024.3280855",
      "Url": "https://doi.org/10.1145/3236024.3280855",
      "Abstract": "In many areas, such as image recognition, natural language processing, search, recommendation, autonomous cars, systems software and infrastructure, and even Software Engineering tools themselves, Software 2.0 (= programming using learned models) is quickly swallowing Software 1.0 (= programming using handcrafted algorithms). Where the Software 1.0 Engineer formally specifies their problem, carefully designs algorithms, composes systems out of subsystems or decomposes complex systems into smaller components, the Software 2.0 Engineer amasses training data and simply feeds it into an ML algorithm that will synthesize an approximation of the function whose partial extensional definition is that training data. Instead of code as the artifact of interest, in Software 2.0 it is all about the data where compilation of source code is replaced by training models with data. This new style of programming has far-reaching consequences for traditional software engineering practices. Everything we have learned about life cycle models, project planning and estimation, requirements analysis, program design, construction, debugging, testing, maintenance and implementation, … runs the danger of becoming obsolete. One way to try to prepare for the new realities of software engineering is not to zero in on the differences between Software 1.0 and Software 2.0 but instead focus on their similarities. If you carefully look at what a neural net actually represents, you realize that in essence it is a pure function, from multi-dimensional arrays of floating point numbers to multi-dimensional arrays of floating point numbers (tensors). What is special about these functions is that they are differentiable (yes, exactly as you remember from middle school calculus), which allows them to be trained using back propagation. The programming language community has also discovered that there is a deep connection between back propagation and continuations. Moreover, when you look closely at how Software 2.0 Engineers construct complex neural nets like CNNs, RNNs, LSTMs, … you recognize they are (implicitly) using high-order combinators like map, fold, zip, scan, recursion, conditionals, function composition, … to compose complex neural network architectures out of simple building blocks. Constructing neural networks using pure and higher-order differentiable functions and training them using reverse-mode automatic differentiation is unsurprisingly called Differentiable Programming. This talk will illustrate the deep programming language principles behind Differentiable Programming, which will hopefully inspire the working Software 1.0 engineer to pay serious attention to the threats and opportunities of Software 2.0.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "1",
      "Series": "ESEC/FSE 2018",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Lake Buena Vista, FL, USA",
      "Manual_Tags": "Deep Learning; Functional Programming; Differentiable Programming; Automatic Differentiation; Continuation Passing Style; Programming Languages; TestMachine Learning"
    },
    {
      "Key": "JMALM669",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Chen, Di; Fu, Wei; Krishna, Rahul; Menzies, Tim",
      "Title": "Applications of psychological science for actionable analytics",
      "Publication Title": "Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "ISBN": "978-1-4503-5573-5",
      "DOI": "10.1145/3236024.3236050",
      "Url": "https://doi.org/10.1145/3236024.3236050",
      "Abstract": "According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of \"heuristic\"s (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics. We find that FFTs are remarkably effective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of FFTs are not effected (while the performance of other learners can vary wildly). Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, FFTs could be used as a standard baseline learner against which other software analytics tools are compared.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "456–467",
      "Series": "ESEC/FSE 2018",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Lake Buena Vista, FL, USA",
      "Manual_Tags": "empirical studies; defect prediction; Decision trees; heuristics; psychological science; software analytics"
    },
    {
      "Key": "5G4DCHL8",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "OConnor, TJ; Mohamed, Reham; Miettinen, Markus; Enck, William; Reaves, Bradley; Sadeghi, Ahmad-Reza",
      "Title": "HomeSnitch: behavior transparency and control for smart home IoT devices",
      "Publication Title": "Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks",
      "ISBN": "978-1-4503-6726-4",
      "DOI": "10.1145/3317549.3323409",
      "Url": "https://doi.org/10.1145/3317549.3323409",
      "Abstract": "The widespread adoption of smart home IoT devices has led to a broad and heterogeneous market with flawed security designs and privacy concerns. While the quality of IoT device software is unlikely to be fixed soon, there is great potential for a network-based solution that helps protect and inform consumers. Unfortunately, the encrypted and proprietary protocols used by devices limit the value of traditional network-based monitoring techniques. In this paper, we present HomeSnitch, a building block for enhancing smart home transparency and control by classifying IoT device communication by semantic behavior (e.g., heartbeat, firmware check, motion detection). HomeSnitch ignores payload content (which is often encrypted) and instead identifies behaviors using features of connection-oriented application data unit exchanges, which represent application-layer dialog between clients and servers. We evaluate HomeSnitch against an independent labeled corpus of IoT device network flows and correctly detect over 99% of behaviors. We further deployed HomeSnitch in a home environment and empirically evaluated its ability to correctly classify known behaviors as well as discover new behaviors. Through these efforts, we demonstrate the utility of network-level services to classify behaviors of and enforce control on smart home devices.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "128–138",
      "Series": "WiSec '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Miami, Florida"
    },
    {
      "Key": "DEVRLQJG",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Pajola, Luca; Pasa, Luca; Conti, Mauro",
      "Title": "Threat is in the Air: Machine Learning for Wireless Network Applications",
      "Publication Title": "Proceedings of the ACM Workshop on Wireless Security and Machine Learning",
      "ISBN": "978-1-4503-6769-1",
      "DOI": "10.1145/3324921.3328783",
      "Url": "https://doi.org/10.1145/3324921.3328783",
      "Abstract": "With the spread of wireless application, huge amount of data is generated every day. Thanks to its elasticity, machine learning is becoming a fundamental brick in this field, and many of applications are developed with the use of it and the several techniques that it offers. However, machine learning suffers on different problems and people that use it often are not aware of the possible threats. Often, an adversary tries to exploit these vulnerabilities in order to obtain benefits; because of this, adversarial machine learning is becoming wide studied in the scientific community. In this paper, we show state-of-the-art adversarial techniques and possible countermeasures, with the aim of warning people regarding sensible argument related to the machine learning.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "16–21",
      "Series": "WiseML 2019",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Miami, FL, USA",
      "Manual_Tags": "security; machine learning; adversarial machine learning; Wireless network applications"
    },
    {
      "Key": "CQBXYKGR",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Hui, Tian; Farokhi, Farhad; Ohrimenko, Olga",
      "Title": "Information Leakage from Data Updates in Machine Learning Models",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3605764.3623905",
      "Url": "https://doi.org/10.1145/3605764.3623905",
      "Abstract": "In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the updated model. Moreover, we observe that data records with rare values are more vulnerable to attacks, which points to the disparate vulnerability of privacy attacks in the update setting. When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model. These observations point to vulnerability of machine learning models to attribute inference attacks in the update setting.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "35–41",
      "Series": "AISec '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "machine learning; privacy; attribute inference; data update"
    },
    {
      "Key": "EAPU5EBP",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Grieco, Gustavo; Dinaburg, Artem",
      "Title": "Toward Smarter Vulnerability Discovery Using Machine Learning",
      "Publication Title": "Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-6004-3",
      "DOI": "10.1145/3270101.3270107",
      "Url": "https://doi.org/10.1145/3270101.3270107",
      "Abstract": "A Cyber Reasoning System (CRS) is designed to automatically find and exploit software vulnerabilities in complex software. To be effective, CRSs integrate multiple vulnerability detection tools (VDTs), such as symbolic executors and fuzzers. Determining which VDTs can best find bugs in a large set of target programs, and how to optimally configure those VDTs, remains an open and challenging problem. Current solutions are based on heuristics created by security analysts that rely on experience, intuition and luck. In this paper, we present Central Exploit Organizer (CEO), a proof-of-concept tool to optimize VDT selection. CEO uses machine learning to optimize the selection and configuration of the most suitable vulnerability detection tool. We show that CEO can predict the relative effectiveness of a given vulnerability detection tool, configuration, and initial input. The estimation accuracy presents an improvement between 11% and 21% over random selection. We are releasing CEO and our dataset as open source to encourage further research.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "48–56",
      "Series": "AISec '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Toronto, Canada",
      "Manual_Tags": "machine learning; vulnerability management"
    },
    {
      "Key": "6G7RBNA9",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Kan, Zeliang; Pendlebury, Feargus; Pierazzi, Fabio; Cavallaro, Lorenzo",
      "Title": "Investigating Labelless Drift Adaptation for Malware Detection",
      "Publication Title": "Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-8657-9",
      "DOI": "10.1145/3474369.3486873",
      "Url": "https://doi.org/10.1145/3474369.3486873",
      "Abstract": "The evolution of malware has long plagued machine learning-based detection systems, as malware authors develop innovative strategies to evade detection and chase profits. This induces concept drift as the test distribution diverges from the training, causing performance decay that requires constant monitoring and adaptation.In this work, we analyze the adaptation strategy used by DroidEvolver, a state-of-the-art learning system that self-updates using pseudo-labels to avoid the high overhead associated with obtaining a new ground truth. After removing sources of experimental bias present in the original evaluation, we identify a number of flaws in the generation and integration of these pseudo-labels, leading to a rapid onset of performance degradation as the model poisons itself. We propose DroidEvolver++, a more robust variant of DroidEvolver, to address these issues and highlight the role of pseudo-labels in addressing concept drift. We test the tolerance of the adaptation strategy versus different degrees of pseudo-label noise and propose the adoption of methods to ensure only high-quality pseudo-labels are used for updates.Ultimately, we conclude that the use of pseudo-labeling remains a promising solution to limitations on labeling capacity, but great care must be taken when designing update mechanisms to avoid negative feedback loops and self-poisoning which have catastrophic effects on performance.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "123–134",
      "Series": "AISec '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "machine learning; malware detection; online learning"
    },
    {
      "Key": "WGRKKZFC",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Ganz, Tom; Härterich, Martin; Warnecke, Alexander; Rieck, Konrad",
      "Title": "Explaining Graph Neural Networks for Vulnerability Discovery",
      "Publication Title": "Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-8657-9",
      "DOI": "10.1145/3474369.3486866",
      "Url": "https://doi.org/10.1145/3474369.3486866",
      "Abstract": "Graph neural networks (GNNs) have proven to be an effective tool for vulnerability discovery that outperforms learning-based methods working directly on source code. Unfortunately, these neural networks are uninterpretable models, whose decision process is completely opaque to security experts, which obstructs their practical adoption. Recently, several methods have been proposed for explaining models of machine learning. However, it is unclear whether these methods are suitable for GNNs and support the task of vulnerability discovery. In this paper we present a framework for evaluating explanation methods on GNNs. We develop a set of criteria for comparing graph explanations and linking them to properties of source code. Based on these criteria, we conduct an experimental study of nine regular and three graph-specific explanation methods. Our study demonstrates that explaining GNNs is a non-trivial task and all evaluation criteria play a role in assessing their efficacy. We further show that graph-specific explanations relate better to code semantics and provide more information to a security expert than regular methods.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "145–156",
      "Series": "AISec '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "machine learning; software security"
    },
    {
      "Key": "LJGW73FL",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Millar, Stuart; Podgurskii, Denis; Kuykendall, Dan; Martínez del Rincón, Jesús; Miller, Paul",
      "Title": "Optimising Vulnerability Triage in DAST with Deep Learning",
      "Publication Title": "Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-9880-0",
      "DOI": "10.1145/3560830.3563724",
      "Url": "https://doi.org/10.1145/3560830.3563724",
      "Abstract": "False positives generated by vulnerability scanners are an industry-wide challenge in web application security. Accordingly, this paper presents a novel multi-view deep learning architecture to optimise Dynamic Application Security Testing (DAST) vulnerability triage, with task-specific design decisions exploiting the structure of traffic exchanges between our rules-based DAST scanner and a given web app. Leveraging convolutional neural networks, natural language processing and word embeddings, our model learns separate yet complementary internal feature representations of these exchanges before fusing them together to make a prediction of a verified vulnerability or a false positive. Given the amount of time and cognitive effort required to constantly manually review high volumes of DAST results correctly, the addition of this deep learning capability to a rules-based scanner creates a hybrid system that enables expert analysts to rank scan results, deprioritise false positives and concentrate on likely real vulnerabilities. This improves productivity and reduces remediation time, resulting in stronger security postures. Evaluations are conducted on a real-world dataset containing 91,324 findings of 74 different vulnerability types curated from DAST scans on nineteen organisations. Results show our multi-view architecture significantly reduces both the false positive rate by 20% and the false negative rate by 40% on average across all organisations compared to the single-view approach.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "137–147",
      "Series": "AISec'22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Los Angeles, CA, USA",
      "Manual_Tags": "deep learning; convolutional neural networks; dast; web application security"
    },
    {
      "Key": "4MCCFYNU",
      "Item Type": "conferencePaper",
      "Publication Year": "2021",
      "Author": "Drees, Jan Peter; Gupta, Pritha; Hüllermeier, Eyke; Jager, Tibor; Konze, Alexander; Priesterjahn, Claudia; Ramaswamy, Arunselvan; Somorovsky, Juraj",
      "Title": "Automated Detection of Side Channels in Cryptographic Protocols: DROWN the ROBOTs!",
      "Publication Title": "Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-8657-9",
      "DOI": "10.1145/3474369.3486868",
      "Url": "https://doi.org/10.1145/3474369.3486868",
      "Abstract": "Currently most practical attacks on cryptographic protocols like TLS are based on side channels, such as padding oracles. Some well-known recent examples are DROWN, ROBOT and Raccoon (USENIX Security 2016, 2018, 2021). Such attacks are usually found by careful and time-consuming manual analysis by specialists.In this paper, we consider the question of how such attacks can be systematically detected and prevented before (large-scale) deployment. We propose a new, fully automated approach, which uses supervised learning to identify arbitrary patterns in network protocol traffic. In contrast to classical scanners, which search for known side channels, the detection of general patterns might detect new side channels, even unexpected ones, such as those from the ROBOT attack.To analyze this approach, we develop a tool to detect Bleichenbacher-like padding oracles in TLS server implementations, based on an ensemble of machine learning algorithms. We verify that the approach indeed detects known vulnerabilities successfully and reliably. The tool also provides detailed information about detected patterns to developers, to assist in removing a potential padding oracle. Due to the automation, the approach scales much better than manual analysis and could even be integrated with a CI/CD pipeline of a development environment, for example.",
      "Date": "2021",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "169–180",
      "Series": "AISec '21",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Republic of Korea",
      "Manual_Tags": "machine learning; tls; bleichenbacher; side channel"
    },
    {
      "Key": "6VIKDIWA",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Greshake, Kai; Abdelnabi, Sahar; Mishra, Shailesh; Endres, Christoph; Holz, Thorsten; Fritz, Mario",
      "Title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3605764.3623985",
      "Url": "https://doi.org/10.1145/3605764.3623985",
      "Abstract": "Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "79–90",
      "Series": "AISec '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "large language models; indirect prompt injection"
    },
    {
      "Key": "CDD682XL",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Gibert, Daniel; Zizzo, Giulio; Le, Quan",
      "Title": "Certified Robustness of Static Deep Learning-based Malware Detectors against Patch and Append Attacks",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3605764.3623914",
      "Url": "https://doi.org/10.1145/3605764.3623914",
      "Abstract": "Machine learning-based (ML) malware detectors have been shown to be susceptible to adversarial malware examples. Given the vulnerability of deep learning detectors to small changes on the input file, we propose a practical and certifiable defense against patch and append attacks on malware detection. Our defense is inspired by the concept of (de)randomized smoothing, a certifiable defense against patch attacks on image classifiers, which we adapt by: (1) presenting a novel chunk-based smoothing scheme that operates on subsequences of bytes within an executable; (2) deriving a certificate that measures the robustness against patch attacks and append attacks. Our approach works as follows: (i) during the training phase, a base classifier is trained to make classifications on a subset of continguous bytes or chunk of bytes from an executable; (ii) at test time, an executable is divided into non-overlapping chunks of fixed size and our detection system classifies the original executable as the majority vote over the predicted classes of the chunks. Leveraging the fact that patch and append attacks can only influence a certain number of chunks, we derive meaningful large robustness certificates against both attacks. To demonstrate the suitability of our approach we have trained a classifier with our chunk-based scheme on the BODMAS dataset. We show that the proposed chunk-based smoothed classifier is more robust against the benign injection attack and state-of-the-art evasion attacks in comparison to a non-smoothed classifier.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "173–184",
      "Series": "AISec '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "machine learning; malware detection; evasion attacks; certified robustness; adversarial defense; randomized smoothing"
    },
    {
      "Key": "3RXTEH8L",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Alperin, Kenneth; Wollaber, Allan; Ross, Dennis; Trepagnier, Pierre; Leonard, Leslie",
      "Title": "Risk Prioritization by Leveraging Latent Vulnerability Features in a Contested Environment",
      "Publication Title": "Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-6833-9",
      "DOI": "10.1145/3338501.3357365",
      "Url": "https://doi.org/10.1145/3338501.3357365",
      "Abstract": "Cyber network defenders face an overwhelming volume of software vulnerabilities. Resource limitations preclude them mitigating all but a small number of vulnerabilities on an enterprise network, so proper prioritization of defensive actions are of paramount importance. Current methods of risk prioritization are predominantly expert-based, and many include leveraging Common Vulnerability Scoring System (CVSS) risk scores. These scores are assigned by subject matter experts according to conventional methods of qualifying risk. Vulnerability mitigation strategies are then often applied in CVSS score order. Our vulnerability assessment system, in contrast, takes a predominantly data-driven approach. In general, we associate a risk metric of vulnerabilities with existence of corresponding exploits. Our assumption is that if an entity has invested time and money to exploit a particular vulnerability, this is a critical gauge of that vulnerability's importance, and hence risk.Prior work presented a model that allows for the creation of prioritized vulnerabilities based on their association-likelihood with exploits, outperforming then-current methods. Because the initial approach only leveraged one vulnerability feature, we extended the vulnerability feature space by incorporating additional features derived from natural language processing. The importance metric is still given by a vulnerability-exploit relationship, but by processing text descriptions and other available information, our system became significantly more accurate and predictive. We next propose a mechanism that customizes vulnerability risks according to their exploitation likelihood in a contested environment given site-specific threat intelligence information, namely, attacks by an Advanced Persistent Threat (APT) group. Utilizing held-back data, we then demonstrate that latently similar vulnerabilities, which could be targeted by the same adversary, see higher risk ratings.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "49–57",
      "Series": "AISec'19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: London, United Kingdom",
      "Manual_Tags": "machine learning; natural language processing; vulnerability; exploit; risk model"
    },
    {
      "Key": "34N35DW9",
      "Item Type": "conferencePaper",
      "Publication Year": "2017",
      "Author": "Chen, Pin-Yu; Zhang, Huan; Sharma, Yash; Yi, Jinfeng; Hsieh, Cho-Jui",
      "Title": "ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models",
      "Publication Title": "Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "978-1-4503-5202-4",
      "DOI": "10.1145/3128572.3140448",
      "Url": "https://doi.org/10.1145/3128572.3140448",
      "Abstract": "Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner's attack) and significantly outperforms existing black-box attacks via substitute models.",
      "Date": "2017",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "15–26",
      "Series": "AISec '17",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Dallas, Texas, USA",
      "Manual_Tags": "adversarial learning; black-box attack; deep learning; neural network; substitute model"
    },
    {
      "Key": "3PDJMPV3",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Imgrund, Erik; Ganz, Tom; Härterich, Martin; Pirch, Lukas; Risse, Niklas; Rieck, Konrad",
      "Title": "Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery",
      "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3605764.3623915",
      "Url": "https://doi.org/10.1145/3605764.3623915",
      "Abstract": "Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "149–160",
      "Series": "AISec '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
      "Manual_Tags": "vulnerability discovery; large language models; causal learning; confounding effect; overfitting"
    },
    {
      "Key": "MSJ5I8CR",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Fan, Ming; Jia, Ang; Liu, Jingwen; Liu, Ting; Chen, Wei",
      "Title": "When representation learning meets software analysis",
      "Publication Title": "Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages",
      "ISBN": "978-1-4503-8125-3",
      "DOI": "10.1145/3416506.3423578",
      "Url": "https://doi.org/10.1145/3416506.3423578",
      "Abstract": "In recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). Especially, representation learning, which can learn vectors from the syntactic and semantics of the code, offers much convenience and promotion for the downstream tasks such as code search and vulnerability detection. In this work, we introduce our two applications of leveraging representation learning for software analysis, including defect prediction and vulnerability detection.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "17–18",
      "Series": "RL+SE&amp;PL 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual, USA",
      "Manual_Tags": "vulnerability detection; representation learning; defect prediction"
    },
    {
      "Key": "E3TNY6X3",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Shi, Yi; Davaslioglu, Kemal; Sagduyu, Yalin E.",
      "Title": "Over-the-air membership inference attacks as privacy threats for deep learning-based wireless signal classifiers",
      "Publication Title": "Proceedings of the 2nd ACM Workshop on Wireless Security and Machine Learning",
      "ISBN": "978-1-4503-8007-2",
      "DOI": "10.1145/3395352.3404070",
      "Url": "https://doi.org/10.1145/3395352.3404070",
      "Abstract": "This paper presents how to leak private information from a wireless signal classifier by launching an over-the-air membership inference attack (MIA). As machine learning (ML) algorithms are used to process wireless signals to make decisions such as PHY-layer authentication, the training data characteristics (e.g., device-level information) and the environment conditions (e.g., channel information) under which the data is collected may leak to the ML model. As a privacy threat, the adversary can use this leaked information to exploit vulnerabilities of the ML model following an adversarial ML approach. In this paper, the MIA is launched against a deep learning-based classifier that uses waveform, device, and channel characteristics (power and phase shifts) in the received signals for RF fingerprinting. By observing the spectrum, the adversary builds first a surrogate classifier and then an inference model to determine whether a signal of interest has been used in the training data of the receiver (e.g., a service provider). The signal of interest can then be associated with particular device and channel characteristics to launch subsequent attacks. The probability of attack success is high (more than 88% depending on waveform and channel conditions) in identifying signals of interest (and potentially the device and channel information) used to build a target classifier. These results show that wireless signal classifiers are vulnerable to privacy threats due to the over-the-air information leakage of their ML models.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "61–66",
      "Series": "WiseML '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Linz, Austria",
      "Manual_Tags": "deep learning; adversarial machine learning; membership inference attack; wireless signal classification"
    },
    {
      "Key": "H4H4YSKT",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Liu, Jinxin; Kantarci, Burak; Adams, Carlisle",
      "Title": "Machine learning-driven intrusion detection for Contiki-NG-based IoT networks exposed to NSL-KDD dataset",
      "Publication Title": "Proceedings of the 2nd ACM Workshop on Wireless Security and Machine Learning",
      "ISBN": "978-1-4503-8007-2",
      "DOI": "10.1145/3395352.3402621",
      "Url": "https://doi.org/10.1145/3395352.3402621",
      "Abstract": "Wide adoption of Internet of Things (IoT) devices and applications encounters security vulnerabilities as roadblocks. The heterogeneous nature of IoT systems prevents common benchmarks, such as the NSL-KDD dataset, from being used to test and verify the performance of different Network Intrusion Detection Systems (NIDS). In order to bridge this gap, in this paper, we examine specific attacks in the NSL-KDD dataset that can impact sensor nodes and networks in IoT settings. Furthermore, in order to detect the introduced attacks, we study eleven machine learning algorithms and report the results. Through numerical analysis, we show that tree-based methods and ensemble methods outperform the rest of the studied machine learning methods. Among the supervised algorithms, XGBoost ranks the first with 97% accuracy, 90.5% Matthews correlation coefficient (MCC), and 99.6% Area Under the Curve (AUC) performance. Moreover, a notable research finding of this study is that the Expectation-Maximization (EM) algorithm, which is an unsupervised method, also performs reasonably well in the detection of the attacks in the NSL-KDD dataset and outperforms the accuracy of the Naïve Bayes classifier by 22.0%.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Pages": "25–30",
      "Series": "WiseML '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Linz, Austria",
      "Manual_Tags": "internet of things; machine learning; cybersecurity; DoS attacks; probe attacks"
    },
    {
      "Key": "EN5IARAS",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Torres, Gildo; Yang, Zhiliu; Blasingame, Zander; Bruska, James; Liu, Chen",
      "Title": "Detecting Non-Control-Flow Hijacking Attacks Using Contextual Execution Information",
      "Publication Title": "Proceedings of the 8th International Workshop on Hardware and Architectural Support for Security and Privacy",
      "ISBN": "978-1-4503-7226-8",
      "DOI": "10.1145/3337167.3337168",
      "Url": "https://doi.org/10.1145/3337167.3337168",
      "Abstract": "In recent years, we see a rise of non-control-flow hijacking attacks, which manipulate key data elements to corrupt the integrity of a victim application while upholding a valid control-flow during its execution. Consequently, they are more difficult to be detected hence prevented with traditional mitigation techniques that target control-oriented attacks. In this work, we propose a methodology for the detection of non-control-flow hijacking attacks via employing low-level hardware information formatted as time series. Using architectural and micro-architectural hardware event counts, we model the regular execution behavior of the application(s) of interest, in an effort to detect abnormal execution behavior taking place at the vicinity of the vulnerability. We employed three distinct anomaly detection models: a traditional support vector machine (SVM), an echo state network (ESN), and a heavily modified k-nearest neighbors (KNN) model. We evaluated the proposed methodology using seven real-world non-control-flow hijacking exploits that target two vulnerabilities in modern web servers and three vulnerabilities in the OpenSSL library. Because our proposed detection methodology employs the contextual information across the temporal domain, we are able to achieve an average classification accuracy of 99.36%, with a false positive rate (FPR) of 0.79% and false negative rate (FNR) of 0.53%, respectively.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "HASP '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Phoenix, AZ, USA",
      "Manual_Tags": "Anomaly Detection; Machine Learning; Data-Only Attacks; Encryption-Downgrade Attacks; Hardware Performance Counters"
    },
    {
      "Key": "HMLVMWNN",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Ozdagli, Ali I; Barreto, Carlos; Koutsoukos, Xenofon",
      "Title": "@PAD: adversarial training of power systems against denial-of-service attacks",
      "Publication Title": "Proceedings of the 7th Symposium on Hot Topics in the Science of Security",
      "ISBN": "978-1-4503-7561-0",
      "DOI": "10.1145/3384217.3385616",
      "Url": "https://doi.org/10.1145/3384217.3385616",
      "Abstract": "In this work, we study the vulnerabilities of protection systems that can detect cyber-attacks in power grid systems. We show that machine learning-based discriminators are not resilient against Denial-of-Service (DoS) attacks. In particular, we demonstrate that an adversarial actor can launch DoS attacks on specific sensors, render their measurements useless and cause the attack detector to classify a more sophisticated cyber-attack as a normal event. As a result of this, the system operator may fail to take action against attack-related faults leading to a decrease in the operation performance. To realize a DoS attack, we present an optimization problem to determine which sensors to attack within a given budget such that the existing classifier can be deceived. For linear classifiers, this optimization problem can be formulated as a mixed-integer linear programming problem. In this paper, we extend this optimization problem to find attacks for more complex classifiers such as neural networks. We demonstrate that a neural network, in particular, with RELU activation functions, can be represented as a set of logic formulas using Disjunctive Normal Form, and the optimization problem can be used to efficiently compute a DoS attack. In addition, we propose a defense model that improves the resilience of neural networks against DoS through adversarial training. Finally, we evaluate the efficiency of the approach using a dataset for classification in power systems.",
      "Date": "2020",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "HotSoS '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Lawrence, Kansas"
    },
    {
      "Key": "8MDZ7MPS",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Sotgiu, Angelo; Pintor, Maura; Biggio, Battista",
      "Title": "Explainability-based Debugging of Machine Learning for Vulnerability Discovery",
      "Publication Title": "Proceedings of the 17th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-9670-7",
      "DOI": "10.1145/3538969.3543809",
      "Url": "https://doi.org/10.1145/3538969.3543809",
      "Abstract": "Machine learning has been successfully used for increasingly complex and critical tasks, achieving high performance and efficiency that would not be possible for human operators. Unfortunately, recent studies have shown that, despite its power, this technology tends to learn spurious correlations from data, making it weak and susceptible to manipulation. Explainability techniques are often used to identify the most relevant features contributing to the decision. However, this is often done by taking examples one by one and trying to show the problem locally. To mitigate this issue, we propose in this paper a systematic method to leverage explainability techniques and build on their results to highlight problems in the model design and training. With an empirical analysis on the Devign dataset, we validate the proposed methodology with a CodeBERT model trained for vulnerability discovery, showing that, despite its impressive performances, spurious correlations consistently steer its decision.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vienna, Austria",
      "Manual_Tags": "machine learning; neural networks; datasets; code vulnerability detection"
    },
    {
      "Key": "GUACVCTH",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Tsinganos, Nikolaos; Sakellariou, Georgios; Fouliras, Panagiotis; Mavridis, Ioannis",
      "Title": "Towards an Automated Recognition System for Chat-based Social Engineering Attacks in Enterprise Environments",
      "Publication Title": "Proceedings of the 13th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-6448-5",
      "DOI": "10.1145/3230833.3233277",
      "Url": "https://doi.org/10.1145/3230833.3233277",
      "Abstract": "Increase in usage of electronic communication tools (email, IM, Skype, etc.) in enterprise environments has created new attack vectors for social engineers. Billions of people are now using electronic equipment in their everyday workflow which means billions of potential victims of Social Engineering (SE) attacks. Human is considered the weakest link in cybersecurity chain and breaking this defense is nowadays the most accessible route for malicious internal and external users. While several methods of protection have already been proposed and applied, none of these focuses on chat-based SE attacks while at the same time automation in the field is still missing. Social engineering is a complex phenomenon that requires interdisciplinary research combining technology, psychology, and linguistics. Attackers treat human personality traits as vulnerabilities and use the language as their weapon to deceive, persuade and finally manipulate the victims as they wish. Hence, a holistic approach is required to build a reliable SE attack recognition system. In this paper we present the current state-of-the-art on SE attack recognition systems, we dissect a SE attack to recognize the different stages, forms, and attributes and isolate the critical enablers that can influence a SE attack to work. Finally, we present our approach for an automated recognition system for chat-based SE attacks that is based on Personality Recognition, Influence Recognition, Deception Recognition, Speech Act and Chat History.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Hamburg, Germany",
      "Manual_Tags": "Social Engineering; Deception; Personality; Persuasion; Speech Act"
    },
    {
      "Key": "ICT7A5Q7",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Kronjee, Jorrit; Hommersom, Arjen; Vranken, Harald",
      "Title": "Discovering software vulnerabilities using data-flow analysis and machine learning",
      "Publication Title": "Proceedings of the 13th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-6448-5",
      "DOI": "10.1145/3230833.3230856",
      "Url": "https://doi.org/10.1145/3230833.3230856",
      "Abstract": "We present a novel method for static analysis in which we combine data-flow analysis with machine learning to detect SQL injection (SQLi) and Cross-Site Scripting (XSS) vulnerabilities in PHP applications. We assembled a dataset from the National Vulnerability Database and the SAMATE project, containing vulnerable PHP code samples and their patched versions in which the vulnerability is solved. We extracted features from the code samples by applying data-flow analysis techniques, including reaching definitions analysis, taint analysis, and reaching constants analysis. We used these features in machine learning to train various probabilistic classifiers. To demonstrate the effectiveness of our approach, we built a tool called WIRECAML, and compared our tool to other tools for vulnerability detection in PHP code. Our tool performed best for detecting both SQLi and XSS vulnerabilities. We also tried our approach on a number of open-source software applications, and found a previously unknown vulnerability in a photo-sharing web application.",
      "Date": "2018",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Hamburg, Germany",
      "Manual_Tags": "machine learning; vulnerability detection; Software security; data-flow analysis; static code analysis"
    },
    {
      "Key": "NF6HW3LY",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Iliou, Christos; Kostoulas, Theodoros; Tsikrika, Theodora; Katos, Vasilios; Vrochidis, Stefanos; Kompatsiaris, Ioannis",
      "Title": "Web Bot Detection Evasion Using Deep Reinforcement Learning",
      "Publication Title": "Proceedings of the 17th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-9670-7",
      "DOI": "10.1145/3538969.3538994",
      "Url": "https://doi.org/10.1145/3538969.3538994",
      "Abstract": "Web bots are vital for the web as they can be used to automate several actions, some of which would have otherwise been impossible or very time consuming. These actions can be benign, such as website testing and web indexing, or malicious, such as unauthorised content scraping, scalping, vulnerability scanning, and more. To detect malicious web bots, recent approaches examine the visitors’ fingerprint and behaviour. For the latter, several values (i.e., features) are usually extracted from visitors’ web logs and used as input to train machine learning models. In this research we show that web bots can use recent advances in machine learning, and, more specifically, Reinforcement Learning (RL), to effectively evade behaviour-based detection techniques. To evaluate these evasive bots, we examine (i) how well they can evade a pre-trained bot detection framework, (ii) how well they can still evade detection after the detection framework is re-trained on new behaviours generated from the evasive web bots, and (iii) how bots perform if re-trained again on the re-trained detection framework. We show that web bots can repeatedly evade detection and adapt to the re-trained detection framework to showcase the importance of considering such types of bots when designing web bot detection frameworks.",
      "Date": "2022",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Vienna, Austria",
      "Manual_Tags": "reinforcement learning; advanced web bots; evasive web bots; web bot detection; web logs"
    },
    {
      "Key": "UMMRQL4M",
      "Item Type": "conferencePaper",
      "Publication Year": "2023",
      "Author": "Lopes Antunes, David; Llopis Sanchez, Salvador",
      "Title": "The Age of fighting machines: the use of cyber deception for Adversarial Artificial Intelligence in Cyber Defence",
      "Publication Title": "Proceedings of the 18th International Conference on Availability, Reliability and Security",
      "ISBN": "9.7984E+12",
      "DOI": "10.1145/3600160.3605077",
      "Url": "https://doi.org/10.1145/3600160.3605077",
      "Abstract": "Cyber deception has emerged as a valuable technique in the field of cybersecurity, closely linked with adversarial Artificial Intelligence. In an era of pervasive automation, it is getting prominence as a research topic aimed at understanding how novel machine learning algorithms can be deceived using adversarial attacks that exploit vulnerabilities of their models. To this end, the paper describes the state-of-the-art of cyber deception for adversarial AI purposes, focusing on its benefits, challenges, and advanced techniques. In addition, this exploratory research attempts to extend its applicability to the fact that an appropriate and timely discovery of adversarial plans and associated actions may enhance own cyber resilience by introducing analytical findings of the adversary's intent into decision-making for cyber situational awareness. The study of adversarial thinking is as old as history and is one of the most relevant subjects rapidly incorporated into the operational planning process – a methodology to understand the operational environment. Adversarial knowledge is used for adapting own cyber defences in response to the cyber threat landscape.",
      "Date": "2023",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '23",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: <conf-loc>, <city>Benevento</city>, <country>Italy</country>, </conf-loc>",
      "Manual_Tags": "cyber deception; adversarial AI; adversarial intent; adversarial reasoning; cyber defence; decision superiority; information advantage"
    },
    {
      "Key": "MY9UYJ7Y",
      "Item Type": "conferencePaper",
      "Publication Year": "2019",
      "Author": "Iliou, Christos; Kostoulas, Theodoros; Tsikrika, Theodora; Katos, Vasilis; Vrochidis, Stefanos; Kompatsiaris, Yiannis",
      "Title": "Towards a framework for detecting advanced Web bots",
      "Publication Title": "Proceedings of the 14th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-7164-3",
      "DOI": "10.1145/3339252.3339267",
      "Url": "https://doi.org/10.1145/3339252.3339267",
      "Abstract": "Automated programs (bots) are responsible for a large percentage of website traffic. These bots can either be used for benign purposes, such as Web indexing, Website monitoring (validation of hyperlinks and HTML code), feed fetching Web content and data extraction for commercial use or for malicious ones, including, but not limited to, content scraping, vulnerability scanning, account takeover, distributed denial of service attacks, marketing fraud, carding and spam. To ensure their security, Web servers try to identify bot sessions and apply special rules to them, such as throttling their requests or delivering different content. The methods currently used for the identification of bots are based either purely on rule-based bot detection techniques or a combination of rule-based and machine learning techniques. While current research has developed highly adequate methods for Web bot detection, these methods' adequacy when faced with Web bots that try to remain undetected hasn't been studied. For this reason, we created and evaluated a Web bot detection framework on its ability to detect conspicuous bots separately from its ability to detect advanced Web bots. We assessed the proposed framework performance using real HTTP traffic from a public Web server. Our experimental results show that the proposed framework has significant ability to detect Web bots that do not try to hide their bot identity using HTTP Web logs (balanced accuracy in a false-positive intolerant server &gt; 95%). However, detecting advanced Web bots that present a browser fingerprint and may present a humanlike behaviour as well is considerably more difficult.",
      "Date": "2019",
      "Date Added": "3/10/24 6:14",
      "Date Modified": "3/10/24 6:14",
      "Series": "ARES '19",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Canterbury, CA, United Kingdom",
      "Manual_Tags": "Advanced Web bots; Evasive Web bots; humanlike behaviour; Web bot detection"
    },
    {
      "Key": "2GDG7AUP",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Widhalm, Dominik; Goeschka, Karl M.; Kastner, Wolfgang",
      "Title": "SoK: a taxonomy for anomaly detection in wireless sensor networks focused on node-level techniques",
      "Publication Title": "Proceedings of the 15th International Conference on Availability, Reliability and Security",
      "ISBN": "978-1-4503-8833-7",
      "DOI": "10.1145/3407023.3407027",
      "Url": "https://doi.org/10.1145/3407023.3407027",
      "Abstract": "Wireless sensor networks play an important role in today's world: When measuring physical conditions, the quality of the sensor readings ultimately impacts the quality of various data analytical services. To maintain data correctness and quality, run-time measures such as anomaly detection techniques are gaining significance. In particular, the detection of threatening node anomalies caused by sensor node faults has become a crucial task.The detection of faulty sensor nodes is a non-trivial task because wireless sensor networks typically consist of low-cost embedded systems with strictly limited resources, especially regarding their energy budget. Thus, efficient and lightweight approaches that meet the requirements of sensor networks are required.In this SoK paper, we contribute with a novel taxonomy of anomaly detection approaches focused on wireless sensor networks and a meta-survey of related classification schemes. To the best of our knowledge, our taxonomy is a comprehensive super-set of all previously published taxonomies in this field. Based on the taxonomy, we present new insights in node-level anomaly detection approaches and the applicability of immune-inspired techniques, and we lay out related research challenges.",
      "Date": "2020",
      "Date Added": "3/10/24 6:15",
      "Date Modified": "3/10/24 6:15",
      "Series": "ARES '20",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual Event, Ireland",
      "Manual_Tags": "taxonomy; anomaly detection; computational intelligence; node-level anomaly; wireless sensor networks"
    },
    {
      "Key": "UD6TI48C",
      "Item Type": "conferencePaper",
      "Publication Year": "2018",
      "Author": "Rieck, Konrad",
      "Title": "Family Reunion: Adversarial Machine Learning meets Digital Watermarking",
      "Publication Title": "Proceedings of the 2nd International Workshop on Multimedia Privacy and Security",
      "ISBN": "978-1-4503-5988-7",
      "DOI": "10.1145/3267357.3267366",
      "Url": "https://doi.org/10.1145/3267357.3267366",
      "Abstract": "Artificial intelligence is increasingly employed in security-critical systems, such as autonomous cars and drones. Unfortunately, many machine learning techniques suffer from vulnerabilities that enable an adversary to thwart their successful application, either during the training or prediction phase. In this talk, we investigate this threat and discuss attacks against machine learning, such as ad- versarial perturbations and data poisoning. Surprisingly, several of the attacks are not entirely novel, and similar concepts have been developed independently for attacking digital watermarks in multimedia security. We review these similarities and provide links between the two research areas that may open new directions for improving both, machine learning and multimedia security.",
      "Date": "2018",
      "Date Added": "3/10/24 6:15",
      "Date Modified": "3/10/24 6:15",
      "Pages": "1",
      "Series": "MPS '18",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Toronto, Canada",
      "Manual_Tags": "machine learning; adversarial learning; digital watermarking"
    },
    {
      "Key": "GWJ54US2",
      "Item Type": "conferencePaper",
      "Publication Year": "2022",
      "Author": "Li, Annie; Endres, Madeline; Weimer, Westley",
      "Title": "Debugging with stack overflow: web search behavior in novice and expert programmers",
      "Publication Title": "Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training",
      "ISBN": "978-1-4503-9225-9",
      "DOI": "10.1145/3510456.3514147",
      "Url": "https://doi.org/10.1145/3510456.3514147",
      "Abstract": "Debugging can be challenging for novice and expert programmers alike. Programmers routinely turn to online resources such as Stack Overflow for help, but understanding of debugging search practices, as well as tool support to find debugging resources, remains limited. Existing tools that mine online help forums are generally not aimed at novices, and programmers face varying levels of success when looking for online resources. Furthermore, training online code search skills is pedagogically challenging, as we have little understanding of how expertise impacts programmers' web search behavior while debugging code.We help fill these knowledge gaps with the results of a study of 40 programmers investigating differences in Stack Overflow search behavior at three levels of expertise: novices, experienced programmers who are novices in Python (the language we use in our study), and experienced Python programmers. We observe significant differences between all three levels in their ability to find posts helpful for debugging a given error, with both general and language-specific expertise facilitating Stack Overflow search efficacy and debugging success. We also conduct an exploratory investigation of factors that correlate with this difference, such as the display rank of the selected link and the number of links checked per search query. We conclude with an analysis of how online search behavior and results vary by Python error type. Our findings can inform online code search pedagogy, as well as inform the development of future automated tools.",
      "Date": "2022",
      "Date Added": "3/10/24 6:15",
      "Date Modified": "3/10/24 6:15",
      "Pages": "69–81",
      "Series": "ICSE-SEET '22",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Pittsburgh, Pennsylvania",
      "Manual_Tags": "debugging; controlled human study; online search behavior; programming experience; stack overflow"
    },
    {
      "Key": "6SWDSDTP",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Villalobos-Arias, Leonardo; Quesada-López, Christian; Guevara-Coto, Jose; Martínez, Alexandra; Jenkins, Marcelo",
      "Title": "Evaluating hyper-parameter tuning using random search in support vector machines for software effort estimation",
      "Publication Title": "Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering",
      "ISBN": "978-1-4503-8127-7",
      "DOI": "10.1145/3416508.3417121",
      "Url": "https://doi.org/10.1145/3416508.3417121",
      "Abstract": "Studies in software effort estimation&nbsp;(SEE) have explored the use of hyper-parameter tuning for machine learning algorithms&nbsp;(MLA) to improve the accuracy of effort estimates. In other contexts random search&nbsp;(RS) has shown similar results to grid search, while being less computationally-expensive. In this paper, we investigate to what extent the random search hyper-parameter tuning approach affects the accuracy and stability of support vector regression&nbsp;(SVR) in SEE. Results were compared to those obtained from ridge regression models and grid search-tuned models. A case study with four data sets extracted from the ISBSG 2018 repository shows that random search exhibits similar performance to grid search, rendering it an attractive alternative technique for hyper-parameter tuning. RS-tuned SVR achieved an increase of 0.227 standardized accuracy&nbsp;(SA) with respect to default hyper-parameters. In addition, random search improved prediction stability of SVR models to a minimum ratio of 0.840. The analysis showed that RS-tuned SVR attained performance equivalent to GS-tuned SVR. Future work includes extending this research to cover other hyper-parameter tuning approaches and machine learning algorithms, as well as using additional data sets.",
      "Date": "2020",
      "Date Added": "3/10/24 6:15",
      "Date Modified": "3/10/24 6:15",
      "Pages": "31–40",
      "Series": "PROMISE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual, USA",
      "Manual_Tags": "empirical study; support vector machines; Software effort estimation; grid search; hyper-parameter tuning; random search"
    },
    {
      "Key": "3B6VPN2N",
      "Item Type": "conferencePaper",
      "Publication Year": "2020",
      "Author": "Aljamaan, Hamoud; Alazba, Amal",
      "Title": "Software defect prediction using tree-based ensembles",
      "Publication Title": "Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering",
      "ISBN": "978-1-4503-8127-7",
      "DOI": "10.1145/3416508.3417114",
      "Url": "https://doi.org/10.1145/3416508.3417114",
      "Abstract": "Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.",
      "Date": "2020",
      "Date Added": "3/10/24 6:15",
      "Date Modified": "3/10/24 6:15",
      "Pages": "1–10",
      "Series": "PROMISE 2020",
      "Publisher": "Association for Computing Machinery",
      "Place": "New York, NY, USA",
      "Extra": "event-place: Virtual, USA",
      "Manual_Tags": "Classification; Machine Learning; Ensemble Learning; Prediction; Bagging; Boosting; Software Defect"
    }
  ]