"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Stealing Hyperparameters in Machine Learning","B. Wang; N. Z. Gong","ECE Department, Iowa State University; ECE Department, Iowa State University","2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","36","52","Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418595","Machine learning;Hyperparameter stealing attack;Machine learning as a service","Linear programming;Biological system modeling;Training;Machine learning;Machine learning algorithms;Testing;Predictive models","","218","4","58","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"Machine Learning Security in Industry: A Quantitative Survey","K. Grosse; L. Bieringer; T. R. Besold; B. Biggio; K. Krombholz","EPFL, Lausanne, Switzerland; QuantPi, Saarbrücken, Germany; Department of Industrial Engineering and Innovation Sciences, Eindhoven University of Technology, Eindhoven, Netherlands; Department of Electrical and Electronic Engineering, University of Cagliari, Cagliari, Italy; CISPA Helmholtz Center of Information Security, Saarbrücken, Germany","IEEE Transactions on Information Forensics and Security","14 Mar 2023","2023","18","","1749","1762","Despite the large body of academic work on machine learning security, little is known about the occurrence of attacks on machine learning systems in the wild. In this paper, we report on a quantitative study with 139 industrial practitioners. We analyze attack occurrence and concern and evaluate statistical hypotheses on factors influencing threat perception and exposure. Our results shed light on real-world attacks on deployed machine learning. On the organizational level, while we find no predictors for threat exposure in our sample, the amount of implement defenses depends on exposure to threats or expected likelihood to become a target. We also provide a detailed analysis of practitioners’ replies on the relevance of individual machine learning attacks, unveiling complex concerns like unreliable decision making, business information leakage, and bias introduction into models. Finally, we find that on the individual level, prior knowledge about machine learning security influences threat perception. Our work paves the way for more research about adversarial machine learning in practice, but yields also insights for regulation and auditing.","1556-6021","","10.1109/TIFS.2023.3251842","Bundesministerium für Klimaschutz, Umwelt, Energie, Mobilität, Innovation und Technologie (BMK), Bundesministerium für Digitalisierung und Wirtschaftsstandort (BMDW), and the Province of Upper Austria in the Frame of the COMET Program Managed by Forschungsförderungsgesellschaft (FFG) in the COMET Module S3AI; Fondazione di Sardegna under the Project TrustML: Towards Machine Learning that Humans Can Trust(grant numbers:CUP: F73C22001320007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057473","Adversarial machine learning;machine learning security;quantitative user study","Security;Organizations;Machine learning;Data models;Training data;Training;Production","","3","","43","IEEE","2 Mar 2023","","","IEEE","IEEE Journals"
"Comparison of Statistical and Machine Learning Techniques for Physical Layer Authentication","L. Senigagliesi; M. Baldi; E. Gambi","Dipartimento di Ingegneria dell’Informazione, Università Politecnica delle Marche, Ancona, Italy; Dipartimento di Ingegneria dell’Informazione, Università Politecnica delle Marche, Ancona, Italy; Dipartimento di Ingegneria dell’Informazione, Università Politecnica delle Marche, Ancona, Italy","IEEE Transactions on Information Forensics and Security","11 Dec 2020","2021","16","","1506","1521","In this article we consider authentication at the physical layer, in which the authenticator aims at distinguishing a legitimate supplicant from an attacker on the basis of the characteristics of a set of parallel wireless channels, which are affected by time-varying fading. Moreover, the attacker's channel has a spatial correlation with the supplicant's one. In this setting, we assess and compare the performance achieved by different approaches under different channel conditions. We first consider the use of two different statistical decision methods, and we prove that using a large number of references (in the form of channel estimates) affected by different levels of time-varying fading is not beneficial from a security point of view. We then consider classification methods based on machine learning. In order to face the worst case scenario of an authenticator provided with no forged messages during training, we consider one-class classifiers. When instead the training set includes some forged messages, we resort to more conventional binary classifiers, considering the cases in which such messages are either labelled or not. For the latter case, we exploit clustering algorithms to label the training set. The performance of both nearest neighbor (NN) and support vector machine (SVM) classification techniques is evaluated. Through numerical examples, we show that under the same probability of false alarm, one-class classification (OCC) algorithms achieve the lowest probability of missed detection when a small spatial correlation exists between the main channel and the adversary one, while statistical methods are advantageous when the spatial correlation between the two channels is large.","1556-6021","","10.1109/TIFS.2020.3033454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237955","Clustering;machine learning;physical layer authentication;wireless communications","Authentication;Fading channels;Training;Machine learning;Clustering algorithms;Machine learning algorithms;Protocols","","36","","40","IEEE","23 Oct 2020","","","IEEE","IEEE Journals"
"The Cost of Privacy in Asynchronous Differentially-Private Machine Learning","F. Farokhi; N. Wu; D. Smith; M. A. Kaafar","Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia; CSIRO’s Data61, Eveleigh, NSW, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia","IEEE Transactions on Information Forensics and Security","4 Feb 2021","2021","16","","2118","2129","We consider training machine learning models using data located on multiple private and geographically-scattered servers with different privacy settings. Due to the distributed nature of the data, communicating with all collaborating private data owners simultaneously may prove challenging or altogether impossible. We consider differentially-private asynchronous algorithms for collaboratively training machine-learning models on multiple private datasets. The asynchronous nature of the algorithms implies that a central learner interacts with the private data owners one-on-one whenever they are available for communication without needing to aggregate query responses to construct gradients of the entire fitness function. Therefore, the algorithm efficiently scales to many data owners. We define the cost of privacy as the difference between the fitness of a privacy-preserving machine-learning model and the fitness of trained machine-learning model in the absence of privacy concerns. We demonstrate that the cost of privacy has an upper bound that is inversely proportional to the combined size of the training datasets squared and the sum of the privacy budgets squared. We validate the theoretical results with experiments on financial and medical datasets. The experiments illustrate that collaboration among more than 10 data owners with at least 10,000 records with privacy budgets greater than or equal to 1 results in a superior machine-learning model in comparison to a model trained in isolation on only one of the datasets, illustrating the value of collaboration and the cost of the privacy. The number of the collaborating datasets can be lowered if the privacy budget is higher.","1556-6021","","10.1109/TIFS.2021.3050603","Optus Macquarie University Cyber Security Hub; Next Generation Technology Funding from the Defense Science and Technology Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319401","Machine learning;differential privacy;stochastic gradient algorithm;asynchronous","Training;Data models;Distributed databases;Biological system modeling;Degradation;Privacy;Machine learning","","6","","41","IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"Efficient Dropout-Resilient Aggregation for Privacy-Preserving Machine Learning","Z. Liu; J. Guo; K. -Y. Lam; J. Zhao","School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore","IEEE Transactions on Information Forensics and Security","21 Mar 2023","2023","18","","1839","1854","Machine learning (ML) has been widely recognized as an enabler of the global trend of digital transformation. With the increasing adoption of data-hungry machine learning algorithms, personal data privacy has emerged as one of the key concerns that could hinder the success of digital transformation. As such, Privacy-Preserving Machine Learning (PPML) has received much attention of the machine learning community, from academic researchers to industry practitioners to government regulators. However, organizations are faced with the dilemma that, on the one hand, they are encouraged to share data to enhance ML performance, but on the other hand, they could potentially be breaching the relevant data privacy regulations. Practical PPML typically allows multiple participants to individually train their ML models, which are then aggregated to construct a global model in a privacy-preserving manner, e.g., based on multi-party computation or homomorphic encryption. Nevertheless, in most important applications of large-scale PPML, e.g., by aggregating clients’ gradients to update a global model for federated learning, such as consumer behavior modeling of mobile application services, some participants are inevitably resource-constrained mobile devices, which may drop out of the PPML system due to their mobility nature (Yang et al., 2019). Therefore, the resilience of privacy-preserving aggregation has become an important problem to be tackled because of its real-world application potential and impacts. In this paper, we propose a scalable privacy-preserving aggregation scheme that can tolerate dropout by participants at any time, and is secure against both semi-honest and active malicious adversaries by setting proper system parameters. By replacing communication-intensive building blocks with a seed homomorphic pseudo-random generator, and relying on the additive homomorphic property of Shamir secret sharing scheme, our scheme outperforms state-of-the-art schemes by up to  $6.37\times $  in runtime and provides a stronger dropout-resilience. The simplicity of our scheme makes it attractive both for implementation and for further improvements.","1556-6021","","10.1109/TIFS.2022.3163592","National Research Foundation, Singapore, under its Strategic Capability Research Centres Funding Initiative; Singapore Ministry of Education Academic Research Fund(grant numbers:Tier 1 RG24/20,Tier 1 RG97/20,Tier 1 RG90/22,Tier 2 MOE2019-T2-1-176); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757847","Secure aggregation;privacy-preserving machine learning;dropout-resilience;HPRG","Servers;Protocols;Cryptography;Computational modeling;Privacy;Machine learning;Runtime","","21","","61","IEEE","14 Apr 2022","","","IEEE","IEEE Journals"
"The Value of Collaboration in Convex Machine Learning with Differential Privacy","N. Wu; F. Farokhi; D. Smith; M. A. Kaafar",Macquarie University; CSIRO’s Data61; CSIRO’s Data61; CSIRO’s Data61,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","304","317","In this paper, we apply machine learning to distributed private data owned by multiple data owners, entities with access to non-overlapping training datasets. We use noisy, differentially-private gradients to minimize the fitness cost of the machine learning model using stochastic gradient descent. We quantify the quality of the trained model, using the fitness cost, as a function of privacy budget and size of the distributed datasets to capture the trade-off between privacy and utility in machine learning. This way, we can predict the outcome of collaboration among privacy-aware data owners prior to executing potentially computationally-expensive machine learning algorithms. Particularly, we show that the difference between the fitness of the trained machine learning model using differentially-private gradient queries and the fitness of the trained machine model in the absence of any privacy concerns is inversely proportional to the size of the training datasets squared and the privacy budget squared. We successfully validate the performance prediction with the actual performance of the proposed privacy-aware learning algorithms, applied to: financial datasets for determining interest rates of loans using regression; and detecting credit card frauds using support vector machines.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152691","Machine learning;Differential privacy;Stochastic gradient algorithm.","Data privacy;Training;Data models;Computational modeling;Prediction algorithms;Privacy;Collaboration","","50","","34","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Automatic Characterization of Exploitable Faults: A Machine Learning Approach","S. Saha; D. Jap; S. Patranabis; D. Mukhopadhyay; S. Bhasin; P. Dasgupta","Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Physical Analysis & Cryptographic Engineering Labs, Nanyang Technical University, Singapore; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Physical Analysis & Cryptographic Engineering Labs, Nanyang Technical University, Singapore; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India","IEEE Transactions on Information Forensics and Security","31 Oct 2018","2019","14","4","954","968","Characterizing the fault space of a cipher to filter out a set of faults potentially exploitable for fault attacks (FA), is a problem with immense practical value. A quantitative knowledge of the exploitable fault space is desirable in several applications, such as security evaluation, cipher construction and implementation, design, testing of countermeasures, and so on. In this paper, we investigate this problem in the context of block ciphers. The formidable size of the fault space of a block cipher mandates the use of an automation strategy to solve this problem, which should be able to characterize each individual fault instance quickly. On the other hand, the automation strategy is expected to be applicable to most of the block cipher constructions. Existing techniques for automated fault attacks do not satisfy both of these goals simultaneously, and hence are not directly applicable in the context of exploitable fault characterization. In this paper, we present a supervised machine learning assisted automated framework, which successfully addresses both of the criteria mentioned. The key idea is to extrapolate the knowledge of some existing FAs on a cipher to rapidly figure out new attack instances. Experimental validation of this idea on two state-of-the-art block ciphers - PRESENT and LED - establishes that our approach is able to provide fairly good accuracy in identifying exploitable fault instances at a reasonable cost. Utilizing this observation, we propose a statistical framework for exploitable fault space characterization, which can provide an estimate of the success rate of an attacker corresponding to the given fault model and fault location. The framework also returns test vectors leading toward successful attacks. As a potential application, the effect of different S-Boxes on the fault space of a cipher is evaluated utilizing the framework.","1556-6021","","10.1109/TIFS.2018.2868245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452997","Security;block cipher;fault attack;machine learning","Ciphers;Tools;Mathematical model;Machine learning;Testing","","19","","45","IEEE","31 Aug 2018","","","IEEE","IEEE Journals"
"Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection","X. Chen; C. Li; D. Wang; S. Wen; J. Zhang; S. Nepal; Y. Xiang; K. Ren","Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Data61, CSIRO, Epping, NSW, Australia; Faculty of Science, Engineering and Technology, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Computer Science and Engineering, University at Buffalo, State University of New York, Buffalo, NY, USA","IEEE Transactions on Information Forensics and Security","8 Oct 2019","2020","15","","987","1001","Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc.), and the perturbations can only be implemented by simply modifying application’s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK’s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.","1556-6021","","10.1109/TIFS.2019.2932228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782574","Android malware detection;adversarial machine learning","Malware;Feature extraction;Detectors;Machine learning;Perturbation methods;Semantics;Tools","","174","","39","IEEE","31 Jul 2019","","","IEEE","IEEE Journals"
"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning","M. Jagielski; A. Oprea; B. Biggio; C. Liu; C. Nita-Rotaru; B. Li","Northeastern University, Boston, MA; Northeastern University, Boston, MA; Univ. Cagliary, Cagliary, Italy; UC Berkeley, Berkeley, CA; Northeastern University, Boston, MA; UC Berkeley, Berkeley, CA","2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","19","35","As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418594","adversarial machine learning;poisoning attacks;robust linear regression","Training;Linear regression;Machine learning;Training data;Robustness;Data models;Predictive models","","351","","55","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"Drynx: Decentralized, Secure, Verifiable System for Statistical Queries and Machine Learning on Distributed Datasets","D. Froelicher; J. R. Troncoso-Pastoriza; J. S. Sousa; J. -P. Hubaux","Laboratory for Data Security, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Laboratory for Data Security, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Laboratory for Data Security, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Laboratory for Data Security, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Information Forensics and Security","10 Apr 2020","2020","15","","3035","3050","Data sharing has become of primary importance in many domains such as big-data analytics, economics and medical research, but remains difficult to achieve when the data are sensitive. In fact, sharing personal information requires individuals’ unconditional consent or is often simply forbidden for privacy and security reasons. In this paper, we propose Drynx, a decentralized system for privacy-conscious statistical analysis on distributed datasets. Drynx relies on a set of computing nodes to enable the computation of statistics such as standard deviation or extrema, and the training and evaluation of machine-learning models on sensitive and distributed data. To ensure data confidentiality and the privacy of the data providers, Drynx combines interactive protocols, homomorphic encryption, zero-knowledge proofs of correctness, and differential privacy. It enables an efficient and decentralized verification of the input data and of all the system’s computations thus provides auditability in a strong adversarial model in which no entity has to be individually trusted. Drynx is highly modular, dynamic and parallelizable. Our evaluation shows that it enables the training of a logistic regression model on a dataset (12 features and 600,000 records) distributed among 12 data providers in less than 2 seconds. The computations are distributed among 6 computing nodes, and Drynx enables the verification of the query execution’s correctness in less than 22 seconds.","1556-6021","","10.1109/TIFS.2020.2976612","Strategic Focal Area Personalized Health and Related Technologies (PHRT) of the ETH Domain(grant numbers:2017-201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019831","Decentralized system;distributed datasets;privacy;statistics;machine learning;homomorphic encryption;zero-knowledge proofs;differential privacy","Computational modeling;Machine learning;Encryption;Privacy","","29","","91","IEEE","2 Mar 2020","","","IEEE","IEEE Journals"
"A Machine Learning Approach to Prevent Malicious Calls over Telephony Networks","H. Li; X. Xu; C. Liu; T. Ren; K. Wu; X. Cao; W. Zhang; Y. Yu; D. Song","Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of California, Berkeley; TouchPal Inc.; TouchPal Inc.; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of California, Berkeley","2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","53","69","Malicious calls, i.e., telephony spams and scams, have been a long-standing challenging issue that causes billions of dollars of annual financial loss worldwide. This work presents the first machine learning-based solution without relying on any particular assumptions on the underlying telephony network infrastructures. The main challenge of this decade-long problem is that it is unclear how to construct effective features without the access to the telephony networks' infrastructures. We solve this problem by combining several innovations. We first develop a TouchPal user interface on top of a mobile App to allow users tagging malicious calls. This allows us to maintain a large-scale call log database. We then conduct a measurement study over three months of call logs, including 9 billion records. We design 29 features based on the results, so that machine learning algorithms can be used to predict malicious calls. We extensively evaluate different state-of-the-art machine learning approaches using the proposed features, and the results show that the best approach can reduce up to 90% unblocked malicious calls while maintaining a precision over 99.99% on the benign call traffic. The results also show the models are efficient to implement without incurring a significant latency overhead. We also conduct ablation analysis, which reveals that using 10 out of the 29 features can reach a performance comparable to using all features.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418596","robocall prevention;machine learning;telephony network","Telephony;Machine learning;Forestry;User interfaces;Databases;Servers;Neural networks","","26","","41","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning","A. Salem; G. Cherubin; D. Evans; B. Köpf; A. Paverd; A. Suri; S. Tople; S. Zanella-Béguelin",Microsoft; Microsoft; University of Virginia; Microsoft; Microsoft; University of Virginia; Microsoft; Microsoft,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","327","345","Deploying machine learning models in production may allow adversaries to infer sensitive information about training data. There is a vast literature analyzing different types of inference risks, ranging from membership inference to reconstruction attacks. Inspired by the success of games (i.e. probabilistic experiments) to study security properties in cryptography, some authors describe privacy inference risks in machine learning using a similar game-based style. However, adversary capabilities and goals are often stated in subtly different ways from one presentation to the other, which makes it hard to relate and compose results. In this paper, we present a game-based framework to systematize the body of knowledge on privacy inference risks in machine learning. We use this framework to (1) provide a unifying structure for definitions of inference risks, (2) formally establish known relations among definitions, and (3) to uncover hitherto unknown relations that would have been difficult to spot otherwise.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179281","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179281","privacy;machine learning;differential privacy;membership inference;attribute inference;property inference","Privacy;Data privacy;Training data;Machine learning;Games;Production;Probabilistic logic","","","","76","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Machine Learning in Wavelet Domain for Electromagnetic Emission Based Malware Analysis","N. Chawla; H. Kumar; S. Mukhopadhyay","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","3 Jun 2021","2021","16","","3426","3441","This paper presents a signal processing and machine learning (ML) based methodology to leverage Electromagnetic (EM) emissions from an embedded device to remotely detect a malicious application running on the device and classify the application into a malware family. We develop Fast Fourier Transform (FFT) based feature extraction followed by Support Vector Machine (SVM) and Random Forest (RF) based ML models to detect a malware. We further propose methods to learn characteristic behavior of different malwares from EM traces to reveal similarities to known malware families and improve efficiency of malware analysis. We propose to use Discrete Wavelet Transform (DWT) based feature extraction from spectrograms of EM side-channel traces and perform ML on the extracted features to learn fine-grained patterns of malware families. The experimental demonstration on Open-Q 820 development platform demonstrate 0.99 F1 score in detecting malware and 0.88 F1 score in uniquely classifying malwares among 8 malware family evaluated using Support Vector Machines (SVM) and Random Forest (RF) Machine Learning(ML) models. We also demonstrate capability of proposed framework in identifying new unknown applications with 0.99 recall and unknown malware family with 0.87 recall.","1556-6021","","10.1109/TIFS.2021.3080510","Semiconductor Research Corporation through the Texas Analog Center of Excellence(grant numbers:2810.002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432941","Android;classification;electromagnetic side-channel;Internet of Things (IoT);machine learning;malware detection;malware family;wavelet transform;spectrogram","Malware;Feature extraction;Spectrogram;Monitoring;Discrete wavelet transforms;Support vector machines;Detectors","","11","","42","IEEE","17 May 2021","","","IEEE","IEEE Journals"
"Machine Learning Cryptanalysis of a Quantum Random Number Generator","N. D. Truong; J. Y. Haw; S. M. Assad; P. K. Lam; O. Kavehei","Nano-Neuro-Inspired Research Laboratory, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; Centre of Excellence for Quantum Computation and Communication Technology, Research School of Physics and Engineering, The Australian National University, Canberra, ACT, Australia; Centre of Excellence for Quantum Computation and Communication Technology, Research School of Physics and Engineering, The Australian National University, Canberra, ACT, Australia; Centre of Excellence for Quantum Computation and Communication Technology, Research School of Physics and Engineering, The Australian National University, Canberra, ACT, Australia; Nano-Neuro-Inspired Research Laboratory, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Information Forensics and Security","1 Aug 2018","2019","14","2","403","414","Random number generators (RNGs) that are crucial for cryptographic applications have been the subject of adversarial attacks. These attacks exploit environmental information to predict generated random numbers that are supposed to be truly random and unpredictable. Though quantum random number generators (QRNGs) are based on the intrinsic indeterministic nature of quantum properties, the presence of classical noise in the measurement process compromises the integrity of a QRNG. In this paper, we develop a predictive machine learning (ML) analysis to investigate the impact of deterministic classical noise in different stages of an optical continuous variable QRNG. Our ML model successfully detects inherent correlations when the deterministic noise sources are prominent. After appropriate filtering and randomness extraction processes are introduced, our QRNG system, in turn, demonstrates its robustness against ML. We further demonstrate the robustness of our ML approach by applying it to uniformly distributed random numbers from the QRNG and a congruential RNG. Hence, our result shows that ML has potentials in benchmarking the quality of RNG devices.","1556-6021","","10.1109/TIFS.2018.2850770","University of Sydney; Commonwealth Scientific and Industrial Research Organisation(grant numbers:PN50041400); Australian Research Council(grant numbers:DP140103448); Australian Research Council(grant numbers:CE110001027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396276","Quantum random number generator;machine learning;cryptoanalysis","Entropy;Machine learning;Generators;Convolution;Cryptography;Feature extraction;Training","","29","","31","IEEE","26 Jun 2018","","","IEEE","IEEE Journals"
"SAIL: Analyzing Structural Artifacts of Logic Locking Using Machine Learning","P. Chakraborty; J. Cruz; A. Alaql; S. Bhunia","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; King Abdulaziz City for Science and Technology (KACST), Communication and Information Technology Research Institute, Riyadh, Saudi Arabia; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Information Forensics and Security","13 Aug 2021","2021","16","","3828","3842","Obfuscation or Logic locking (LL) is a technique for protecting hardware intellectual property (IP) blocks against diverse threats, including IP theft, reverse engineering, and malicious modifications. State-of-the-art locking techniques primarily focus on securing a design from unauthorized usage by disabling correct functionality – they often do not directly address hiding design intent through structural transformations. They rely on the synthesis tool to introduce structural changes. We observe that this process is insufficient as the resulting changes in circuit topology are: (1) local and (2) predictable. In this paper, we analyze the structural transformations introduced by LL and introduce a potential attack, called SAIL, that can exploit structural artifacts introduced by LL. SAIL uses machine learning (ML) guided structural recovery that exposes a critical vulnerability in these techniques. Through this attack, we demonstrate that the gate-level structure of a locked design can be retrieved in most parts through a systematic set of steps. The proposed attack is applicable to most forms of logic locking, and significantly more powerful than existing attacks, e.g., SAT-based attacks, since it does not require the availability of golden functional responses (e.g., an unlocked IC). Evaluation on benchmark circuits shows that we can recover an average of about 92%, up to 97%, transformations (Top-10 R-Metric) introduced by logic locking. We show that this attack is scalable, flexible, and versatile. Additionally, to evaluate the SAIL attack resilience of a locked design, we present the SIVA-Metric that is fast in terms of computation speed and does not require any training. We also propose possible mitigation steps for incorporating SAIL resilience into a locked design.","1556-6021","","10.1109/TIFS.2021.3096028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478898","Hardware obfuscation;logic locking;hardware security;cybersecurity;machine learning","Logic gates;Resilience;Machine learning;Hardware;Interference;Benchmark testing;Training","","12","","44","IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"Poligraph: Intrusion-Tolerant and Distributed Fake News Detection System","G. Shan; B. Zhao; J. R. Clavin; H. Zhang; S. Duan","Department of Management Information System, Temple University, Philadelphia, PA, USA; Institute for Advanced Study, Tsinghua University, Beijing, China; University of Maryland, Baltimore County, Baltimore, MD, USA; Department of Information Systems, Shandong Institute of Blockchain, Jinan, China; Institute for Advanced Study and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","15 Dec 2021","2022","17","","28","41","We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally ( ${4\%}$ – ${7\%}$ ) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.","1556-6021","","10.1109/TIFS.2021.3131026","Shandong Key Research and Development Program(grant numbers:2020ZLYS09); National Key Research and Development Program of China(grant numbers:2018YFA0704701); Shandong Key Research and Development Program(grant numbers:2020ZLYS09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627681","Reliability;fault tolerance;machine learning (ML)","Feature extraction;Social networking (online);Throughput;Machine learning;Data models;Visualization;Buildings","","10","","86","IEEE","25 Nov 2021","","","IEEE","IEEE Journals"
"Comments on “Dropping Activation Outputs with Localized First-Layer Deep Network for Enhancing User Privacy and Data Security”","X. Tan; H. Li; L. Wang; Z. Xu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","17 Jul 2020","2020","15","","3938","3939","Inference based on deep learning models is usually implemented by exposing sensitive user data to the outside models, which of course gives rise to acute privacy concerns. To deal with these concerns, Dong et al. recently proposed an approach, namely the dropping-activation-outputs (DAO) first layer. This approach was claimed to be a non-invertible transformation, such that the privacy of user data could not be compromised. However, In this paper, we prove that the DAO first layer, in fact, can generally be inverted, and hence fails to preserve privacy. We also provide a countermeasure against the privacy vulnerabilities that we examined.","1556-6021","","10.1109/TIFS.2020.2988156","National Key Research and Development Program of China(grant numbers:2017YFB0801900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068271","Deep learning;data privacy","Data privacy;Artificial neural networks;Data models;Computational modeling;Data security;Machine learning;Privacy","","3","","5","IEEE","15 Apr 2020","","","IEEE","IEEE Journals"
"Zero-Shot Machine Unlearning","V. S. Chundawat; A. K. Tarun; M. Mandal; M. Kankanhalli","School of Computing, National University of Singapore, Queenstown, Singapore; School of Computing, National University of Singapore, Queenstown, Singapore; School of Computing, National University of Singapore, Queenstown, Singapore; School of Computing, National University of Singapore, Queenstown, Singapore","IEEE Transactions on Information Forensics and Security","25 Apr 2023","2023","18","","2345","2354","Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. These methods remove the information of the forget data from the model while maintaining the model efficacy on the retain data. The zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code is available here: https://github.com/ayu987/zero-shot-unlearning","1556-6021","","10.1109/TIFS.2023.3265506","National Research Foundation, Singapore, under its Strategic Capability Research Centers Funding Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097553","Machine unlearning;machine learning security and privacy;data privacy","Data models;Training;Data privacy;Training data;Computational modeling;Regulation;Machine learning","","5","","45","IEEE","7 Apr 2023","","","IEEE","IEEE Journals"
"Differentially Private ADMM Algorithms for Machine Learning","F. Shang; T. Xu; Y. Liu; H. Liu; L. Shen; M. Gong","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","4 Oct 2021","2021","16","","4733","4745","In this paper, we study efficient differentially private alternating direction methods of multipliers (ADMM) via gradient perturbation for many centralized machine learning problems. For smooth convex loss functions with (non)-smooth regularization, we propose the first differentially private ADMM (DP-ADMM) algorithm with the performance guarantee of  $(\epsilon,\delta)$ -differential privacy ( $(\epsilon,\delta)$ -DP). From the viewpoint of theoretical analysis, we use the Gaussian mechanism and the conversion relationship between Rényi Differential Privacy (RDP) and DP to perform a comprehensive privacy analysis for our algorithm. Then we establish a new criterion to prove the convergence of the proposed algorithms including DP-ADMM. We also give the utility analysis of our DP-ADMM. Moreover, we propose a new accelerated DP-ADMM (DP-AccADMM) algorithm with the Nesterov’s acceleration technique. Finally, we conduct numerical experiments on many real-world datasets to show the privacy-utility tradeoff of the two proposed algorithms, and all the comparative analysis shows that DP-AccADMM converges faster and has a better utility than DP-ADMM, when the privacy budget  $\epsilon $  is larger than a threshold.","1556-6021","","10.1109/TIFS.2021.3113768","National Natural Science Foundation of China(grant numbers:61876220,61876221,61976164,62036006); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT_15R53); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2020JM-194); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540875","Differentially private;alternating direction method of multipliers (ADMM);gradient perturbation;momentum acceleration;Gaussian mechanism","Privacy;Perturbation methods;Differential privacy;Convergence;Approximation algorithms;Convex functions;Machine learning algorithms","","8","","65","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Disguising Attacks with Explanation-Aware Backdoors","M. Noppel; L. Peter; C. Wressnegger","KASTEL Security Research Labs, Karlsruhe Institute of Technology, Germany; KASTEL Security Research Labs, Karlsruhe Institute of Technology, Germany; KASTEL Security Research Labs, Karlsruhe Institute of Technology, Germany","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","664","681","Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate how to fully disguise the adversarial operation of a machine learning model. Similar to neural backdoors, we change the model’s prediction upon trigger presence but simultaneously fool an explanation method that is applied post-hoc for analysis. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of these explanation-aware backdoors for gradient- and propagation-based explanation methods in the image domain, before we resume to conduct a red-herring attack against malware classification.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179308","Ministry of Education; Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179308","Explainable-Machine-Learning;Backdooring;Adversarial-Machine-Learning","Privacy;Analytical models;Computational modeling;Machine learning;Predictive models;Malware;Computer security","","1","","100","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning","M. Nasr; S. Songi; A. Thakurta; N. Papernot; N. Carlin","University of Massachusetts, Amherst; Google Brain; Google Brain; Google Brain; Google Brain","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","866","882","Differentially private (DP) machine learning allows us to train models on private data while limiting data leakage. DP formalizes this data leakage through a cryptographic game, where an adversary must predict if a model was trained on a dataset D, or a dataset D′ that differs in just one example. If observing the training algorithm does not meaningfully increase the adversary's odds of successfully guessing which dataset the model was trained on, then the algorithm is said to be differentially private. Hence, the purpose of privacy analysis is to upper bound the probability that any adversary could successfully guess which dataset the model was trained on.In our paper, we instantiate this hypothetical adversary in order to establish lower bounds on the probability that this distinguishing game can be won. We use this adversary to evaluate the importance of the adversary capabilities allowed in the privacy analysis of DP training algorithms.For DP-SGD, the most common method for training neural networks with differential privacy, our lower bounds are tight and match the theoretical upper bound. This implies that in order to prove better upper bounds, it will be necessary to make use of additional assumptions. Fortunately, we find that our attacks are significantly weaker when additional (realistic) restrictions are put in place on the adversary's capabilities. Thus, in the practical setting common to many real-world deployments, there is a gap between our lower bounds and the upper bounds provided by the analysis: differential privacy is conservative and adversaries may not be able to leak as much information as suggested by the theoretical bound.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519424","Differentially-private-(DP)-machine-learning;Differentially-private;machine-learning;deep-learning;DP-SGD;membership-inference","Training;Deep learning;Privacy;Differential privacy;Upper bound;Toxicology;Games","","24","","57","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Dynamic Differential Privacy for ADMM-Based Distributed Classification Learning","T. Zhang; Q. Zhu","Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","1","172","187","Privacy-preserving distributed machine learning becomes increasingly important due to the recent rapid growth of data. This paper focuses on a class of regularized empirical risk minimization machine learning problems, and develops two methods to provide differential privacy to distributed learning algorithms over a network. We first decentralize the learning algorithm using the alternating direction method of multipliers, and propose the methods of dual variable perturbation and primal variable perturbation to provide dynamic differential privacy. The two mechanisms lead to algorithms that can provide privacy guarantees under mild conditions of the convexity and differentiability of the loss function and the regularizer. We study the performance of the algorithms, and show that the dual variable perturbation outperforms its primal counterpart. To design an optimal privacy mechanism, we analyze the fundamental tradeoff between privacy and accuracy, and provide guidelines to choose privacy parameters. Numerical experiments using customer information database are performed to corroborate the results on privacy and utility tradeoffs and design.","1556-6021","","10.1109/TIFS.2016.2607691","National Science Foundation(grant numbers:CNS-1544782); NYU Research Challenge Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563366","Machine learning;ADMM;distributed computing;privacy;differential privacy;dynamic programming","Privacy;Heuristic algorithms;Data privacy;Algorithm design and analysis;Machine learning algorithms;Risk management;Databases","","128","","24","IEEE","8 Sep 2016","","","IEEE","IEEE Journals"
"Doing good by fighting fraud: Ethical anti-fraud systems for mobile payments","Z. A. Din; H. Venugopalan; H. Lin; A. Wushensky; S. Liu; S. T. King","University of California, Davis; University of California, Davis; Bouncer Technologies; Bouncer Technologies; Bouncer Technologies; University of California, Davis","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1623","1640","App builders commonly use security challenges, a form of step-up authentication, to add security to their apps. However, the ethical implications of this type of architecture has not been studied previously.In this paper, we present a large-scale measurement study of running an existing anti-fraud security challenge, Boxer, in real apps running on mobile devices. We find that although Boxer does work well overall, it is unable to scan effectively on devices that run its machine learning models at less than one frame per second (FPS), blocking users who use inexpensive devices.With the insights from our study, we design Daredevil, a new anti-fraud system for scanning payment cards that works well across the broad range of performance characteristics and hardware configurations found on modern mobile devices. Daredevil reduces the number of devices that run at less than one FPS by an order of magnitude compared to Boxer, providing a more equitable system for fighting fraud.In total, we collect data from 5,085,444 real devices spread across 496 real apps running production software and interacting with real users.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519475","Deep-Learning;Credit-card-fraud;Mobile-apps;Ethical-machine-learning;Machine-learning-bias","Deep learning;Machine learning algorithms;Software algorithms;Production;Reliability engineering;Mobile handsets;User experience","","2","","55","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"New Primitives for Actively-Secure MPC over Rings with Applications to Private Machine Learning","I. Damgård; D. Escudero; T. Frederiksen; M. Keller; P. Scholl; N. Volgushev","Aarhus University; Aarhus University; Alexandra Institute; Data61, CSIRO; Aarhus University; Alexandra Institute","2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","1102","1120","At CRYPTO 2018 Cramer et al. presented SPDZ2k , a new secret-sharing based protocol for actively secure multi-party computation against a dishonest majority, that works over rings instead of fields. Their protocol uses slightly more communication than competitive schemes working over fields. However, implementation-wise, their approach allows for arithmetic to be carried out using native 32 or 64-bit CPU operations rather than modulo a large prime. The authors thus conjectured that the increased communication would be more than made up for by the increased efficiency of implementations. In this work we answer their conjecture in the affirmative. We do so by implementing their scheme, and designing and implementing new efficient protocols for equality test, comparison, and truncation over rings. We further show that these operations find application in the machine learning domain, and indeed significantly outperform their field-based competitors. In particular, we implement and benchmark oblivious algorithms for decision tree and support vector machine (SVM) evaluation.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835310","MPC;decision-trees;SVM","Protocols;Support vector machines;Machine learning;Decision trees;Cryptography;Logic gates","","51","","48","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Towards Making Systems Forget with Machine Unlearning","Y. Cao; J. Yang",Columbia University; Columbia University,"2015 IEEE Symposium on Security and Privacy","20 Jul 2015","2015","","","463","480","Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly. This paper focuses on making learning systems forget, the process of which we call machine unlearning, or simply unlearning. We present a general, efficient unlearning approach by transforming learning algorithms used by a system into a summation form. To forget a training data sample, our approach simply updates a small number of summations -- asymptotically faster than retraining from scratch. Our approach is general, because the summation form is from the statistical query learning in which many machine learning algorithms can be implemented. Our approach also applies to all stages of machine learning, including feature selection and modeling. Our evaluation, on four diverse learning systems and real-world workloads, shows that our approach is general, effective, fast, and easy to use.","2375-1207","978-1-4673-6949-7","10.1109/SP.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163042","Machine Unlearning;Forgetting System;Adversarial Machine Learning","Training data;Data models;Machine learning algorithms;Data privacy;Learning systems;Computational modeling;Feature extraction","","131","","80","IEEE","20 Jul 2015","","","IEEE","IEEE Conferences"
"SNAP: Efficient Extraction of Private Properties with Poisoning","H. Chaudhari; J. Abascal; A. Oprea; M. Jagielski; F. Tramèr; J. Ullman",Northeastern University; Northeastern University; Northeastern University; Google Research; ETH Zurich; Northeastern University,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","400","417","Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed [1] –[3], but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. [3]. For example, on the Census dataset, SNAP achieves 34% higher success rate than [3] while being 56.5× faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP’s generality and effectiveness.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179334","privacy-attacks;data-poisoning;property-inference;data-poisoning-in-machine-learning","Training;Privacy;Data privacy;Analytical models;Toxicology;Computational modeling;Machine learning","","4","","46","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"DP-ADMM: ADMM-Based Distributed Learning With Differential Privacy","Z. Huang; R. Hu; Y. Guo; E. Chan-Tin; Y. Gong","School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, TX, USA; DDepartment of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, Loyola University Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Information Forensics and Security","9 Oct 2019","2020","15","","1002","1012","Alternating direction method of multipliers (ADMM) is a widely used tool for machine learning in distributed settings where a machine learning model is trained over distributed data sources through an interactive process of local computation and message passing. Such an iterative process could cause privacy concerns of data owners. The goal of this paper is to provide differential privacy for ADMM-based distributed machine learning. Prior approaches on differentially private ADMM exhibit low utility under high privacy guarantee and assume the objective functions of the learning problems to be smooth and strongly convex. To address these concerns, we propose a novel differentially private ADMM-based distributed learning algorithm called DP-ADMM, which combines an approximate augmented Lagrangian function with time-varying Gaussian noise addition in the iterative process to achieve higher utility for general objective functions under the same differential privacy guarantee. We also apply the moments accountant method to analyze the end-to-end privacy loss. The theoretical analysis shows that the DP-ADMM can be applied to a wider class of distributed learning problems, is provably convergent, and offers an explicit utility-privacy tradeoff. To our knowledge, this is the first paper to provide explicit convergence and utility properties for differentially private ADMM-based distributed learning algorithms. The evaluation results demonstrate that our approach can achieve good convergence and model accuracy under high end-to-end differential privacy guarantee.","1556-6021","","10.1109/TIFS.2019.2931068","National Science Foundation(grant numbers:CNS-1850523); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772211","Machine learning;ADMM;distributed algorithms;privacy;differential privacy;and moments accountant","Privacy;Differential privacy;Machine learning;Convergence;Convex functions;Standards;Approximation algorithms","","91","","41","IEEE","25 Jul 2019","","","IEEE","IEEE Journals"
"Reliable and Modeling Attack Resistant Authentication of Arbiter PUF in FPGA Implementation With Trinary Quadruple Response","S. S. Zalivaka; A. A. Ivaniuk; C. -H. Chang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Faculty of Computer Systems and Networks, Belarusian State University of Informatics and Radioelectronics, Minsk, Belarus; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","7 Nov 2018","2019","14","4","1109","1123","Field programmable gate array (FPGA) is a potential hotbed for malicious and counterfeit hardware infiltration. Arbiter-based physical unclonable function (A-PUF) has been widely regarded as a suitable lightweight security primitive for FPGA bitstream encryption and device authentication. Unfortunately, the metastability of flip-flop gives rise to poor A-PUF reliability in FPGA implementation. Its linear additive path delays are also vulnerable to modeling attacks. Most reliability enhancement techniques tend to increase the response predictability and ease machine learning attacks. This paper presents a robust device authentication method based on the FPGA implementation of a reliability enhanced A-PUF with trinary digit (trit) quadruple responses. A two flip-flop arbiter is used to produce a trit for metastability detection. By considering the ordered responses to all four combinations of first and last challenge bits, each quadruple response can be compressed into a quadbit that represents one of the five classes of trit quadruple response with greater reproducibility. This challenge-response quadruple classification not only greatly reduces the burden of error correction at the device but also enables a precise A-PUF model to be built at the server without having to store the complete challenge-response pair (CRP) set for authentication. Besides, the real challenge to the A-PUF is generated internally by a lossy, nonlinear, and irreversible maximum length signature generator at both the server and device sides to prevent the naked CRP from being machine learned by the attacker. The A-PUF with short repetition code of length five has been tested to achieve a reliability of 1.0 over the full operating temperature range of the target FPGA board with lower hardware resource utilization than other modeling attack resilient strong PUFs. The proposed authentication protocol has also been experimentally evaluated to be practically secure against various machine learning attacks including evolutionary strategy covariance matrix adaptation.","1556-6021","","10.1109/TIFS.2018.2870835","Singapore Ministry of Education Academic Research Fund (AcRF) Tier II(grant numbers:MOE 2015-T2-2-013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466897","Arbiter PUF;reliability enhancement;machine learning attack resistance;authentication protocol","Field programmable gate arrays;Reliability;Authentication;Hardware;Servers;Machine learning;Resistance","","74","","60","IEEE","16 Sep 2018","","","IEEE","IEEE Journals"
"RTrap: Trapping and Containing Ransomware With Machine Learning","G. O. Ganfure; C. -F. Wu; Y. -H. Chang; W. -K. Shih","Department of Computer Science, Dire Dawa Institute of Technology, Dire Dawa, Ethiopia; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu City, Taiwan; Institute of Information Science, Academia Sinica, Taipei City, Taiwan; Department of Computer Science, Dire Dawa Institute of Technology, Dire Dawa, Ethiopia","IEEE Transactions on Information Forensics and Security","7 Feb 2023","2023","18","","1433","1448","With advances in social engineering tricks and other technical shortcomings, ransomware attacks have become a severe cybercrime affecting organizations of all shapes and sizes. Although the security teams are making plenty of ransomware detection tools, the ransomware incident report shows they are ineffective in detecting emerging ransomware attacks. This work presents “RTrap,” a systematic framework to detect and contain ransomware efficiently and effectively via machine learning-generated deceptive files. Using a data-driven decoy file selection and generation strategy, RTrap plants deceptive decoy files across the directory to lure the ransomware to access it. RTrap also introduced a lightweight decoy watcher to monitor generated decoy files in real time. As the timing of the ransomware attack is not known to the victim in advance, and the ransomware encryption process is speedy, the proposed decoy-watcher executes an automatic/automated response after the detection promptly. The experiment shows that RTrap can detect ransomware with an average 18 file loss per 10311 legitimate user files.","1556-6021","","10.1109/TIFS.2023.3240025","National Science and Technology Council(grant numbers:111-2223-E-001-001,111-2923-E-002-014-MY3,111-2221-E-001-013-MY3,112-2927-I-001-508,112-2222-EA49-002-MY2); Academia Sinica(grant numbers:AS-IA-111-M01,AS-GCS-110-08); Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026355","Deception-based detection;ransomware detection;affinity propagation;machine learning;adaptive decoy files","Ransomware;Feature extraction;Cryptography;Codes;Organizations;Encryption;Behavioral sciences","","8","","52","IEEE","26 Jan 2023","","","IEEE","IEEE Journals"
"PUF-Phenotype: A Robust and Noise-Resilient Approach to Aid Group-Based Authentication With DRAM-PUFs Using Machine Learning","O. Millwood; J. Miskelly; B. Yang; P. Gope; E. B. Kavun; C. Lin","Department of Computer Science, The University of Sheffield, Regent Court Campus, Sheffield, U.K; Centre for Secure Information Technologies, Queen’s University Belfast, Belfast, U.K; Department of Computer Science, The University of Sheffield, Regent Court Campus, Sheffield, U.K; Department of Computer Science, The University of Sheffield, Regent Court Campus, Sheffield, U.K; Secure Intelligent Systems Research Group, FIM, University of Passau, Passau, Germany; Department of Computer Science, The University of Sheffield, Regent Court Campus, Sheffield, U.K","IEEE Transactions on Information Forensics and Security","27 Apr 2023","2023","18","","2451","2465","As the demand for highly secure and dependable lightweight systems increases in the modern world, Physically Unclonable Functions (PUFs) continue to promise a lightweight alternative to high-cost encryption techniques and secure key storage. While the security features promised by PUFs are highly attractive for secure system designers, they have been shown to be vulnerable to various sophisticated attacks - most notably Machine Learning (ML) based modelling attacks (ML-MA) which attempt to digitally clone the PUF behaviour and thus undermine their security. More recent ML-MA have even exploited publicly known helper data required for PUF error correction in order to predict PUF responses without requiring knowledge of response data. In response to this, research is beginning to emerge regarding the authentication of PUF devices with the assistance of ML as opposed to traditional PUF techniques of storage and comparison of pre-known Challenge-Response pairs (CRPs). In this article, we propose a classification system using ML based on a novel ‘PUF-Phenotype’ concept to accurately identify the origin and determine the validity of noisy memory-derived (DRAM) PUF responses as an alternative to helper data-reliant denoising techniques. To our best knowledge, we are the first to perform classification over multiple devices per model to enable a group-based PUF authentication scheme. We achieve up to 98% classification accuracy using a modified deep convolutional neural network (CNN) for feature extraction in conjunction with several well-established classifiers. We also experimentally verified the performance of our model on a Raspberry Pi device to determine the suitability of deploying our proposed model in a resource-constrained environment.","1556-6021","","10.1109/TIFS.2023.3266624","Engineering and Physical Sciences Research Council (EPSRC) Secure IoT Processor Platform with Remote Attestation (SIPP) Project(grant numbers:EP/S030867/1); Royal Society Research(grant numbers:RGS\R1\221183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099450","Physically unclonable functions (PUF);PUF-phenotype;DRAM-PUF;machine learning;error correction","Authentication;Security;Noise measurement;Feature extraction;Random access memory;Noise reduction;Data models","","4","","36","IEEE","12 Apr 2023","","","IEEE","IEEE Journals"
"SecureML: A System for Scalable Privacy-Preserving Machine Learning","P. Mohassel; Y. Zhang",Visa Research; University of Maryland,"2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","19","38","Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958569","Privacy-preserving machine learning;secure computation","Training;Logistics;Protocols;Data models;Privacy;Linear regression;Neural networks","","850","11","48","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU","S. Tan; B. Knott; Y. Tian; D. J. Wu",University of Virginia; Facebook AI Research; University of Virginia; University of Virginia,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1021","1038","We introduce CryptGPU, a system for privacy-preserving machine learning that implements all operations on the GPU (graphics processing unit). Just as GPUs played a pivotal role in the success of modern deep learning, they are also essential for realizing scalable privacy-preserving deep learning. In this work, we start by introducing a new interface to losslessly embed cryptographic operations over secret-shared values (in a discrete domain) into floating-point operations that can be processed by highly-optimized CUDA kernels for linear algebra. We then identify a sequence of ""GPU-friendly"" cryptographic protocols to enable privacy-preserving evaluation of both linear and non-linear operations on the GPU. Our microbenchmarks indicate that our private GPU-based convolution protocol is over 150× faster than the analogous CPU-based protocol; for non-linear operations like the ReLU activation function, our GPU-based protocol is around 10× faster than its CPU analog. With CryptGPU, we support private inference and training on convolutional neural networks with over 60 million parameters as well as handle large datasets like ImageNet. Compared to the previous state-of-the-art, our protocols achieve a 2× to 8× improvement in private inference for large networks and datasets. For private training, we achieve a 6× to 36× improvement over prior state-of-the-art. Our work not only showcases the viability of performing secure multiparty computation (MPC) entirely on the GPU to newly enable fast privacy-preserving machine learning, but also highlights the importance of designing new MPC primitives that can take full advantage of the GPU’s computing capabilities.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519386","privacy;machine-learning;GPU;multi-party-computation","Deep learning;Training;Privacy;Systematics;Graphics processing units;Linear algebra;Cryptography","","48","","62","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning","L. Zhou; Z. Wang; H. Cui; Q. Song; Y. Yu","Huawei Technology, Shanghai, China; Huawei Technology, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Technology, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","534","551","The overhead of non-linear functions dominates the performance of the secure multiparty computation (MPC) based privacy-preserving machine learning (PPML). This work introduces a family of novel secure three-party computation (3PC) protocols, Bicoptor, which improve the efficiency of evaluating non-linear functions. The basis of Bicoptor is a new sign determination protocol, which relies on a clever use of the truncation protocol proposed in SecureML (S&P 2017). Our 3PC sign determination protocol only requires two communication rounds, and does not involve any preprocessing. Such sign determination protocol is well-suited for computing non-linear functions in PPML, e.g. the activation function ReLU, Maxpool, and their variants. We develop suitable protocols for these non-linear functions, which form a family of GPU-friendly protocols, Bicoptor. All Bicoptor protocols only require two communication rounds without preprocessing. We evaluate Bicoptor under a 3-party LAN network over a public cloud, and achieve more than 370,000 DReLU/ReLU or 41,000 Maxpool (find the maximum value of nine inputs) operations per second. Under the same settings and environment, our ReLU protocol has a one or even two orders of magnitude improvement to the state-of-the-art works, Falcon (PETS 2021) or Edabits (CRYPTO 2020), respectively without batch processing.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179449","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179449","","Privacy;Cloud computing;Protocols;Batch production systems;Machine learning;Throughput;Computational efficiency","","","","28","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Outlier Dirichlet Mixture Mechanism: Adversarial Statistical Learning for Anomaly Detection in the Fog","N. Moustafa; K. -K. R. Choo; I. Radwan; S. Camtepe","School of Engineering and Information Technology, University of New South Wales at ADFA, Canberra, ACT, Australia; School of Information Technology & Mathematical Sciences, University of South Australia, Adelaide, SA, Australia; College of Business and Economics, The Australian National University, Canberra, ACT, Australia; CSIRO Data61, Marsfield, NSW, Australia","IEEE Transactions on Information Forensics and Security","8 May 2019","2019","14","8","1975","1987","Current anomaly detection systems (ADSs) apply statistical and machine learning algorithms to discover zero-day attacks, but such algorithms are vulnerable to advanced persistent threat actors. In this paper, we propose an adversarial statistical learning mechanism for anomaly detection, outlier Dirichlet mixture-based ADS (ODM-ADS), which has three new capabilities. First, it can self-adapt against data poisoning attacks that inject malicious instances in the training phase for disrupting the learning process. Second, it establishes a statistical legitimate profile and considers variations from the baseline of the profile as anomalies using a proposed outlier function. Third, to deal with dynamic and large-scale networks such as Internet of Things and cloud and fog computing, we suggest a framework for deploying the mechanism as Software as a Service in the fog nodes. The fog enables the proposed mechanism to concurrently process streaming data at the edge of the network. The ODM-ADS mechanism is evaluated using both NSL-KDD and UNSW-NB15 datasets, whose findings indicate that ODM-ADS outperforms seven other peer algorithms in terms of accuracy, detection rates, false positive rates, and computational time.","1556-6021","","10.1109/TIFS.2018.2890808","Cloud Technology Endowed Professorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600389","Adversarial statistical/machine learning;outlier detection;Dirichlet mixture model;anomaly detection;fog computing","Cloud computing;Training;Security;Anomaly detection;Machine learning algorithms;Edge computing;Software as a service","","80","","65","IEEE","3 Jan 2019","","","IEEE","IEEE Journals"
"Quantifying Membership Privacy via Information Leakage","S. Saeidian; G. Cervia; T. J. Oechtering; M. Skoglund","Division of Information Science and Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Information Science and Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Information Science and Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Information Forensics and Security","7 May 2021","2021","16","","3096","3108","Machine learning models are known to memorize the unique properties of individual data points in a training set. This memorization capability can be exploited by several types of attacks to infer information about the training data, most notably, membership inference attacks. In this paper, we propose an approach based on information leakage for guaranteeing membership privacy. Specifically, we propose to use a conditional form of the notion of maximal leakage to quantify the information leaking about individual data entries in a dataset, i.e., the entrywise information leakage. We apply our privacy analysis to the Private Aggregation of Teacher Ensembles (PATE) framework for privacy-preserving classification of sensitive data and prove that the entrywise information leakage of its aggregation mechanism is Schur-concave when the injected noise has a log-concave probability density. The Schur-concavity of this leakage implies that increased consensus among teachers in labeling a query reduces its associated privacy cost. Finally, we derive upper bounds on the entrywise information leakage when the aggregation mechanism uses Laplace distributed noise.","1556-6021","","10.1109/TIFS.2021.3073804","Strategic Research Agenda Program, Information and Communication Technology–The Next Generation (SRA ICT–TNG); Swedish Government and KTH Digital Futures Center within the project DataLEASH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406982","Privacy-preserving machine learning;membership inference;maximal leakage;log-concave probability density","Privacy;Differential privacy;Measurement;Training;Machine learning;Data models;Upper bound","","15","","34","CCBY","19 Apr 2021","","","IEEE","IEEE Journals"
"Analyzing Android Encrypted Network Traffic to Identify User Actions","M. Conti; L. V. Mancini; R. Spolaor; N. V. Verde","Dipartimento di Matematica, Università di Padova, Padua, Italy; Dipartimento di Informatica, Sapienza Università di Roma, Rome, Italy; Dipartimento di Matematica, Università di Padova, Padua, Italy; Dipartimento di Informatica, Sapienza Università di Roma, Rome, Italy","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","1","114","125","Mobile devices can be maliciously exploited to violate the privacy of people. In most attack scenarios, the adversary takes the local or remote control of the mobile device, by leveraging a vulnerability of the system, hence sending back the collected information to some remote web service. In this paper, we consider a different adversary, who does not interact actively with the mobile device, but he is able to eavesdrop the network traffic of the device from the network side (e.g., controlling a Wi-Fi access point). The fact that the network traffic is often encrypted makes the attack even more challenging. In this paper, we investigate to what extent such an external attacker can identify the specific actions that a user is performing on her mobile apps. We design a system that achieves this goal using advanced machine learning techniques. We built a complete implementation of this system, and we also run a thorough set of experiments, which show that our attack can achieve accuracy and precision higher than 95%, for most of the considered actions. We compared our solution with the three state-of-the-art algorithms, and confirming that our system outperforms all these direct competitors.","1556-6021","","10.1109/TIFS.2015.2478741","TENACE PRIN Project through the Italian Ministry of Education, University and Research(grant numbers:20103P34XC); European Commission Directorate General Home Affairs through the GAINS Project(grant numbers:HOME/2013/CIPS/AG/4000005057); European Commission through the H2020 SUNFISH Project(grant numbers:644666); EU-India REACH Project(grant numbers:ICI+/2014/342-896); Project entitled Tackling Mobile Malware with Innovative Machine Learning Techniques through the University of Padua; Marie Curie Fellowship through the European Commission(grant numbers:PCIG11-GA-2012-321980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7265055","Cellular phones;information security;privacy","Time series analysis;Cryptography;Privacy;IP networks;Mobile handsets;Machine learning algorithms;Mobile communication","","191","","43","IEEE","14 Sep 2015","","","IEEE","IEEE Journals"
"A Graph-Based Stratified Sampling Methodology for the Analysis of (Underground) Forums","G. Di Tizio; G. A. Siu; A. Hutchings; F. Massacci","University of Trento, Trento, Italy; University of Cambridge, Cambridge, U.K; University of Cambridge, Cambridge, U.K; Department of Computer Science and Technology, University of Trento, Trento, Italy","IEEE Transactions on Information Forensics and Security","8 Sep 2023","2023","18","","5473","5483","Researchers analyze underground forums to study abuse and cybercrime activities. Due to the size of the forums and the domain expertise required to identify criminal discussions, most approaches employ supervised machine learning techniques to automatically classify the posts of interest. Human annotation is costly. How to select samples to annotate that account for the structure of the forum? We present a methodology to generate stratified samples based on information about the centrality properties of the population and evaluate classifier performance. We observe that by employing a sample obtained from a uniform distribution of the post degree centrality metric, we maintain the same level of precision but significantly increase the recall (+30%) compared to a sample whose distribution is respecting the population stratification. We find that classifiers trained with similar samples disagree on the classification of criminal activities up to 33% of the time when deployed on the entire forum.","1556-6021","","10.1109/TIFS.2023.3304424","European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation Program(grant numbers:830929 (CyberSec4Europe),952647 (AssureMOSS)); iCrime(grant numbers:949127); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214599","Cybercrime;machine learning;underground forum","Social networking (online);Measurement;Training;Statistics;Sociology;Instruction sets;Computer crime","","","","54","IEEE","11 Aug 2023","","","IEEE","IEEE Journals"
"Characterizing and Evaluating Adversarial Examples for Offline Handwritten Signature Verification","L. G. Hafemann; R. Sabourin; L. S. Oliveira","Laboratoire d’imagerie, de vision et d’intelligence artificielle, École de Technologie Supérieure, Université du Québec, Montreal, QC, Canada; Laboratoire d’imagerie, de vision et d’intelligence artificielle, École de Technologie Supérieure, Université du Québec, Montreal, QC, Canada; Department of Informatics, Federal University of Paraná, Curitiba, Brazil","IEEE Transactions on Information Forensics and Security","16 May 2019","2019","14","8","2153","2166","The phenomenon of adversarial examples is attracting increasing interest from the machine learning community, due to its significant impact on the security of machine learning systems. Adversarial examples are similar (from a perceptual notion of similarity) to samples from the data distribution, that “fool” a machine learning classifier. For computer vision applications, these are images with carefully crafted but almost imperceptible changes, which are misclassified. In this paper, we characterize this phenomenon under an existing taxonomy of threats to biometric systems, in particular identifying new attacks for offline handwritten signature verification systems. We conducted an extensive set of experiments on four widely used datasets: MCYT-75, CEDAR, GPDS-160, and the Brazilian PUC-PR, considering both a CNN-based system and a system using a handcrafted feature extractor. We found that attacks that aim to get a genuine signature rejected are easy to generate, even in a limited knowledge scenario, where the attacker does not have access to the trained classifier nor the signatures used for training. Attacks that get a forgery to be accepted are harder to produce, and often require a higher level of noise-in most cases, no longer “imperceptible” as previous findings in object recognition. We also evaluated the impact of two countermeasures on the success rate of the attacks and the amount of noise required for generating successful attacks.","1556-6021","","10.1109/TIFS.2019.2894031","Fonds de Recherche du Québec - Nature et Technologies; Conselho Nacional de Desenvolvimento Científico e Tecnológico(grant numbers:206318/2014-6); National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621025","Adversarial machine learning;signature verification;biometrics","Feature extraction;Training;Security;Machine learning;Data models;Forgery;Training data","","31","","37","IEEE","22 Jan 2019","","","IEEE","IEEE Journals"
"On the (In)security of Peer-to-Peer Decentralized Machine Learning","D. Pasquini; M. Raynal; C. Troncoso","SPRING Lab, EPFL, Switzerland; SPRING Lab, EPFL, Switzerland; SPRING Lab, EPFL, Switzerland","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","418","436","In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning—a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users’ local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179291","Fondation Botnar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179291","Collaborative Machine Learning;Privacy attacks;Peer-to-Peer systems","Privacy;Protocols;Federated learning;Collaboration;Performance gain;Benchmark testing;Peer-to-peer computing","","","","79","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"DPlanner: A Privacy Budgeting System for Utility","W. Li; L. Xiang; B. Guo; Z. Li; X. Wang","John Hopcroft Center, Shanghai Jiao Tong University, Shanghai, China; John Hopcroft Center, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; College of Information Engineering, Xiangtan University, Xiangtan, China; Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Information Forensics and Security","23 Jan 2023","2023","18","","1196","1210","Differential mymargin privacy has been deployed to machine learning platforms to preserve the privacy of data in use. A long neglected but important fact is that data privacy is a non-replenishable resource and should be carefully scheduled to maximize its utility gain. In this work, we propose a new privacy budgeting system—DPlanner, which estimates data blocks’ importance to queries and assigns fractional privacy budget to those data blocks contributing most to a query. The scheduler is novelly designed to include two-fold randomness, which satisfies differential privacy with tight budgets, at the same time guarantees the expected utility in the worst-case query sequence when queries arrive in an online fashion. Experiments in a variety of machine learning settings have shown that our DPlanner outperforms the state-of-the-art schedulers by serving at least 25% more queries, or reducing the total privacy consumption by over 50%.","1556-6021","","10.1109/TIFS.2022.3231786","National Key Research and Development Program of China(grant numbers:2021ZD0112801); NSF China(grant numbers:62272306,62032020,62136006,42050105,62020106005,62061146002,61960206002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997560","Differential privacy;scheduling","Data privacy;Privacy;Machine learning;Differential privacy;Schedules;Data models;Training data","","","","44","IEEE","22 Dec 2022","","","IEEE","IEEE Journals"
"A Self-Adaptive Bell–LaPadula Model Based on Model Training With Historical Access Logs","Z. Tang; X. Ding; Y. Zhong; L. Yang; K. Li","National Supercomputing Center in Changsha, Hunan University, Changsha, China; National Supercomputing Center in Changsha, Hunan University, Changsha, China; National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Computer and Communication Engineering, Changsha University of Science and Technology, Hunan, China; National Supercomputing Center in Changsha, Hunan University, Changsha, China","IEEE Transactions on Information Forensics and Security","16 Apr 2018","2018","13","8","2047","2061","In currently popular access control models, the security policies and regulations never change in the running system process once they are identified, which makes it possible for attackers to find the vulnerabilities in a system, resulting in the lack of ability to perceive the system security status and risks in a dynamic manner and exposing the system to such risks. By introducing the maximum entropy (MaxENT) models into the rule optimization for the Bell-LaPadula (BLP) model, this paper proposes an improved BLP model with the self-learning function: MaxENT-BLP. This model first formalizes the security properties, system states, transformational rules, and a constraint model based on the states transition of the MaxENT. After handling the historical system access logs as the original data sets, this model extracts the user requests, current states, and decisions to act as the feature vectors. Second, we use k -fold cross validation to divide all vectors into a training set and a testing set. In this paper, the model training process is based on the Broyden-Fletcher-Goldfarb-Shanno algorithm. And this model contains a strategy update algorithm to adjust the access control rules dynamically according to the access and decision records in a system. Third, we prove that MaxENT-BLP is secure through theoretical analysis. By estimating the precision, recall, and F1-score, the experiments show the availability and accuracy of this model. Finally, this paper provides the process of model training based on deep learning and discussions regarding adversarial samples from the malware classifiers. We demonstrate that MaxENT-BLP is an appropriate choice and has the ability to help running information systems to avoid more risks and losses.","1556-6021","","10.1109/TIFS.2018.2807793","National Natural Science Foundation of China(grant numbers:61572176,L1624040); National Key Research and Development Program of China(grant numbers:2017YFB0202201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295130","Adversarial sample;BLP;machine learning;mandatory access control;maximum entropy model;rule optimization","Hidden Markov models;Training;Access control;Data models;Feature extraction;Machine learning","","18","","37","IEEE","19 Feb 2018","","","IEEE","IEEE Journals"
"Everybody’s Got ML, Tell Me What Else You Have: Practitioners’ Perception of ML-Based Security Tools and Explanations","J. Mink; H. Benkraouda; L. Yang; A. Ciptadi; A. Ahmadzadeh; D. Votipka; G. Wang",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Truera; Blue Hexagon; Tufts University; University of Illinois at Urbana-Champaign,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","2068","2085","Significant efforts have been investigated to develop machine learning (ML) based tools to support security operations. However, they still face key challenges in practice. A generally perceived weakness of machine learning is the lack of explanation, which motivates researchers to develop machine learning explanation techniques. However, it is not yet well understood how security practitioners perceive the benefits and pain points of machine learning and corresponding explanation methods in the context of security operations. To fill this gap and understand ""what is needed"", we conducted semi-structured interviews with 18 security practitioners with diverse roles, duties, and expertise. We find practitioners generally believe that ML tools should be used in conjunction with (instead of replacing) traditional rule-based methods. While ML’s output is perceived as difficult to reason, surprisingly, rule-based methods are not strictly easier to interpret. We also find that only few practitioners considered security (robustness to adversarial attacks) as a key factor for the choice of tools. Regarding ML explanations, while recognizing their values in model verification and understanding security events, practitioners also identify gaps between existing explanation methods and the needs of their downstream tasks. We collect and synthesize the suggestions from practitioners regarding explanation scheme designs, and discuss how future work can help to address these needs.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179321","","Privacy;Pain;Face recognition;Machine learning;Robustness;Security;Task analysis","","2","","97","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Fingerprinting the Fingerprinters: Learning to Detect Browser Fingerprinting Behaviors","U. Iqbal; S. Englehardt; Z. Shafiq","The University of Iowa; Mozilla Corporation; University of California, Davis","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1143","1161","Browser fingerprinting is an invasive and opaque stateless tracking technique. Browser vendors, academics, and standards bodies have long struggled to provide meaningful protections against browser fingerprinting that are both accurate and do not degrade user experience. We propose FP-Inspector, a machine learning based syntactic-semantic approach to accurately detect browser fingerprinting. We show that FP-Inspector performs well, allowing us to detect 26% more fingerprinting scripts than the state-of-the-art. We show that an API-level fingerprinting countermeasure, built upon FP-Inspector, helps reduce website breakage by a factor of 2. We use FP-Inspector to perform a measurement study of browser fingerprinting on top-100K websites. We find that browser fingerprinting is now present on more than 10% of the top-100K websites and over a quarter of the top-10K websites. We also discover previously unreported uses of JavaScript APIs by fingerprinting scripts suggesting that they are looking to exploit APIs in new and unexpected ways.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00017","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519502","fingerprinting;tracking;machine-learning","Privacy;Virtual assistants;Standards organizations;Computer bugs;Prototypes;Machine learning;Fingerprint recognition","","29","","96","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"RAB: Provable Robustness Against Backdoor Attacks","M. Weber; X. Xu; B. Karlaš; C. Zhang; B. Li","ETH Zurich, Switzerland; University of Illinois at Urbana-Champaign, USA; ETH Zurich, Switzerland; ETH Zurich, Switzerland; University of Illinois at Urbana-Champaign, USA","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1311","1328","Recent studies have shown that deep neural net-works (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We theoretically prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179451","Alfred P. Sloan Foundation; European Research Council; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179451","Machine-learning-robustness;Backdoor-attacks;Certified-robustness","Training;Threat modeling;Support vector machines;Privacy;Machine learning algorithms;Smoothing methods;Machine learning","","16","","62","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Statistical Features-Based Real-Time Detection of Drifted Twitter Spam","C. Chen; Y. Wang; J. Zhang; Y. Xiang; W. Zhou; G. Min","University of Electronic Science and Technology of China, Chengdu, China; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","4","914","925","Twitter spam has become a critical problem nowadays. Recent works focus on applying machine learning techniques for Twitter spam detection, which make use of the statistical features of tweets. In our labeled tweets data set, however, we observe that the statistical properties of spam tweets vary over time, and thus, the performance of existing machine learning-based classifiers decreases. This issue is referred to as “Twitter Spam Drift”. In order to tackle this problem, we first carry out a deep analysis on the statistical features of one million spam tweets and one million non-spam tweets, and then propose a novel Lfun scheme. The proposed scheme can discover “changed” spam tweets from unlabeled tweets and incorporate them into classifier's training process. A number of experiments are performed to evaluate the proposed scheme. The results show that our proposed Lfun scheme can significantly improve the spam detection accuracy in real-world scenarios.","1556-6021","","10.1109/TIFS.2016.2621888","ARC Linkage Project(grant numbers:LP120200266); National Natural Science Foundation of China(grant numbers:61401371); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707341","Social network security;twitter spam detection;machine learning","Twitter;Feature extraction;Uniform resource locators;Security;Market research;Malware","","98","","44","OAPA","26 Oct 2016","","","IEEE","IEEE Journals"
"Log-Based Anomaly Detection With Robust Feature Extraction and Online Learning","S. Han; Q. Wu; H. Zhang; B. Qin; J. Hu; X. Shi; L. Liu; X. Yin","School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Engineering and Information Technology, University of New South Wales, Canberra, NSW, Australia; INSC&BNRist, Tsinghua University, Beijing, China; Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing University of Posts and Telecommunications, Nanjing, China; DCST, Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","18 Feb 2021","2021","16","","2300","2311","Cloud technology has brought great convenience to enterprises as well as customers. System logs record notable events and are becoming valuable resources to track and investigate system status. Detecting anomaly from logs as fast as possible can improve the quality of service significantly. Although many machine learning algorithms (e.g., SVM, Logistic Regression) have high detection accuracy, we find that they assume data are clean and might have high training time. Facing these challenges, in this paper, we propose Robust Online Evolving Anomaly Detection (ROEAD) framework which adopts Robust Feature Extractor (RFE) to remove the effects of noise and Online Evolving Anomaly Detection (OEAD) to dynamic update parameters. We propose Online Evolving SVM (OES) algorithm as the example of online anomaly detection methods. We analyze the performance of OES in theory and prove the performance difference between OES and the best hypothesis tends to zero as time goes infinity. We compare the performance of ROEAD against state-of-the-art anomaly detection algorithms using public log datasets. The results demonstrate that ROEAD is able to remove the effects of noise and OES can improve the detection accuracy by more than 40%.","1556-6021","","10.1109/TIFS.2021.3053371","National Key Research and Development Program of China(grant numbers:2020YFB10056,2019QY(Y)0602,2017YFB1400700,2017YFB0802500); Natural Science Foundation of China(grant numbers:62002009,61932011,61972019,61772538,61532021,91646203,61672083); Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, NJUPT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330769","Online learning;ROEAD;RFE","Anomaly detection;Feature extraction;Training;Support vector machines;Semantics;Machine learning algorithms;Noise measurement","","30","","50","IEEE","21 Jan 2021","","","IEEE","IEEE Journals"
"Achieving Privacy-Preserving and Verifiable Support Vector Machine Training in the Cloud","C. Hu; C. Zhang; D. Lei; T. Wu; X. Liu; L. Zhu","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Information Systems, Singapore Management University, Bras Basah, Singapore; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Information Forensics and Security","15 Jun 2023","2023","18","","3476","3491","With the proliferation of machine learning, the cloud server has been employed to collect massive data and train machine learning models. Several privacy-preserving machine learning schemes have been suggested recently to guarantee data and model privacy in the cloud. However, these schemes either mandate the involvement of the data owner in model training or utilize high-cost cryptographic techniques, resulting in excessive computational and communication overheads. Furthermore, none of the existing work considers the malicious behavior of the cloud server during model training. In this paper, we propose the first privacy-preserving and verifiable support vector machine training scheme by employing a two-cloud platform. Specifically, based on the homomorphic verification tag, we design a verification mechanism to enable verifiable machine learning training. Meanwhile, to improve the efficiency of model training, we combine homomorphic encryption and data perturbation to design an efficient multiplication operation for the encryption domain. A rigorous theoretical analysis demonstrates the security and reliability of our scheme. The experimental results indicate that our scheme can reduce computational and communication overheads by at least 43.94% and 99.58%, respectively, compared to state-of-the-art SVM training methods.","1556-6021","","10.1109/TIFS.2023.3283104","National Natural Science Foundation of China(grant numbers:62202051,62102027); National Key Research and Development Program of China(grant numbers:2021YFB2700500,2021YFB2700503); China Postdoctoral Science Foundation(grant numbers:2021M700435,2021TQ0042,2021TQ0041); Shandong Provincial Key Research and Development Program(grant numbers:2021CXGC010106); Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:2022B1212010005); Beijing Institute of Technology (BIT) Research Fund Program for Young Scholars; BIT Research and Innovation Promoting Project(grant numbers:2022YCXZ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144365","Privacy-preserving;support vector machine;data perturbation;homomorphic encryption;verification mechanism","Support vector machines;Training;Data models;Computational modeling;Servers;Machine learning;Cryptography","","23","","47","IEEE","5 Jun 2023","","","IEEE","IEEE Journals"
"Neural Network Modeling Attacks on Arbiter-PUF-Based Designs","N. Wisiol; B. Thapaliya; K. T. Mursi; J. -P. Seifert; Y. Zhuang","Security in Telecommunications, Technische Universität Berlin, Berlin, Germany; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Security in Telecommunications, Technische Universität Berlin, Berlin, Germany; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Information Forensics and Security","1 Aug 2022","2022","17","","2719","2731","By revisiting, improving, and extending recent neural-network based modeling attacks on XOR Arbiter PUFs from the literature, we show that XOR Arbiter PUFs, (XOR) Feed-Forward Arbiter PUFs, and Interpose PUFs can be attacked faster, up to larger security parameters, and with an order of magnitude fewer challenge-response pairs than previously known both in simulation and in silicon data. To support our claim, we discuss the differences and similarities of recently proposed modeling attacks and offer a fair comparison of the performance of these attacks by implementing all of them using the popular machine learning framework Keras and comparing their performance against the well-studied Logistic Regression attack. Our findings show that neural-network-based modeling attacks have the potential to outperform traditional modeling attacks on PUFs and must hence become part of the standard toolbox for PUF security analysis; the code and discussion in this paper can serve as a basis for the extension of our results to PUF designs beyond the scope of this work.","1556-6021","","10.1109/TIFS.2022.3189533","National Science Foundation(grant numbers:2103563); German Ministry for Education and Research as BBDC 2(grant numbers:01IS18025A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9819893","Physical unclonable function;strong PUFs;machine learning;modeling attacks;arbiter PUF","Security;Data models;Analytical models;Cryptography;Neural networks;Machine learning;Predictive models","","17","","37","IEEE","7 Jul 2022","","","IEEE","IEEE Journals"
"Threat Intelligence Generation Using Network Telescope Data for Industrial Control Systems","O. Cabana; A. M. Youssef; M. Debbabi; B. Lebel; M. Kassouf; R. Atallah; B. L. Agba","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada; Thales Canada Inc., Montreal, Canada; Institut de recherche d’Hydro-Québec, Varennes, Canada; Institut de recherche d’Hydro-Québec, Varennes, Canada; Institut de recherche d’Hydro-Québec, Varennes, Canada","IEEE Transactions on Information Forensics and Security","31 May 2021","2021","16","","3355","3370","Industrial Control Systems (ICSs) are cyber-physical systems that offer attractive targets to threat actors due to the scale of damages, both physical and cyber, that successful exploitation can cause. As such, ICSs often find themselves victims to reconnaissance campaigns - coordinated scanning activity that targets a wide subset of the Internet - that aim to discover vulnerable systems. As these campaigns likely scan broad netblocks of the Internet, some traffic is directed to network telescopes, which are routable, allocated, and unused IP space. In this paper, we explore the threat landscape of ICS devices by analyzing and investigating network telescope traffic. Our network traffic analysis tool takes darknet traffic and generates threat intelligence on scanning campaigns targeting ICSs in the form of campaign fragments, which we leverage in new ways to get more in-depth knowledge of the cybersecurity threats. We investigate the payloads of the identified campaigns using a custom Deep Packet Inspection (DPI) technique to dissect and analyze the packets. We found 13 distinct payload templates and deduced their purpose, and by extension the campaign goals. We use machine learning to classify the sources behind the campaigns and identify threat actors such as botnets, malicious attackers, or researchers, and establish a methodology to rank our campaigns to prioritize our analysis. To conduct our analysis of the threats targeting ICSs, we have leveraged 12.85 TB (330 days) of network traffic received by our observed darknet IP space. Combining these investigative threads, we provide a thorough overview of the threat landscape targeting ICS systems.","1556-6021","","10.1109/TIFS.2021.3078261","NSERC/Hydro-Québec/Thales Industrial Research Chair in Smart Grid Security; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425553","ICS;darknet;machine learning;DPI;scanning;classification","Integrated circuits;Payloads;IP networks;Reconnaissance;Telescopes;Protocols;Machine learning","","10","","51","IEEE","7 May 2021","","","IEEE","IEEE Journals"
"Dynamically Generate Password Policy via Zipf Distribution","Y. Xiao; J. Zeng","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","IEEE Transactions on Information Forensics and Security","3 Mar 2022","2022","17","","835","848","Password composition policies are helpful in strengthening password’s resistance against guessing attacks. Sadly, existing off-the-shelf composition policies often remain static, which creates potential security vulnerability. In this paper, we propose a new adaptive password policy generation framework called HTPG. Based on the Zipf distribution of passwords, HTPG classifies all passwords in data set into two categories, that is, head passwords and tail passwords. We find that head passwords are vulnerable and high-value for attackers because they are most frequently used, while tail passwords have higher strength than head passwords. According to this fact, HTPG dynamically generates policies to enhance head passwords by modifying them so as to be closer to tail passwords on feature space. By introducing the idea of machine learning, we propose a policy sort method based on information gain ratio to help user choose more effective policies in enhancing head passwords. HTPG can effectively improve the security of entire password data set and make the password distribution more uniform. Experiments show that the number of cracked head passwords decreases 69% on average, compared with the original head passwords, by adopting policies generated by HTPG. Surveys on usability show that 80.23% enhanced passwords can be recalled by those who remember the corresponding original passwords.","1556-6021","","10.1109/TIFS.2022.3152357","Shanghai Municipal Natural Science Foundation(grant numbers:151403700); Joint Foundation for Industry and Education, Ministry of Education, China(grant numbers:2017A03021); National Key Research and Development Program of China(grant numbers:20170803203); National Key Research and Development Program of China(grant numbers:20160800101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715109","Authentication;Zipf distribution;machine learning;password policy","Passwords;Magnetic heads;Security;Usability;Machine learning;Generators;Blocklists","","3","","37","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Protecting Sensitive Attributes by Adversarial Training Through Class-Overlapping Techniques","T. -H. Lin; Y. -S. Lee; F. -C. Chang; J. M. Chang; P. -Y. Wu","Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Information Forensics and Security","25 Jan 2023","2023","18","","1283","1294","In recent years, machine learning as a service (MLaaS) has brought considerable convenience to our daily lives. However, these services raise the issue of leaking users’ sensitive attributes, such as race, when provided through the cloud. The present work overcomes this issue by proposing an innovative privacy-preserving approach called privacy-preserving class overlap (PPCO), which incorporates both a Wasserstein generative adversarial network and the idea of class overlapping to obfuscate data for better resilience against the leakage of attribute-inference attacks(i.e., malicious inference on users’ sensitive attributes). Experiments show that the proposed method can be employed to enhance current state-of-the-art works and achieve superior privacy–utility trade-off. Furthermore, the proposed method is shown to be less susceptible to the influence of imbalanced classes in training data. Finally, we provide a theoretical analysis of the performance of our proposed method to give a flavour of the gap between theoretical and empirical performances.","1556-6021","","10.1109/TIFS.2023.3236180","Asian Office of Aerospace Research and Development(grant numbers:FA2386-20-1-4039); Ministry of Science and Technology, Taiwan(grant numbers:109-2634-F-002-038,110-2222-E-002-008,110-2634-F-002-039); National Taiwan University(grant numbers:106R891310,111L880601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015066","Privacy-preserving machine learning;adversarial training;generative adversarial network;class overlap;machine learning as a service;Wasserstein distance;data obfuscation","Data privacy;Privacy;Training;Machine learning;Feature extraction;Cloud computing;Threat modeling","","3","","39","IEEE","11 Jan 2023","","","IEEE","IEEE Journals"
"With Great Dispersion Comes Greater Resilience: Efficient Poisoning Attacks and Defenses for Linear Regression Models","J. Wen; B. Z. H. Zhao; M. Xue; A. Oprea; H. Qian","East China Normal University, Shanghai, China; CSIRO-Data61, University of New South Wales, Sydney, NSW, Australia; The University of Adelaide, Adelaide, SA, Australia; Northeastern University, Boston, MA, USA; East China Normal University, Shanghai, China","IEEE Transactions on Information Forensics and Security","28 Jul 2021","2021","16","","3709","3723","With the rise of third parties in the machine learning pipeline, the service provider in “Machine Learning as a Service” (MLaaS), or external data contributors in online learning, or the retraining of existing models, the need to ensure the security of the resulting machine learning models has become an increasingly important topic. The security community has demonstrated that without transparency of the data and the resulting model, there exist many potential security risks, with new risks constantly being discovered. In this paper, we focus on one of these security risks - poisoning attacks. Specifically, we analyze how attackers may interfere with the results of regression learning by poisoning the training datasets. To this end, we analyze and develop a new poisoning attack algorithm. Our attack, termed Nopt, in contrast with previous poisoning attack algorithms, can produce larger errors with the same proportion of poisoning data-points. Furthermore, we also significantly improve the state-of-the-art defense algorithm, termed TRIM, proposed by Jagielsk et al. (IEEE S&P 2018), by incorporating the concept of probability estimation of clean data-points into the algorithm. Our new defense algorithm, termed Proda, demonstrates an increased effectiveness in reducing errors arising from the poisoning dataset through optimizing ensemble models. We highlight that the time complexity of TRIM had not been estimated; however, we deduce from their work that TRIM can take exponential time complexity in the worst-case scenario, in excess of Proda's logarithmic time. The performance of both our proposed attack and defense algorithms is extensively evaluated on four real-world datasets of housing prices, loans, health care, and bike sharing services. We hope that our work will inspire future research to develop more robust learning algorithms immune to poisoning attacks.","1556-6021","","10.1109/TIFS.2021.3087332","NSFC-ISF Joint Scientific Research Program(grant numbers:61961146004); Innovation Program of Shanghai Municipal Education Commission(grant numbers:2021-01-07-00-08-E00101); Australian Research Council (ARC) Discovery Project(grant numbers:DP210102670); U.S. Army Combat Capabilities Development Command Army Research Laboratory (ARL) through Cyber Security CRA(grant numbers:W911NF-13-2-0045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448089","Data poisoning attacks and defenses;linear regression models;complexity","Training;Linear regression;Data models;Predictive models;Numerical models;Machine learning;Time complexity","","13","","46","IEEE","7 Jun 2021","","","IEEE","IEEE Journals"
"Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems","A. Datta; S. Sen; Y. Zick","Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Pittsburgh, USA","2016 IEEE Symposium on Security and Privacy (SP)","18 Aug 2016","2016","","","598","617","Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque-it is difficult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of influence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals (e.g., a loan decision) and groups (e.g., disparate impact based on gender). Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the marginal influence of individual inputs within such a set (e.g., income). Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting. Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise. Our empirical validation with standard machine learning algorithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available. In particular, they provide better explanations than standard associative measures for a host of scenarios that we consider. Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy.","2375-1207","978-1-5090-0824-7","10.1109/SP.2016.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546525","transparency;fairness;machine learning","Decision making;Atmospheric measurements;Particle measurements;Privacy;Correlation;Machine learning algorithms;Algorithm design and analysis","","273","1","62","IEEE","18 Aug 2016","","","IEEE","IEEE Conferences"
"Your Model Trains on My Data? Protecting Intellectual Property of Training Data via Membership Fingerprint Authentication","G. Liu; T. Xu; X. Ma; C. Wang","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Information Forensics and Security","15 Mar 2022","2022","17","","1024","1037","In recent years, data has become the new oil that fuels various machine learning (ML) applications. Just as the oil refining, providing data to an ML model is a product of massive costs and expertise efforts. However, how to protect the intellectual property (IP) of the training data in ML remains largely open. In this paper, we present MeFA, a novel framework for detecting training data IP embezzlement via Membership Fingerprint Authentication, which is able to determine whether a suspect ML model is trained on the to be protected target data or not. The key observation is that a part of data has a similar influence on the prediction behavior of different ML models. On this basis, MeFA leverages membership inference techniques to extract these data as the fingerprints of the target data and constructs an authentication model to verify the data’s ownership by identifying the obtained membership fingerprints. MeFA has several salient features. It does not assume any knowledge of the suspect model except for its black-box prediction API, through which we can merely get the prediction output of a given input, and also does not require any modification to the dataset or the training process, since it takes advantage of the inherent membership property of the data. As a by-product, MeFA can also serve as a post-protection to verify the ownership of ML models, without modifying the training process of the model. Extensive experiments on three realistic datasets and seven types of ML models validate the effectiveness of MeFA, and demonstrate that it is also robust to scenarios when the training data is partially used or preprocessed with representative membership inference defenses.","1556-6021","","10.1109/TIFS.2022.3155921","National Natural Science Foundation of China(grant numbers:61872416,62171189,62002104,62071192); Fundamental Research Funds for the Central Universities of China(grant numbers:2019kfyXJJS017); Key Research and Development Program of Hubei Province(grant numbers:2020BAB120); Wuhan Yellow Crane Talents (Excellent Young Scholar); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724248","Training data authentication;intellectual property protection;membership inference attack;membership fingerprint;machine learning model","Data models;Predictive models;Training;Computational modeling;Training data;Watermarking;Authentication","","7","","53","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"Semi-Supervised Multivariate Statistical Network Monitoring for Learning Security Threats","J. Camacho; G. Maciá-Fernández; N. M. Fuentes-García; E. Saccenti","Department of Signal Theory, CITIC-University of Granada, Granada, Spain; Department of Signal Theory, CITIC-University of Granada, Granada, Spain; Department of Signal Theory, CITIC-University of Granada, Granada, Spain; Laboratory of Systems and Synthetic Biology, Wageningen University and Research, Wageningen, PB, The Netherlands","IEEE Transactions on Information Forensics and Security","16 May 2019","2019","14","8","2179","2189","This paper presents a semi-supervised approach for intrusion detection. The method extends the unsupervised multivariate statistical network monitoring approach based on the principal component analysis by introducing a supervised optimization technique to learn the optimum scaling in the input data. It inherits the advantages of the unsupervised strategy, capable of uncovering new threats, with that of supervised strategies, capable of learning the pattern of a targeted threat. The supervised learning is based on an extension of the gradient descent method based on partial least squares (PLS). Moreover, we enhance this method by using sparse PLS variants. The practical application of the system is demonstrated on a recently published real case study, showing relevant improvements in detection performance and in the interpretation of the attacks.","1556-6021","","10.1109/TIFS.2019.2894358","Spanish Ministry of Economy and Competitiveness and FEDER funds(grant numbers:TIN2014-60346-R,TIN2017-83494-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628992","Multivariate statistical network monitoring;anomaly detection;intrusion detection;semi-supervised learning;partial least squares regression;principal components analysis","Principal component analysis;Optimization;Anomaly detection;Monitoring;Intrusion detection;Machine learning","","32","","40","IEEE","29 Jan 2019","","","IEEE","IEEE Journals"
"Compressive Privacy Generative Adversarial Network","B. -W. Tseng; P. -Y. Wu","Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Information Forensics and Security","7 Feb 2020","2020","15","","2499","2513","Machine learning as a service (MLaaS) has brought much convenience to our daily lives recently. However, the fact that the service is provided through cloud raises privacy leakage issues. In this work we propose the compressive privacy generative adversarial network (CPGAN), a data-driven adversarial learning framework for generating compressing representations that retain utility comparable to state-of-the-art, with the additional feature of defending against reconstruction attack. This is achieved by applying adversarial learning scheme to the design of compression network (privatizer), whose utility/privacy performances are evaluated by the utility classifier and the adversary reconstructor, respectively. Experimental results demonstrate that CPGAN achieves better utility/privacy trade-off in comparison with the previous work, and is applicable to real-world large datasets.","1556-6021","","10.1109/TIFS.2020.2968188","Ministry of Science and Technology, Taiwan(grant numbers:MOST-107-2634-F-002-008-,MOST-108-2634-F-002-005-); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963921","Compressive privacy;cyber security;privacy preserving machine learning;adversarial learning;generative adversarial networks;machine learning as a service","Data privacy;Privatization;Privacy;Data models;Generative adversarial networks;Stochastic processes;Feature extraction","","23","","71","IEEE","20 Jan 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving Boosting in the Local Setting","S. Wang; J. M. Chang","Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Information Forensics and Security","17 Dec 2021","2021","16","","4451","4465","In machine learning, boosting is one of the most popular methods that is designed to combine multiple base learners into a superior one. The well-known Boosted Decision Tree classifier has been widely adopted in data mining and pattern recognition. With the emerging challenge in privacy, the personal images, browsing history, and financial reports, which are held by individuals and entities are more likely to contain sensitive information. The privacy concern is intensified when the data leaves the hand of owners and is used for further mining. Such privacy issues demand that the machine learning algorithms should be privacy-aware. Recently, Local Differential Privacy has been proposed as an effective privacy protection approach, which allows data owners to perturb the data before any release. In this paper, we propose a distributed privacy-preserving boosting algorithm that can be applied to various types of classifiers. By adopting LDP as a building block, the proposed boosting algorithm leverages the aggregation of the perturbed data shares to build the base learner, which ensures that privacy is well preserved for the participated data owners. Our experiments demonstrate that the proposed algorithm effectively boosts various classifiers and the boosted classifiers maintain a high utility.","1556-6021","","10.1109/TIFS.2021.3097822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9489291","Ensemble learning;boosting;local differential privacy","Boosting;Privacy;Machine learning algorithms;Differential privacy;Decision trees;Data models;Prediction algorithms","","4","","48","IEEE","16 Jul 2021","","","IEEE","IEEE Journals"
"VPiP: Values Packing in Paillier for Communication Efficient Oblivious Linear Computations","W. Wu; J. Wang; Y. Zhang; Z. Liu; L. Zhou; X. Lin","Department of College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-Belval Esch-sur-Alzette, Luxembourg; Department of School of Mathematical Sciences, Peking University, Beijing, China; Department of College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Management Science and Information Systems, Rutgers University, New Brunswick, NJ, USA","IEEE Transactions on Information Forensics and Security","21 Jul 2023","2023","18","","4214","4228","The technique of packing multiple values into one message without losing homomorphic computation properties is the main workhorse that drives many exciting advances in applying lattice-based homomorphic encryption schemes to privacy-preserving Machine-Learning-as-a-Service (MLaaS). However, this technique does not directly work for the classic Paillier homomorphic encryption scheme, limiting the use of the Paillier scheme in the privacy-preserving MLaaS. To enrich the applications of Paillier in privacy-preserving MLaaS, we present a set of new methods for efficient linear computations over packed values under the Paillier scheme, such as vector multiplication, matrix multiplication, and convolutional calculation between ciphertexts and plaintexts. Different from the packing methods of lattice-based schemes, the Paillier packing method naturally allows higher packing capability for values in lower bit-length. This property can significantly benefit privacy-preserving MLaaS, as the values of user inputs and parameters of machine learning models are often quantized into low bits (e.g., 1–8 bits). We conduct comparisons based on different linear computation tasks, the proposed methods under the Paillier scheme clearly outperform the state-of-the-art in terms of communication and computational efficiency, especially in realistic scenarios. For example, compared to one of the recent arts CrypTFlow2 (Rathee et al., 2020), the communication cost of our solution can be  $21.7\times $  smaller at best. Thanks to the reduction of communication cost, the runtime can be  $2.46\times $  faster than CrypTFlow2 at the median-country-speed of current global mobile broadband.","1556-6021","","10.1109/TIFS.2023.3290483","National Key Research and Development Program of China(grant numbers:2020AAA0107703); National Natural Science Foundation of China(grant numbers:62132008,62071222); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20220075,BK20200418); Research Initiation Project of Zhejiang Laboratory(grant numbers:2022PD0AC02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167664","Privacy-preserving;homomorphic encryption;paillier cryptosystem","Servers;Machine learning;Homomorphic encryption;Runtime;Cloud computing;Computational modeling;Computational efficiency","","","","49","IEEE","28 Jun 2023","","","IEEE","IEEE Journals"
"Machine-Learning Attacks on PolyPUFs, OB-PUFs, RPUFs, LHS-PUFs, and PUF–FSMs","J. Delvaux","imec-COSIC, KU Leuven, Leuven, Belgium","IEEE Transactions on Information Forensics and Security","8 May 2019","2019","14","8","2043","2058","A physically unclonable function (PUF) is a circuit of which the input-output behavior is designed to be sensitive to the random variations of its manufacturing process. This building block hence facilitates the authentication of any given device in a population of identically laid-out silicon chips, similar to the biometric authentication of a human. The focus and novelty of this paper is the development of efficient impersonation attacks on the following five Arbiter PUF-based authentication protocols: 1) the so-called Poly PUF protocol of Konigsmark et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2016; 2) the so-called OB-PUF protocol of Gao et al. as presented at the IEEE Conference PerCom 2016; 3) the so-called RPUF protocol of Ye et al. as presented at the IEEE Conference AsianHOST 2016; 4) the so-called LHS-PUF protocol of Idriss and Bayoumi as presented at the IEEE Conference RFID-TA 2017; and 5) the so-called PUF-FSM protocol of Gao et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2018. The common flaw of all five designs is that the use of lightweight obfuscation logic provides insufficient protection against machine-learning attacks.","1556-6021","","10.1109/TIFS.2019.2891223","KU Leuven(grant numbers:C16/15/058); European Research Council (ERC)(grant numbers:695305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603753","Physically unclonable function;machine learning;entity authentication","Protocols;Machine learning;Authentication;Delays;Tin;Sociology;Statistics","","114","","38","IEEE","6 Jan 2019","","","IEEE","IEEE Journals"
"Membership Inference Attacks Against Machine Learning Models","R. Shokri; M. Stronati; C. Song; V. Shmatikov",Cornell Tech; INRIA; Cornell; Cornell Tech,"2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","3","18","We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958568","","Training;Data models;Predictive models;Privacy;Sociology;Statistics;Google","","1546","6","38","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Proof-of-Learning: Definitions and Practice","H. Jia; M. Yaghini; C. A. Choquette-Choo; N. Dullerud; A. Thudi; V. Chandrasekaran; N. Papernot",University of Toronto and Vector Institute; University of Toronto and Vector Institute; University of Toronto and Vector Institute; University of Toronto and Vector Institute; University of Toronto and Vector Institute; University of Wisconsin-Madison; University of Toronto and Vector Institute,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1039","1056","Training machine learning (ML) models typically involves expensive iterative optimization. Once the model’s final parameters are released, there is currently no mechanism for the entity which trained the model to prove that these parameters were indeed the result of this optimization procedure. Such a mechanism would support security of ML applications in several ways. For instance, it would simplify ownership resolution when multiple parties contest ownership of a specific model. It would also facilitate the distributed training across untrusted workers where Byzantine workers might otherwise mount a denial-ofservice by returning incorrect model updates.In this paper, we remediate this problem by introducing the concept of proof-of-learning in ML. Inspired by research on both proof-of-work and verified computations, we observe how a seminal training algorithm, stochastic gradient descent, accumulates secret information due to its stochasticity. This produces a natural construction for a proof-of-learning which demonstrates that a party has expended the compute require to obtain a set of model parameters correctly. In particular, our analyses and experiments show that an adversary seeking to illegitimately manufacture a proof-of-learning needs to perform at least as much work than is needed for gradient descent itself.We also instantiate a concrete proof-of-learning mechanism in both of the scenarios described above. In model ownership resolution, it protects the intellectual property of models released publicly. In distributed training, it preserves availability of the training procedure. Our empirical evaluation validates that our proof-of-learning mechanism is robust to variance induced by the hardware (e.g., ML accelerators) and software stacks.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00106","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519402","machine-learning;security;proof-of-work","Training;Privacy;Computational modeling;Stochastic processes;Machine learning;Intellectual property;Software","","20","","90","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Unsupervised Machine Learning-Based Detection of Covert Data Integrity Assault in Smart Grid Networks Utilizing Isolation Forest","S. Ahmed; Y. Lee; S. -H. Hyun; I. Koo","University of Ulsan, Ulsan, South Korea; University of Ulsan, Ulsan, South Korea; University of Ulsan, Ulsan, South Korea; University of Ulsan, Ulsan, South Korea","IEEE Transactions on Information Forensics and Security","14 Jun 2019","2019","14","10","2765","2777","Being one of the most multifaceted cyber-physical systems, smart grids (SGs) are arguably more prone to cyber-threats. A covert data integrity assault (CDIA) on a communications network may be lethal to the reliability and safety of SG operations. They are intelligently designed to sidestep the traditional bad data detector in power control centers, and this type of assault can compromise the integrity of the data, causing a false estimation of the state that further severely distresses the entire power system operation. In this paper, we propose an unsupervised machine learning-based scheme to detect CDIAs in SG communications networks utilizing non-labeled data. The proposed scheme employs a state-of-the-art algorithm, called isolation forest, and detects CDIAs based on the hypothesis that the assault has the shortest average path length in a constructed random forest. To tackle the dimensionality issue from the growth in power systems, we use a principal component analysis-based feature extraction technique. The evaluation of the proposed scheme is carried out through standard IEEE 14-bus, 39-bus, 57-bus, and 118-bus systems. The simulation results show that the proposed scheme is proficient at handling non-labeled historical measurement datasets and results in a significant improvement in attack detection accuracy.","1556-6021","","10.1109/TIFS.2019.2902822","National Research Foundation of Korea(grant numbers:NRF-2018R1A2B6001714); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660426","Cyber-security;cyber-assaults;machine learning;principal component analysis;state estimation;smart grids;isolation forest","Power measurement;Smart grids;Data integrity;Feature extraction;State estimation;Communication networks","","172","","69","IEEE","5 Mar 2019","","","IEEE","IEEE Journals"
"UNSAIL: Thwarting Oracle-Less Machine Learning Attacks on Logic Locking","L. Alrahis; S. Patnaik; J. Knechtel; H. Saleh; B. Mohammad; M. Al-Qutayri; O. Sinanoglu","Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, NY, USA; Division of Engineering, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Division of Engineering, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates","IEEE Transactions on Information Forensics and Security","23 Feb 2021","2021","16","","2508","2523","Logic locking aims to protect the intellectual property (IP) of integrated circuit (IC) designs throughout the globalized supply chain. The SAIL attack, based on tailored machine learning (ML) models, circumvents combinational logic locking with high accuracy and is amongst the most potent attacks as it does not require a functional IC acting as an oracle. In this work, we propose UNSAIL, a logic locking technique that inserts key-gate structures with the specific aim to confuse ML models like those used in SAIL. More specifically, UNSAIL serves to prevent attacks seeking to resolve the structural transformations of synthesis-induced obfuscation, which is an essential step for logic locking. Our approach is generic; it can protect any local structure of key-gates against such ML-based attacks in an oracle-less setting. We develop a reference implementation for the SAIL attack and launch it on both traditionally locked and UNSAIL-locked designs. For SAIL, two ML models have been proposed (which we implement accordingly), namely a change-prediction model and a reconstruction model; the change-prediction model is used to determine which key-gate structures to restore using the reconstruction model. Our study on benchmarks ranging from the ISCAS-85 and ITC-99 suites to the OpenRISC Reference Platform System-on-Chip (ORPSoC) confirms that UNSAIL degrades the accuracy of the change-prediction model and the reconstruction model by an average of 20.13 and 17 percentage points (pp), respectively. When the aforementioned models are combined, which is the most powerful scenario for SAIL, UNSAIL reduces the attack accuracy of SAIL by an average of 11pp. We further demonstrate that UNSAIL thwarts other oracle-less attacks, i.e., SWEEP and the redundancy attack, indicating the generic nature and strength of our approach. Detailed layout-level evaluations illustrate that UNSAIL incurs minimal area and power overheads of 0.26% and 0.61%, respectively, on the million-gate ORPSoC design.","1556-6021","","10.1109/TIFS.2021.3057576","Khalifa University of Science, Technology and Research(grant numbers:RC2-2018-020); Center for Cyber Security at NYU New York/Abu Dhabi (NYU/NYUAD); Global Ph.D. Fellowship at NYU/NYUAD; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350294","Logic locking;hardware security;IP protection;hardware obfuscation;machine learning","Integrated circuit modeling;Logic gates;Redundancy;Supply chains;Security;System-on-chip;Predictive models","","24","","37","IEEE","8 Feb 2021","","","IEEE","IEEE Journals"
"Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models","G. Liu; T. Xu; R. Zhang; Z. Wang; C. Wang; L. Liu","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","21 Nov 2023","2024","19","","427","440","Machine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner’s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model’s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model’s training data in an unsupervised manner, without any priori knowledge about the target model’s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.","1556-6021","","10.1109/TIFS.2023.3324772","National Natural Science Foundation of China(grant numbers:62272183,52031009,62171189,U20A20181); Key Research and Development Program of Hubei Province(grant numbers:2023BAB074,2021BAA026); Special Fund for Wuhan Yellow Crane Talents (Excellent Young Scholar);; Georgia Tech through USA NSF CISE(grant numbers:2302720,2312758,2038029); IBM Faculty Award; and; CISCO Edge AI Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285881","Machine learning (ML);membership inference attack (MIA);black-box model;autoencoder","Data models;Training;Predictive models;Training data;Computational modeling;Closed box;Feature extraction","","1","","41","IEEE","16 Oct 2023","","","IEEE","IEEE Journals"
"Proof of Unlearning: Definitions and Instantiation","J. Weng; S. Yao; Y. Du; J. Huang; J. Weng; C. Wang","College of Cyber Security, Jinan University, Guangzhou, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Information Forensics and Security","12 Feb 2024","2024","19","","3309","3323","The “Right to be Forgotten” rule in machine learning (ML) practice enables some individual data to be deleted from a trained model, as pursued by recently developed machine unlearning techniques. To truly comply with the rule, a natural and necessary step is to verify if the individual data are indeed deleted after unlearning. Yet, previous parameter-space verification metrics may be easily evaded by a distrustful model trainer. Thus, Thudi et al. recently present a call to action on algorithm-level verification in USENIX Security’22. We respond to the call, by reconsidering the unlearning problem in the scenario of machine learning as a service (MLaaS), and proposing a new definition framework for Proof of Unlearning (PoUL) on algorithm level. Specifically, our PoUL definitions (i) enforce correctness properties on both the pre and post phases of unlearning, so as to prevent the state-of-the-art forging attacks; (ii) highlight proper practicality requirements of both the prover and verifier sides with minimal invasiveness to the off-the-shelf service pipeline and computational workloads. Under the definition framework, we subsequently present a trusted hardware-empowered instantiation using SGX enclave, by logically incorporating an authentication layer for tracing the data lineage with a proving layer for supporting the audit of learning. We customize authenticated data structures to support large out-of-enclave storage with simple operation logic, and meanwhile, enable proving complex unlearning logic with affordable memory footprints in the enclave. We finally validate the feasibility of the proposed instantiation with a proof-of-concept implementation and multi-dimensional performance evaluation.","1556-6021","","10.1109/TIFS.2024.3358993","National Natural Science Foundation of China(grant numbers:62302192,U23A20303); Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China(grant numbers:61825203,62332007,U22B2028); Science and Technology Major Project of Tibetan Autonomous Region of China(grant numbers:XZ202201ZD0006G); National Joint Engineering Research Center of Network Security Detection and Protection Technology; Guangdong Key Laboratory of Data Security and Privacy Preserving; Guangdong Hong Kong Joint Laboratory for Data Security and Privacy Protection; Engineering Research Center of Trustworthy Artificial Intelligence (AI), Ministry of Education; Research Grants Council (RGC) of Hong Kong(grant numbers:11217620,11218521,11218322,R6021-20F,R1012-21,RFS2122-1S04,C2004-21G,C1029-22G,N_CityU139/21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416191","Authentication;data integrity;trusted computing;machine learning (ML)","Data models;Predictive models;Servers;Computational modeling;Authentication;Prediction algorithms;Closed box","","1","","91","IEEE","29 Jan 2024","","","IEEE","IEEE Journals"
"Real-Time Malicious Traffic Detection With Online Isolation Forest Over SD-WAN","P. Zhang; F. He; H. Zhang; J. Hu; X. Huang; J. Wang; X. Yin; H. Zhu; Y. Li","School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; Institute for Network Sciences and Cyberspace (INSC), Tsinghua University, Beijing, China; School of Engineering and Information Technology, University of New South Wales at the Australian Defence Force Academy (UNSW@ADFA), ACT, Canberra, Australia; School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China; Institute for Network Sciences and Cyberspace (INSC), Tsinghua University, Beijing, China; Department of Computer Science and Technology (DCST), Tsinghua University, Beijing, China; Research Institute, China Telecom Corporation Ltd., Beijing, China; Institute for Network Sciences and Cyberspace (INSC), Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","4 Apr 2023","2023","18","","2076","2090","Software Defined Network (SDN) has been widely used in modern network architecture. The SD-WAN is considered as a technology that has a potential to revolutionize the WAN service usage by utilizing the SDN philosophy. Attacks within SD-WAN can affect the network and block the entire services. In this paper, we propose a machine learning based anomalous traffic detection framework named OADSD over SD-WAN that can achieve task independently and has the ability of adapting to the environment. The OADSD adopts Distributed Dynamic Feature Extraction (DDFE) to extract representative features directly from the raw traffic, and proposes the On-demand Evolving Isolation Forest (OEIF) to make the system adapt to an environment. We provide a theoretical analysis of the performance of the OADSD. We also conduct comprehensive experiments to evaluate the performance of the OADSD with real world public datasets as well as a small real testbed. Our experiments under real world public datasets show that, the OADSD can accurately detect various kinds of attacks with a high performance. Compared with the state-of-the-art systems, the OADSD can achieve up to 60% accuracy improvement.","1556-6021","","10.1109/TIFS.2023.3262121","National Natural Science Foundation of China(grant numbers:62002009); Tsinghua University-China Telecom Joint Research Institute for Next Generation Internet Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081489","Malicious traffic detection;traffic feature extraction;online learning","Feature extraction;Training;Machine learning;Anomaly detection;Image edge detection;Wide area networks;Forestry","","3","","49","CCBY","27 Mar 2023","","","IEEE","IEEE Journals"
"Throwing Darts in the Dark? Detecting Bots with Limited Data using Neural Data Augmentation","S. T. K. Jan; Q. Hao; T. Hu; J. Pu; S. Oswal; G. Wang; B. Viswanath",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Virginia Tech; Virginia Tech; Radware; University of Illinois at Urbana-Champaign; Virginia Tech,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","1190","1206","Machine learning has been widely applied to building security applications. However, many machine learning models require the continuous supply of representative labeled data for training, which limits the models' usefulness in practice. In this paper, we use bot detection as an example to explore the use of data synthesis to address this problem. We collected the network traffic from 3 online services in three different months within a year (23 million network requests). We develop a stream-based feature encoding scheme to support machine learning models for detecting advanced bots. The key novelty is that our model detects bots with extremely limited labeled data. We propose a data synthesis method to synthesize unseen (or future) bot behavior distributions. The synthesis method is distribution-aware, using two different generators in a Generative Adversarial Network to synthesize data for the clustered regions and the outlier regions in the feature space. We evaluate this idea and show our method can train a model that outperforms existing methods with only 1% of the labeled data. We show that data synthesis also improves the model's sustainability over time and speeds up the retraining. Finally, we compare data synthesis and adversarial retraining and show they can work complementary with each other to improve the model generalizability.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152805","","CAPTCHAs;Data models;IP networks;Machine learning;Security;Training;Encoding","","18","","82","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Towards Practical Privacy-Preserving Decision Tree Training and Evaluation in the Cloud","L. Liu; R. Chen; X. Liu; J. Su; L. Qiao","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China","IEEE Transactions on Information Forensics and Security","31 Mar 2020","2020","15","","2914","2929","Due to the capacity of storing massive data and providing huge computing resources, cloud computing has been a desirable platform for doing machine learning. However, the issue of data privacy is far from being well solved and thus has been a general concern in the cloud-aided machine learning. In this work, we investigate the study of how to efficiently do decision tree training and evaluation in the cloud and meanwhile achieve privacy preservation. Unlike existing cloud server-assisted model training approaches, in our proposed solution, the whole training process is mostly done by the cloud service provider who owns the machine learning model. Since the cloud cannot directly divide the encrypted dataset according to the best attributes selected, we propose a new method for decision tree training without dataset splitting. Precisely, we design three methods for decision tree training with the different tradeoff between privacy and efficiency. In all of these methods, the outsourced data are not revealed to the cloud service provider. We also propose a privacy-preserving decision tree evaluation scheme where the cloud service provider learns nothing about the user's input and the classification result while the trained model is kept secret to the user who could only learn the classification result. Compared with previous decision tree evaluation work, our scheme achieves desirable privacy preservation against both the user and the cloud service provider, and also minimizes the user's computation and communication costs. Moreover, besides protecting the data confidentiality, our proposed scheme also supports off-line users and thus has good scalability. The real-world dataset-based experimental results demonstrate that our system is of desirable utility and efficiency.","1556-6021","","10.1109/TIFS.2020.2980192","National Natural Science Foundation of China(grant numbers:61702541,61872087,61822202,61872089,61702105,61806216,61972412); National Basic Research Program of China (973 Program)(grant numbers:2017YFB0802300); China Academy of Space Technology(grant numbers:YESS20170128); National University of Defense Technology(grant numbers:ZK17-03-46,ZK19-38); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9032115","Privacy-preserving training;decision tree;cloud computing;data security and privacy","Training;Decision trees;Cryptography;Cloud computing;Machine learning;Additives;Privacy","","40","","45","IEEE","11 Mar 2020","","","IEEE","IEEE Journals"
"Anonymous and Efficient Authentication Scheme for Privacy-Preserving Distributed Learning","Y. Jiang; K. Zhang; Y. Qian; L. Zhou","College of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Omaha, NE, USA; Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Omaha, NE, USA; College of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Information Forensics and Security","24 Jun 2022","2022","17","","2227","2240","Distributed learning is proposed as a promising technique to reduce heavy data transmissions in centralized machine learning. By allowing the participants training the model locally, raw data is unnecessarily uploaded to the centralized cloud server, reducing the risks of privacy leakage as well. However, the existing studies have shown that an adversary is able to derive the raw data by analyzing the obtained machine learning models. To tackle this challenge, the state-of-the-art solutions mainly depend on differential privacy and encryption techniques (e.g., homomorphic encryption). Whereas, differential privacy degrades data utility and leads to inaccurate learning, while encryption based approaches are not effective to all machine learning algorithms due to the limited operations and excessive computation cost. In this work, we propose a novel scheme to resolve the privacy issues from the anonymous authentication approach. Different from the two types of existing solutions, this approach is generalized to all machine learning algorithms without reducing data utility, while guaranteeing privacy preservation. In addition, it can be integrated with detection schemes against data poisoning attacks and free-rider attacks, being more practical for distributed learning. To this end, we first design a pairing-based certificateless signature scheme. Based on the signature scheme, we further propose an anonymous and efficient authentication protocol which supports dynamic batch verification. The proposed protocol guarantees the desired security properties while being computationally efficient. Formal security proof and analysis have been provided to demonstrate the achieved security properties, including confidentiality, anonymity, mutual authentication, unlinkability, unforgeability, forward security, backward security, and non-repudiation. In addition, the performance analysis reveals that our proposed protocol significantly reduces the time consumption in batch verification, achieving high computational efficiency.","1556-6021","","10.1109/TIFS.2022.3181848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792404","Distributed learning;privacy preservation;anonymous authentication;efficiency","Authentication;Protocols;Data models;Servers;Computational modeling;Privacy;Vehicular ad hoc networks","","15","","33","IEEE","9 Jun 2022","","","IEEE","IEEE Journals"
"ENF-Based Region-of-Recording Identification for Media Signals","A. Hajj-Ahmad; R. Garg; M. Wu","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Intel Corporation, Chandler, AZ, USA; Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","6","1125","1136","The electric network frequency (ENF) is a signature of power distribution networks that can be captured by multimedia signals recorded near electrical activities. This has led to the emergence of multiple forensic applications based on the use of ENF signals. Examples of such applications include validating the time-of-recording of an ENF-containing multimedia signal or estimating its recording location based on concurrent reference signals from power grids. In this paper, we examine a novel ENF-based application that infers the power grid in which the ENF-containing multimedia signal was recorded without relying on the availability of concurrent power references. We investigate features based on the statistical differences in ENF variations between different power grids to serve as signatures for the region-of-recording of the media signal. We use these features in a multiclass machine learning implementation that is able to identify the grid-of-recording of a signal with high accuracy. In addition, we explore techniques for building multiconditional learning systems that can adapt to changes in the noise environment between the training and testing data.","1556-6021","","10.1109/TIFS.2015.2398367","National Science Foundation (NSF) through the University of Maryland ADVANCE Seed Research Grant(grant numbers:1008117); NSF(grant numbers:1309623,1320803); University of Maryland Kulkarni Research Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027197","Electric network frequency;information forensics;location-of-recording estimation;power grids;machine learning","Feature extraction;Testing;Power grids;Training;Support vector machines;Multimedia communication;Forensics","","51","","20","IEEE","29 Jan 2015","","","IEEE","IEEE Journals"
"An Explainable AI-Based Intrusion Detection System for DNS Over HTTPS (DoH) Attacks","T. Zebin; S. Rezvy; Y. Luo","School of Computing Science, University of East Anglia, Norwich, U.K; Department of Computer Science, School of Science, Technology and Health, York St John University, York, U.K; Faculty of Science and Technology, Middlesex University London, London, U.K","IEEE Transactions on Information Forensics and Security","24 Jun 2022","2022","17","","2339","2349","Over the past few years, Domain Name Service (DNS) remained a prime target for hackers as it enables them to gain first entry into networks and gain access to data for exfiltration. Although the DNS over HTTPS (DoH) protocol has desirable properties for internet users such as privacy and security, it also causes a problem in that network administrators are prevented from detecting suspicious network traffic generated by malware and malicious tools. To support their efforts in maintaining a secure network, in this paper, we have implemented an explainable AI solution using a novel machine learning framework. We have used the publicly available CIRA-CIC-DoHBrw-2020 dataset for developing an accurate solution to detect and classify the DNS over HTTPS attacks. Our proposed balanced and stacked Random Forest achieved very high precision (99.91%), recall (99.92%) and F1 score (99.91%) for the classification task at hand. Using explainable AI methods, we have additionally highlighted the underlying feature contributions in an attempt to provide transparent and explainable results from the model.","1556-6021","","10.1109/TIFS.2022.3183390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796558","Secure computing;machine learning;intrusion detection system;explainable AI","Tunneling;Servers;Security;Cryptography;Protocols;Computer crime;Feature extraction","","33","","24","IEEE","15 Jun 2022","","","IEEE","IEEE Journals"
"Pilot Contamination Attack Detection for NOMA in 5G mm-Wave Massive MIMO Networks","N. Wang; L. Jiao; A. Alipour-Fanid; M. Dabaghchian; K. Zeng","Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA","IEEE Transactions on Information Forensics and Security","16 Dec 2019","2020","15","","1363","1378","Power non-orthogonal multiple access (NOMA) has been considered as a new enabling technology in 5G communication. In this paper, we introduce the problem of pilot contamination attack (PCA) on NOMA in millimeter wave (mmWave) and massive MIMO 5G communication. Due to the new characteristics of NOMA such as superposed signals with multi-users, PCA detection faces new challenges. By harnessing the sparseness and statistics of mmWave and massive MIMO virtual channel, we propose two effective PCA detection schemes for NOMA tackling static and dynamic environments, respectively. For the static environment, the problem of PCA detection is formulated as a binary hypothesis test of the virtual channel sparsity. For the dynamic environment, the statistic of the peaks in the virtual channel is leveraged to distinguish the contamination state from the normal state. A peak estimation algorithm and a machine learning based detection framework are proposed to achieve high detection performance. To further optimize the proposed scheme, a feature selection algorithm and an optimization model considering the detection accuracy and detection delay are presented. Simulation results evaluate and confirm the effectiveness of the proposed detection schemes. The detection rate can approach 100% with 10-3 false alarm rate in the static environment and above 95% in the dynamic environment under various system parameters.","1556-6021","","10.1109/TIFS.2019.2939742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825836","Non-orthogonal multiple access (NOMA);mulitple-input and mulitple-output (MIMO);5G mobile communication;millimeter wave communication;network security;physical layer;machine learning","Principal component analysis;NOMA;5G mobile communication;Massive MIMO;Channel estimation;Phase shift keying","","29","","22","IEEE","5 Sep 2019","","","IEEE","IEEE Journals"
"Seeing Traffic Paths: Encrypted Traffic Classification With Path Signature Features","S. -J. Xu; G. -G. Geng; X. -B. Jin; D. -J. Liu; J. Weng","College of Information Science and Technology and the College of Cyber Security, Jinan University, Guangzhou, China; College of Information Science and Technology and the College of Cyber Security, Jinan University, Guangzhou, China; Department of Electrical and Electronic Engineering, Xi’an Jiaotong-Liverpool University, Suzhou, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; College of Information Science and Technology and the College of Cyber Security, Jinan University, Guangzhou, China","IEEE Transactions on Information Forensics and Security","17 Jun 2022","2022","17","","2166","2181","Although many network traffic protection methods have been developed to protect user privacy, encrypted traffic can still reveal sensitive user information with sophisticated analysis. In this paper, we propose ETC-PS, a novel encrypted traffic classification method with path signature. We first construct the traffic path with a session packet length sequence to represent the interactions between the client and the server. Then, path transformations are conducted to exhibit its structure and obtain different information. A multiscale path signature is finally computed as a kind of distinctive feature to train the traditional machine learning classifier, which achieves highly robust accuracy and low training overhead. Six publicly available datasets with different traffic types of HTTPS/1, HTTPS/2, QUIC, VPN, non-VPN, Tor, and non-Tor are used to conduct closed-world and open-world evaluations to verify the effectiveness of ETC-PS. The experimental results demonstrate that ETC-PS is superior to the state-of-the-art methods in terms of accuracy,  $f_{1}$  score, time complexity, and stability.","1556-6021","","10.1109/TIFS.2022.3179955","NSFC(grant numbers:92067108); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011314); National Natural Science Foundation of China(grant numbers:U1804159); “Qing Lan” Project in Jiangsu Universities; Science and Technology Development Fund of Macau(grant numbers:SKL-IOTSC-2018-2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786755","Encrypted traffic classification;path signature feature;machine learning","Cryptography;Feature extraction;Virtual private networks;Ports (computers);Payloads;Privacy;Monitoring","","11","","62","IEEE","2 Jun 2022","","","IEEE","IEEE Journals"
"Cryptomining Cannot Change Its Spots: Detecting Covert Cryptomining Using Magnetic Side-Channel","A. Gangwal; M. Conti","Department of Mathematics, University of Padua, Padua, Italy; Department of Mathematics, University of Padua, Padua, Italy","IEEE Transactions on Information Forensics and Security","23 Jan 2020","2020","15","","1630","1639","With new cryptocurrencies being frequently introduced to the market, the demand for cryptomining - a fundamental operation associated with most of the cryptocurrencies - has initiated a new stream of earning financial gains. The cost associated with the lucrative cryptomining has driven general masses to unethically mine cryptocurrencies using “plundered” resources in the public organizations (e.g., universities) as well as in the corporate sector that follows Bring Your Own Device (BYOD) culture. Such exploitation of the resources causes financial detriment to the affected organizations, which often discover the abuse when the damage has already been done. In this paper, we present a novel approach that leverages magnetic side-channel to detect covert cryptomining. Our proposed approach works even when the examiner does not have login-access or root-privileges on the suspect device. It merely requires the physical proximity of the examiner and a magnetic sensor, which is often available on smartphones. The fundamental idea of our approach is to profile the magnetic field emission of a processor for the set of available mining algorithms. We built a complete implementation of our system using advanced machine learning techniques. In our experiments, we included all the cryptocurrencies supported by the top-10 mining pools, which collectively comprise the largest share (84% during Q3 2018) of the cryptomining market. Moreover, we tested our methodology primarily on two different laptops. By using the data recorded from the magnetometer of an ordinary smartphone, our classifier achieved an average precision of over 88% and an average F1 score of 87%. Apart from our primary goal - which is to identify covert cryptomining - we also performed four additional experiments to further evaluate our approach. We found that due to its underlying design, our system is future-ready and can readily adapt even to zero-day cryptocurrencies.","1556-6021","","10.1109/TIFS.2019.2945171","EU LOCARD Project(grant numbers:H2020-SU-SEC-2018-832735); Huawei Project Secure Remote OTA Updates for In-Vehicle Software Systems(grant numbers:HIRPO 2018040400359-2018); Fondazione Cassa di Risparmio di Padova e Rovigo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854845","Altcoin;Bitcoin;cryptocurrency;detection;machine learning;mining","Cryptography;Smart phones;Time series analysis;Magnetic sensors;Central Processing Unit;Magnetoacoustic effects;Hardware","","10","","46","IEEE","2 Oct 2019","","","IEEE","IEEE Journals"
"Transfer-Path-Based Hardware-Reuse Strong PUF Achieving Modeling Attack Resilience With200 Million Training CRPs","C. Xu; J. Zhang; M. -K. Law; X. Zhao; Pui-In Mak; R. P. Martins","State Key Laboratory of Analog and Mixed-Signal VLSI, Institute of Microelectronics and the Faculty of Science and Technology—ECE, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, Institute of Microelectronics and the Faculty of Science and Technology—ECE, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI, Institute of Microelectronics and the Faculty of Science and Technology—ECE, University of Macau, Macau, China; College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; State Key Laboratory of Analog and Mixed-Signal VLSI, Institute of Microelectronics and the Faculty of Science and Technology—ECE, University of Macau, Macau, China; State Key Laboratory of Analog and Mixed-Signal VLSI and the Faculty of Science and Technology--ECE, University of Macau, Macau, China","IEEE Transactions on Information Forensics and Security","12 Apr 2023","2023","18","","2188","2203","This paper presents a hardware-reuse strong physical unclonable function (PUF) based on the intrinsic transfer paths (TPs) of a conventional digital multiplier to achieve a strong modeling attack resilience. With the multiplier input employed as the PUF challenge and the path delay as the entropy source, all the possible valid propagation paths from distinct input/output pairs can serve as PUF primitives. We can quantize the path delay using a time-to-digital converter (TDC), and select the suitable TDC output bits as the PUF response. We further propose a lightweight dynamic obfuscation algorithm (DOA) and a secure mutual authentication protocol to counteract modeling attacks. The proposed strong PUF using a  $32\times 32$  multiplier as implemented in the Xilinx ZYNQ-7000 SoC features a total of 2048 intrinsic PUF primitives, while achieving a response stream (RS) with an average of 1024 responses per TDC output bit per challenge. With  $Bit(5)$  and  $Bit(6)$  of the TDC output selected for PUF response generation, they demonstrate a measured reliability and uniqueness of up to 98.31% and 49.34%, respectively, with their excellent randomness performance as validated by the NIST SP800-22 tests. Under machine learning (ML)-based modeling attack with artificial neural network (ANN), the measured prediction accuracy of both  $Bit(5)$  and  $Bit(6)$  can still be maintained at  $\sim 50\%$  with a total of >200 million CRPs as the training set.","1556-6021","","10.1109/TIFS.2023.3263621","Macao Science and Technology Development Fund(grant numbers:0011/2019/APJ,0148/2020/A3,SKL-AMSV(UM)-2023-2025); Research Committee of University of Macau(grant numbers:CPG2023-00005-IME); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089859","Physical unclonable function (PUF);transfer path (TP);response stream (RS);field-programmable gate array (FPGA);hardware reuse;machine learning (ML) attack;multiplier","Delays;Logic gates;Physical unclonable function;Training;Reliability;Security;Artificial neural networks","","3","","39","IEEE","31 Mar 2023","","","IEEE","IEEE Journals"
"A Natural Language Processing Approach for Instruction Set Architecture Identification","D. Sahabandu; J. S. Mertoguno; R. Poovendran","Department of Electrical and Computer Engineering, Network Security Laboratory, University of Washington, Seattle, WA, USA; Institute for Information Security and Privacy, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Network Security Laboratory, University of Washington, Seattle, WA, USA","IEEE Transactions on Information Forensics and Security","20 Jul 2023","2023","18","","4086","4099","Binary analysis of software is a critical step in cyber forensics applications such as program vulnerability assessment and malware detection. This involves interpreting instructions executed by software and often necessitates converting the software’s binary file data to assembly language. The conversion process requires information about the binary file’s target instruction set architecture (ISA). However, ISA information might not be included in binary files due to compilation errors, partial downloads, or adversarial corruption of file metadata. Machine learning (ML) is a promising methodology that can be used to identify the target ISA using binary data in the object code section of binary files. In this paper we propose a binary code feature extraction model to improve the accuracy and scalability of ML-based ISA identification methods. Our feature extraction model can be used in the absence of domain knowledge about the ISAs. Specifically, we adapt models from natural language processing (NLP) to i) identify successive byte patterns commonly observed in binary codes, ii) estimate the significance of each byte pattern to a binary file, and iii) estimate the relevance of each byte pattern in distinguishing between ISAs. We introduce character-level features of encoded binaries to identify fine-grained bit patterns inherent to each ISA. We evaluate our approach using two different datasets: binaries from 12 ISAs and 23 ISAs. Empirical evaluations show that using our byte-level features in ML-based ISA identification results in ~98% accuracy compared to the ~91% accuracy of state-of-the-art features based on byte-histograms and byte pattern signatures. We observe that character-level features allow reducing the size of the feature set by up to 16x while maintaining accuracy of ISA identification above 97%.","1556-6021","","10.1109/TIFS.2023.3288456","ONR(grant numbers:N00014-20-1-2636,N00014-23-1-2386); DARPA SSITH(grant numbers:D22AC00123-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159173","Computer architecture;natural language processing(NLP);machine learning (ML)","Feature extraction;Codes;Natural language processing;Computer architecture;Source coding;Registers;Reduced instruction set computing","","1","","61","IEEE","21 Jun 2023","","","IEEE","IEEE Journals"
"No Bot Expects the DeepCAPTCHA! Introducing Immutable Adversarial Examples, With Applications to CAPTCHA Generation","M. Osadchy; J. Hernandez-Castro; S. Gibson; O. Dunkelman; D. Pérez-Cabo","Computer Science Department, University of Haifa, Haifa, Israel; School of Computing, University of Kent, Canterbury, U.K.; School of Physical Sciences, University of Kent, Canterbury, U.K.; Computer Science Department, University of Haifa, Haifa, Israel; Gradiant, Campus Universitario de Vigo, Vigo, Spain","IEEE Transactions on Information Forensics and Security","25 Jul 2017","2017","12","11","2640","2653","Recent advances in deep learning (DL) allow for solving complex AI problems that used to be considered very hard. While this progress has advanced many fields, it is considered to be bad news for Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs), the security of which rests on the hardness of some learning problems. In this paper, we introduce DeepCAPTCHA, a new and secure CAPTCHA scheme based on adversarial examples, an inherit limitation of the current DL networks. These adversarial examples are constructed inputs, either synthesized from scratch or computed by adding a small and specific perturbation called adversarial noise to correctly classified items, causing the targeted DL network to misclassify them. We show that plain adversarial noise is insufficient to achieve secure CAPTCHA schemes, which leads us to introduce immutable adversarial noise-an adversarial noise that is resistant to removal attempts. In this paper, we implement a proof of concept system, and its analysis shows that the scheme offers high security and good usability compared with the best previously existing CAPTCHAs.","1556-6021","","10.1109/TIFS.2017.2718479","U.K. Engineering and Physical Sciences Research Council(grant numbers:EP/M013375/1); Israeli Ministry of Science and Technology(grant numbers:3-11858); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954632","CAPTCHA;deep learning;CNN;adversarial examples;HIP","CAPTCHAs;Security;Robustness;Machine learning;Tools;Usability","","123","","53","CCBY","21 Jun 2017","","","IEEE","IEEE Journals"
"MBTree: Detecting Encryption RATs Communication Using Malicious Behavior Tree","C. Dong; Z. Lu; Z. Cui; B. Liu; K. Chen","Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China","IEEE Transactions on Information Forensics and Security","1 Jul 2021","2021","16","","3589","3603","Network trace signature matching is one reliable approach to detect active Remote Control Trojan, (RAT). Compared to statistical-based detection of malicious network traces in the face of known RATs, the signature-based method can achieve more stable performance and thus more reliability. However, with the development of encrypted technologies and disguise tricks, current methods suffer inaccurate signature descriptions and inflexible matching mechanisms. In this paper, we propose to tackle above problems by presenting MBTree, an approach to detect encryption RATs Command and Control (C&C) communication based on host-level network trace behavior. MBTree first models the RAT network behaviors as the malicious set by automatically building the multiple level tree, MLTree from distinctive network traces of each sample. Then, MBTree employs a detection algorithm to detect malicious network traces that are similar to any MLTrees in the malicious set. To illustrate the effectiveness of our proposed method, we adopt theoretical analysis of MBTree from the probability perspective. In addition, we have implemented MBTree to evaluate it on five datasets which are reorganized in a sophisticated manner for comprehensive assessment. The experimental results demonstrate the accurate and robust of MBTree, especially in the face of new emerging benign applications.","1556-6021","","10.1109/TIFS.2021.3071595","National Key Research and Development Program of China(grant numbers:2019QY1303,2019QY1301); Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDC02040100,NSFC U1836211); Beijing Natural Science Foundation(grant numbers:JQ18011); Youth Innovation Promotion Association CAS; Program of Key Laboratory of Network Assessment Technology; Chinese Academy of Sciences; Program of Beijing Key Laboratory of Network Security and Protection Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398652","Encrypted traffic;malicious traffic;Trojan detection;signature;network behavior;command and control","Rats;Machine learning;Encryption;Feature extraction;Payloads;Manuals;Deep learning","","16","","51","IEEE","7 Apr 2021","","","IEEE","IEEE Journals"
"To Act or Not to Act: An Adversarial Game for Securing Vehicle Platoons","G. Sun; T. Alpcan; B. I. P. Rubinstein; S. Camtepe","Department of Electrical and Electronic Engineering, University of Melbourne, Melbourne, VIC, Australia; Department of Electrical and Electronic Engineering, University of Melbourne, Melbourne, VIC, Australia; Department of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; CSIRO Data61, Epping, NSW, Australia","IEEE Transactions on Information Forensics and Security","17 Nov 2023","2024","19","","163","177","Vehicle platooning systems are vulnerable to malicious attacks that exploit vehicle-to-vehicle (V2V) communication, causing potential instability and increased collision risks. Conventional machine learning (ML) detection methods show promise but can be circumvented by intelligent adversaries. In this paper, we present a novel, end-to-end attack detection and mitigation approach that uniquely incorporates advancements in (adversarial) machine learning, control theory, and game theory. We employ a non-cooperative security game with imperfect information to model complex attack/defense interactions. This aids in making informed decisions regarding detector deployment and attack mitigation, even amidst possibly misleading attack detection reports. We model our control system reconfiguration attack mitigation approach as a switched system and provide a n in-depth stability analysis. The simulations conducted in a sophisticated simulator demonstrate our approach’s potential for real-world online deployment. Our game-based defense formulation significantly improves inter-vehicle distance and defense utilities against both cyber-physical and adversarially-masked attacks while reducing the distance disturbance caused by the ambient traffic by up to 87% compared to baseline defense approaches.","1556-6021","","10.1109/TIFS.2023.3320610","Next Generation Technologies Fund Program in partnership with the Defence Science and Technology Group, Department of Defence, Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO) Data61 Ph.D. Scholarship Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266364","Cyber-physical security;adversarial attack and defense;security game;switched systems;attack detection and mitigation","Games;Machine learning;Detectors;Security;Training;Stability analysis;Control systems","","","","57","IEEE","28 Sep 2023","","","IEEE","IEEE Journals"
"BABD: A Bitcoin Address Behavior Dataset for Pattern Analysis","Y. Xiang; Y. Lei; D. Bao; T. Li; Q. Yang; W. Liu; W. Ren; K. -K. R. Choo","School of Computer Science, China University of Geosciences, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Engineering and Information Technology, The University of Melbourne, Melbourne, VIC, Australia; School of Computer Science, China University of Geosciences, Wuhan, China; NSFOCUS Technologies Group Company Ltd., Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Information Forensics and Security","3 Jan 2024","2024","19","","2171","2185","Cryptocurrencies have dramatically increased adoption in mainstream applications in various fields such as financial and online services, however, there are still a few amounts of cryptocurrency transactions that involve illicit or criminal activities. It is essential to identify and monitor addresses associated with illegal behaviors to ensure the security and stability of the cryptocurrency ecosystem. In this paper, we propose a framework to build a dataset comprising Bitcoin transactions between 12 July 2019 and 26 May 2021. This dataset (hereafter referred to as BABD-13) contains 13 types of Bitcoin addresses, 5 categories of indicators with 148 features, and 544,462 labeled data, which is the largest labeled Bitcoin address behavior dataset publicly available to our knowledge. We also propose a novel and efficient subgraph generation algorithm called BTC-SubGen to extract a  ${k}$ -hop subgraph from the entire Bitcoin transaction graph constructed by the directed heterogeneous multigraph starting from a specific Bitcoin address node. We then conduct 13-class classification tasks on BABD-13 by five machine learning models namely  ${k}$ -nearest neighbors algorithm, decision tree, random forest, multilayer perceptron, and XGBoost, the results show that the accuracy rates are between 93.24% and 97.13%. In addition, we study the relations and importance of the proposed features and analyze how they affect the effect of machine learning models. Finally, we conduct a preliminary analysis of the behavior patterns of different types of Bitcoin addresses using concrete features and find several meaningful and explainable modes.","1556-6021","","10.1109/TIFS.2023.3347894","Yunnan Key Laboratory of Blockchain Application Technology(grant numbers:202105AG070005,YNB202303); CCF-NSFOCUS Kun-Peng Scientific Research Fund(grant numbers:CCF-NSFOCUS2021008); Key Laboratory of Data Protection and Intelligent Management, Ministry of Education, Sichuan University; Fundamental Research Funds for the Central Universities(grant numbers:SCU2023D008); Opening Project of Nanchang Innovation Institute, Peking University(grant numbers:NCII2022A02); Monash International Tuition Scholarship from Monash University; Cloud Technology Endowed Professorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375557","Cryptocurrency;Bitcoin transaction;subgraph generation algorithm;machine learning;behavior pattern","Bitcoin;Behavioral sciences;Feature extraction;Ransomware;Machine learning algorithms;Indexes;Geology","","2","","50","IEEE","28 Dec 2023","","","IEEE","IEEE Journals"
"Privacy-Preserving Deep Learning via Weight Transmission","L. T. Phong; T. T. Phuong","National Institute of Information and Communications Technology (NICT), Tokyo, Japan; National Institute of Information and Communications Technology (NICT), Tokyo, Japan","IEEE Transactions on Information Forensics and Security","2 Jul 2019","2019","14","11","3003","3015","This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method, because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from the existing systems in the following features: 1) any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; 2) gradients computed by SGD are not shared but the weight parameters are shared instead; and 3) robustness against colluding parties even in the extreme case that only one honest party exists. One of our systems requires a shared symmetric key among the data owners (trainers) to ensure the secrecy of the weight parameters against a central server. We prove that our systems, while privacy preserving, achieve the same learning accuracy as SGD and, hence, retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets and show that our systems outperform the previous system in terms of learning accuracies.","1556-6021","","10.1109/TIFS.2019.2911169","Japan Science and Technology Agency(grant numbers:JPMJCR168A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691476","Privacy preservation;stochastic gradient descent;distributed trainers;neural networks","Servers;Cryptography;Deep learning;Data privacy;Neural networks;Privacy","","74","","46","IEEE","14 Apr 2019","","","IEEE","IEEE Journals"
"A Defensive Strategy Against Beam Training Attack in 5G mmWave Networks for Manufacturing","S. Dinh-Van; T. M. Hoang; B. B. Cebecioglu; D. S. Fowler; Y. K. Mo; M. D. Higgins","Warwick Manufacturing Group, School of Engineering, The University of Warwick, Coventry, U.K; Department of Electrical Engineering, University of Colorado Denver, Denver, CO, USA; School of Computing and Digital Technology, Birmingham City University, Birmingham, U.K; Warwick Manufacturing Group, The University of Warwick, Coventry, U.K; Warwick Manufacturing Group, School of Engineering, The University of Warwick, Coventry, U.K; Warwick Manufacturing Group, School of Engineering, The University of Warwick, Coventry, U.K","IEEE Transactions on Information Forensics and Security","13 Apr 2023","2023","18","","2204","2217","Millimeter-wave (mmWave) carriers are an essential building block of fifth-generation (5G) systems. Satisfactory performance of the communications over the mmWave spectrum requires an alignment between the signal beam of the transmitter and receiver, achieved via beam training protocols. Nevertheless, beam training is vulnerable to jamming attacks, where the attacker intends to send jamming signals over different spatial directions to confuse legitimate nodes. This paper focuses on defending against this attack in smart factories where a moving Automated Guided Vehicle (AGV) communicates with a base station via a mmWave carrier. We introduce a defensive strategy to cope with jamming attacks, including two stages: jamming detection and jamming mitigation. Developed based on autoencoders, both algorithms can learn the characteristics/features of the received signals at the AGV. They can be employed consecutively before performing the downlink data transmission. In particular, once a jamming attack is identified, the jamming mitigation can be utilized to retrieve the corrupted received signal strength vector, allowing a better decision during the beam training operation. In addition, the proposed algorithm is straightforward and fully compliant with the existing beam training protocols in 5G New Radio. The numerical results show that not only the proposed defensive strategy can capture more than 80% of attack events, but it also improves the average signal-to-interference-plus-noise-ratio significantly, i.e., up to 15 dB.","1556-6021","","10.1109/TIFS.2023.3265341","Warwick Manufacturing Group Centre High Value Manufacturing Catapult, The University of Warwick, Coventry, U.K; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093907","Attack detection;beam training;beam training attack;machine learning;mmWave;PHY-layer security;5G","Jamming;Training;5G mobile communication;Millimeter wave communication;Wireless communication;Security;Manufacturing","","2","","31","IEEE","6 Apr 2023","","","IEEE","IEEE Journals"
"Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications","B. Flowers; R. M. Buehrer; W. C. Headley","Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA","IEEE Transactions on Information Forensics and Security","5 Dec 2019","2020","15","","1102","1113","Recent advancements in radio frequency machine learning (RFML) have demonstrated the use of raw in-phase and quadrature (IQ) samples for multiple spectrum sensing tasks. Yet, deep learning techniques have been shown, in other applications, to be vulnerable to adversarial machine learning (ML) techniques, which seek to craft small perturbations that are added to the input to cause a misclassification. The current work differentiates the threats that adversarial ML poses to RFML systems based on where the attack is executed from: direct access to classifier input, synchronously transmitted over the air (OTA), or asynchronously transmitted from a separate device. Additionally, the current work develops a methodology for evaluating adversarial success in the context of wireless communications, where the primary metric of interest is bit error rate and not human perception, as is the case in image recognition. The methodology is demonstrated using the well known Fast Gradient Sign Method to evaluate the vulnerabilities of raw IQ based Automatic Modulation Classification and concludes RFML is vulnerable to adversarial examples, even in OTA attacks. However, RFML domain specific receiver effects, which would be encountered in an OTA attack, can present significant impairments to adversarial evasion.","1556-6021","","10.1109/TIFS.2019.2934069","Bradley Masters Fellowship through the Bradley Department of Electrical and Computer Engineering at Virginia Tech; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792120","Cognitive radio security;machine learning;modulation classification","Perturbation methods;Receivers;Transmitters;Wireless communication;Modulation","","77","","34","IEEE","8 Aug 2019","","","IEEE","IEEE Journals"
"Adversarial XAI Methods in Cybersecurity","A. Kuppa; N. -A. Le-Khac","School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland","IEEE Transactions on Information Forensics and Security","25 Oct 2021","2021","16","","4924","4938","Machine Learning methods are playing a vital role in combating ever-evolving threats in the cybersecurity domain. Explanation methods that shed light on the decision process of black-box classifiers are one of the biggest drivers in the successful adoption of these models. Explaining predictions that address ‘Why?/Why Not?’ questions help users/stakeholders/analysts understand and accept the predicted outputs with confidence and build trust. Counterfactual explanations are gaining popularity as an alternative method to help users to not only understand the decisions of black-box models (why?) but also to provide a mechanism to highlight mutually exclusive data instances that would change the outcomes (why not?). Recent Explainable Artificial Intelligence literature has focused on three main areas: (a) creating and improving explainability methods that help users better understand how the internal of ML models work as well as their outputs; (b) attacks on interpreters with a white-box setting; (c) defining the relevant properties, metrics of explanations generated by models. Nevertheless, there is no thorough study of how the model explanations can introduce new attack surfaces to the underlying systems. A motivated adversary can leverage the information provided by explanations to launch membership inference, and model extraction attacks to compromise the overall privacy of the system. Similarly, explanations can also facilitate powerful evasion attacks such as poisoning and back door attacks. In this paper, we cover this gap by tackling various cybersecurity properties and threat models related to counterfactual explanations. We propose a new black-box attack that leverages Explainable Artificial Intelligence (XAI) methods to compromise the confidentiality and privacy properties of underlying classifiers. We validate our approach with datasets and models used in the cyber security domain to demonstrate that our method achieves the attacker’s goal under threat models which reflect the real-world settings.","1556-6021","","10.1109/TIFS.2021.3117075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555622","XAI;cybersecurity;counterfactual explanations;adversarial attacks;poisoning attacks;model stealing;membership inference attacks","Predictive models;Data models;Analytical models;Privacy;Computer security;Password;Numerical models;Adversarial machine learning","","33","","88","CCBYNCND","1 Oct 2021","","","IEEE","IEEE Journals"
"The Best Defense Is a Good Offense: Adversarial Attacks to Avoid Modulation Detection","M. Z. Hameed; A. György; D. Gündüz","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; DeepMind, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Information Forensics and Security","22 Oct 2020","2021","16","","1074","1087","We consider a communication scenario, in which an intruder tries to determine the modulation scheme of the intercepted signal. Our aim is to minimize the accuracy of the intruder, while guaranteeing that the intended receiver can still recover the underlying message with the highest reliability. This is achieved by perturbing channel input symbols at the encoder, similarly to adversarial attacks against classifiers in machine learning. In image classification, the perturbation is limited to be imperceptible to a human observer, while in our case the perturbation is constrained so that the message can still be reliably decoded by the legitimate receiver, which is oblivious to the perturbation. Simulation results demonstrate the viability of our approach to make wireless communication secure against state-of-the-art intruders (using deep learning or decision trees) with minimal sacrifice in the communication performance. On the other hand, we also demonstrate that using diverse training data and curriculum learning can significantly boost the accuracy of the intruder.","1556-6021","","10.1109/TIFS.2020.3025441","Imperial College London President’s Ph.D. Scholarship; European Research Council (ERC) through the Starting Grant BEACON(grant numbers:677854); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201397","Secure communication;deep learning;adversarial attacks;modulation classification","Modulation;Receivers;Transmitters;Perturbation methods;Machine learning;Jamming;Bit error rate","","30","","37","IEEE","21 Sep 2020","","","IEEE","IEEE Journals"
"High Intrinsic Dimensionality Facilitates Adversarial Attack: Theoretical Evidence","L. Amsaleg; J. Bailey; A. Barbe; S. M. Erfani; T. Furon; M. E. Houle; M. Radovanović; X. V. Nguyen","Inria, CNRS, IRISA, Campus de Beaulieu, Univ Rennes, Rennes, France; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Laboratoire de Physique, École Normale Supérieure de Lyon, Lyon, France; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Inria, CNRS, IRISA, Campus de Beaulieu, Univ Rennes, Rennes, France; National Institute of Informatics, Tokyo, Japan; Faculty of Sciences, University of Novi Sad, Novi Sad, Serbia; NVIDIA Corporation, Santa Clara, CA, USA","IEEE Transactions on Information Forensics and Security","7 Oct 2020","2021","16","","854","865","Machine learning systems are vulnerable to adversarial attack. By applying to the input object a small, carefully-designed perturbation, a classifier can be tricked into making an incorrect prediction. This phenomenon has drawn wide interest, with many attempts made to explain it. However, a complete understanding is yet to emerge. In this paper we adopt a slightly different perspective, still relevant to classification. We consider retrieval, where the output is a set of objects most similar to a user-supplied query object, corresponding to the set of k-nearest neighbors. We investigate the effect of adversarial perturbation on the ranking of objects with respect to a query. Through theoretical analysis, supported by experiments, we demonstrate that as the intrinsic dimensionality of the data domain rises, the amount of perturbation required to subvert neighborhood rankings diminishes, and the vulnerability to adversarial attack rises. We examine two modes of perturbation of the query: either `closer' to the target point, or `farther' from it. We also consider two perspectives: `query-centric', examining the effect of perturbation on the query's own neighborhood ranking, and `target-centric', considering the ranking of the query point in the target's neighborhood set. All four cases correspond to practical scenarios involving classification and retrieval.","1556-6021","","10.1109/TIFS.2020.3023274","European(grant numbers:CHIST-ERA ID_IOT); Australian Research Council(grant numbers:DP140101969); ANR-AID Chaire SAIDA; JSPS Kakenhi Kiban (B) Research(grant numbers:18H03296); Serbian National Project(grant numbers:OI174023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194069","Adversarial attack;intrinsic dimensionality;nearest neighbor","Perturbation methods;Feature extraction;Machine learning;Neural networks;Databases;Content-based retrieval;Learning systems","","15","","51","IEEE","10 Sep 2020","","","IEEE","IEEE Journals"
"Sampling Rate Distribution for Flow Monitoring and DDoS Detection in Datacenter","R. Biswas; S. Kim; J. Wu","Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Transactions on Information Forensics and Security","23 Feb 2021","2021","16","","2524","2534","Monitoring all the internal flows in a datacenter is important to protect a victim against internal distributed denial-of-service (DDoS) attacks. Unused virtual machines (VMs) in a datacenter are used as monitors and flows are copied to the monitors from software defined networking (SDN) switches by adding some special rules. In such a system, a VM runs a machine learning method to detect DDoS behavior but it can only process a limited number/amount of flows. When the amount of flows is beyond the capacities of all monitor VMs, the system sub-samples each flow probabilistically. The sampling rate affects the DDoS detection rate of the monitors. Besides, the DDoS detection rates of different types of flows are different for the same sampling rate. A uniform sampling rate might not produce a good overall DDoS detection rate. Assigning different sampling rates to different flows may produce the best result. In this paper, we propose a flow grouping approach based on behavioral similarity among the VMs followed by hierarchical clustering of VMs. The sampling rate is uniform among all the flows in a group. We investigate the relationship between the sampling rate and the DDoS detection rate. Then, we formulate an optimization problem for finding an optimal sampling rate distribution and solve it using mix-integer linear programming. We conduct extensive experiments with Hadoop and Spark and present results that support the feasibility of our model.","1556-6021","","10.1109/TIFS.2021.3054522","NSF(grant numbers:CNS 1824440,CNS 1828363,CNS 1757533,CNS 1618398,CNS 1651947,CNS 1564128); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335605","Botnet;DDoS defense;DDoS;flooding attack;network security;sampling rate distribution","Monitoring;Computer crime;Denial-of-service attack;Servers;Sparks;Probabilistic logic;Machine learning","","14","","23","IEEE","25 Jan 2021","","","IEEE","IEEE Journals"
"SmarPer: Context-Aware and Automatic Runtime-Permissions for Mobile Devices","K. Olejnik; I. Dacosta; J. S. Machado; K. Huguenin; M. E. Khan; J. -P. Hubaux","Raytheon BBN Technologies, Cambridge, MA, USA; EPFL, School of Computer and Communication Sciences (IC), Lausanne, Switzerland; EPFL, School of Computer and Communication Sciences (IC), Lausanne, Switzerland; Faculty of Business and Economics (HEC), UNIL, Lausanne, Switzerland; Rikagaku Kenkyujo Yokohama Campus, Yokohama, Kanagawa, JP; EPFL, School of Computer and Communication Sciences (IC), Lausanne, Switzerland","2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","1058","1076","Permission systems are the main defense that mobile platforms, such as Android and iOS, offer to users to protect their private data from prying apps. However, due to the tension between usability and control, such systems have several limitations that often force users to overshare sensitive data. We address some of these limitations with SmarPer, an advanced permission mechanism for Android. To address the rigidity of current permission systems and their poor matching of users' privacy preferences, SmarPer relies on contextual information and machine learning methods to predict permission decisions at runtime. Note that the goal of SmarPer is to mimic the users' decisions, not to make privacy-preserving decisions per se. Using our SmarPer implementation, we collected 8,521 runtime permission decisions from 41 participants in real conditions. With this unique data set, we show that using an efficient Bayesian linear regression model results in a mean correct classification rate of 80% (±3%). This represents a mean relative reduction of approximately 50% in the number of incorrect decisions when compared with a user-defined static permission policy, i.e., the model used in current permission systems. SmarPer also focuses on the suboptimal trade-off between privacy and utility, instead of only ""allow"" or ""deny"" type of decisions, SmarPer also offers an ""obfuscate"" option where users can still obtain utility by revealing partial information to apps. We implemented obfuscation techniques in SmarPer for different data types and evaluated them during our data collection campaign. Our results show that 73% of the participants found obfuscation useful and it accounted for almost a third of the total number of decisions. In short, we are the first to show, using a large dataset of real in situ permission decisions, that it is possible to learn users' unique decision patterns at runtime using contextual information while supporting data obfuscation, this is an important step towards automating the management of permissions in smartphones.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958625","mobile privacy;machine learning;permission systems;privacy preferences","Runtime;Privacy;Mobile communication;Androids;Humanoid robots;Data models;Smart phones","","57","","58","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Model Stealing Attacks Against Inductive Graph Neural Networks","Y. Shen; X. He; Y. Han; Y. Zhang",Norton Research Group; CISPA Helmholtz Center for Information Security; INRIA; CISPA Helmholtz Center for Information Security,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1175","1192","Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary’s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833607","Helmholtz Association; Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833607","","Privacy;Computational modeling;Machine learning;Benchmark testing;Reconstruction algorithms;Data models;Graph neural networks","","13","","98","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"ALScA: A Framework for Using Auxiliary Learning Side-Channel Attacks to Model PUFs","W. Liu; Y. Zhang; Y. Tang; H. Wang; Q. Wei","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; Zhengzhou Xinda Institute of Advanced Technology, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","IEEE Transactions on Information Forensics and Security","26 Dec 2022","2023","18","","804","817","Physical unclonable functions (PUFs) have emerged as potent hardware primitives owing to their intrinsic properties of being secret key-free, clone-proof, and lightweight. However, PUFs cannot avoid the threats of machine learning modeling and side-channel attacks (SCAs). Nevertheless, almost all attacks neglect the correlations between the mathematical model and side-channel models introduced by PUF internal parameters; thus, such attacks fail to exploit related data and struggle in modeling complex PUFs. To address this problem, we propose a framework for using auxiliary learning SCAs to model strong PUFs by learning multiple related tasks together. Side-channel information predictions are introduced as auxiliary tasks to facilitate the primary task of predicting response. The parameters hard for the primary task to learn can be shared by the auxiliary tasks that learn the same parameters more straightforwardly. Based on the proposed framework, we design a specific auxiliary learning power SCA that employs power level prediction as the auxiliary task. The proposed attack is implemented with the hard-parameter sharing and hierarchy sharing deep neural networks. Experimental results demonstrate that the proposed attack succeeds in modeling XOR APUF, MPUF, and iPUF and outperforms the state-of-the-art methods in modeling MPUF and iPUF. We evaluate the influences of task relatedness, architecture, and loss weight ratio. Furthermore, we propose a fine-grained classification-based method to generate the auxiliary task with an enhanced relationship to the primary task. According to the response, the class corresponding to a specific side-channel state is further divided into two subclasses. Experimental results demonstrate that the generated auxiliary task promotes performance and alleviates the adverse effects of improper architecture and parameters.","1556-6021","","10.1109/TIFS.2022.3227445","National Natural Science Foundation(grant numbers:61871405,61802431); Project of Shanghai Science and Technology Innovation Action Program(grant numbers:22511101300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973338","Auxiliary learning;deep neural network;multi-task;physical unclonable function;side-channel attack","Task analysis;Mathematical models;Side-channel attacks;Computer architecture;Predictive models;Resists;Neural networks","","1","","48","IEEE","7 Dec 2022","","","IEEE","IEEE Journals"
"AISCM-FH: AI-Enabled Secure Communication Mechanism in Fog Computing-Based Healthcare","M. Wazid; A. K. Das; S. Shetty; J. J. P. C. Rodrigues; M. Guizani","Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, India; Center for Security, Theory and Algorithmic Research, International Institute of Information Technology, Hyderabad, India; Department of Modeling, Simulation and Visualization Engineering, Virginia Modeling, Analysis and Simulation Center and the Center for Cybersecurity Education and Research, Old Dominion University, Suffolk, VA, USA; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, United Arab Emirates","IEEE Transactions on Information Forensics and Security","7 Dec 2022","2023","18","","319","334","Fog computing-based Internet of Things (IoT) architecture is useful for various types of delay efficient network communications and services, like digital healthcare. However, there are privacy and security issues with the fog computing-based healthcare systems, which can further increase the risk of leakage of sensitive healthcare data. Therefore, a security mechanism, such as access control for fog computing-based healthcare systems, is needed to protect its data against various potential attacks. Moreover, the blockchain technology can be used to solve the digital healthcare’s data integrity related problems. The use of Artificial Intelligence (AI) further makes the system more effective in case of prediction of health related diseases. In this paper, an AI-enabled secure communication mechanism in fog computing-based healthcare system (in short, AISCM-FH) has been proposed. The security analysis of the proposed AISCM-FH is provided using the standard random oracle model and also with the heuristic (non-mathematical) security analysis. A pragmatic study determines the impact of the proposed AISCM-FH on key performance indicators. Moreover, we include a detailed performance comparison of AISCM-FH with other relevant existing schemes to show that it has low communication and computation costs, and provides superior security and extra functionality attributes as compared to those for other competing existing approaches.","1556-6021","","10.1109/TIFS.2022.3220959","DoD Center of Excellence in AI and Machine Learning (CoE-AIML) through the U.S. Army Research Laboratory(grant numbers:W911NF-20-2-0277); FCT/MCTES through National Funds and co-funded EU Funds(grant numbers:UIDB/50008/2020); Brazilian National Council for Research and Development (CNPq)(grant numbers:313036/2020-9); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943289","Smart healthcare;Internet of Things (IoT);fog computing;security;access control and key management;simulation","Medical services;Internet of Things;Security;Servers;Peer-to-peer computing;Logic gates;Computational modeling","","15","","34","IEEE","9 Nov 2022","","","IEEE","IEEE Journals"
"Reconstructing Training Data with Informed Adversaries","B. Balle; G. Cherubin; J. Hayes",DeepMind; Microsoft Research; DeepMind,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1138","1156","Given access to a machine learning model, can an adversary reconstruct the model’s training data? This work studies this question from the lens of a powerful informed adversary who knows all the training data points except one. By instantiating concrete attacks, we show it is feasible to reconstruct the remaining data point in this stringent threat model. For convex models (e.g. logistic regression), reconstruction attacks are simple and can be derived in closed-form. For more general models (e.g. neural networks), we propose an attack strategy based on training a reconstructor network that receives as input the weights of the model under attack and produces as output the target data point. We demonstrate the effectiveness of our attack on image classifiers trained on MNIST and CIFAR-10, and systematically investigate which factors of standard machine learning pipelines affect reconstruction success. Finally, we theoretically investigate what amount of differential privacy suffices to mitigate reconstruction attacks by informed adversaries. Our work provides an effective reconstruction attack that model developers can use to assess memorization of individual points in general settings beyond those considered in previous works (e.g. generative language models or access to training gradients); it shows that standard models have the capacity to store enough information to enable high-fidelity reconstruction of training data points; and it demonstrates that differential privacy can successfully mitigate such attacks in a parameter regime where utility degradation is minimal.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833677","machine learning;neural networks;reconstruction attacks;differential privacy","Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning;Data models","","14","","70","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"CNN-Based Adversarial Embedding for Image Steganography","W. Tang; B. Li; S. Tan; M. Barni; J. Huang","School of Electronics and Information Technology, Sun Yat-sen University, Guangdong, China; National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","8 May 2019","2019","14","8","2074","2087","Steganographic schemes are commonly designed in a way to preserve image statistics or steganalytic features. Since most of the state-of-the-art steganalytic methods employ a machine learning (ML)-based classifier, it is reasonable to consider countering steganalysis by trying to fool the ML classifiers. However, simply applying perturbations on stego images as adversarial examples may lead to the failure of data extraction and introduce unexpected artifacts detectable by other classifiers. In this paper, we present a steganographic scheme with a novel operation called adversarial embedding (ADV-EMB), which achieves the goal of hiding a stego message while at the same time fooling a convolutional neural network (CNN)-based steganalyzer. The proposed method works under the conventional framework of distortion minimization. In particular, ADV-EMB adjusts the costs of image elements modifications according to the gradients back propagated from the target CNN steganalyzer. Therefore, modification direction has a higher probability to be the same as the inverse sign of the gradient. In this way, the so-called adversarial stego images are generated. Experiments demonstrate that the proposed steganographic scheme achieves better security performance against the target adversary-unaware steganalyzer by increasing its missed detection rate. In addition, it deteriorates the performance of other adversary-aware steganalyzers, opening the way to a new class of modern steganographic schemes capable of overcoming powerful CNN-based steganalysis.","1556-6021","","10.1109/TIFS.2019.2891237","National Natural Science Foundation of China(grant numbers:U1636202,61572329,61872244,61772349); Shenzhen Research Development Program(grant numbers:JCYJ20160328144421330); Alibaba Group through the Alibaba Innovative Research (AIR) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603808","Steganography;steganalysis;adversarial machine learning","Distortion;Perturbation methods;Deep learning;Feature extraction;Minimization;Security;Image coding","","173","","51","OAPA","6 Jan 2019","","","IEEE","IEEE Journals"
"Functionality-Preserving Black-Box Optimization of Adversarial Windows Malware","L. Demetrio; B. Biggio; G. Lagorio; F. Roli; A. Armando","Department of Electrical and Electronic Engineering, PRA Lab, University of Cagliari, Cagliari, Italy; Department of Electrical and Electronic Engineering, PRA Lab, University of Cagliari, Cagliari, Italy; Computer Security Laboratory (CSecLab), University of Genoa, Genoa, Italy; Department of Electrical and Electronic Engineering, PRA Lab, University of Cagliari, Cagliari, Italy; Computer Security Laboratory (CSecLab), University of Genoa, Genoa, Italy","IEEE Transactions on Information Forensics and Security","3 Jun 2021","2021","16","","3469","3478","Windows malware detectors based on machine learning are vulnerable to adversarial examples, even if the attacker is only given black-box query access to the model. The main drawback of these attacks is that: ( i) they are query-inefficient, as they rely on iteratively applying random transformations to the input malware; and ( ii) they may also require executing the adversarial malware in a sandbox at each iteration of the optimization process, to ensure that its intrusive functionality is preserved. In this paper, we overcome these issues by presenting a novel family of black-box attacks that are both query-efficient and functionality-preserving, as they rely on the injection of benign content (which will never be executed) either at the end of the malicious file, or within some newly-created sections. Our attacks are formalized as a constrained minimization problem which also enables optimizing the trade-off between the probability of evading detection and the size of the injected payload. We empirically investigate this trade-off on two popular static Windows malware detectors, and show that our black-box attacks can bypass them with only few queries and small payloads, even when they only return the predicted labels. We also evaluate whether our attacks transfer to other commercial antivirus solutions, and surprisingly find that they can evade, on average, more than 12 commercial antivirus engines. We conclude by discussing the limitations of our approach, and its possible future extensions to target malware classifiers based on dynamic analysis.","1556-6021","","10.1109/TIFS.2021.3082330","Progetti di Rilevante Interesse Nazionale (PRIN) 2017 Project RexLearn through the Italian Ministry of Education, University and Research(grant numbers:2017TWNMH2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437194","Adversarial examples;malware detection;evasion attacks;black-box optimization;machine learning","Malware;Detectors;Optimization;Operating systems;Feature extraction;Payloads;Minimization","","73","","33","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"Fine-Grained Webpage Fingerprinting Using Only Packet Length Information of Encrypted Traffic","M. Shen; Y. Liu; L. Zhu; X. Du; J. Hu","School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, School of Computer Science, Beijing, China; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; School of Engineering and IT, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Information Forensics and Security","4 Feb 2021","2021","16","","2046","2059","Encrypted web traffic can reveal sensitive information of users, such as their browsing behaviors. Existing studies on encrypted traffic analysis focus on website fingerprinting. We claim that fine-grained webpage fingerprinting, which speculates specific webpages on a same website visited by a victim, allows exploiting more user private information, e.g., shopping interests in an online shopping mall. Since webpages from the same website usually have very similar traffic traces that make them indistinguishable, existing solutions may end up with low accuracy. In this paper, we propose FineWP, a novel fine-grained webpage fingerprinting method. We make an observation that the length information of packets in bidirectional client-server interactions can be distinctive features for webpage fingerprinting. The extracted features are then fed into traditional machine learning models to train classifiers, which achieve both high accuracy and low training overhead. We collect two real-world traffic datasets and construct closed- and open-world evaluations to verify the effectiveness of FineWP. The experimental results demonstrate that FineWP is superior to the state-of-the-art methods in terms of accuracy, time complexity and stability.","1556-6021","","10.1109/TIFS.2020.3046876","National Key Research and Development Program of China(grant numbers:2020YFB1006101); Beijing Nova Program(grant numbers:Z201100006820006); NSFC Projects(grant numbers:61972039,61872041); Beijing Natural Science Foundation(grant numbers:4192050); Zhejiang Lab Open Fund(grant numbers:2020AA3AB04); ARC Funds(grant numbers:DP190103660,DP200103207,LP180100663); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305740","Webpage fingerprinting;encrypted traffic classification;machine learning;convolutional neural networks","Feature extraction;Uplink;Servers;Loading;Training;Fingerprint recognition;Protocols","","52","","36","IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks","J. Chen; X. Zhang; R. Zhang; C. Wang; L. Liu","Internet Technology and Engineering Research and Development Center (ITEC), School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Internet Technology and Engineering Research and Development Center (ITEC), School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Internet Technology and Engineering Research and Development Center (ITEC), School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","3 Jun 2021","2021","16","","3412","3425","Machine learning techniques have been widely applied to various applications. However, they are potentially vulnerable to data poisoning attacks, where sophisticated attackers can disrupt the learning procedure by injecting a fraction of malicious samples into the training dataset. Existing defense techniques against poisoning attacks are largely attack-specific: they are designed for one specific type of attacks but do not work for other types, mainly due to the distinct principles they follow. Yet few general defense strategies have been developed. In this paper, we propose De-Pois, an attack-agnostic defense against poisoning attacks. The key idea of De-Pois is to train a mimic model the purpose of which is to imitate the behavior of the target model trained by clean samples. We take advantage of Generative Adversarial Networks (GANs) to facilitate informative training data augmentation as well as the mimic model construction. By comparing the prediction differences between the mimic model and the target model, De-Pois is thus able to distinguish the poisoned samples from clean ones, without explicit knowledge of any ML algorithms or types of poisoning attacks. We implement four types of poisoning attacks and evaluate De-Pois with five typical defense methods on different realistic datasets. The results demonstrate that De-Pois is effective and efficient for detecting poisoned data against all the four types of poisoning attacks, with both the accuracy and F1-score over 0.9 on average.","1556-6021","","10.1109/TIFS.2021.3080522","National Natural Science Foundation of China(grant numbers:61872416,52031009,62002104,62071192); Fundamental Research Funds for the Central Universities of China(grant numbers:2019kfyXJJS017); special fund for Wuhan Yellow Crane Talents (Excellent Young Scholar); fund of Hubei Key Laboratory of Transportation Internet of Things(grant numbers:2019IOT004); National Science Foundation(grant numbers:NSF 2038029,NSF 1564097); IBM faculty award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431105","Machine learning;data poisoning attack;attack-agnostic defense;generative adversarial network","Data models;Training;Testing;Predictive models;Computational modeling;Training data;Task analysis","","44","","43","IEEE","14 May 2021","","","IEEE","IEEE Journals"
"Large-Scale Empirical Study of Important Features Indicative of Discovered Vulnerabilities to Assess Application Security","M. Zhang; X. de Carné de Carnavalet; L. Wang; A. Ragab","Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, QC, Canada; Department of Industrial Electronics and Control Engineering, Menoufia University, Menouf, Egypt","IEEE Transactions on Information Forensics and Security","24 May 2019","2019","14","9","2315","2330","Existing research on vulnerability discovery models shows that the existence of vulnerabilities inside an application may be linked to certain features, e.g., size or complexity, of that application. However, the applicability of such features to demonstrate the relative security between two applications is not well studied, which may depend on multiple factors in a complex way. In this paper, we perform the first large-scale empirical study of the correlation between various features of applications and the abundance of vulnerabilities. Unlike existing work, which typically focuses on one particular application, resulting in limited successes, we focus on the more realistic issue of assessing the relative security level among different applications. To the best of our knowledge, this is the most comprehensive study of 780 real-world applications involving 6498 vulnerabilities. We apply seven feature selection methods to nine feature subsets selected among 34 collected features, which are then fed into six types of machine learning models, producing 523 estimations. The predictive power of important features is evaluated using four different performance measures. This paper reflects that the complexity of applications is not the only factor in vulnerability discovery and the human-related factors contribute to explaining the number of discovered vulnerabilities in an application.","1556-6021","","10.1109/TIFS.2019.2895963","Natural Sciences and Engineering Research Council of Canada(grant numbers:N01035); Vanier Canada Graduate Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629314","Software vulnerability analysis;vulnerability discovery model;software security;machine learning","Feature extraction;Correlation;Measurement;Complexity theory;Software;Predictive models;Security","","23","","49","IEEE","29 Jan 2019","","","IEEE","IEEE Journals"
"Guided Malware Sample Analysis Based on Graph Neural Networks","Y. -H. Chen; S. -C. Lin; S. -C. Huang; C. -L. Lei; C. -Y. Huang","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Information Forensics and Security","20 Jul 2023","2023","18","","4128","4143","Malicious binaries have caused data and monetary loss to people, and these binaries keep evolving rapidly nowadays. With tons of new unknown attack binaries, one essential daily task for security analysts and researchers is to analyze and effectively identify malicious parts and report the critical behaviors within the binaries. While manual analysis is slow and ineffective, automated malware report generation is a long-term goal for malware analysts and researchers. This study moves one step toward the goal by identifying essential functions in malicious binaries to accelerate and even automate the analyzing process. We design and implement an expert system based on our proposed graph neural network called MalwareExpert. The system pinpoints the essential functions of an analyzed sample and visualizes the relationships between involved parts. We evaluate our proposed approach using executable binaries in the Windows operating system. The evaluation results show that our approach has a competitive detection performance (97.3% accuracy and 96.5% recall rate) compared to existing malware detection models. Moreover, it gives an intuitive and easy-to-understand explanation of the model predictions by visualizing and correlating essential functions. We compare the identified essential functions reported by our system against several expert-made malware analysis reports from multiple sources. Our qualitative and quantitative analyses show that the pinpointed functions indicate accurate directions. In the best case, the top 2% of functions reported from the system can cover all expert-annotated functions in three steps. We believe that the MalwareExpert system has shed light on automated program behavior analysis.","1556-6021","","10.1109/TIFS.2023.3283913","National Science and Technology Council (NSTC) of Taiwan(grant numbers:111-2628-E-A49-006-MY2,112-2634-F-A49-001-MBK,112-2218-E-A49-023); Taiwan Academic Cybersecurity Center (TACC) Project at the National Yang Ming Chiao Tung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145856","Graph neural network;machine learning for security;malware analysis;reverse engineering","Malware;Feature extraction;Analytical models;Behavioral sciences;Security;Machine learning;Training","","2","","73","IEEE","7 Jun 2023","","","IEEE","IEEE Journals"
"Detecting AI Trojans Using Meta Neural Analysis","X. Xu; Q. Wang; H. Li; N. Borisov; C. A. Gunter; B. Li",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","103","120","In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks, but they make undesirable assumptions about the attack strategies or require direct access to the trained models, which restricts their utility in practice.This paper addresses these challenges by introducing a Meta Neural Trojan Detection (MNTD) pipeline that does not make assumptions on the attack strategies and only needs black-box access to models. The strategy is to train a meta-classifier that predicts whether a given target model is Trojaned. To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution. We then dynamically optimize a query set together with the meta-classifier to distinguish between Trojaned and benign models.We evaluate MNTD with experiments on vision, speech, tabular data and natural language text datasets, and against different Trojan attacks such as data poisoning attack, model manipulation attack, and latent attack. We show that MNTD achieves 97% detection AUC score and significantly outperforms existing detection approaches. In addition, MNTD generalizes well and achieves high detection performance against unforeseen attacks. We also propose a robust MNTD pipeline which achieves around 90% detection AUC even when the attacker aims to evade the detection with full knowledge of the system.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519467","","Privacy;Pipelines;Neural networks;Natural languages;Machine learning;Predictive models;Data models","","74","","57","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Helen: Maliciously Secure Coopetitive Learning for Linear Models","W. Zheng; R. A. Popa; J. E. Gonzalez; I. Stoica",UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","724","738","Many organizations wish to collaboratively train machine learning models on their combined datasets for a common benefit (e.g., better medical research, or fraud detection). However, they often cannot share their plaintext datasets due to privacy concerns and/or business competition. In this paper, we design and build Helen, a system that allows multiple parties to train a linear model without revealing their data, a setting we call coopetitive learning. Compared to prior secure training systems, Helen protects against a much stronger adversary who is malicious and can compromise m−1 out of m parties. Our evaluation shows that Helen can achieve up to five orders of magnitude of performance improvement when compared to training using an existing state-of-the-art secure multi-party computation framework.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835215","Training;Cryptography;Privacy;Linear-models;Security","Training;Cryptography;Protocols;Organizations;Machine learning;Data models;Convex functions","","64","2","71","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"HEDGE: Efficient Traffic Classification of Encrypted and Compressed Packets","F. Casino; K. -K. R. Choo; C. Patsakis","Department of Informatics, University of Piraeus, Pireas, Greece; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Informatics, University of Piraeus, Pireas, Greece","IEEE Transactions on Information Forensics and Security","27 Jun 2019","2019","14","11","2916","2926","As the size and source of network traffic increase, so does the challenge of monitoring and analyzing network traffic. Therefore, sampling algorithms are often used to alleviate these scalability issues. However, the use of high entropy data streams, through the use of either encryption or compression, further compounds the challenge as current state-of-the-art algorithms cannot accurately and efficiently differentiate between encrypted and compressed packets. In this paper, we propose a novel traffic classification method named High Entropy DistinGuishEr (HEDGE) to distinguish between compressed and encrypted traffic. HEDGE is based on the evaluation of the randomness of the data streams and can be applied to individual packets without the need to have access to the entire stream. The findings from the evaluation show that our approach outperforms current state of the art. We also make available our statistically sound dataset, based on known benchmarks, to the wider research community.","1556-6021","","10.1109/TIFS.2019.2911156","Horizon 2020 Framework Programme(grant numbers:780498); European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH – CREATE – INNOVATE MELITY(grant numbers:T1EDK-01958); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691576","Encryption;high entropy sources;compression;classification;traffic analysis","Entropy;Encryption;Payloads;Benchmark testing;Real-time systems","","51","","61","IEEE","14 Apr 2019","","","IEEE","IEEE Journals"
"iPrivJoin: An ID-Private Data Join Framework for Privacy-Preserving Machine Learning","Y. Liu; B. Zhang; Y. Ma; Z. Ma; Z. Wu","School of Cyber Engineering, Xidian University, Xi’an, China; School of Cyber Engineering, Zhejiang University, Hangzhou, China; Blue Elephant Tech., Hangzhou, China; School of Cyber Engineering, Xidian University, Xi’an, China; School of Cyber Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","24 Jul 2023","2023","18","","4300","4312","The world has observed an increasing trend in the development of Privacy-Preserving Machine Learning (PPML) for cross-silo collaborative model training over sensitive data. As the first essential step of cross-silo PPML, it is critical that the parties can align their dataset with privacy assurance, i.e., private data join. However, the existing private data join methods typically leak the ID information in the dataset intersection, which often raises privacy concerns. In this work, we propose iPrivJoin: a novel framework of ID-private data join for PPML. Compared with naively using circuit-based Private Set Intersection (circuit-PSI) for data join, the proposed framework has two advantages: 1) data volume reduction. iPrivJoin utilizes oblivious shuffle to securely trim off the redundant data that is outside the intersection, while the entire dataset needs to be carried to further process in the circuit-PSI based approach. 2) efficiency improvement. iPrivJoin introduces a new private encoding technique to avoid the expensive circuit evaluation that is needed in circuit-PSI. As a result, compared with directly using circuit-PSI, PPML with iPrivJoin enjoys approximately  $3\times $  of speedup. Moreover, we propose a new oblivious shuffle protocol, which may be of independent interest. It achieves  $1.44\times $  of speedup to the state-of-the-art in the real-world WAN network setting.","1556-6021","","10.1109/TIFS.2023.3288455","National Key Research and Development Program of China(grant numbers:2022YFB3103500); National Natural Science Foundation of China(grant numbers:U21A20464,61872283,62261160651); Natural Science Basic Research Program of Shaanxi Province(grant numbers:2021JC-22); CNKLSTISS; China 111 Project(grant numbers:B16037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159165","Private data join;private set intersection;oblivious shuffle","Data privacy;Training;Encoding;Protocols;Integrated circuit modeling;Cryptography;Privacy","","","","40","IEEE","21 Jun 2023","","","IEEE","IEEE Journals"
"SiRnn: A Math Library for Secure RNN Inference","D. Rathee; M. Rathee; R. K. Kiran Goli; D. Gupta; R. Sharma; N. Chandran; A. Rastogi",Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1003","1020","Complex machine learning (ML) inference algorithms like recurrent neural networks (RNNs) use standard functions from math libraries like exponentiation, sigmoid, tanh, and reciprocal of square root. Although prior work on secure 2-party inference provides specialized protocols for convolutional neural networks (CNNs), existing secure implementations of these math operators rely on generic 2-party computation (2PC) protocols that suffer from high communication. We provide new specialized 2PC protocols for math functions that crucially rely on lookup-tables and mixed-bitwidths to address this performance overhead; our protocols for math functions communicate up to 423× less data than prior work. Furthermore, our math implementations are numerically precise, which ensures that the secure implementations preserve model accuracy of cleartext. We build on top of our novel protocols to build SiRnn, a library for end-to-end secure 2-party DNN inference, that provides the first secure implementations of an RNN operating on time series sensor data, an RNN operating on speech data, and a state-of-the-art ML architecture that combines CNNs and RNNs for identifying all heads present in images. Our evaluation shows that SiRnn achieves up to three orders of magnitude of performance improvement when compared to inference of these models using an existing state-of-the-art 2PC framework.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519413","privacy-preserving machine learning;secure two-party computation;recurrent neural networks;math functions;mixed-bitwidths;secure inference","Privacy;Protocols;Recurrent neural networks;Machine learning algorithms;Time series analysis;Libraries;Magnetic heads","","26","","115","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Applications of Explicit Non-Linear Feature Maps in Steganalysis","M. Boroumand; J. Fridrich","Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA","IEEE Transactions on Information Forensics and Security","3 Jan 2018","2018","13","4","823","833","Currently, the most popular detectors of content-adaptive image steganography are built using machine learning with images represented with rich features. Such high-dimensional descriptors, however, prevent utilization of more complex and potentially more accurate machine learning paradigms, such as kernelized support vector machines, due to infeasibly expensive training. In this paper, we demonstrate that explicit non-linear feature maps coupled with simple classifiers improve the accuracy of current steganalysis detectors built as binary classifiers as well as quantitative detectors in the form of payload regressors. The non-linear map is obtained by approximating a symmetric positive semi-definite kernel on selected pairs of cover features. Exponential forms of kernels derived from symmetrized Ali-Silvey distances improve the detection accuracy of binary detectors and lower the error of quantitative detectors across all tested steganographic schemes on grayscale and color images. The learned non-linear map only weakly depends on the cover source and its learning has a low computational complexity. The technique can also be used for unsupervised feature dimensionality reduction. For payload regressors, the dimensionality can be significantly reduced while simultaneously decreasing the estimation error.","1556-6021","","10.1109/TIFS.2017.2766580","Air Force Office of Scientific Research(grant numbers:FA9550-09-1-0147); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8082553","Steganalysis;adaptive steganography;explicit transformation;Nyström approximation;support vector machine","Kernel;Detectors;Feature extraction;Training;Payloads;Support vector machines;Additives","","20","","52","IEEE","25 Oct 2017","","","IEEE","IEEE Journals"
"Laplacian Smoothing Stochastic ADMMs With Differential Privacy Guarantees","Y. Liu; J. Geng; F. Shang; W. An; H. Liu; Q. Zhu; W. Feng","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Peng Cheng Laboratory, Shenzhen, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Transactions on Information Forensics and Security","18 May 2022","2022","17","","1814","1826","Many machine learning tasks such as structured sparse coding and multi-task learning can be converted into an equality constrained optimization problem. The stochastic alternating direction method of multipliers (SADMM) is a popular algorithm to solve such large-scale problems, and has been successfully used in many real-world applications. However, existing SADMMs fail to take into consideration an important issue in their designs, i.e., protecting sensitive information. To address this challenging issue, this paper proposes a novel differential privacy stochastic ADMM framework for solving equality constrained machine learning problems. In particular, to further lift the utility in privacy-preserving equality constrained optimization, a Laplacian smoothing operation is also introduced into our differential privacy ADMM framework, and it can smooth out the Gaussian noise used in the Gaussian mechanism. Then we propose an efficient differentially private variance reduced stochastic ADMM (DP-VRADMM) algorithm with Laplacian smoothing for both strongly convex and general convex objectives. As a by-product, we also present a new differentially private stochastic ADMM algorithm with DP guarantees. In theory, we provide both private guarantees and utility guarantees for the proposed algorithms, which show that Laplacian smoothing can improve the utility bounds of our algorithms. Experimental results on real-world datasets verify our theoretical results and the effectiveness of our algorithms.","1556-6021","","10.1109/TIFS.2022.3170271","National Natural Science Foundation of China(grant numbers:61876221,61876220,61976164,62072334,61836009); Natural Science Basic Research Program of Shaanxi(grant numbers:2022GY-061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762729","Alternating direction method of multipliers (ADMM);differential privacy;variance reduction;Laplacian smoothing;utility guarantees","Smoothing methods;Laplace equations;Privacy;Differential privacy;Stochastic processes;Optimization;Convergence","","1","","45","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"Efficient Homomorphic Evaluation on Large Intervals","J. H. Cheon; W. Kim; J. H. Park","Department of Mathematical Sciences, Seoul National University, Seoul, South Korea; Department of Mathematical Sciences, Seoul National University, Seoul, South Korea; Department of Mathematical Sciences, Seoul National University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","15 Jul 2022","2022","17","","2553","2568","Homomorphic encryption (HE) is being widely used for privacy-preserving computation. Since HE schemes only support polynomial operations, it is prevalent to use polynomial approximations of non-polynomial functions. We cannot monitor the intermediate values during the homomorphic evaluation; as a consequence, we should utilize polynomial approximations with sufficiently large approximation intervals to prevent the failure of the evaluation. However, the large approximation interval potentially accompanies computational overheads, and it is a serious bottleneck of HE application on real-world data. In this work, we introduce domain extension polynomials (DEPs) that extend the domain interval of functions by a factor of  $k$  while preserving the feature of the original function on its original domain interval. By repeatedly iterating the domain-extension process with DEPs, we can extend with  $O(\log {K})$  operations the domain of a given function by a factor of  $K$  while the feature of the original function is preserved in its original domain interval. By using DEPs, we can efficiently evaluate in an encrypted state a function that converges at infinities, i.e.,  $\lim _{x\to \infty }f(x)$  and  $\lim _{x\to -\infty }f(x)$  exist in  $\mathbb {R}$ . To uniformly approximate the function on  $[-R,R]$ , our method exploits  $O(\log {R})$  operations and  $O(1)$  memory. This is more efficient than the previous approach, the minimax approximation and Paterson-Stockmeyer algorithm, which uses  $\Omega (\sqrt {R})$  multiplications and  $\Omega (\sqrt {R})$  memory for the evaluation. As another application of DEPs, we also suggest a method to manage the risky outliers from a large interval  $[-R,R]$  by using  $O(\log {R})$  additional multiplications. As a real-world application, we trained the logistic regression classifier on large public datasets in an encrypted state by using our method. We exploit our method to the evaluation of the logistic function on large intervals, e.g., [−7683, 7683].","1556-6021","","10.1109/TIFS.2022.3188145","Institute of Information & Communications Technology Planning & Evaluation (IITP) Grant; Korea Government [Ministry of Science and ICT (MSIT)] (Development and Library Implementation of Fully Homomorphic Machine Learning Algorithms Supporting Neural Network Learning Over Encrypted Data)(grant numbers:2020-0-00840); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813691","Homomorphic encryption;composite polynomial approximation;logistic regression","Cryptography;Logistics;Approximation algorithms;Computational efficiency;Machine learning algorithms;Monitoring;Encryption","","","","31","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"Glyph: Efficient ML-Based Detection of Heap Spraying Attacks","F. Pierazzi; S. Cristalli; D. Bruschi; M. Colajanni; M. Marchetti; A. Lanzi","King’s College London, London, U.K.; Computer Science Department, University of Milan, Milan, Italy; Computer Science Department, University of Milan, Milan, Italy; Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy; Computer Science Department, University of Milan, Milan, Italy","IEEE Transactions on Information Forensics and Security","5 Oct 2020","2021","16","","740","755","Heap spraying is probably the most simple and effective memory corruption attack, which fills the memory with malicious payloads and then jumps at a random location in hopes of starting the attacker's routines. To counter this threat, GRAFFITI has been recently proposed as the first OS-agnostic framework for monitoring memory allocations of arbitrary applications at runtime; however, the main contributions of GRAFFITI are on the monitoring system, and its detection engine only considers simple heuristics which are tailored to certain attack vectors and are easily evaded. In this article, we aim to overcome this limitation and propose GLYPH as the first ML-based heap spraying detection system, which is designed to be effective, efficient, and resilient to evasive attackers. GLYPH relies on the information monitored by GRAFFITI, and we investigate the effectiveness of different feature spaces based on information entropy and memory n-grams, and discuss the several engineering challenges we have faced to make GLYPH efficient with an overhead compatible with that of GRAFFITI. To evaluate GLYPH, we build a representative dataset with several variants of heap spraying attacks, and assess GLYPH's resilience against evasive attackers through selective hold-out experiments. Results show that GLYPH achieves high accuracy in detecting spraying and is able to generalize well, outperforming the state-of-the-art approach for heap spraying detection, NOZZLE. Finally, we thoroughly discuss the trade-offs between detection performance and runtime overhead of GLYPH's different configurations.","1556-6021","","10.1109/TIFS.2020.3017925","Italian Ministry of Foreign Affairs and International Cooperation(grant numbers:PGR00814); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171343","Heap spraying;memory exploitation;machine learning;memory monitoring;detection","Spraying;Monitoring;Runtime;Resource management;Operating systems;Feature extraction;Engines","","3","","61","IEEE","19 Aug 2020","","","IEEE","IEEE Journals"
"XMD: An Expansive Hardware-Telemetry-Based Mobile Malware Detector for Endpoint Detection","H. Kumar; B. Chakraborty; S. Sharma; S. Mukhopadhyay","Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","9 Oct 2023","2023","18","","5906","5919","Hardware-based Malware Detectors (HMDs) have shown promise in detecting malicious workloads. However, the current HMDs focus solely on the CPU core of a System-on-Chip (SoC) and, therefore, do not exploit the full potential of the hardware telemetry. In this paper, we propose XMD, an HMD that uses an expansive set of telemetry channels extracted from the different subsystems of SoC. XMD exploits the thread-level profiling power of the CPU-core telemetry, and the global profiling power of non-core telemetry channels, to achieve significantly better detection performance than currently used Hardware Performance Counter (HPC) based detectors. We leverage the concept of manifold hypothesis to analytically prove that adding non-core telemetry channels improves the separability of the benign and malware classes, resulting in performance gains. We train and evaluate XMD using hardware telemetries collected from 723 benign applications and 1033 malware samples on a commodity Android Operating System (OS)-based mobile device. XMD improves over currently used HPC-based detectors by 32.91% for the in-distribution test data. XMD achieves the best detection performance of 86.54% with a false positive rate of 2.9%, compared to the detection rate of 80%, offered by the best performing signature-based Anti-Virus(AV) on VirusTotal, on the same set of malware samples.","1556-6021","","10.1109/TIFS.2023.3318969","Semiconductor Research Corporation and TxACE(grant numbers:#2810.002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262063","Malware detection;machine learning;security;Android OS","Telemetry;Hardware;Performance evaluation;Malware;Behavioral sciences;Resists;Monitoring","","","","58","IEEE","25 Sep 2023","","","IEEE","IEEE Journals"
"Machine Unlearning via Representation Forgetting With Parameter Self-Sharing","W. Wang; C. Zhang; Z. Tian; S. Yu","School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Information Forensics and Security","30 Nov 2023","2024","19","","1099","1111","Machine unlearning enables data owners to remove the contribution of their specified samples from trained models. However, existing methods fail to strike an optimal balance between erasure effectiveness and model utility preservation. Previous studies focused on removing the impact of user-specified data from the model as much as possible to implement unlearning. These methods usually result in significant model utility degradation, commonly called catastrophic unlearning. To address the issue, we systematically consider machine unlearning and formulate it as a two-objective optimization problem that involves forgetting the erased data and retaining the previously learned knowledge, highlighting accuracy preservation during the unlearning process. We propose an unlearning method called representation-forgetting unlearning with parameter self-sharing (RFU-SS) to achieve the two-objective unlearning goal. Firstly, we design a representation-forgetting unlearning (RFU) method that aims to remove the contribution of specified samples from a trained representation by minimizing the mutual information between the representation and the erased data. The representation is learned using the information bottleneck (IB) method. RFU is tailored to the IB structure models for ease of introduction. Secondly, we customize a parameter self-sharing structural optimization method for RFU (i.e., RFU-SS) to simultaneously optimize the forgetting and retention objectives to find the optimal balance. Extensive experimental results demonstrate a significant effectiveness improvement of RFU-SS over the state-of-the-art methods. RFU-SS almost eliminates catastrophic unlearning, reducing model accuracy degradation from over 6% to less than 0.2% on the MNIST dataset with an even better removal effect. The source code is available at https://github.com/wwq5-code/RFU-SS.git.","1556-6021","","10.1109/TIFS.2023.3331239","Australia(grant numbers:ARC DP200101374,LP190100676); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312776","Machine unlearning;representation forgetting;multi-objective optimization;machine learning","Data models;Training;Degradation;Optimization;Computational modeling;Mutual information;Task analysis","","","","45","IEEE","8 Nov 2023","","","IEEE","IEEE Journals"
"Machine Learning-Based Delay-Aware UAV Detection and Operation Mode Identification Over Encrypted Wi-Fi Traffic","A. Alipour-Fanid; M. Dabaghchian; N. Wang; P. Wang; L. Zhao; K. Zeng","Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Computer Science, Morgan State University, Baltimore, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Information Science and Technology, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA","IEEE Transactions on Information Forensics and Security","10 Feb 2020","2020","15","","2346","2360","The consumer unmanned aerial vehicle (UAV) market has grown significantly over the past few years. Despite its huge potential in spurring economic growth by supporting various applications, the increase of consumer UAVs poses potential risks to public security and personal privacy. To minimize the risks, efficiently detecting and identifying invading UAVs is in urgent need for both invasion detection and forensics purposes. Aiming to complement the existing physical detection mechanisms, we propose a machine learning-based framework for fast UAV identification over encrypted Wi-Fi traffic. It is motivated by the observation that many consumer UAVs use Wi-Fi links for control and video streaming. The proposed framework extracts features derived only from packet size and inter-arrival time of encrypted Wi-Fi traffic, and can efficiently detect UAVs and identify their operation modes. In order to reduce the online identification time, our framework adopts a re-weighted ℓ1-norm regularization, which considers the number of samples and computation cost of different features. This framework jointly optimizes feature selection and prediction performance in a unified objective function. To tackle the packet inter-arrival time uncertainty when optimizing the trade-off between the detection accuracy and delay, we utilize maximum likelihood estimation (MLE) method to estimate the packet inter-arrival time. We collect a large number of real-world Wi-Fi data traffic of eight types of consumer UAVs and conduct extensive evaluation on the performance of our proposed method. Evaluation results show that our proposed method can detect and identify tested UAVs within 0.15-0.35s with high accuracy of 85.7-95.2%. The UAV detection range is within the physical sensing range of 70m and 40m in the line-of-sight (LoS) and non-line-of-sight (NLoS) scenarios, respectively. The operation mode of UAVs can be identified with high accuracy of 88.5-98.2%.","1556-6021","","10.1109/TIFS.2019.2959899","National Security Agency(grant numbers:H98230-16-1-0356,H98230-18-1-0343); Virginia’s Commonwealth Cyber Initiative (CCI); National Science Foundation(grant numbers:1755850,1841520,1907805); Jeffress Trust Award; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933072","Unmanned aerial vehicle (UAV) detection;machine learning;encrypted Wi-Fi traffic classification","Unmanned aerial vehicles;Wireless fidelity;Feature extraction;Cryptography;Delays;Radar detection","","44","","39","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"A Machine Learning-Based Digital Twin for Anti-Counterfeiting Applications With Copy Detection Patterns","Y. Belousov; G. Quétant; B. Pulfer; R. Chaban; J. Tutt; O. Taran; T. Holotyak; S. Voloshynovskiy","Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland; Centre Universitaire d’Informatique, Department of Computer Science, University of Geneva, Geneva, Switzerland","IEEE Transactions on Information Forensics and Security","13 Feb 2024","2024","19","","3395","3408","In this paper, we present a new approach to model a printing-imaging channel using a machine learning-based “digital twin” for copy detection patterns (CDP). The CDP are considered as modern anti-counterfeiting features in multiple applications. Our digital twin is formulated within the information-theoretic framework of TURBO initially developed for high energy physics simulations, using variational approximations of mutual information for both encoder and decoder in the bidirectional exchange of information. This model extends various architectural designs, including paired pix2pix and unpaired CycleGAN, for image-to-image translation. Applicable to any type of printing and imaging devices, the model needs only training data comprising digital templates sent to a printing device and data acquired by an imaging device. The data can be paired, unpaired, or hybrid, ensuring architectural flexibility and scalability for multiple practical setups. We explore the influence of various architectural factors, metrics, and discriminators on the overall system’s performance in generating and predicting printed CDP from their digital versions and vice versa. We also performed a comparison with several state-of-the-art methods for image-to-image translation applications. The simulation code and extended results are publicly available at https://gitlab.unige.ch/sip-group/digital-twin.","1556-6021","","10.1109/TIFS.2024.3361798","Swiss National Science Foundation (SNSF), Information-theoretic analysis of deep identification systems(grant numbers:200021_182063); SNSF Sinergia, Robust Deep Density Models for High-Energy Particle Physics and Solar Flare Analysis (RODEM)(grant numbers:CRSII5_193716); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418902","Copy detection patterns;machine learning;digital twin;information theory;variational approximation","Imaging;Authentication;Printing;Digital twins;Mathematical models;Training;Semiconductor device modeling","","","","62","IEEE","2 Feb 2024","","","IEEE","IEEE Journals"
"Learning from Mutants: Using Code Mutation to Learn and Monitor Invariants of a Cyber-Physical System","Y. Chen; C. M. Poskitt; J. Sun","Singapore University of Technology and Design, Singapore, Singapore; Singapore University of Technology and Design, Singapore, Singapore; Singapore University of Technology and Design, Singapore, Singapore","2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","648","660","Cyber-physical systems (CPS) consist of sensors, actuators, and controllers all communicating over a network; if any subset becomes compromised, an attacker could cause significant damage. With access to data logs and a model of the CPS, the physical effects of an attack could potentially be detected before any damage is done. Manually building a model that is accurate enough in practice, however, is extremely difficult. In this paper, we propose a novel approach for constructing models of CPS automatically, by applying supervised machine learning to data traces obtained after systematically seeding their software components with faults (""mutants""). We demonstrate the efficacy of this approach on the simulator of a real-world water purification plant, presenting a framework that automatically generates mutants, collects data traces, and learns an SVM-based model. Using cross-validation and statistical model checking, we show that the learnt model characterises an invariant physical property of the system. Furthermore, we demonstrate the usefulness of the invariant by subjecting the system to 55 network and code-modification attacks, and showing that it can detect 85% of them from the data logs generated at runtime.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418629","cyber physical systems;water treatment systems;invariants;anomaly detection;attestation;system modelling;machine learning;mutation testing;attacks","Sensors;Software;Actuators;Data models;Feature extraction;Monitoring;Model checking","","86","","48","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"F-BLEAU: Fast Black-Box Leakage Estimation","G. Cherubin; K. Chatzikokolakis; C. Palamidessi","EPFL; University of Athens; INRIA, École Polytechnique","2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","835","852","We consider the problem of measuring how much a system reveals about its secret inputs. We work in the black-box setting: we assume no prior knowledge of the system's internals, and we run the system for choices of secrets and measure its leakage from the respective outputs. Our goal is to estimate the Bayes risk, from which one can derive some of the most popular leakage measures (e.g., min-entropy leakage). The state-of-the-art method for estimating these leakage measures is the frequentist paradigm, which approximates the system's internals by looking at the frequencies of its inputs and outputs. Unfortunately, this does not scale for systems with large output spaces, where it would require too many input-output examples. Consequently, it also cannot be applied to systems with continuous outputs (e.g., time side channels, network traffic). In this paper, we exploit an analogy between Machine Learning (ML) and black-box leakage estimation to show that the Bayes risk of a system can be estimated by using a class of ML methods: the universally consistent learning rules; these rules can exploit patterns in the input-output examples to improve the estimates' convergence, while retaining formal optimality guarantees. We focus on a set of them, the nearest neighbor rules; we show that they significantly reduce the number of black-box queries required for a precise estimation whenever nearby outputs tend to be produced by the same secret; furthermore, some of them can tackle systems with continuous outputs. We illustrate the applicability of these techniques on both synthetic and real-world data, and we compare them with the state-of-the-art tool, leakiEst, which is based on the frequentist approach.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835250","quantitative-information-flow;estimation;security-bounds;machine-learning;leakage;privacy;side-channels","Maximum likelihood estimation;Tools;Frequency measurement;Convergence;Privacy","","14","","37","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis","M. Fan; W. Wei; X. Xie; Y. Liu; X. Guan; T. Liu","School of Cyber Science and Engineering, MoEKLINNS, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, MoEKLINNS, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Cyber Science and Engineering, MoEKLINNS, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, MoEKLINNS, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Information Forensics and Security","5 Oct 2020","2021","16","","838","853","With the rapid growth of Android malware, many machine learning-based malware analysis approaches are proposed to mitigate the severe phenomenon. However, such classifiers are opaque, non-intuitive, and difficult for analysts to understand the inner decision reason. For this reason, a variety of explanation approaches are proposed to interpret predictions by providing important features. Unfortunately, the explanation results obtained in the malware analysis domain cannot achieve a consensus in general, which makes the analysts confused about whether they can trust such results. In this work, we propose principled guidelines to assess the quality of five explanation approaches by designing three critical quantitative metrics to measure their stability, robustness, and effectiveness. Furthermore, we collect five widely-used malware datasets and apply the explanation approaches on them in two tasks, including malware detection and familial identification. Based on the generated explanation results, we conduct a sanity check of such explanation approaches in terms of the three metrics. The results demonstrate that our metrics can assess the explanation approaches and help us obtain the knowledge of most typical malicious behaviors for malware analysis.","1556-6021","","10.1109/TIFS.2020.3021924","National Key Research and Development Program of China(grant numbers:2016YFB1000903); National Natural Science Foundation of China(grant numbers:61902306,61532004,61532015,61632015,61602369,U1766215,61772408,61702414,61833015); China Postdoctoral Science Foundation(grant numbers:2019TQ0251,2020M673439); Innovative Research Group of the National Natural Science Foundation of China(grant numbers:61721002); Ministry of Education Innovation Research Team (IRT_17R86); Chinese academy of engineering “The Online and Offline Mixed Educational Service System for ‘The Belt and Road’ Training in MOOC China” and Project of China Knowledge Centre for Engineering Science and Technology; Key Research Program of State Grid Shaanxi Electric Power Company; National Research Foundation, Prime Minister’s Office, Singapore under its National Cybersecurity Research and Development Program, NRF Investigatorship(grant numbers:NRF2018NCR-NCR005-0001,NRFI06-2020-0022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186721","Android malware;explanation approaches;stability;robustness;effectiveness","Malware;Measurement;Androids;Humanoid robots;Perturbation methods;Analytical models;Robustness","","27","","49","IEEE","4 Sep 2020","","","IEEE","IEEE Journals"
"COPPTCHA: COPPA Tracking by Checking Hardware-Level Activity","K. Basu; S. S. Hussain; U. Gupta; R. Karri","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, USA; Institute for Information Security and Privacy, Georgia Institute of Technology, Atlanta, USA; Intel Corporation, Santa Clara, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, USA","IEEE Transactions on Information Forensics and Security","22 Apr 2020","2020","15","","3213","3226","User privacy is an extremely important concern for mobile applications. Recently, the Federal Trade Commission (FTC) has penalized multiple mobile application developers, such as TikTok and BabyBus for violating privacy regulations. Privacy concerns are more critical for children, who do not comprehend the risks associated with transmitting private information like geospatial location. The Children's Online Privacy Protection Act (COPPA) is an online privacy regulation platform to monitor data usage by mobile applications designed for children. Existing research on detecting whether an application complies with certain privacy regulations is performed either by analyzing the application binary or by dynamic monitoring of network at runtime. However, as explained in related work, both methods have their respective demerits. We propose COPPTCHA, a Hardware performance counter (HPC)-based technique to detect whether a children's app abides by the COPPA regulations. HPCs are special purpose registers found in all processors that measure system level events. Since the proposed method is hardware-based, it is difficult to undermine it compared to software-based COPPA compliance detection. COPPTCHA has no hardware overhead, since HPC data collection is integral to all industry standard processors. The HPC readings of applications running on a smartphone are classified using machine learning based classifiers to detect COPPA compliance. Our experiments employing a Moto-G4 smartphone shows that COPPTCHA can detect COPPA-violating apps with ≥ 99% accuracy.","1556-6021","","10.1109/TIFS.2020.2983287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049424","Social factors;Demography;Technology Social factors;Privacy","Privacy;Advertising;Smart phones;Mobile applications;Regulation;Standards;Monitoring","","8","","47","IEEE","27 Mar 2020","","","IEEE","IEEE Journals"
"Multiclass Classification-Based Side-Channel Hybrid Attacks on Strong PUFs","W. Liu; R. Wang; X. Qi; L. Jiang; J. Jing","State Key Laboratory of Mathematic Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematic Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematic Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematic Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematic Engineering and Advanced Computing, Zhengzhou, China","IEEE Transactions on Information Forensics and Security","15 Mar 2022","2022","17","","924","937","Physical unclonable functions (PUFs) are promising solutions for low-cost device authentication; hence, ignoring the security of PUFs is becoming increasingly difficult. Generally, strong PUFs are vulnerable to classical machine learning (ML) attacks; however, classical ML attacks do not perform well on strong PUFs with complex structures. Side-channel analysis (SCA) hybrid attacks provide efficient approaches to modeling XOR APUF. However, owing to the inadequate exploitation of all available data, recent SCA hybrid attacks may fail on novel PUF designs, such as MPUF and iPUF. Thus, herein, we introduce a method that combines challenge-response pairs with side-channel information to construct challenge-synthetic-feature pairs (CSPs) via feature cross, thereby making it possible to model strong PUFs through multiclass classification. We propose multiclass classification-based SCA hybrid attacks to model strong PUFs with complex structures. When provided with CSPs, the proposed hybrid attacks use a feed-forward neural network with a softmax activation function to build combined models of PUFs. The combined models predict class labels for given challenges and then reveal responses through simple mappings from these labels. Experimental results show that the proposed attacks could model 16-XOR APUF, (128,5)-MPUF, (8,8)-iPUF, and (2,16)-iPUF with accuracies exceeding 94%. Compared with state-of-the-art modeling techniques, the proposed attack has advantages in terms of modeling accuracy, time cost, and the size of required training data.","1556-6021","","10.1109/TIFS.2022.3152393","National Natural Science Foundation(grant numbers:61871405,61802431); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732975","Physical unclonable function;side-channel analysis;multiclass classification;synthetic feature;feed-forward neural network","Mathematical models;Reliability;Data models;Neural networks;Security;Complexity theory;Analytical models","","6","","43","IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"An Evaluation Method of the Anti-Modeling-Attack Capability of PUFs","Y. Chen; X. Cui; Y. Liu; X. Cui","Key Laboratory of Integrated Microsystems, Peking University Shenzhen Graduate School, Shenzhen, China; Key Laboratory of Integrated Microsystems, Peking University Shenzhen Graduate School, Shenzhen, China; Key Laboratory of Integrated Microsystems, Peking University Shenzhen Graduate School, Shenzhen, China; Institute of Microelectronics, Peking University, Beijing, China","IEEE Transactions on Information Forensics and Security","21 Mar 2023","2023","18","","1773","1788","The physical unclonable function (PUF) is regarded as the root of trust of hardware systems. However, it suffers from the modeling attacks based on machine learning (ML) algorithms. Subsequently, the anti-modeling-attack PUF is of great concern from academia and industries in recent years. In practice, the security of a given PUF is evaluated after the pertinent attacks. However, these evaluation methods are not helpful for the PUF design, because the relationship between the PUF structure and the anti-modeling-attack capability is not established explicitly. This work proposes a security evaluation method of PUF based on the mimic attack and the Probably Approximately Correct (PAC) theory. The anti-modeling-attack capability of PUF is measured by the corresponding area enclosed by the evaluation curve. Twenty representative types of PUFs with different sizes are evaluated by the proposed method. It shows that the proposed method is effective, because the evaluation results are consistent with the difficulties of modeling attacks for the corresponding PUFs in the design practices. And the evaluation results are able to assist the PUF design.","1556-6021","","10.1109/TIFS.2023.3254434","Shenzhen Science and Technology Innovation Committee(grant numbers:JCYJ20220818100814033,KQTD20200820113105004); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010155002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064080","PUF;modeling attack;ANN;evaluation model","Physical unclonable function;Numerical models;Security;Training;Support vector machines;Predictive models;Picture archiving and communication systems","","","","46","IEEE","8 Mar 2023","","","IEEE","IEEE Journals"
"Secure Distributed Computing With Straggling Servers Using Polynomial Codes","H. Yang; J. Lee","Communications and Machine Learning Laboratory, Seoul National University, Seoul, South Korea; Communications and Machine Learning Laboratory, Seoul National University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","23 Jul 2018","2019","14","1","141","150","In this paper, we consider a secure distributed computing scenario in which a master wants to perform matrix multiplication of confidential inputs with multiple workers in parallel. In such a setting, a master does not want to reveal information about the two input matrices to the workers in an information-theoretic sense. We propose a secure distributed computing scheme that can efficiently cope with straggling effects by applying polynomial codes on sub-tasks assigned to workers. The achievable recovery threshold, i.e., the number of workers that a master needs to wait for to get the final product, of our proposed scheme is revealed to be order-optimal to the number of workers. Moreover, we derive the achievable recovery threshold of the proposed scheme is within a constant multiplicative factor from information-theoretic lower bound. As a byproduct, we extend our strategy to secure distributed computing for convolution tasks on confidential data.","1556-6021","","10.1109/TIFS.2018.2846601","Basic Science Research Program through NRF; MSIP(grant numbers:NRF-2017R1A2B2007102); Technology Innovation Program through MOTIE(grant numbers:10051928); Bio-Mimetic Robot Research Center through DAPA(grant numbers:UD130070ID); INMAC; BK21-plus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382305","Distributed computing;data security;polynomial codes","Task analysis;Convolution;Distributed databases;Computational modeling;Manganese;Data security","","80","","29","OAPA","12 Jun 2018","","","IEEE","IEEE Journals"
"Private Information Retrieval for Secure Distributed Storage Systems","H. Yang; W. Shin; J. Lee","Communications and Machine Learning Laboratory, Seoul National University, Seoul, South Korea; Department of Electronics Engineering, Pusan National University, Busan, South Korea; Communications and Machine Learning Laboratory, Seoul National University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","5 Jun 2018","2018","13","12","2953","2964","In this paper, we investigate a private information retrieval (PIR) problem for secure distributed storage systems in the presence of an eavesdropper. We design the secure distributed database and the corresponding PIR scheme, which protect not only user privacy (concealing the index of the desired message) from the databases, but also data security (concealing the messages themselves) from an eavesdropper. In our proposed scheme, we use a secret sharing scheme in storing the messages for data security at each of the databases. We consider two different scenarios on whether the databases are aware of the index sets of the secret shares stored in other databases. The key idea in designing an efficient PIR procedure is to exploit the secret shares of undesired messages as a side information by means of storing the secret shares at multiple databases. In particular, it is shown that the rates of the proposed PIR schemes are within a constant multiplicative factor from the derived upper-bound on the capacity of PIR problem.","1556-6021","","10.1109/TIFS.2018.2833050","Basic Science Research Program through NRF, MSIP(grant numbers:NRF-2017R1A2B2007102); Technology Innovation Program through MOTIE(grant numbers:10051928); Bio-Mimetic Robot Research Center through DAPA(grant numbers:UD130070ID); INMAC; BK21-plus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8353857","Private information retrieval;data security;user privacy;distributed storage system","Cryptography;Distributed databases;Data privacy;Secure storage;Information retrieval","","61","","30","IEEE","3 May 2018","","","IEEE","IEEE Journals"
"Private Coded Matrix Multiplication","M. Kim; H. Yang; J. Lee","Department of Electrical and Computer Engineering, Communications and Machine Learning Lab., Seoul National University, Seoul, South Korea; School of Electronic Engineering, Kumoh National Institute of Technology, Gumi, South Korea; Department of Electrical and Computer Engineering, Communications and Machine Learning Lab., Seoul National University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","14 Aug 2020","2020","15","","1434","1443","In distributed computing system for the master-worker framework, an erasure code is able to mitigate the effects of slow workers, also called stragglers. The distributed computing system combined with coding is referred to as coded computation. For a matrix multiplication, we consider a variation of coded computation that ensures the master's privacy from the workers, which is referred to as private coded matrix multiplication. In the private coded matrix multiplication, the master needs to compute a matrix multiplication on its own matrix and one of the matrices in a library exclusively shared by the external workers. After the master recovers the matrix multiplication through coded matrix multiplication, the workers should not know which matrix in the library was desired by the master, which implies that the master's privacy is ensured. Our problem is a special case of linear private computation, where a linear combination of matrices in the library should be concealed. We propose a private coded matrix multiplication scheme, based on the conventional coded matrix multiplication scheme. In terms of computation time and communication load, we compare our proposed scheme with a conventional robust private information retrieval scheme and private computation schemes.","1556-6021","","10.1109/TIFS.2019.2940895","National Research Foundation of Korea(grant numbers:NRF-2017R1A2B2007102); Ministry of Science and ICT (Korea Government); Technology Innovation Program(grant numbers:10051928); Ministry of Trade, Industry and Energy; Bio-Mimetic Robot Research Center; Defense Acquisition Program Administration(grant numbers:UD130070ID); Samsung Electronics AI Grant; Ministry of Trade, Industry and Energy (Korea Government)-Institute of Information and Communications Technology Planning and Evaluation (Korea)(grant numbers:2019-0-01367 (BabyMind)); Institute of New Media And Communications; BK21-plus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832193","Distributed computing;coded computation;private information retrieval;erasure codes;information theory;privacy","Libraries;Privacy;Encoding;Distributed computing;Data privacy;Artificial intelligence;Databases","","25","","32","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"A Novel Serial Multimodal Biometrics Framework Based on Semisupervised Learning Techniques","Q. Zhang; Y. Yin; D. -C. Zhan; J. Peng","Machine Learning and Data Mining Laboratory, Shandong University, Jinan, China; Machine Learning and Data Mining Laboratory, Shandong University, Jinan, China; National Key Laboratory for Novel Software Technology, Lamda Group, Nanjing University, Nanjing, China; School of Computer Science and Technology, Shandong University, Jinan, China","IEEE Transactions on Information Forensics and Security","19 May 2017","2014","9","10","1681","1694","We propose in this paper a novel framework for serial multimodal biometric systems based on semisupervised learning techniques. The proposed framework addresses the inherent issues of user inconvenience and system inefficiency in parallel multimodal biometric systems. Further, it advances the serial multimodal biometric systems by promoting the discriminating power of the weaker but more user convenient trait(s) and saving the use of the stronger but less user convenient trait(s) whenever possible. This is in contrast to other existing serial multimodal biometric systems that suggest optimized orderings of the traits deployed and parameterizations of the corresponding matchers but ignore the most important requirements of common applications. In terms of methodology, we propose to use semisupervised learning techniques to strengthen the matcher(s) on the weaker trait(s), utilizing the coupling relationship between the weaker and the stronger traits. A dimensionality reduction method for the weaker trait(s) based on dependence maximization is proposed to achieve this purpose. Experiments on two prototype systems clearly demonstrate the advantages of the proposed framework and methodology.","1556-6021","","10.1109/TIFS.2014.2346703","National Natural Science Foundation of China(grant numbers:61070097,61173069,61105043); Shandong Natural Science Funds for Distinguished Young Scholar(grant numbers:JQ201316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6874510","Serial multimodal biometrics;user convenience;semi-supervised learning;dimensionality reduction","Biometrics (access control);Feature extraction;Face;Face recognition;Authentication;Reliability;Educational institutions","","23","","53","OAPA","8 Aug 2014","","","IEEE","IEEE Journals"
"Bottlenecks CLUB: Unifying Information-Theoretic Trade-Offs Among Complexity, Leakage, and Utility","B. Razeghi; F. P. Calmon; D. Gunduz; S. Voloshynovskiy","Department of Computer Science, University of Geneva, Geneva, Switzerland; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K; Department of Computer Science, University of Geneva, Geneva, Switzerland","IEEE Transactions on Information Forensics and Security","6 Apr 2023","2023","18","","2060","2075","Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems. In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new interpretation of the popular generative and discriminative models, (iii) constructs new insights for the generative compression models, and (iv) can be used to obtain fair generative models. We first formulate the CLUB model as a complexity-constrained privacy-utility optimization problem. We then connect it with the closely related bottleneck problems, namely information bottleneck (IB), privacy funnel (PF), deterministic IB (DIB), conditional entropy bottleneck (CEB), and conditional PF (CPF). We show that the CLUB model generalizes all these problems as well as most other information-theoretic privacy models. Then, we construct the deep variational CLUB (DVCLUB) models by employing neural networks to parameterize variational approximations of the associated information quantities. Building upon these information quantities, we present unified objectives of the supervised and unsupervised DVCLUB models. Leveraging the DVCLUB model in an unsupervised setup, we then connect it with state-of-the-art generative models, such as variational auto-encoders (VAEs), generative adversarial networks (GANs), as well as the Wasserstein GAN (WGAN), Wasserstein auto-encoder (WAE), and adversarial auto-encoder (AAE) models through the optimal transport (OT) problem. We then show that the DVCLUB model can also be used in fair representation learning problems, where the goal is to mitigate the undesired bias during the training phase of a machine learning model. We conduct extensive quantitative experiments on colored-MNIST and CelebA datasets.","1556-6021","","10.1109/TIFS.2023.3262112","Swiss NSF(grant numbers:200021_182063); U.S. NSF(grant numbers:CIF 1900750,CAREER 1845852); U.K. Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/T023600/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10081397","Information-theoretic privacy;statistical inference;information bottleneck;obfuscation;generative models","Training;Privacy;Machine learning algorithms;Neural networks;Generative adversarial networks;Mathematical models;Loss measurement","","6","","75","IEEE","27 Mar 2023","","","IEEE","IEEE Journals"
"Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision","X. Ji; Y. Cheng; Y. Zhang; K. Wang; C. Yan; W. Xu; K. Fu","Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Security and Privacy Research Group (SPQR), University of Michigan","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","160","175","Autonomous vehicles increasingly exploit computer-vision-based object detection systems to perceive environments and make critical driving decisions. To increase the quality of images, image stabilizers with inertial sensors are added to alleviate image blurring caused by camera jitters. However, such a trend opens a new attack surface. This paper identifies a system-level vulnerability resulting from the combination of the emerging image stabilizer hardware susceptible to acoustic manipulation and the object detection algorithms subject to adversarial examples. By emitting deliberately designed acoustic signals, an adversary can control the output of an inertial sensor, which triggers unnecessary motion compensation and results in a blurred image, even if the camera is stable. The blurred images can then induce object misclassification affecting safety-critical decision making. We model the feasibility of such acoustic manipulation and design an attack framework that can accomplish three types of attacks, i.e., hiding, creating, and altering objects. Evaluation results demonstrate the effectiveness of our attacks against four academic object detectors (YOLO V3/V4/V5 and Fast R-CNN), and one commercial detector (Apollo). We further introduce the concept of AMpLe attacks, a new class of system-level security vulnerabilities resulting from a combination of adversarial machine learning and physics-based injection of information-carrying signals into hardware.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00091","Analog Devices; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519394","","Computer vision;Inertial sensors;Object detection;Detectors;Cameras;Acoustics;Hardware","","14","","59","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Intriguing Properties of Adversarial ML Attacks in the Problem Space","F. Pierazzi; F. Pendlebury; J. Cortellazzi; L. Cavallaro",King's College London; King's College London; King's College London; King's College London,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","1332","1349","Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that ""adversarial-malware as a service"" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152781","adversarial machine learning;problem space;input space;malware;program analysis;evasion","Malware;Perturbation methods;Robustness;Androids;Humanoid robots;Semantics","","104","","76","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Homogeneous and Heterogeneous Feed-Forward XOR Physical Unclonable Functions","S. V. S. Avvaru; Z. Zeng; K. K. Parhi","Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA","IEEE Transactions on Information Forensics and Security","7 Feb 2020","2020","15","","2485","2498","Physical unclonable functions (PUFs) are hardware security primitives that are used for device authentication and cryptographic key generation. Standard XOR PUFs typically contain multiple standard arbiter PUFs as components, and are more secure than standard arbiter PUFs or feed-forward (FF) arbiter PUFs (FF PUFs). This paper proposes design of feed-forward XOR PUFs (FFXOR PUFs) where each component PUF is a FF PUF. Various homogeneous and heterogeneous FFXOR PUFs are presented and evaluated in terms of four fundamental properties of PUFs: uniqueness, attack-resistance, reliability and randomness. Certain key issues pertaining to XOR PUFs such as their vulnerability to machine learning attacks and instability in responses are investigated. Other important challenges like the lack of uniqueness in FF PUFs and the asymmetry in FPGA arbiter PUFs are addressed and it is shown that FFXOR PUFs can naturally overcome these problems. It is shown that heterogeneous FFXOR PUFs (i.e., FFXOR PUFs with non-identical components) can be resilient to state-of-the-art machine learning attacks. We also present systematic reliability analysis of FFXOR PUFs and demonstrate that soft-response thresholding can be used as an effective countermeasure to overcome the degraded reliability bottleneck. Observations from simulations are further verified through hardware implementation of 64-bit FFXOR PUFs on Xilinx Artix-7 FPGA.","1556-6021","","10.1109/TIFS.2020.2968113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963980","Hardware security;arbiter PUF;feed-forward PUF;XOR PUF;FPGA PUF;FFXOR PUFs;homogeneous;heterogeneous;reliability;uniqueness;security;attack-resistance;randomness","Reliability;Standards;Delays;Field programmable gate arrays;Physical unclonable function;Machine learning","","51","","53","IEEE","20 Jan 2020","","","IEEE","IEEE Journals"
"A Robust Approach for Securing Audio Classification Against Adversarial Attacks","M. Esmaeilpour; P. Cardinal; A. Lameiras Koerich","Department of Software and IT Engineering, École de Technologie Supérieure, University of Quebec, Montreal, Canada; Department of Software and IT Engineering, École de Technologie Supérieure, University of Quebec, Montreal, Canada; Department of Software and IT Engineering, École de Technologie Supérieure, University of Quebec, Montreal, Canada","IEEE Transactions on Information Forensics and Security","4 Feb 2020","2020","15","","2147","2159","Adversarial audio attacks can be considered as a small perturbation unperceptive to human ears that is intentionally added to an audio signal and causes a machine learning model to make mistakes. This poses a security concern about the safety of machine learning models since the adversarial attacks can fool such models toward the wrong predictions. In this paper we first review some strong adversarial attacks that may affect both audio signals and their 2D representations and evaluate the resiliency of deep learning models and support vector machines (SVM) trained on 2D audio representations such as short time Fourier transform, discrete wavelet transform (DWT) and cross recurrent plot against several state-of-the-art adversarial attacks. Next, we propose a novel approach based on pre-processed DWT representation of audio signals and SVM to secure audio systems against adversarial attacks. The proposed architecture has several preprocessing modules for generating and enhancing spectrograms including dimension reduction and smoothing. We extract features from small patches of the spectrograms using the speeded up robust feature (SURF) algorithm which are further used to transform into cluster distance distribution using the K-Means++ algorithm. Finally, SURF-generated vectors are encoded by this codebook and the resulting codewords are used for training a SVM. All these steps yield to a novel approach for audio classification that provides a good tradeoff between accuracy and resilience. Experimental results on three environmental sound datasets show the competitive performance of the proposed approach compared to the deep neural networks both in terms of accuracy and robustness against strong adversarial attacks.","1556-6021","","10.1109/TIFS.2019.2956591","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN 2016-04855,RGPIN 2016-06628); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8922608","Spectrograms;environmental sound classification;adversarial attack;K-means++;support vector machines (SVM);convolutional denoising autoencoder","Support vector machines;Machine learning;Robustness;Perturbation methods;Predictive models;Optimization;Two dimensional displays","","38","","47","IEEE","4 Dec 2019","","","IEEE","IEEE Journals"
"Debiasing Android Malware Datasets: How Can I Trust Your Results If Your Dataset Is Biased?","T. C. Miranda; P. -F. Gimenez; J. -F. Lalande; V. V. T. Tong; P. Wilke","CentraleSupélec, Inria, CNRS, University of Rennes 1, IRISA, Rennes, France; CentraleSupélec, Inria, CNRS, University of Rennes 1, IRISA, Rennes, France; CentraleSupélec, Inria, CNRS, University of Rennes 1, IRISA, Rennes, France; CentraleSupélec, Inria, CNRS, University of Rennes 1, IRISA, Rennes, France; CentraleSupélec, Inria, CNRS, University of Rennes 1, IRISA, Rennes, France","IEEE Transactions on Information Forensics and Security","17 Jun 2022","2022","17","","2182","2197","Android security has received a lot of attention over the last decade, especially malware investigation. Researchers attempt to highlight applications’ security-relevant characteristics to better understand malware and effectively distinguish malware from benign applications. The accuracy and the completeness of their proposals are evaluated experimentally on malware and goodware datasets. Thus, the quality of these datasets is of critical importance: if the datasets are outdated or not representative of the studied population, the conclusions may be flawed. We specify different types of experimental scenarios. Some of them require unlabeled but representative datasets of the entire population. Others require datasets labeled with valuable characteristics that may be difficult to compute, such as malware datasets. We discuss the irregularities of datasets used in experiments, questioning the validity of the performances reported in the literature. This article focuses on providing guidelines for designing debiased datasets. First, we propose guidelines for building representative datasets from unlabeled ones. Second, we propose and experiment a debiasing algorithm that, given a biased labeled dataset and a target representative dataset, builds a representative and labeled dataset. Finally, from the previous debiased datasets, we produce datasets for experiments on Android malware detection or classification with machine learning algorithms. Experiments show that debiased datasets perform better when classifying with machine learning algorithms.","1556-6021","","10.1109/TIFS.2022.3180184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787514","Datasets;malware;experiments","Malware;Statistics;Sociology;Machine learning algorithms;Classification algorithms;Training;Security","","4","","81","CCBY","3 Jun 2022","","","IEEE","IEEE Journals"
"FedRecovery: Differentially Private Machine Unlearning for Federated Learning Frameworks","L. Zhang; T. Zhu; H. Zhang; P. Xiong; W. Zhou","Centre for Cyber Security and Privacy and the School of Computer Science, The University of Technology, Sydney, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, The University of Technology, Sydney, NSW, Australia; Department of Operational Research and Scientific Computation, Faculty of Science, Beijing University of Technology, Beijing, China; Department of Computer Science and Technology, School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China; Centre for Cyber Security and Privacy and the School of Computer Science, The University of Technology, Sydney, NSW, Australia","IEEE Transactions on Information Forensics and Security","10 Aug 2023","2023","18","","4732","4746","Over the past decades, the abundance of personal data has led to the rapid development of machine learning models and important advances in artificial intelligence (AI). However, alongside all the achievements, there are increasing privacy threats and security risks that may cause significant losses for data providers. Recent legislation requires that the private information about a user should be removed from a database as well as machine learning models upon certain deletion requests. While erasing data records from memory storage is straightforward, it is often challenging to remove the influence of particular data samples from a model that has already been trained. Machine unlearning is an emerging paradigm that aims to make machine learning models “forget” what they have learned about particular data. Nevertheless, the unlearning issue for federated learning has not been completely addressed due to its special working mode. First, existing solutions crucially rely on retraining-based model calibration, which is likely unavailable and can pose new privacy risks for federated learning frameworks. Second, today’s efficient unlearning strategies are mainly designed for convex problems, which are incapable of handling more complicated learning tasks like neural networks. To overcome these limitations, we took advantage of differential privacy and developed an efficient machine unlearning algorithm named FedRecovery. The FedRecovery erases the impact of a client by removing a weighted sum of gradient residuals from the global model, and tailors the Gaussian noise to make the unlearned model and retrained model statistically indistinguishable. Furthermore, the algorithm neither requires retraining-based fine-tuning nor needs the assumption of convexity. Theoretical analyses show the rigorous indistinguishability guarantee. Additionally, the experiment results on real-world datasets demonstrate that the FedRecovery is efficient and is able to produce a model that performs similarly to the retrained one.","1556-6021","","10.1109/TIFS.2023.3297905","Australian Research Council Discovery(grant numbers:DP200100946,DP230100246); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189868","Machine unlearning;differential privacy;federated learning","Data models;Federated learning;Computational modeling;Mathematical models;Data privacy;Training;Servers","","1","","51","IEEE","21 Jul 2023","","","IEEE","IEEE Journals"
"SpecView: Malware Spectrum Visualization Framework With Singular Spectrum Transformation","J. Yu; Y. He; Q. Yan; X. Kang","Guangdong Key Laboratory of Information Security, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Information Security Technology, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Guangdong Key Laboratory of Information Security Technology, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Information Forensics and Security","9 Nov 2021","2021","16","","5093","5107","With the rapid development of automation tools including polymorphic and metamorphic engines, generic packers, and genetic programming, many variants of malware have emerged, which pose a significant threat to the Internet security. To effectively detect malware variants, researchers have developed visualization-based approaches that can visualize malware adaptations for in-depth malware analysis. However, most existing visualization approaches rely on the binary image of a malware sample, which fail to provide an effective texture feature representation and thus often result in low efficiency in coping with challenging malware samples. In this paper, we propose SpecView, a malware spectrum visualization framework with singular spectrum transformation. SpecView converts malware binary code into one-dimensional time series spectrum data, and leverages the singular spectrum transformation method to obtain the structural changes preserved in the time series spectrum data. Then, we utilize the particle swarm optimization algorithm to optimize the singular spectrum transformation performance in SpecView. We apply SpecView in the task of malware classification. Extensive experimental results show that SpecView is effective and efficient in malware classification on the Malimg, Malheur, Drebin, and PRAGuard Malgenome Class Encryption datasets, with classification accuracy exceeding 99%, and it can effectively identify malware variants that use evasive techniques such as packer and encryption obfuscation. The proposed method outperforms the state-of-the-art methods on all datasets and the classification accuracy reaches 100% for 5 malware families packed by the UPX packer on the Malimg dataset, as well as 9 malware families that use Class Encryption obfuscation techniques on the PRAGuard Malgenome Class Encryption datasets.","1556-6021","","10.1109/TIFS.2021.3124725","NSFC(grant numbers:62072484,61772571); Chinese National Key Research and Development Project(grant numbers:2019QY2203); National Science Foundation(grant numbers:CNS-1950171); Key Project of Hanshan Normal University(grant numbers:XN202035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607026","Malware spectrum;malware visualization;singular spectrum transformation;particle swarm optimization;malware classification","Time series analysis;Internet security;Genetic programming;Data visualization;Computer security;Malware;Encryption;Particle swarm optimization","","9","","65","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"LDS-FL: Loss Differential Strategy Based Federated Learning for Privacy Preserving","T. Wang; Q. Yang; K. Zhu; J. Wang; C. Su; K. Sato","School of Intelligent Systems Engineering, Sun Yat-Sen University, Shenzhen, China; School of Intelligent Systems Engineering, Sun Yat-Sen University, Shenzhen, China; School of Intelligent Systems Engineering, Sun Yat-Sen University, Shenzhen, China; School of Intelligent Systems Engineering, Sun Yat-Sen University, Shenzhen, China; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; R-CCS RIKEN, Kobe, Japan","IEEE Transactions on Information Forensics and Security","4 Dec 2023","2024","19","","1015","1030","Federated Learning (FL) has attracted extraordinary attention from the industry and academia due to its advantages in privacy protection and collaboratively training on isolated datasets. Since machine learning algorithms usually try to find an optimal hypothesis to fit the training data, attackers also can exploit the shared models and reversely analyze users’ private information. However, there is still no good solution to solve the privacy-accuracy trade-off, by making information leakage more difficult and meanwhile can guarantee the convergence of learning. In this work, we propose a Loss Differential Strategy (LDS) for parameter replacement in FL. The key idea of our strategy is to maintain the performance of the Private Model to be preserved through parameter replacement with multi-user participation, while the efficiency of privacy attacks on the model can be significantly reduced. To evaluate the proposed method, we have conducted comprehensive experiments on four typical machine learning datasets to defend against membership inference attack. For example, the accuracy on MNIST is near 99%, while it can reduce the accuracy of attack by 10.1% compared with FedAvg. Compared with other traditional privacy protection mechanisms, our method also outperforms them in terms of accuracy and privacy preserving.","1556-6021","","10.1109/TIFS.2023.3322328","National Natural Science Foundation of China(grant numbers:62072485); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011294); Guangdong Provincial Key Laboratory of Fire Science and Intelligent Emergency Technology, Guangzhou, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272663","Federated learning;loss differential strategy;privacy-preserving;deep learning","Privacy;Modeling;Servers;Data models;Computational modeling;Federated learning;Training","","1","","47","IEEE","5 Oct 2023","","","IEEE","IEEE Journals"
"Exploiting Unintended Feature Leakage in Collaborative Learning","L. Melis; C. Song; E. De Cristofaro; V. Shmatikov",UCL; Cornell University; UCL & Alan Turing Institute; Cornell Tech,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","691","706","Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835269","privacy;collaborative-learning;deep-learning;security;inference-attacks","Training;Training data;Data models;Servers;Collaborative work;Task analysis;Computational modeling","","620","","68","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"AdGraph: A Graph-Based Approach to Ad and Tracker Blocking","U. Iqbal; P. Snyder; S. Zhu; B. Livshits; Z. Qian; Z. Shafiq",University of Iowa; Brave Software; UC Riverside; Brave Software; UC Riverside; University of Iowa,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","763","776","User demand for blocking advertising and tracking online is large and growing. Existing tools, both deployed and described in research, have proven useful, but lack either the completeness or robustness needed for a general solution. Existing detection approaches generally focus on only one aspect of advertising or tracking (e.g. URL patterns, code structure), making existing approaches susceptible to evasion.In this work we present AdGraph, a novel graph-based machine learning approach for detecting advertising and tracking resources on the web. AdGraph differs from existing approaches by building a graph representation of the HTML structure, network requests, and JavaScript behavior of a webpage, and using this unique representation to train a classifier for identifying advertising and tracking resources. Because AdGraph considers many aspects of the context a network request takes place in, it is less susceptible to the single-factor evasion techniques that flummox existing approaches.We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly accurate, able to replicate the labels of human-generated filter lists with 95.33% accuracy, and can even identify many mistakes in filter lists. We implement AdGraph as a modification to Chromium. AdGraph adds only minor overhead to page loading and execution, and is actually faster than stock Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we conclude that AdGraph is both accurate enough and performant enough for online use, breaking comparable or fewer websites than popular filter list based approaches.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152669","","Advertising;Chromium;Uniform resource locators;Browsers;Privacy;Tools;Robustness","","28","","65","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Efficient Coded Multi-Party Computation at Edge Networks","E. Vedadi; Y. Keshtkarjahromi; H. Seferoglu","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Seagate Technology, Shakopee, MN, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA","IEEE Transactions on Information Forensics and Security","30 Nov 2023","2024","19","","807","820","Multi-party computation (MPC) is promising for designing privacy-preserving machine learning algorithms at edge networks. An emerging approach is coded-MPC (CMPC), which advocates the use of coded computation to improve the performance of MPC in terms of the required number of workers involved in computations. The current approach for designing CMPC algorithms is to merely combine efficient coded computation constructions with MPC. We show that this approach fails short of being efficient; e.g., entangled polynomial codes are not necessarily better than PolyDot codes in MPC setting, while they are always better for coded computation. Motivated by this observation, we propose a new construction; Adaptive Gap Entangled (AGE) polynomial codes for MPC. We show through analysis and simulations that MPC with AGE codes always perform better than existing CMPC algorithms in terms of the required number of workers as well as computation, storage, and communication overhead.","1556-6021","","10.1109/TIFS.2023.3326970","National Science Foundation (NSF)(grant numbers:CNS-1801708,CCF-1942878,CNS-2112471); Seagate Agreement(grant numbers:00118496.0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292690","Adaptive gap entangled polynomial codes;multi-party computation;coded computation;edge computing;privacy","Codes;Servers;Privacy;Machine learning algorithms;Computational efficiency;Computational modeling;Data privacy","","","","53","CCBY","23 Oct 2023","","","IEEE","IEEE Journals"
"2PCLA: Provable Secure and Privacy Preserving Enhanced Certificateless Authentication Scheme for Distributed Learning","Y. Ma; Q. Cheng; X. Luo","Fourth Department, Information Engineering University, Zhengzhou, China; Fourth Department, Information Engineering University, Zhengzhou, China; Henan Province Key Laboratory of Cyberspace Situation Awareness, Zhengzhou, China","IEEE Transactions on Information Forensics and Security","9 Oct 2023","2023","18","","5876","5889","Distributed learning (DL) emerges as machine learning and the Internet of Things develop quickly and widely. As edge servers pre-process and pre-learn the statistics, global servers can reduce costs, improve efficiency and output more precise results. However, to acquire high-quality and adequate data, servers should collect information from a number of end devices, which naturally leads to confidentiality and privacy problems during information transmission. If the private information or the data are compromised by malicious attackers, the users’ security and the network operation will all be in danger. To resolve this thorny challenge, numerous schemes have been put forward, adopting different cryptography technologies and aiming at aspects of security. However, many state-of-the-art schemes can hardly satisfy the security demands and are pointed out to be defective. Lately, Jiang et al. made an effort and proposed a certificateless signature scheme, as well as an authentication scheme for the purpose of solving the privacy issues. Unfortunately, in this paper, we point out that their schemes can hardly resist forgery attacks and ephemeral key leakage attacks. Further, we will propose an improved scheme noted as 2PCLA and change the method of generating the session key. Theoretical analysis and formal security analysis utilizing Tamarin analysis tool are provided to prove the security of 2PCLA scheme. Performance evaluation has been done from both theoretical and experimental perspectives. The assessment results illustrate that 2PCLA can balance security properties with execution efficiency relatively well.","1556-6021","","10.1109/TIFS.2023.3318952","National Key Research and Development Program of China(grant numbers:2022YFB3102900); National Natural Science Foundation of China(grant numbers:61872449,62172433,62172435); Science Foundation for the Excellent Youth Scholars of Henan Province(grant numbers:222300420099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262019","Distributed learning (DL);certificateless-based cryptography;privacy preservation;authentication","Security;Servers;Authentication;Privacy;Protocols;Machine learning;Immune system","","","","26","IEEE","25 Sep 2023","","","IEEE","IEEE Journals"
"TI-PUF: Toward Side-Channel Resistant Physical Unclonable Functions","A. Aghaie; A. Moradi","Horst Görtz Institute for IT Security, Ruhr-Universität Bochum, Bochum, Germany; Horst Görtz Institute for IT Security, Ruhr-Universität Bochum, Bochum, Germany","IEEE Transactions on Information Forensics and Security","17 Jun 2020","2020","15","","3470","3481","One of the main motivations behind introducing PUFs was their ability to resist physical attacks. Among them, cloning was the major concern of related scientific literature. Several primitive PUF designs have been introduced to the community, and several machine learning attacks have been shown capable of modeling such constructions. Although a few works have expressed how to make use of Side-Channel Analysis (SCA) leakage of PUF constructions to significantly improve the modeling attacks, little attention has been paid to provide corresponding countermeasures. In this paper, we present a generic technique to operate any PUF primitive in an SCA-secure fashion. We, for the first time, make it possible to apply a provably-secure masking countermeasure - Threshold Implementation (TI) - on a strong PUF design. As a case study, we concentrate on the Interpose PUF and based on practical experiments on an FPGA prototype, we demonstrate the ability of our construction to prevent the recovery of intermediate values through SCA measurements.","1556-6021","","10.1109/TIFS.2020.2986887","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) through the Germany???s Excellence Strategy(grant numbers:EXC 2092 CASA - 390781972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063465","Physical unclonable function;side-channel analysis;threshold implementation;masking;modeling;machine learning","Resistance;Cryptography;Reliability;Protocols;Machine learning;Analytical models;Semiconductor device measurement","","19","","50","IEEE","10 Apr 2020","","","IEEE","IEEE Journals"
"Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks","N. Papernot; P. McDaniel; X. Wu; S. Jha; A. Swami","Department of Computer Science and Engineering, Penn State University; Department of Computer Science and Engineering, Penn State University; Computer Sciences Department, University of Wisconsin-Madison; Computer Sciences Department, University of Wisconsin-Madison; United States Army Research Laboratory, Adelphi, Maryland","2016 IEEE Symposium on Security and Privacy (SP)","18 Aug 2016","2016","","","582","597","Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.","2375-1207","978-1-5090-0824-7","10.1109/SP.2016.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546524","","Training;Computer architecture;Machine learning;Security;Automobiles;Computational modeling;Neural networks","","1374","11","44","IEEE","18 Aug 2016","","","IEEE","IEEE Conferences"
"SIRAJ: A Unified Framework for Aggregation of Malicious Entity Detectors","S. Thirumuruganathan; M. Nabeel; E. Choo; I. Khalil; T. Yu",Qatar Computing Research Institute; Qatar Computing Research Institute; Qatar Computing Research Institute; Qatar Computing Research Institute; Qatar Computing Research Institute,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","507","521","High-quality intelligence of Internet threat (e.g., malware files, malicious domains, phishing URLs and malicious IPs) are important for both security practitioners and the research community. Given the agility of attackers, the scale of the Internet, and the fast-evolving landscape of threats, one could not rely solely on a single source (such as an anti-malware engine or an IP blacklist) for obtaining accurate, up-to-date, and comprehensive threat analysis. Instead, we need to aggregate the analysis from multiple sources. However, it is non-trivial to do such aggregation effectively. A common practice is to label an indicator (malware, domains, URLs, etc.) as malicious if it is marked by a number of sources above an ad-hoc certain threshold. Often, this results in sub-optimal performance as it assumes that all sources are of similar quality/expertise, independent, and temporally stable, which unfortunately are often not true in practice. A natural alternative is to train a supervised machine learning model. However, this approach needs a sufficiently large amount of manually labeled ground truth, which is time-consuming to collect and has to be updated frequently, resulting in substantial recurring costs. In this paper, we propose SIRAJ, a novel framework for aggregating the detection output of various intelligence sources such as anti-malware engines. SIRAJ is based on the pretrain and fine-tune paradigm. Specifically, we use self-supervised learning-based approaches to learn a pre-trained embedding model that converts multi-source inputs into a high-dimensional embedding. The embeddings are learned through three carefully designed pretext tasks that imbue them with knowledge about dependencies between scanners and their temporal dynamics. The learned embeddings could be used for diverse downstream machine learning tasks. SIRAJ is designed to be general and can be used for diverse domains such as URLs, malware, and IPs. Further, SIRAJ works well even when there is limited to no labeled data available. Through extensive experiments, we show that our learned representations can produce results comparable to supervised methods while only requiring as little as 100 labeled samples. Importantly, the results show that SIRAJ accurately detects threat indicators much earlier than the baseline algorithms, a feat that is critical against short-lived indicators like Phishing URLs.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833725","threat-intelligence-aggregation;ground-truth-generation;malicious-entities;truth-discovery;self-supervised-learning","Technological innovation;Phishing;Aggregates;Machine learning;Malware;Internet;Blocklists","","2","","61","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Robust Smartphone App Identification via Encrypted Network Traffic Analysis","V. F. Taylor; R. Spolaor; M. Conti; I. Martinovic","Department of Computer Science, University of Oxford, Oxford, U.K.; Department of Mathematics, University of Padova, Padua, Italy; Department of Mathematics, University of Padova, Padua, Italy; Department of Computer Science, University of Oxford, Oxford, U.K.","IEEE Transactions on Information Forensics and Security","17 Nov 2017","2018","13","1","63","78","The apps installed on a smartphone can reveal much information about a user, such as their medical conditions, sexual orientation, or religious beliefs. In addition, the presence or absence of particular apps on a smartphone can inform an adversary, who is intent on attacking the device. In this paper, we show that a passive eavesdropper can feasibly identify smartphone apps by fingerprinting the network traffic that they send. Although SSL/TLS hides the payload of packets, side-channel data, such as packet size and direction is still leaked from encrypted connections. We use machine learning techniques to identify smartphone apps from this side-channel data. In addition to merely fingerprinting and identifying smartphone apps, we investigate how app fingerprints change over time, across devices, and across different versions of apps. In addition, we introduce strategies that enable our app classification system to identify and mitigate the effect of ambiguous traffic, i.e., traffic in common among apps, such as advertisement traffic. We fully implemented a framework to fingerprint apps and ran a thorough set of experiments to assess its performance. We fingerprinted 110 of the most popular apps in the Google Play Store and were able to identify them six months later with up to 96% accuracy. Additionally, we show that app fingerprints persist to varying extents across devices and app versions.","1556-6021","","10.1109/TIFS.2017.2737970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006282","Cellular phones;information security;privacy","Cryptography;Performance evaluation;Object recognition;Web pages;IP networks;Feature extraction;Robustness","","223","","35","IEEE","9 Aug 2017","","","IEEE","IEEE Journals"
"Semantics-Based Online Malware Detection: Towards Efficient Real-Time Protection Against Malware","S. Das; Y. Liu; W. Zhang; M. Chandramohan","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Hong Kong University of Science and Technology, Hong Kong; Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","2","289","302","Recently, malware has increasingly become a critical threat to embedded systems, while the conventional software solutions, such as antivirus and patches, have not been so successful in defending the ever-evolving and advanced malicious programs. In this paper, we propose a hardware-enhanced architecture, GuardOL, to perform online malware detection. GuardOL is a combined approach using processor and field-programmable gate array (FPGA). Our approach aims to capture the malicious behavior (i.e., high-level semantics) of malware. To this end, we first propose the frequency-centric model for feature construction using system call patterns of known malware and benign samples. We then develop a machine learning approach (using multilayer perceptron) in FPGA to train classifier using these features. At runtime, the trained classifier is used to classify the unknown samples as malware or benign, with early prediction. The experimental results show that our solution can achieve high classification accuracy, fast detection, low power consumption, and flexibility for easy functionality upgrade to adapt to new malware samples. One of the main advantages of our design is the support of early prediction-detecting 46% of malware within first 30% of their execution, while 97% of the samples at 100% of their execution, with <;3% false positives.","1556-6021","","10.1109/TIFS.2015.2491300","National Research Foundation, Prime Minister’s Office, Singapore under its National Cybersecurity R&D Program and administered by the National Cybersecurity R&D Directorate(grant numbers:NRF2014NCR-NCR001-30); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299317","Malware Detection;Hardware-enhanced Architecture;Runtime Security;Early Prediction;Reconfigurable Malware Detection;Malware detection;hardware-enhanced architecture;runtime security;early prediction;reconfigurable malware detection","Malware;Feature extraction;Software;Semantics;Runtime;Field programmable gate arrays;Training","","128","","52","IEEE","15 Oct 2015","","","IEEE","IEEE Journals"
"I Know What You Saw Last Minute—Encrypted HTTP Adaptive Video Streaming Title Classification","R. Dubin; A. Dvir; O. Pele; O. Hadar","Department of Communication Systems Engineering, Ben-Gurion University of the Negev, Beer Sheva, Israel; Center for Cyber Technologies, Ariel University, Ariel, Israel; Department of Electrical and Electronics Engineering, Ariel University, Ariel, Israel; Department of Electrical and Electronics Engineering, Ariel University, Ariel, Israel","IEEE Transactions on Information Forensics and Security","29 Aug 2017","2017","12","12","3039","3049","Desktops can be exploited to violate privacy. There are two main types of attack scenarios: active and passive. We consider the passive scenario where the adversary does not interact actively with the device, but is able to eavesdrop on the network traffic of the device from the network side. In the near future, most Internet traffic will be encrypted and thus passive attacks are challenging. Previous research has shown that information can be extracted from encrypted multimedia streams. This includes video title classification of non HTTP adaptive streams. This paper presents algorithms for encrypted HTTP adaptive video streaming title classification. We show that an external attacker can identify the video title from video HTTP adaptive streams sites, such as YouTube. To the best of our knowledge, this is the first work that shows this. We provide a large data set of 15000 YouTube video streams of 2100 popular video titles that was collected under real-world network conditions. We present several machine learning algorithms for the task and run a thorough set of experiments, which shows that our classification accuracy is higher than 95%. We also show that our algorithms are able to classify video titles that are not in the training set as unknown and some of the algorithms are also able to eliminate false prediction of video titles and instead report unknown. Finally, we evaluate our algorithm robustness to delays and packet losses at test time and show that our solution is robust to these changes.","1556-6021","","10.1109/TIFS.2017.2730819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987775","HTTP adaptive video streaming;HTTP2;encrypted traffic;classification;YouTube","Streaming media;YouTube;Cryptography;Algorithm design and analysis;Wireless fidelity;Bit rate;Internet","","60","","63","IEEE","21 Jul 2017","","","IEEE","IEEE Journals"
"Anomaly Detection in Real-Time Multi-Threaded Processes Using Hardware Performance Counters","P. Krishnamurthy; R. Karri; F. Khorrami","Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA","IEEE Transactions on Information Forensics and Security","24 Sep 2019","2020","15","","666","680","We propose a novel methodology for real-time monitoring of software running on embedded processors in cyber-physical systems (CPS). The approach uses real-time monitoring of hardware performance counters (HPC) and applies to multi-threaded and interrupt-driven processes typical in programmable logic controller (PLC) implementation of real-time controllers. The methodology uses a black-box approach to profile the target process using HPCs. The time series of HPC measurements over a time window under known-good operating conditions is used to train a machine learning classifier. At run-time, this trained classifier classifies the time series of HPC measurements as baseline (i.e., probabilistically corresponding to a model learned from the training data) or anomalous. The baseline versus anomalous labels over successive time windows offer robustness against the stochastic variability of code execution on the embedded processor and detect code modifications. We demonstrate effectiveness of the approach on an embedded PLC in a hardware-in-the-loop (HITL) testbed emulating a benchmark industrial process. In addition, to illustrate the scalability of the approach, we also apply the methodology to a second PLC platform running a representative embedded control process.","1556-6021","","10.1109/TIFS.2019.2923577","Office of Naval Research(grant numbers:N00014-15-1-2182,N00014-17-1-2006); Defense Advanced Research Projects Agency(grant numbers:FA8750-16-C-0179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737990","Anomaly detection;cyber security;programmable logic controller;malware;resilient control","Malware;Monitoring;Real-time systems;Program processors;Process control;Time series analysis;Hardware","","44","","46","IEEE","17 Jun 2019","","","IEEE","IEEE Journals"
"PFLF: Privacy-Preserving Federated Learning Framework for Edge Computing","H. Zhou; G. Yang; H. Dai; G. Liu","School of Computer Science, Nanjing University of Post and Telecommunication, Nanjing, China; School of Computer Science, Nanjing University of Post and Telecommunication, Nanjing, China; School of Computer Science, Nanjing University of Post and Telecommunication, Nanjing, China; School of Network and Communication Engineering, Jinling Institute of Technology, Anhui, Wuhu, China","IEEE Transactions on Information Forensics and Security","30 May 2022","2022","17","","1905","1918","Federated learning (FL) can protect clients’ privacy from leakage in distributed machine learning. Applying federated learning to edge computing can protect the privacy of edge clients and benefit edge computing. Nevertheless, eavesdroppers can analyze the parameter information to specify clients’ private information and model features. And it is difficult to achieve a high privacy level, convergence, and low communication overhead during the entire process in the FL framework. In this paper, we propose a novel privacy-preserving federated learning framework for edge computing (PFLF). In PFLF, each client and the application server add noise before sending the data. To protect the privacy of clients, we design a flexible arrangement mechanism to count the optimal training times for clients. We prove that PFLF guarantees the privacy of clients and servers during the entire training process. Then, we theoretically prove that PFLF has three main properties: 1) For a given privacy level and model aggregation times, there is an optimal number of participating times for clients; 2) There is an upper and lower bound of convergence; 3) PFLF achieves low communication overhead by designing a flexible participation training mechanism. Simulation experiments confirm the correctness of our theoretical analysis. Therefore, PFLF helps design a framework to balance privacy levels and convergence and achieve low communication overhead when there is a part of clients dropping out of training.","1556-6021","","10.1109/TIFS.2022.3174394","National Natural Science Foundation of China(grant numbers:61872197,61972209); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX21 0789); Natural Research Foundation of Nanjing University of Posts and Telecommunications(grant numbers:NY217119); Anhui Provincial Key Laboratory of Network and Information Security(grant numbers:AHNIS2020002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772495","Federated learning;differential privacy;convergence performance;information leakage;edge computing","Privacy;Servers;Convergence;Training;Collaborative work;Edge computing;Computational modeling","","21","","46","IEEE","11 May 2022","","","IEEE","IEEE Journals"
"Reverse JPEG Compatibility Attack","J. Butora; J. Fridrich","Department of Electrical and Computer Engineering, Binghamton University, Binghamton, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, USA","IEEE Transactions on Information Forensics and Security","16 Dec 2019","2020","15","","1444","1454","A novel steganalysis method for JPEG images is introduced that is universal in the sense that it reliably detects any type of steganography as well as small payloads. It is limited to quality factors 99 and 100. The detection statistic is formed from the rounding errors in the spatial domain after decompressing the JPEG image. The attack works whenever, during compression, the discrete cosine transform is applied to integer-valued signal. Reminiscent of the well-established JPEG compatibility steganalysis, we call the new approach the “reverse JPEG compatibility attack.” While the attack is introduced and analyzed under simplifying assumptions using reasoning based on statistical signal detection, the best detection in practice is obtained with machine learning tools. Experiments on diverse datasets of both grayscale and color images, five steganographic schemes, and with a variety of JPEG compressors demonstrate the universality and applicability of this steganalysis method in practice.","1556-6021","","10.1109/TIFS.2019.2940904","National Science Foundation(grant numbers:1561446); Defense Advanced Research Projects Agency(grant numbers:FA8750-16-2-0173); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832246","Steganography;steganalysis;JPEG;quality factor 100;reverse compatibility;rounding errors;deep learning","Transform coding;Discrete cosine transforms;Quantization (signal);Image coding;Gaussian distribution;Detectors;Payloads","","20","","44","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"Improving the Efficiency of SVM Classification With FHE","J. -C. Bajard; P. Martins; L. Sousa; V. Zucca","Laboratoire d’informatique de Paris 6, CNRS, Sorbonne Université, Paris, France; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, Australia","IEEE Transactions on Information Forensics and Security","23 Jan 2020","2020","15","","1709","1722","In an ever more data-centric economy, machine learning models have risen in importance. With the large amounts of data companies collect, they are able to develop highly accurate models to predict the behaviours of their customers. It is thus important to safeguard the data used to build these models to prevent competitors from mimicking their services. In addition, as this type of techniques finds its way into areas that need to deal with more sensitive information, like the medical industry, the privacy of the data that needs to be classified also has to be ensured. Herein, this topic is addressed by homomorphically evaluating Support Vector Machine (SVM) models, in a way that guarantees that a client learns nothing about the model except for the classification of his data, and that the service provider learns nothing about the data. Whereas, previously, Fully Homomorphic Encryption (FHE) has mostly focused on either bit-wise or value-wise computations, SVMs present an additional challenge since they combine both: during an initial phase a kernel function is evaluated that makes use of real arithmetic, and during a second phase the sign bit has to be extracted. Novel techniques are herein proposed that allow for speedups of up to 2.7 and 6.6 for the evaluation of polynomials and the determination of sign, respectively, in comparison to the state of the art. Finally, it is shown that the proposed techniques do not deteriorate the classification accuracy of the SVM models.","1556-6021","","10.1109/TIFS.2019.2946097","Fundação para a Ciência e a Tecnologia(grant numbers:UID/CEC/50021/2019); Fundação para a Ciência e a Tecnologia(grant numbers:SFRH/BD/103791/2014); ANR grant(grant numbers:ARRAND 15-CE39-0002-01); Pessoa/Hubert Curien Programme(grant numbers:4335 (FCT)/40832XC (CAMPUSFRANCE)); Sorbonne University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861379","Support vector machine;homomorphic encryption;computer arithmetic;parallel algorithms","Support vector machines;Kernel;Cryptography;Biological system modeling;Data models;Companies;Computational modeling","","15","","27","IEEE","7 Oct 2019","","","IEEE","IEEE Journals"
"Leia: A Lightweight Cryptographic Neural Network Inference System at the Edge","X. Liu; B. Wu; X. Yuan; X. Yi","School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia","IEEE Transactions on Information Forensics and Security","13 Jan 2022","2022","17","","237","252","The advances in machine learning have revealed its great potential for emerging mobile applications such as face recognition and voice assistant. Models trained via a Neural Network (NN) can offer accurate and efficient inference services for mobile users. Unfortunately, the current deployment of such service encounters privacy concerns. Directly offloading the model to the mobile device violates model privacy of the model owner, while feeding user input to the service compromises user privacy. To address this issue, we propose Leia, a lightweight cryptographic NN inference system at the edge. Leia is designed from two mobile-friendly perspectives. First, it leverages the paradigm of edge computing wherein the inference procedure keeps the model closer to the mobile user to foster low latency service. Specifically, Leia’s architecture consists of two non-colluding edge services to obliviously perform NN inference on the encoded user data and model. Second, Leia’s realization makes the judicious use of potentially constrained computational and communication resources in edge devices. We adapt the Binarized Neural Network (BNN), a trending flavor of NN with low inference overhead, and purely choose the lightweight secret sharing techniques to realize secure blocks of BNN. We implement Leia and deploy it on Raspberry Pi. Empirical evaluations on benchmark and medical datasets via various models demonstrate the practicality of Leia.","1556-6021","","10.1109/TIFS.2021.3138611","Australian Research Council (ARC) Discovery Projects(grant numbers:DP200103308,DP180103251,DP190102835); ARC Linkage Project(grant numbers:LP160101766); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663203","Secure computation;privacy-preserving mobile application;neural network inference;edge computing","Artificial neural networks;Cryptography;Computational modeling;Protocols;Servers;Privacy;Mobile handsets","","14","","52","IEEE","24 Dec 2021","","","IEEE","IEEE Journals"
"PrivEdge: From Local to Distributed Private Training and Prediction","A. S. Shamsabadi; A. Gascón; H. Haddadi; A. Cavallaro","Centre for Intelligent Sensing, Queen Mary University of London, London, U.K.; The Alan Turing Institute, London, U.K.; Faculty of Engineering, Imperial College London, London, U.K.; Centre for Intelligent Sensing, Queen Mary University of London, London, U.K.","IEEE Transactions on Information Forensics and Security","17 Jul 2020","2020","15","","3819","3831","Machine Learning as a Service (MLaaS) operators provide model training and prediction on the cloud. MLaaS applications often rely on centralised collection and aggregation of user data, which could lead to significant privacy concerns when dealing with sensitive personal data. To address this problem, we propose PrivEdge, a technique for privacy-preserving MLaaS that safeguards the privacy of users who provide their data for training, as well as users who use the prediction service. With PrivEdge, each user independently uses their private data to locally train a one-class reconstructive adversarial network that succinctly represents their training data. As sending the model parameters to the service provider in the clear would reveal private information, PrivEdge secret-shares the parameters among two non-colluding MLaaS providers, to then provide cryptographically private prediction services through secure multi-party computation techniques. We quantify the benefits of PrivEdge and compare its performance with state-of-the-art centralised architectures on three privacy-sensitive image-based tasks: individual identification, writer identification, and handwritten letter recognition. Experimental results show that PrivEdge has high precision and recall in preserving privacy, as well as in distinguishing between private and non-private images. Moreover, we show the robustness of PrivEdge to image compression and biased training data. The source code is available at https://github.com/smartcameras/PrivEdge.","1556-6021","","10.1109/TIFS.2020.2988132","The Alan Turing Institute through an Enrichment Scheme; The Alan Turing Institute(grant numbers:EPSRC EP/N510129/1); U.K. Government’s Defence and Security Programme; EPSRC Databox and DADA(grant numbers:EP/N028260/1,EP/R03351X/1); Huawei Technologies; The Alan Turing Institute funded by the EPSRC, through the PRIMULA Project(grant numbers:EP/N510129/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069213","Distributed learning;privacy;one-class classifier;generative adversarial network;multi-party computation","Training;Feature extraction;Computational modeling;Predictive models;Cryptography;Data privacy;Data models","","8","","85","IEEE","16 Apr 2020","","","IEEE","IEEE Journals"
"Understanding the Manipulation on Recommender Systems through Web Injection","Y. Zhang; J. Xiao; S. Hao; H. Wang; S. Zhu; S. Jajodia","Department of Electrical and Computer Engineering, University of Delaware, Newark, USA; Department of Computer Science, Boise State University, Boise, USA; Department of Computer Science, Old Dominion University, Norfolk, USA; Department of Electrical and Computer Engineering, Virginia Tech, Arlington, USA; Department of Computer Science and Engineering, Penn State University, University Park, USA; Center for Secure Information Systems, George Mason University, Fairfax, USA","IEEE Transactions on Information Forensics and Security","16 Jul 2020","2020","15","","3807","3818","Recommender systems have been increasingly used in a variety of web services, providing a list of recommended items in which a user may have an interest. While important, recommender systems are vulnerable to various malicious attacks. In this paper, we study a new security vulnerability in recommender systems caused by web injection, through which malicious actors stealthily tamper any unprotected in-transit HTTP webpage content and force victims to visit specific items in some web services (even running HTTPS), e.g., YouTube. By doing so, malicious actors can promote their targeted items in those web services. To obtain a deeper understanding on the recommender systems of our interest (including YouTube, Yelp, Taobao, and 360 App market), we first conduct a measurement-based analysis on several real-world recommender systems by leveraging machine learning algorithms. Then, web injection is implemented in three different types of devices (i.e., computer, router, and proxy server) to investigate the scenarios where web injection could occur. Based on the implementation of web injection, we demonstrate that it is feasible and sometimes effective to manipulate the real-world recommender systems through web injection. We also present several countermeasures against such manipulations.","1556-6021","","10.1109/TIFS.2019.2954737","Army Research Office(grant numbers:W911NF-13-1-0421,W911NF-19-1-0049); National Science Foundation(grant numbers:CNS-1618117,CNS-1822094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907865","Recommender systems;recommendation manipulation;web injection","Recommender systems;YouTube;Web services;Browsers;Pollution;Videos;History","","7","","40","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Reverse Engineering of Printed Electronics Circuits: From Imaging to Netlist Extraction","A. T. Erozan; M. Hefenbrock; M. Beigl; J. Aghassi-Hagmann; M. B. Tahoori","Chair of Dependable Nano Computing, Karlsruhe Institute of Technology, Karlsruhe, Germany; Chair of Pervasive Computing Systems-TECO, Karlsruhe Institute of Technology, Karlsruhe, Germany; Chair of Pervasive Computing Systems-TECO, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Nanotechnology, Karlsruhe Institute of Technology, Eggenstein-Leopoldshafen, Germany; Chair of Dependable Nano Computing, Karlsruhe Institute of Technology, Karlsruhe, Germany","IEEE Transactions on Information Forensics and Security","17 Sep 2019","2020","15","","475","486","Printed electronics (PE) circuits have several advantages over silicon counterparts for the applications where mechanical flexibility, extremely low-cost, large area, and custom fabrication are required. The custom (personalized) fabrication is a key feature of this technology, enabling customization per application, even in small quantities due to low-cost printing compared with lithography. However, the personalized and on-demand fabrication, the non-standard circuit design, and the limited number of printing layers with larger geometries compared with traditional silicon chip manufacturing open doors for new and unique reverse engineering (RE) schemes for this technology. In this paper, we present a robust RE methodology based on supervised machine learning, starting from image acquisition all the way to netlist extraction. The results show that the proposed RE methodology can reverse engineer the PE circuits with very limited manual effort and is robust against non-standard circuit design, customized layouts, and high variations resulting from the inherent properties of PE manufacturing processes.","1556-6021","","10.1109/TIFS.2019.2922237","Ministerium für Wissenschaft, Forschung und Kunst Baden-Württemberg; Helmholtz Association(grant numbers:VI530); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735876","Printed electronics;personal fabrication;reverse engineering;supervised learning","Fabrication;Transistors;Printing;Silicon;Layout;Reverse engineering;Integrated circuits","","6","","65","IEEE","12 Jun 2019","","","IEEE","IEEE Journals"
"An Efficient Privacy-Enhancing Cross-Silo Federated Learning and Applications for False Data Injection Attack Detection in Smart Grids","H. -Y. Tran; J. Hu; X. Yin; H. R. Pota","School of Engineering and Information Technology, The University of New South Wales Canberra at ADFA, Canberra, ACT, Australia; School of Engineering and Information Technology, The University of New South Wales Canberra at ADFA, Canberra, ACT, Australia; School of Information and Communication Technology, Griffith University, Gold Coast, QLD, Australia; School of Engineering and Information Technology, The University of New South Wales Canberra at ADFA, Canberra, ACT, Australia","IEEE Transactions on Information Forensics and Security","26 Apr 2023","2023","18","","2538","2552","Federated Learning is a prominent machine learning paradigm which helps tackle data privacy issues by allowing clients to store their raw data locally and transfer only their local model parameters to an aggregator server to collaboratively train a shared global model. However, federated learning is vulnerable to inference attacks from dishonest aggregators who can infer information about clients’ training data from their model parameters. To deal with this issue, most of the proposed schemes in literature either require a non-colluded server setting, a trusted third-party to compute master secret keys or a secure multiparty computation protocol which is still inefficient over multiple iterations of computing an aggregation model. In this work, we propose an efficient cross-silo federated learning scheme with strong privacy preservation. By designing a double-layer encryption scheme which has no requirement to compute discrete logarithm, utilizing secret sharing only at the establishment phase and in the iterations when parties rejoin, and accelerating the computation performance via parallel computing, we achieve an efficient privacy-preserving federated learning protocol, which also allows clients to dropout and rejoin during the training process. The proposed scheme is demonstrated theoretically and empirically to provide provable privacy against an honest-but-curious aggregator server and simultaneously achieve desirable model utilities. The scheme is applied to false data injection attack detection (FDIA) in smart grids. This is a more secure cross-silo FDIA federated learning resilient to the local private data inference attacks than the existing works.","1556-6021","","10.1109/TIFS.2023.3267892","ARC Discovery(grant numbers:DP190103660,DP200103207); ARC Linkage(grant numbers:LP180100663); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103703","Privacy-preserving;federated learning;encryption;secret sharing;false data injection attack detection","Federated learning;Computational modeling;Privacy;Data models;Cryptography;Servers;Training","","5","","39","CCBY","17 Apr 2023","","","IEEE","IEEE Journals"
"FedDual: Pair-Wise Gossip Helps Federated Learning in Large Decentralized Networks","Q. Chen; Z. Wang; H. Wang; X. Lin","State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China; School of Computer Science, University of Guelph, Guelph, ON, Canada","IEEE Transactions on Information Forensics and Security","7 Dec 2022","2023","18","","335","350","There is a significant recent interest in collaboratively training a machine learning (ML) model without collecting data to a central server. Federated learning (FL) emerges as an efficient solution mitigating systemic privacy risks and communication costs. However, conventional FL inherited from parameter server designs relies too much on a central server, which may lead to privacy risks, communication bottlenecks, or a single point of failure. In this paper, we propose an asynchronous and hierarchical local gradient aggregation and global model update algorithm, FedDual, under three different security considerations for FL in large decentralized networks. Particularly, FedDual preserves privacy by introducing local differential privacy (LDP) and aggregates local gradients asynchronously and hierarchically via a pair-wise gossip algorithm, which is more competitive than previous gossip-based decentralized FL methods in terms of privacy preservation and communication efficiency, and offers more computational efficiency compared to existing blockchain-assisted decentralized FL methods. Further, we devise a noise cutting trick based on Private Set Intersection (PSI) to mitigate the prediction performance loss of the global model caused by the leveraged LDP. Rigorous analyses show that FedDual helps decentralized FL achieve the same convergence rate of  $\mathcal {O}\left({\frac {1}{T}}\right) $  as centralized ML theoretically. Ingenious experiments on MNIST, CIFAR-10, and FEMNIST confirm that the model prediction performance gained from FedDual is close to centralized ML. More importantly, the proposed noise cutting trick helps FedDual to train better global models than LDP-based FL methods in terms of prediction performance and convergence rate.","1556-6021","","10.1109/TIFS.2022.3222935","NSFC(grant numbers:62172319,U19B200073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954055","Federated learning;decentralized networks;privacy-preserving;security;efficiency","Security;Servers;Privacy;Computational modeling;Training;Data models;Convergence","","4","","51","IEEE","17 Nov 2022","","","IEEE","IEEE Journals"
"Monitoring Device Current to Characterize Trim Operations of Solid-State Drives","J. Shey; J. A. Blanco; O. Walker; T. W. Tedesso; H. T. Ngo; R. Rakvic; K. D. Fairbanks","Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Electrical and Computer Engineering Department, United States Naval Academy, Annapolis, MD, USA; Applied Physics Laboratory, Johns Hopkins University, Laurel, MD","IEEE Transactions on Information Forensics and Security","31 Jan 2019","2019","14","5","1296","1306","Solid-state drives (SSDs) are pervasive in modern computing and have supplanted hard disk drives in many applications. Substantial changes in architecture have brought about not only improvements in speed and energy usage but also new security concerns. The presence of proprietary firmware onboard SSD controllers in particular raises the possibility that data believed by a user or operating system to be deleted physically remains on the drive and can thus be recovered. This security issue has a direct application to malware detection, digital forensics, and consumer privacy. To begin to address this, we propose a novel, noninvasive side-channel approach to infer the SSD trim operation. We demonstrate that it is possible to infer the trim operation with better than 99% accuracy using current probe measurements in conjunction with machine learning techniques. We find that the sampling frequency can be reduced to 200 kSps while maintaining greater than 80% of the total power in the 0-1 MHz band. The classifier accordingly uses only information in the frequency range between 0 and 100 kHz in achieving its high accuracy. We also validate our current probe measurement technique by comparing it with an in-line resistor.","1556-6021","","10.1109/TIFS.2018.2876835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501955","Solid state drive;SSD;power consumption;energy;modeling;current probe;TRIM","Security;Malware;Transistors;Drives;Digital forensics;Probes;Current measurement","","4","","51","USGov","21 Oct 2018","","","IEEE","IEEE Journals"
"Toward Early and Accurate Network Intrusion Detection Using Graph Embedding","X. Hu; W. Gao; G. Cheng; R. Li; Y. Zhou; H. Wu","School of Cyber Science and Engineering and Key Laboratory of Computer Network and Information Integration, Ministry of Education of China, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; Institute of Science and Engineering, Kanazawa University, Kakuma, Kanazawa, Japan; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China","IEEE Transactions on Information Forensics and Security","28 Sep 2023","2023","18","","5817","5831","Early and accurate detection of network intrusions is crucial to ensure network security and stability. Existing network intrusion detection methods mainly use conventional machine learning or deep learning technology to classify intrusions based on the statistical features of network flows. The feature extraction relies on expert experience and cannot be performed until the end of network flows, which delays intrusion detection. The existing graph-based intrusion detection methods require global network traffic to construct communication graphs, which is complex and time-consuming. Besides, the existing deep learning-based and graph-based intrusion detection methods resort to massive training samples. This paper proposes Graph2vec+RF, an early and accurate network intrusion detection method based on graph embedding technology. We construct a flow graph from the initial several interactive packets for each bidirectional network flow instead, adopt graph embedding technology, graph2vec, to learn the vector representation of the flow graph and classify the graph vectors with Random Forest (RF). Graph2vec+RF automatically extracts flow graph features using subgraph structures and relies on only a small number of the initial interactive packets per bidirectional network flow without requiring massive training samples to achieve early and accurate network intrusion detection. Our experimental results on the CICIDS2017 and CICIDS2018 datasets show that our proposed Graph2vec+RF outperforms the state-of-the-art methods in terms of accuracy, recall, precision, and F1-score.","1556-6021","","10.1109/TIFS.2023.3318960","Jiangsu Province Natural Science Foundation Project(grant numbers:BK20231413); National Natural Science Foundation of China(grant numbers:61602114,62172093); Special Funds for Basic Scientific Research Operations of Central Universities(grant numbers:2242023k30053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262084","Network intrusion detection;graph embedding;graph2vec;network traffic analysis;network security","Feature extraction;Network intrusion detection;Radio frequency;Deep learning;Network security;Ensemble learning;Training","","1","","59","IEEE","25 Sep 2023","","","IEEE","IEEE Journals"
"Enabling Fraud Prediction on Preliminary Data Through Information Density Booster","H. Zhu; C. Wang","Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China","IEEE Transactions on Information Forensics and Security","22 Sep 2023","2023","18","","5706","5720","In online lending services, fraud prediction is an especially critical step to control loss risk and improve processing efficiency. Unfortunately, it is definitely challenging since the ex-ante prediction actually needs to be made only based on the most basic information of applicants. This work figures out that the essential difficulty here is the low information density of data associations which contain the useful information for fraud prediction. Accordingly, we propose a novel multi-stage data representation scheme, called AI2Vec (Applicant Information Vectoring), as an information density booster. It can gradually boost information density of associations by simultaneously decreasing the scale of information carriers and increasing the amount of useful information. The qualified performance of our AI2Vec is validated by the experiments over real-life data from a prestigious online lending platform. It can help commonly-used machine learning classifiers outperform the state-of-the-art methods, including the method of pilot platform with manual feature engineering by the subject matter experts.","1556-6021","","10.1109/TIFS.2023.3300523","National Natural Science Foundation of China (NSFC)(grant numbers:62372328,61972287); Special Security Foundation of Shanghai Municipal Commission of Economy and Information Technology(grant numbers:214102); Open Fund of Key Laboratory of Industrial Internet of Things and Networked Control, Ministry of Education(grant numbers:2021FF08); Program of Shanghai Academic Research Leader(grant numbers:22XD1423700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198384","Fraud prediction;online lending services;information density;network representation learning","Fraud;Representation learning;History;Task analysis;Semantics;Data mining;Boosting","","","","52","IEEE","1 Aug 2023","","","IEEE","IEEE Journals"
"SoK: Fully Homomorphic Encryption Compilers","A. Viand; P. Jattke; A. Hithnawi",ETH Zurich; ETH Zurich; ETH Zurich,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1092","1108","Fully Homomorphic Encryption (FHE) allows a third party to perform arbitrary computations on encrypted data, learning neither the inputs nor the computation results. Hence, it provides resilience in situations where computations are carried out by an untrusted or potentially compromised party. This powerful concept was first conceived by Rivest et al. in the 1970s. However, it remained unrealized until Craig Gentry presented the first feasible FHE scheme in 2009.The advent of the massive collection of sensitive data in cloud services, coupled with a plague of data breaches, moved highly regulated businesses to increasingly demand confidential and secure computing solutions. This demand, in turn, has led to a recent surge in the development of FHE tools. To understand the landscape of recent FHE tool developments, we conduct an extensive survey and experimental evaluation to explore the current state of the art and identify areas for future development.In this paper, we survey, evaluate, and systematize FHE tools and compilers. We perform experiments to evaluate these tools’ performance and usability aspects on a variety of applications. We conclude with recommendations for developers intending to develop FHE-based applications and a discussion on future directions for FHE tools development.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519484","SoK;FHE;Fully-Homomorphic-Encryption;Compilers","Privacy;Tools;Data breach;Encryption;Usability;Surges;Resilience","","32","","85","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Authorship Attribution for Social Media Forensics","A. Rocha; W. J. Scheirer; C. W. Forstall; T. Cavalcante; A. Theophilo; B. Shen; A. R. B. Carvalho; E. Stamatatos","Institute of Computing, University of Campinas, Campinas, Brazil; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Institute of Computing, University of Campinas, Campinas, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Institute of Computing, University of Campinas, Campinas, Brazil; Department of Information and Communication Systems Engineering, University of the Aegean, Karlovasi, Greece","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","1","5","33","The veil of anonymity provided by smartphones with pre-paid SIM cards, public Wi-Fi hotspots, and distributed networks like Tor has drastically complicated the task of identifying users of social media during forensic investigations. In some cases, the text of a single posted message will be the only clue to an author's identity. How can we accurately predict who that author might be when the message may never exceed 140 characters on a service like Twitter? For the past 50 years, linguists, computer scientists, and scholars of the humanities have been jointly developing automated methods to identify authors based on the style of their writing. All authors possess peculiarities of habit that influence the form and content of their written works. These characteristics can often be quantified and measured using machine learning algorithms. In this paper, we provide a comprehensive review of the methods of authorship attribution that can be applied to the problem of social media forensics. Furthermore, we examine emerging supervised learning-based methods that are effective for small sample sizes, and provide step-by-step explanations for several scalable approaches as instructional case studies for newcomers to the field. We argue that there is a significant need in forensics for new authorship attribution algorithms that can exploit context, can process multi-modal data, and are tolerant to incomplete knowledge of the space of all possible authors at training time.","1556-6021","","10.1109/TIFS.2016.2603960","Brazilian Coordination for the Improvement of Higher Education and Personnel — CAPES (DeepEyes project); São Paulo Research Foundation – FAPESP (through the DéjàVu Project)(grant numbers:15/19222-9); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555393","Authorship attribution;forensics;social media;machine learning;computational linguistics;stylometry","Media;Forensics;Internet;Writing;Feature extraction;Context;Speech","","142","","200","IEEE","29 Aug 2016","","","IEEE","IEEE Journals"
"ALDOCX: Detection of Unknown Malicious Microsoft Office Documents Using Designated Active Learning Methods Based on New Structural Feature Extraction Methodology","N. Nissim; A. Cohen; Y. Elovici","Malware Laboratory, Cyber Security Research Center, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Malware Laboratory, Cyber Security Research Center, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Malware Laboratory, Cyber Security Research Center, Ben-Gurion University of the Negev, Beer-Sheva, Israel","IEEE Transactions on Information Forensics and Security","18 Jan 2017","2017","12","3","631","646","Attackers increasingly take advantage of innocent users who tend to casually open email messages assumed to be benign, carrying malicious documents. Recent targeted attacks aimed at organizations utilize the new Microsoft Word documents (*.docx). Anti-virus software fails to detect new unknown malicious files, including malicious docx files. In this paper, we present ALDOCX, a framework aimed at accurate detection of new unknown malicious docx files that also efficiently enhances the framework's detection capabilities over time. Detection relies upon our new structural feature extraction methodology (SFEM), which is performed statically using meta-features extracted from docx files. Using machine-learning algorithms with SFEM, we created a detection model that successfully detects new unknown malicious docx files. In addition, because it is crucial to maintain the detection model's updatability and incorporate new malicious files created daily, ALDOCX integrates our active-learning (AL) methods, which are designed to efficiently assist anti-virus vendors by better focusing their experts' analytical efforts and enhance detection capability. ALDOCX identifies and acquires new docx files that are most likely malicious, as well as informative benign files. These files are used for enhancing the knowledge stores of both the detection model and the anti-virus software. The evaluation results show that by using ALDOCX and SFEM, we achieved a high detection rate of malicious docx files (94.44% TPR) compared with the anti-virus software (85.9% TPR)-with very low FPR rates (0.19%). ALDOCX's AL methods used only 14% of the labeled docx files, which led to a reduction of 95.5% in security experts' labeling efforts compared with the passive learning and the support vector machine (SVM)-Margin (existing active-learning method). Our AL methods also showed a significant improvement of 91% in number of unknown docx malware acquired, compared with the passive learning and the SVM-Margin, thus providing an improved updating solution for the detection model, as well as the anti-virus software widely used within organizations.","1556-6021","","10.1109/TIFS.2016.2631905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762928","Active learning;machine learning;structural;documents;microsoft office files;docx;malware;malicious","Organizations;Feature extraction;Electronic mail;Security;Malware;Portable document format","","64","","21","OAPA","1 Dec 2016","","","IEEE","IEEE Journals"
"Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks With Adversarial Traces","M. S. Rahman; M. Imani; N. Mathews; M. Wright","Global Cybersecurity Institute, Rochester Institute of Technology, Rochester, NY, USA; Anomali Inc., Redwood City, CA, USA; Global Cybersecurity Institute, Rochester Institute of Technology, Rochester, NY, USA; Global Cybersecurity Institute, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Information Forensics and Security","28 Dec 2020","2021","16","","1594","1609","Website Fingerprinting (WF) is a type of traffic analysis attack that enables a local passive eavesdropper to infer the victim's activity, even when the traffic is protected by a VPN or an anonymity system like Tor. Leveraging a deep-learning classifier, a WF attacker can gain over 98% accuracy on Tor traffic. In this paper, we explore a novel defense, Mockingbird, based on the idea of adversarial examples that have been shown to undermine machine-learning classifiers in other domains. Since the attacker gets to design and train his attack classifier based on the defense, we first demonstrate that at a straightforward technique for generating adversarial-example based traces fails to protect against an attacker using adversarial training for robust classification. We then propose Mockingbird, a technique for generating traces that resists adversarial training by moving randomly in the space of viable traces and not following more predictable gradients. The technique drops the accuracy of the state-of-the-art attack hardened with adversarial training from 98% to 42-58% while incurring only 58% bandwidth overhead. The attack accuracy is generally lower than state-of-the-art defenses, and much lower when considering Top-2 accuracy, while incurring lower bandwidth overheads.","1556-6021","","10.1109/TIFS.2020.3039691","National Science Foundation (NSF)(grant numbers:1423163,1722743,1816851,1433736); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265277","Anonymity system;defense;privacy;adversarial machine learning;deep learning","Training;Deep learning;Monitoring;Fingerprint recognition;Bandwidth;Reliability;Privacy","","38","","52","IEEE","20 Nov 2020","","","IEEE","IEEE Journals"
"Reconstruction Attacks Against Mobile-Based Continuous Authentication Systems in the Cloud","M. Al-Rubaie; J. M. Chang","Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","12","2648","2663","Continuous authentication for mobile devices using behavioral biometrics is being suggested to complement initial authentication for securing mobile devices, and the cloud services accessed through them. This area has been studied over the past few years, and low error rates were achieved; however, it was based on training and testing using support vector machine (SVM) and other non-privacy-preserving machine learning algorithms. To stress the importance of carefully designed privacy-preserving systems, we investigate the possibility of reconstructing gestures raw data from users' authentication profiles or synthesized samples' testing results. We propose two types of reconstruction attacks based on whether actual user samples are available to the adversary (as in SVM profiles) or not. We also propose two algorithms to reconstruct raw data: a numerical-based algorithm that is specific to one compromised system, and a randomization-based algorithm that can work against almost any compromised system. For our experiments, we selected one compromised and four attacked gesture-based continuous authentication systems from the recent literature. The experiments, performed using a public data set, showed that the attacks were feasible, with a median ranging from 80% to 100% against one attacked system using all types of attacks and algorithms, and a median ranging from 73% to 100% against all attacked systems using the randomization-based algorithm and the negative support vector attack. Finally, we analyze the results, and provide recommendations for building active authentication systems that could resist reconstruction attacks.","1556-6021","","10.1109/TIFS.2016.2594132","DARPA Active Authentication Program; Brandeis Program(grant numbers:FA8750-12-2-0200,N66001-15-C-4068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523420","Mobile devices;continuous authentication;gestures;privacy;reconstruction attacks;machine learning","Authentication;Image reconstruction;Support vector machines;Mobile handsets;Testing;Algorithm design and analysis;Biometrics (access control)","","35","","37","IEEE","27 Jul 2016","","","IEEE","IEEE Journals"
"Finding the Needle in the Haystack: Metrics for Best Trace Selection in Unsupervised Side-Channel Attacks on Blinded RSA","A. Kulow; T. Schamberger; L. Tebelmann; G. Sigl","Department of Electrical and Computer Engineering, Chair of Security in Information Technology, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Chair of Security in Information Technology, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Chair of Security in Information Technology, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Chair of Security in Information Technology, Technical University of Munich, Munich, Germany","IEEE Transactions on Information Forensics and Security","31 May 2021","2021","16","","3254","3268","For asymmetric ciphers, such as RSA and ECC, side-channel attacks on the underlying exponentiation are mitigated by countermeasures like constant-time implementation and blinding. This restricts an attacker to a single side-channel trace for an attack as a different representation of the private key is used for each exponentiation. In this work, we propose an unsupervised machine learning framework for side-channel attacks on asymmetric cryptography that analyzes leakage in multiple side-channel traces, identifying the best trace for key retrieval. We apply Principal Component Analysis (PCA) preprocessing followed by a classification step that assigns segments of traces to elementary operations of the Square and Multiply exponentiation of RSA. In order to estimate the attack complexity for each trace in terms of key enumeration effort, we introduce two new metrics: The Entropy-based Cost Function (EBCF) is used to select a trace for the attack as well as bits which have to be brute-forced if not all bits can be determined correctly from this single trace. To reduce brute-force complexity further, we introduce Illegal Sequence Detection (ISD) to remove brute-force candidates which do not fit to the Square-and-Multiply scheme. We first provide a proof of concept for 320-bit key length traces and, moving towards a more realistic scenario, retrieve the key from a 1024-bit RSA implementation protected by message and exponent blinding. We are able to select the trace with the least remaining brute-force complexity from 1000 power measurements of the signature generation with randomized inputs and blinding values on a 32-bit ARM Cortex-M4 microcontroller.","1556-6021","","10.1109/TIFS.2021.3074884","German Federal Ministry of Education and Research through the Project SIKRIN-KRYPTOV(grant numbers:16KIS1070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410592","Side-channel analysis;RSA;exponentiation;unsupervised machine learning;PCA;best trace selection","Complexity theory;Entropy;Power measurement;Cryptography;Principal component analysis;Measurement;Side-channel attacks","","5","","31","IEEE","22 Apr 2021","","","IEEE","IEEE Journals"
"Federated Supervised Principal Component Analysis","W. Briguglio; W. A. Yousef; I. Traore; M. Mamun","Department of ECE, University of Victoria, Victoria, BC, Canada; Department of ECE, University of Victoria, Victoria, BC, Canada; Department of ECE, University of Victoria, Victoria, BC, Canada; National Research Council of Canada, Fredericton, NB, Canada","IEEE Transactions on Information Forensics and Security","21 Nov 2023","2024","19","","646","660","In federated learning, standard machine learning (ML) techniques are modified so they can be applied to data held by separate participants without the need for exchanging said data and while preserving privacy. Other data modelling techniques, such as singular value decomposition, have been similarly federated, enabling federated principal component analysis (PCA), which is a popular preprocessing step for ML tasks. Supervised PCA improves on standard PCA by using labeled data to retain more relevant information for supervised ML problems. However, a federated version of supervised PCA does not exist in the literature. In this paper, we propose a federated version of supervised PCA and its dual and kernel variations, called FeS-PCA, dual FeS-PCA, and FeSK-PCA, respectively. We used random orthogonal matrix masking to keep FeS-PCA and dual FeS-PCA private, while FeSK-PCA was kept private using an approximation of the standard approach. We tested our proposed approaches by recreating visualization, classification, and regression experiments from the original unfederated supervised PCA paper. We further added a real-world federated dataset to test the scalability and fidelity of our approach. Our analysis and results indicate that FeS-PCA and dual FeS-PCA are faithful, lossless, and private versions of their unfederated counterparts. Furthermore, despite being an approximation, FeSK-PCA achieves nearly identical performance to standard kernel SPCA in many cases. This is in addition to the added benefit of a reduced runtime and smaller memory footprint.","1556-6021","","10.1109/TIFS.2023.3326981","National Research Council (NRC) under the Digital Health and Geospatial Analytics (DHGA)(grant numbers:DHGA-111-1); Digital Research Alliance of Canada’s; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292699","Federated learning;principal component analysis (PCA);privacy-preserving machine learning;supervised PCA","Principal component analysis;Data privacy;Privacy;Matrix decomposition;Task analysis;Standards;Kernel","","","","38","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"DRAM-Based PUF Utilizing the Variation of Adjacent Cells","E. Abulibdeh; L. Younes; B. Mohammad; K. Humood; H. Saleh; M. Al-Qutayri","Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, The University of Edinburgh, Edinburgh, Scotland, U.K.; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates; Department of Electrical Engineering and Computer Science (EECS), Khalifa University, Abu Dhabi, United Arab Emirates","IEEE Transactions on Information Forensics and Security","2 Feb 2024","2024","19","","2909","2918","The Physical Unclonable Function (PUF) is a security mechanism that takes advantage of the physical variations in a device to create a unique response that can be used as a device signature or secure key. However, many DRAM-based PUFs violate the operating rules of commodity DRAM to exploit a source of entropy in the DRAM read path. This work proposes a fast and reliable DRAM-based PUF that evaluates the variation of adjacent cells and produces the response through the normal read operation. The proposed design is implemented using 65-nm technology, and a detailed statistical SPICE simulation verifies its validity. The statistical analysis shows that the proposed PUF achieves 54.19% uniformity and 49.43% uniqueness, with 98% of the investigated responses achieving a Shannon entropy of 0.95. Additionally, the proposed design generates the response by  $45~\mu s$ , which is at least 66.7 times faster than existing systems. Furthermore, the proposed design uses the relative behavior of cells, which allows for stable responses against temperature and voltage variations, eliminating the need for error correction codes. The proposed PUF also shows resiliency against machine learning-based modeling attacks, as the prediction accuracy does not exceed 55% over 5K Challenge-Response Pairs (CRPs). The area overhead is negligible as the proposed design uses standard circuits, with the addition of only one  $2\times 1$  multiplexer at the inputs of the row buffer.","1556-6021","","10.1109/TIFS.2024.3354115","Technology Innovation Institute (TII), Abu Dhabi, United Arab Emirates(grant numbers:EX2021-005); System on Chip at Khalifa University, Abu Dhabi, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10399912","PUF;DRAM;uniformity;reliability;uniqueness;variation;delay;generation;machine-learning attacks","Random access memory;Physical unclonable function;Transistors;Entropy;Capacitors;Voltage;Software","","","","39","IEEE","15 Jan 2024","","","IEEE","IEEE Journals"
"Deep Abstraction and Weighted Feature Selection for Wi-Fi Impersonation Detection","M. E. Aminanto; R. Choi; H. C. Tanuwidjaja; P. D. Yoo; K. Kim","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Centre for Electronic Warfare, Information and Cyber, Cranfield Defence and Security, Defence Academy of the United Kingdom, Shrivenham, U.K.; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Information Forensics and Security","18 Dec 2017","2018","13","3","621","636","The recent advances in mobile technologies have resulted in Internet of Things (IoT)-enabled devices becoming more pervasive and integrated into our daily lives. The security challenges that need to be overcome mainly stem from the open nature of a wireless medium, such as a Wi-Fi network. An impersonation attack is an attack in which an adversary is disguised as a legitimate party in a system or communications protocol. The connected devices are pervasive, generating high-dimensional data on a large scale, which complicates simultaneous detections. Feature learning, however, can circumvent the potential problems that could be caused by the large-volume nature of network data. This paper thus proposes a novel deep-feature extraction and selection (D-FES), which combines stacked feature extraction and weighted feature selection. The stacked autoencoding is capable of providing representations that are more meaningful by reconstructing the relevant information from its raw inputs. We then combine this with modified weighted feature selection inspired by an existing shallow-structured machine learner. We finally demonstrate the ability of the condensed set of features to reduce the bias of a machine learner model as well as the computational complexity. Our experimental results on a well-referenced Wi-Fi network benchmark data set, namely, the Aegean Wi-Fi Intrusion data set, prove the usefulness and the utility of the proposed D-FES by achieving a detection accuracy of 99.918% and a false alarm rate of 0.012%, which is the most accurate detection of impersonation attacks reported in the literature.","1556-6021","","10.1109/TIFS.2017.2762828","Institute for Information & communications Technology Promotion through the Korea Government (MSIT) (Research on Communication Technology using Bio-Inspired Algorithm and 2017-0-00555, Towards Provable-secure Multi-party Authenticated Key Exchange Protocol based on Lattices in a Quantum World)(grant numbers:2013-0-00396); National Research Foundation of Korea through the Korea Government (MSIT)(grant numbers:NRF-2015R1A-2A2A01006812); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067440","Intrusion detection system;impersonation attack;deep learning;feature extraction;stacked autoencoder;large-scale Wi-Fi networks","Feature extraction;Wireless fidelity;Support vector machines;Wireless networks;Computational modeling;Security;Machine learning","","158","","71","IEEE","13 Oct 2017","","","IEEE","IEEE Journals"
"An Automatic Cost Learning Framework for Image Steganography Using Deep Reinforcement Learning","W. Tang; B. Li; M. Barni; J. Li; J. Huang","Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","7 Oct 2020","2021","16","","952","967","Automatic cost learning for steganography based on deep neural networks is receiving increasing attention. Steganographic methods under such a framework have been shown to achieve better security performance than methods adopting hand-crafted costs. However, they still exhibit some limitations that prevent a full exploitation of their potentiality, including using a function-approximated neural-network-based embedding simulator and a coarse-grained optimization objective without explicitly using pixel-wise information. In this article, we propose a new embedding cost learning framework called SPAR-RL (Steganographic Pixel-wise Actions and Rewards with Reinforcement Learning) that overcomes the above limitations. In SPAR-RL, an agent utilizes a policy network which decomposes the embedding process into pixel-wise actions and aims at maximizing the total rewards from a simulated steganalytic environment, while the environment employs an environment network for pixel-wise reward assignment. A sampling process is utilized to emulate the message embedding of an optimal embedding simulator. Through the iterative interactions between the agent and the environment, the policy network learns a secure embedding policy which can be converted into pixel-wise embedding costs for practical message embedding. Experimental results demonstrate that the proposed framework achieves state-of-the-art security performance against various modern steganalyzers, and outperforms existing cost learning frameworks with regard to learning stability and efficiency.","1556-6021","","10.1109/TIFS.2020.3025438","NSFC(grant numbers:61872244,U1636202,61772349); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2019B151502001); Guangdong Research and Development Program in Key Areas(grant numbers:2019B010139003); Shenzhen Research and Development Program(grant numbers:JCYJ20180305124325555,GJHZ20180928155814437); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205850","Steganography;steganalysis;reinforcement learning;embedding policy;automatic cost learning","Distortion;Machine learning;Neural networks;Learning (artificial intelligence);Gallium nitride;Generative adversarial networks;Security","","82","","48","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Network Traffic Fingerprinting Based on Approximated Kernel Two-Sample Test","J. Kohout; T. Pevný","Cisco Systems, Inc., San Jose, CA, USA; Cisco Systems, Inc., San Jose, CA, USA","IEEE Transactions on Information Forensics and Security","18 Dec 2017","2018","13","3","788","801","Many applications and communication protocols exhibit unique communication patterns that can be exploited to identify them in network traffic. This paper proposes a method to represent these patterns compactly, such that they can be used in different analytical tasks. The method treats each communication as a set of observations of a random variable with unknown probability distribution. This view allows us to derive the representation from a distance between two probability distributions used in maximum mean discrepancy—a non-parametric kernel test. The representation (and distance) can be then easily used in various algorithms for identification of communicating application and data analysis, independently of the specific type of input data.","1556-6021","","10.1109/TIFS.2017.2768018","Research Grant GAČR(grant numbers:15-08916S); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089373","Communication fingerprinting;maximum mean discrepancy;application identification","Kernel;Probability distribution;Inspection;Machine learning algorithms;Protocols;Memory management;Random variables","","10","","52","IEEE","30 Oct 2017","","","IEEE","IEEE Journals"
"Optimal Adversarial Policies in the Multiplicative Learning System With a Malicious Expert","S. R. Etesami; N. Kiyavash; V. Leon; H. V. Poor","Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; College of Management of Technology, EPFL, Lausanne, Switzerland; Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Information Forensics and Security","10 Feb 2021","2021","16","","2276","2287","We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts' advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. The system's loss is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of 1+O√(ln N)/N, where N is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in O(N3). We also discuss a generalization of our model to multi-expert settings. Our results provide a new direction for vulnerability assessment of commonly-used learning algorithms to internal adversarial attacks.","1556-6021","","10.1109/TIFS.2021.3052360","U.S. National Science Foundation(grant numbers:EPCN-1944403,CCF-1908308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328196","Adversarial learning;expert advice;Markov decision process;dynamic programming;approximation ratio","Learning systems;Prediction algorithms;Machine learning algorithms;Motion pictures;Approximation algorithms;Analytical models;Training","","1","","23","IEEE","18 Jan 2021","","","IEEE","IEEE Journals"
"MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-Based Malware Detection","A. Rashid; J. Such","Department of Informatics, King’s College London, Strand Campus, London, U.K; Department of Informatics, King’s College London, Strand Campus, London, U.K","IEEE Transactions on Information Forensics and Security","26 Jul 2023","2023","18","","4361","4376","ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper, we present MalProtect, which is a stateful defense against query attacks in the malware detection domain. MalProtect uses several threat indicators to detect attacks. Our results show that it reduces the evasion rate of adversarial query attacks by 80+% in Android and Windows malware, across a range of attacker scenarios. In the first evaluation of its kind, we show that MalProtect outperforms prior stateful defenses, especially under the peak adversarial threat.","1556-6021","","10.1109/TIFS.2023.3293959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177782","Adversarial machine learning;malware detection;machine learning security;deep learning","Malware;Predictive models;Feature extraction;Analytical models;Monitoring;History;Security","","","","90","IEEE","10 Jul 2023","","","IEEE","IEEE Journals"
"A Multimodal Deep Learning Method for Android Malware Detection Using Various Features","T. Kim; B. Kang; M. Rho; S. Sezer; E. G. Im","Department of Computer and Software, Hanyang University, Seoul, South Korea; Centre for Secure Information Technologies, Queen’s University of Belfast, Belfast, U.K.; Department of Computer Science and Engineering, Hanyang University, Seoul, South Korea; Centre for Secure Information Technologies, Queen’s University of Belfast, Belfast, U.K.; Department of Computer Science and Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","13 Sep 2018","2019","14","3","773","788","With the widespread use of smartphones, the number of malware has been increasing exponentially. Among smart devices, android devices are the most targeted devices by malware because of their high popularity. This paper proposes a novel framework for android malware detection. Our framework uses various kinds of features to reflect the properties of android applications from various aspects, and the features are refined using our existence-based or similarity-based feature extraction method for effective feature representation on malware detection. Besides, a multimodal deep learning method is proposed to be used as a malware detection model. This paper is the first study of the multimodal deep learning to be used in the android malware detection. With our detection model, it was possible to maximize the benefits of encompassing multiple feature types. To evaluate the performance, we carried out various experiments with a total of 41 260 samples. We compared the accuracy of our model with that of other deep neural network models. Furthermore, we evaluated our framework in various aspects including the efficiency in model updates, the usefulness of diverse features, and our feature representation method. In addition, we compared the performance of our framework with those of other existing methods including deep learning-based methods.","1556-6021","","10.1109/TIFS.2018.2866319","MSIT (Ministry of Science, ICT), South Korea, under the ITRC (Information Technology Research Center) Support Program(grant numbers:IITP-2018-2013-1-00881); Institute for Information & Communication Technology Promotion (IITP); IITP Grant; Korea Government (MSIT), Development of Defense Technologies against Ransomware(grant numbers:2017-0-00388); National Research Foundation of Korea; Korea Government (MSIP)(grant numbers:NRF-2016R1A2B4015254); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443370","Android malware;intrusion detection;machine learning;malware detection;neural network","Feature extraction;Malware;Androids;Humanoid robots;Machine learning;Neural networks;Libraries","","309","","62","IEEE","21 Aug 2018","","","IEEE","IEEE Journals"
"Federated Learning With Differential Privacy: Algorithms and Performance Analysis","K. Wei; J. Li; M. Ding; C. Ma; H. H. Yang; F. Farokhi; S. Jin; T. Q. S. Quek; H. Vincent Poor","School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; CSIRO Data61, Sydney, NSW, Australia; School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; Singapore University of Technology and Design, Singapore; CSIRO’s Data61, Melbourne, VIC, Australia; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Singapore University of Technology and Design, Singapore; Department of Electrical Engineering, Princeton University, Princeton, NJ","IEEE Transactions on Information Forensics and Security","17 Jun 2020","2020","15","","3454","3469","Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients’ private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients’ side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number  $N$  of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a  $K$ -client random scheduling strategy, where  $K$  ( $1\leq K< N$ ) clients are randomly selected from the  $N$  overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the  $K$ -client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal  $K$  that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels.","1556-6021","","10.1109/TIFS.2020.2988575","National Key Research and Development Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:61872184,61727802); SUTD Growth Plan Grant for AI; U.S. National Science Foundation(grant numbers:CCF-1908308); Princeton Center for Statistics and Machine Learning under a Data X Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069945","Federated learning;differential privacy;convergence performance;information leakage;client selection","Convergence;Privacy;Servers;Training;Analytical models;Distributed databases","","803","","38","IEEE","17 Apr 2020","","","IEEE","IEEE Journals"
"GraphSC: Parallel Secure Computation Made Easy","K. Nayak; X. S. Wang; S. Ioannidis; U. Weinsberg; N. Taft; E. Shi",University of Maryland; University of Maryland; Yahoo!; Facebook; Google; University of Maryland,"2015 IEEE Symposium on Security and Privacy","20 Jul 2015","2015","","","377","394","We propose introducing modern parallel programming paradigms to secure computation, enabling their secure execution on large datasets. To address this challenge, we present Graph SC, a framework that (i) provides a programming paradigm that allows non-cryptography experts to write secure code, (ii) brings parallelism to such secure implementations, and (iii) meets the need for obliviousness, thereby not leaking any private information. Using Graph SC, developers can efficiently implement an oblivious version of graph-based algorithms (including sophisticated data mining and machine learning algorithms) that execute in parallel with minimal communication overhead. Importantly, our secure version of graph-based algorithms incurs a small logarithmic overhead in comparison with the non-secure parallel version. We build Graph SC and demonstrate, using several algorithms as examples, that secure computation can be brought into the realm of practicality for big data analysis. Our secure matrix factorization implementation can process 1 million ratings in 13 hours, which is a multiple order-of-magnitude improvement over the only other existing attempt, which requires 3 hours to process 16K ratings.","2375-1207","978-1-4673-6949-7","10.1109/SP.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163037","secure computation;oblivious algorithms;graph algorithms;parallel algorithms","Algorithm design and analysis;Programming;Parallel processing;Computational modeling;Data mining;Clustering algorithms;Machine learning algorithms","","70","","78","IEEE","20 Jul 2015","","","IEEE","IEEE Conferences"
"Membership Inference Attacks From First Principles","N. Carlini; S. Chien; M. Nasr; S. Song; A. Terzis; F. Tramèr",Google Research; Google Research; Google Research; Google Research; Google Research; Google Research,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1897","1914","A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model’s training dataset. These attacks are currently evaluated using average-case “accuracy” metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., ≤ 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833649","","Measurement;Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning","","52","","73","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Back in Black: Towards Formal, Black Box Analysis of Sanitizers and Filters","G. Argyros; I. Stais; A. Kiayias; A. D. Keromytis",Columbia University; University of Athens; University of Athens; Columbia University,"2016 IEEE Symposium on Security and Privacy (SP)","18 Aug 2016","2016","","","91","109","We tackle the problem of analyzing filter and sanitizer programs remotely, i.e. given only the ability to query the targeted program and observe the output. We focus on two important and widely used program classes: regular expression (RE) filters and string sanitizers. We demonstrate that existing tools from machine learning that are available for analyzing RE filters, namely automata learning algorithms, require a very large number of queries in order to infer real life RE filters. Motivated by this, we develop the first algorithm that infers symbolic representations of automata in the standard membership/equivalence query model. We show that our algorithm provides an improvement of x15 times in the number of queries required to learn real life XSS and SQL filters of popular web application firewall systems such as mod-security and PHPIDS. % Active learning algorithms require the usage of an equivalence oracle, i.e. an oracle that tests the equivalence of a hypothesis with the target machine. We show that when the goal is to audit a target filter with respect to a set of attack strings from a context free grammar, i.e. find an attack or infer that none exists, we can use the attack grammar to implement the equivalence oracle with a single query to the filter. Our construction finds on average 90% of the target filter states when no attack exists and is very effective in finding attacks when they are present. For the case of string sanitizers, we show that existing algorithms for inferring sanitizers modelled as Mealy Machines are not only inefficient, but lack the expressive power to be able to infer real life sanitizers. We design two novel extensions to existing algorithms that allow one to infer sanitizers represented as single-valued transducers. Our algorithms are able to infer many common sanitizer functions such as HTML encoders and decoders. Furthermore, we design an algorithm to convert the inferred models into BEK programs, which allows for further applications such as cross checking different sanitizer implementations and cross compiling sanitizers into different languages supported by the BEK backend. We showcase the power of our techniques by utilizing our black-box inference algorithms to perform an equivalence checking between different HTML encoders including the encoders from Twitter, Facebook and Microsoft Outlook email, for which no implementation is publicly available.","2375-1207","978-1-5090-0824-7","10.1109/SP.2016.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546497","sanitizers;filters;automata;learning;web security","Algorithm design and analysis;Machine learning algorithms;Learning automata;Transducers;Grammar;HTML;Inference algorithms","","23","","49","IEEE","18 Aug 2016","","","IEEE","IEEE Conferences"
"Invisible Probe: Timing Attacks with PCIe Congestion Side-channel","M. Tan; J. Wan; Z. Zhou; Z. Li","School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; University of California, Irvine","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","322","338","PCIe (Peripheral Component Interconnect express) protocol is the de facto protocol to bridge CPU and peripheral devices like GPU, NIC, and SSD drive. There is an increasing demand to install more peripheral devices on a single machine, but the PCIe interfaces offered by Intel CPUs are fixed. To resolve such contention, PCIe switch, PCH (Platform Controller Hub), or virtualization cards are installed on the machine to allow multiple devices to share a PCIe interface. Congestion happens when the collective PCIe traffic from the devices overwhelm the PCIe link capacity, and transmission delay is then introduced.In this work, we found the PCIe delay not only harms device performance but also leaks sensitive information about a user who uses the machine. In particular, as user’s activities might trigger data movement over PCIe (e.g., between CPU and GPU), by measuring PCIe congestion, an adversary accessing another device can infer the victim’s secret indirectly. Therefore, the delay resulted from I/O congestion can be exploited as a side-channel. We demonstrate the threat from PCIe congestion through 2 attack scenarios and 4 victim settings. Specifically, an attacker can learn the workload of a GPU in a remote server by probing a RDMA NIC that shares the same PCIe switch and measuring the delays. Based on the measurement, the attacker is able to know the keystroke timings of the victim, what webpage is rendered on the GPU, and what machine-learning model is running on the GPU. Besides, when the victim is using a low-speed device, e.g., an Ethernet NIC, an attacker controlling an NVMe SSD can launch a similar attack when they share a PCH or virtualization card. The evaluation result shows our attack can achieve high accuracy (e.g., 96.31% accuracy in inferring webpage visited by a victim).","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00059","Fudan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519482","PCIe;Side-Channel","Performance evaluation;Privacy;Protocols;Graphics processing units;Switches;Machine learning;Delays","","12","","103","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"“Adversarial Examples” for Proof-of-Learning","R. Zhang; J. Liu; Y. Ding; Z. Wang; Q. Wu; K. Ren",Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1408","1422","In S&P 21, Jia et al. proposed a new concept/mechanism named proof-of-learning (PoL), which allows a prover to demonstrate ownership of a machine learning model by proving integrity of the training procedure. It guarantees that an adversary cannot construct a valid proof with less cost (in both computation and storage) than that made by the prover in generating the proof. A PoL proof includes a set of intermediate models recorded during training, together with the corresponding data points used to obtain each recorded model. Jia et al. claimed that an adversary merely knowing the final model and training dataset cannot efficiently find a set of intermediate models with correct data points. In this paper, however, we show that PoL is vulnerable to “adversarial examples”! Specifically, in a similar way as optimizing an adversarial example, we could make an arbitrarily-chosen data point “generate” a given model, hence efficiently generating intermediate models with correct data points. We demonstrate, both theoretically and empirically, that we are able to generate a valid proof with significantly less cost than generating a proof by the prover.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833596","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833596","","Training;Privacy;Costs;Computational modeling;Machine learning;Data models;Security","","4","","29","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks","S. Dyrmishi; S. Ghamizi; T. Simonetto; Y. L. Traon; M. Cordy",University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1384","1400","While the literature on security attacks and defenses of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection) and seven datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealistic attacks. We shed light on the patterns that discriminate which unrealistic examples can be used for effective hardening. We release our code, datasets and models to support future research in exploring how to reduce the gap between unrealistic and realistic adversarial attacks.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179316","adversarial attacks;constrained feature space;problem space;hardening","Privacy;Analytical models;Codes;Botnet;Text categorization;Machine learning;Robustness","","1","","46","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"An End-to-End Dense-InceptionNet for Image Copy-Move Forgery Detection","J. -L. Zhong; C. -M. Pun","Department of Computer and Information Science, University of Macau, Macau, China; Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Information Forensics and Security","3 Feb 2020","2020","15","","2134","2146","A novel image copy-move forgery detection scheme using a Dense-InceptionNet is proposed in this paper. Dense-InceptionNet is an end-to-end, multi-dimensional dense-feature connection, Deep Neural Network (DNN). It is the first DNN model to autonomously learn the feature correlations and search the possible forgery snippets through the matching clues. The proposed Dense-InceptionNet consists of Pyramid Feature Extractor (PFE), Feature Correlation Matching (FCM), and Hierarchical Post-Processing (HPP) modules. The PFE module is proposed to extract multi-dimensional and multi-scale dense-features. The features of each layer in this extractor module are directly connected to the preceding layers. The FCM module is proposed to learn the high correlations of deep features and obtain three candidate matching maps. Finally, the HPP module which makes use of three matching maps to obtain a combination of cross-entropies is amenable to better training via backpropagation. Experiments demonstrate that the efficiency of the proposed Dense-InceptionNet is much better than the other state-of-the-art methods while achieving the relative best performance against most known attacks.","1556-6021","","10.1109/TIFS.2019.2957693","Universidade de Macau(grant numbers:MYRG2018-00035-FST,MYRG2019-00086-FST); Fundo para o Desenvolvimento das Ciências e da Tecnologia(grant numbers:041-2017-A1,0019/2019/A); Characteristic Innovation Projects of Universities in Guangdong Province (Natural Science)(grant numbers:2018GKTSCX022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8926513","Copy-move forgery detection;deep neural network;Dense-InceptionNet","Feature extraction;Forgery;Correlation;Training;Neural networks;Transforms;Data mining","","91","","39","IEEE","6 Dec 2019","","","IEEE","IEEE Journals"
"ABSI: An Adaptive Binary Splitting Algorithm for Malicious Meter Inspection in Smart Grid","X. Xia; Y. Xiao; W. Liang","University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA; Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","IEEE Transactions on Information Forensics and Security","7 Aug 2018","2019","14","2","445","458","Electricity theft is a widespread problem that causes tremendous economic losses for all utility companies around the globe. As many countries struggle to update their antique power systems to emerging smart grids, more and more smart meters are deployed throughout the world. Compared with analog meters which can be tampered with by only physical attacks, smart meters can be manipulated by malicious users with both physical and cyber-attacks for the purpose of stealing electricity. Thus, electricity theft will become even more serious in a smart grid than in a traditional power system if utility companies do not implement efficient solutions. The goal of this paper is to identify all malicious users in a neighborhood area in a smart grid within the shortest detection time. We propose an adaptive binary splitting inspection (ABSI) algorithm which adopts a group testing method to locate the malicious users. There are two considered inspection strategies in this paper: a scanning method in which users will be inspected individually, and a binary search method by which a specific number of users will be examined as a whole. During the inspection process of our proposed scheme, the inspection strategy as well as the number of users in the groups to be inspected are adaptively adjusted. Simulation results show that the proposed ABSI algorithm outperforms existing methods.","1556-6021","","10.1109/TIFS.2018.2854703","National Key Research and Development Program of China(grant numbers:2017YFE0101300); National Science Foundation(grant numbers:CNS-1059265); National Natural Science Foundation of China(grant numbers:61374200,71661147005); Ministry of Industry and Information Technology of the People's Republic of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409488","Electricity theft;smart grid;security;group testing","Inspection;Smart grids;Companies;Smart meters;Testing;Meters","","35","","51","IEEE","10 Jul 2018","","","IEEE","IEEE Journals"
"Brain-Inspired Golden Chip Free Hardware Trojan Detection","S. Faezi; R. Yasaei; A. Barua; M. A. A. Faruque","Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Information Forensics and Security","8 Apr 2021","2021","16","","2697","2708","Since 2007, the use of side-channel measurements for detecting Hardware Trojan (HT) has been extensively studied. However, the majority of works either rely on a golden chip, or they rely on methods that are not robust against subtle acceptable changes that would occur over the life-cycle of an integrated circuit (IC). In this paper, we propose using a brain-inspired architecture called Hierarchical Temporal Memory (HTM) for HT detection. Similar to the human brain, our proposed solution is resilient against natural changes that might happen in the side-channel measurements while being able to accurately detect abnormal behavior of the chip when the HT gets triggered. We use a self-referencing method for HT detection, which eliminates the need for the golden chip. The effectiveness of our approach is evaluated using TrustHub benchmarks, which shows 92.20% detection accuracy on average.","1556-6021","","10.1109/TIFS.2021.3062989","Office of Naval Research (ONR)(grant numbers:N00014-17-1-2499); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366548","Hardware Trojan;hierarchical temporal memory;anomaly detection;unsupervised training;side-channel analysis","Integrated circuits;Integrated circuit modeling;Supply chains;Fabrication;Trojan horses;Transistors;Training","","28","","55","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"CruParamer: Learning on Parameter-Augmented API Sequences for Malware Detection","X. Chen; Z. Hao; L. Li; L. Cui; Y. Zhu; Z. Ding; Y. Liu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","3 Mar 2022","2022","17","","788","803","Learning on execution behaviour, i.e., sequences of API calls, is proven to be effective in malware detection. In this paper, we present CruParamer, a deep neural network based malware detection approach for Windows platform that performs learning on sequences of parameter-augmented APIs. It first employs rule-based and clustering-based classification to assess the sensitivity of a parameter to malicious behaviour, and further labels the API following the run-time parameters with varying degrees of sensitivities. Then, it encodes the APIs by concatenating the native embedding and the sensitive embedding of labelled APIs, for characterizing the relationship between successive labelled APIs and their correspondence in terms of security semantics. Finally, it feeds the sequences of API embedding into the deep neural network for training a binary classifier to detect malware. In addition to presenting the design, we have implemented CruParamer and evaluated it on two datasets. The results demonstrate that CruParamer outperforms naïve models when taking raw APIs as input, proving the effectiveness of CruParamer. Moreover, we have evaluated the impact of mimicry and adversarial attacks on our model, and the results verify the robustness of CruParamer.","1556-6021","","10.1109/TIFS.2022.3152360","National Natural Science Foundation of China(grant numbers:61972392,62072453); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2020164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715123","Malware detection;API sequence;run-time parameter;word embedding;deep learning","Malware;Semantics;Security;Sensitivity;Labeling;Feature extraction;Deep learning","","23","","60","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"RAD: A Statistical Mechanism Based on Behavioral Analysis for DDoS Attack Countermeasure","M. Hajimaghsoodi; R. Jalili","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Information Forensics and Security","3 Aug 2022","2022","17","","2732","2745","Nowadays, Distributed Denial of Service (DDoS) attacks are among the most prevailing and costly attacks across the networks which challenge a variety of services. While many defense mechanisms are presented to detect and mitigate DDoS attacks, attackers constantly explore alternative approaches for orchestrating novel DDoS attacks. Distribution of the mechanism and its deployment into different zones can improve the accuracy and coverage of DDoS attack varieties. In this paper, we propose a 3-phase DDoS attack countermeasure, named  $RAD$ , based on a statistical model for scoring users in order to detect DDoS attacks. In the first phase, users are classified into either suspicious or benign based on their traffic behavior, being indicated by the number of flows, packets, concurrent connections, and amount of user-generated traffic. In the second phase, we identify a potential attack state using the drop, jitter, and delay processing parameters. In the third phase, relevant policies are enforced on the suspicious class of users and its effects are assessed continuously in order to reduce false alarms.  $RAD$  is evaluated through the UNB CICDDoS2019 dataset and is compared with four well-known DDoS detection algorithms.  $RAD$  counters DDoS attacks with more than 80% precision, 99% recall, and 89% F1-Measure in CICDDoS2019.","1556-6021","","10.1109/TIFS.2022.3172598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767830","Statistical defense mechanism;user behavioral analysis;DDoS attack countermeasure;event correlation","Denial-of-service attack;Computer crime;Servers;Feature extraction;Entropy;Monitoring;Task analysis","","9","","36","IEEE","3 May 2022","","","IEEE","IEEE Journals"
"Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation","G. -N. Dong; C. -M. Pun; Z. Zhang","Department of Computer and Information Science, University of Macau, Taipa, Macau; Department of Computer and Information Science, University of Macau, Taipa, Macau; Department of Computer and Information Science, University of Macau, Taipa, Macau","IEEE Transactions on Information Forensics and Security","30 Aug 2021","2021","16","","4197","4210","Kinship verification is a long-standing research challenge in computer vision. The visual differences presented to the face have a significant effect on the recognition capabilities of the kinship systems. We argue that aggregating multiple visual knowledge can better describe the characteristics of the subject for precise kinship identification. Typically, the age-invariant features can represent more natural facial details. Such age-related transformations are essential for face recognition due to the biological effects of aging. However, the existing methods mainly focus on employing the single-view image features for kinship identification, while more meaningful visual properties such as race and age are directly ignored in the feature learning step. To this end, we propose a novel deep collaborative multi-modal learning (DCML) to integrate the underlying information presented in facial properties in an adaptive manner to strengthen the facial details for effective unsupervised kinship verification. Specifically, we construct a well-designed adaptive feature fusion mechanism, which can jointly leverage the complementary properties from different visual perspectives to produce composite features and draw greater attention to the most informative components of spatial feature maps. Particularly, an adaptive weighting strategy is developed based on a novel attention mechanism, which can enhance the dependencies between different properties by decreasing the information redundancy in channels in a self-adaptive manner. Moreover, we propose to use self-supervised learning to further explore the intrinsic semantics embedded in raw data and enrich the diversity of samples. As such, we could further improve the representation capabilities of kinship feature learning and mitigate the multiple variations from original visual images. To validate the effectiveness of the proposed method, extensive experimental evaluations conducted on four widely-used datasets show that our DCML method is always superior to some state-of-the-art kinship verification methods.","1556-6021","","10.1109/TIFS.2021.3098165","University of Macau(grant numbers:MYRG2018-00035-FST,MYRG2019-00086-FST); Science and Technology Development Fund, Macau SAR(grant numbers:0034/2019/AMJ,0087/2020/A2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490195","Kinship verification;information security;self-supervised learning","Feature extraction;Task analysis;Visualization;Supervised learning;Correlation;Collaboration;Semantics","","3","","59","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"A High Accuracy and Adaptive Anomaly Detection Model With Dual-Domain Graph Convolutional Network for Insider Threat Detection","X. Li; X. Li; J. Jia; L. Li; J. Yuan; Y. Gao; S. Yu","Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Information Forensics and Security","1 Mar 2023","2023","18","","1638","1652","Insider threat is destructive and concealable, making addressing it a challenging task in cybersecurity. Most existing methods transform user behavior into sequential information and analyze user behavior while neglecting structural information among users, resulting in high false positives. To solve this problem, in this paper, we propose Dual-Domain Graph Convolutional Network (referred to as DD-GCN), a graph-based modularized method for high accuracy and adaptive insider threat detection. The central idea is to convert user features and structural information into heterogeneous graphs in the light of various relationships and take user behavior and relationship into account together. To this end, a weighted feature similarity mechanism is applied to balance the feature similarity of users and original linkages among them so as to generate the fused structure. Next, specific graph embeddings are extracted from the original topology structure and fused structure simultaneously, which convert behavior information into high-level representations. Furthermore, an attention mechanism is applied to learn the adaptive importance weights of the user’s features in the corresponding embedding. The combination and difference constraints are proposed to enhance the learned embeddings’ commonality and the ability to capture different information. Extensive experiments on two real-world datasets clearly show that our proposed DD-GCN extracts the most correlated information from structural topology and feature information substantially, and achieves improved accuracy with a clear margin.","1556-6021","","10.1109/TIFS.2023.3245413","National Natural Science Foundation of China(grant numbers:62202066,62102040,62002028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044727","Insider threat detection;anomaly detection;graph convolutional network","Behavioral sciences;Feature extraction;Convolutional neural networks;Topology;Adaptation models;Network topology;Couplings","","3","","44","IEEE","14 Feb 2023","","","IEEE","IEEE Journals"
"Transcending TRANSCEND: Revisiting Malware Classification in the Presence of Concept Drift","F. Barbero; F. Pendlebury; F. Pierazzi; L. Cavallaro",King’s College London; King’s College London; King’s College London; University College London,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","805","823","Machine learning for malware classification shows encouraging results, but real deployments suffer from performance degradation as malware authors adapt their techniques to evade detection. This phenomenon, known as concept drift, occurs as new malware examples evolve and become less and less like the original training examples. One promising method to cope with concept drift is classification with rejection in which examples that are likely to be misclassified are instead quarantined until they can be expertly analyzed.We propose TRANSCENDENT, a rejection framework built on Transcend, a recently proposed strategy based on conformal prediction theory. In particular, we provide a formal treatment of Transcend, enabling us to refine conformal evaluation theory—its underlying statistical engine—and gain a better understanding of the theoretical reasons for its effectiveness. In the process, we develop two additional conformal evaluators that match or surpass the performance of the original while significantly decreasing the computational overhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that removes sources of experimental bias present in the original evaluation. TRANSCENDENT outperforms state-of-the-art approaches while generalizing across different malware domains and classifiers.To further assist practitioners, we showcase optimal operational settings for a TRANSCENDENT deployment and show how it can be applied to many popular learning algorithms. These insights support both old and new empirical findings, making Transcend a sound and practical solution for the first time. To this end, we release TRANSCENDENT as open source, to aid the adoption of rejection strategies by the security community.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833659","security;machine learning;malware detection","Training;Degradation;Privacy;Pipelines;Machine learning;Prediction theory;Malware","","10","","53","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"NEUZZ: Efficient Fuzzing with Neural Program Smoothing","D. She; K. Pei; D. Epstein; J. Yang; B. Ray; S. Jana",Columbia University; Columbia University; Columbia University; Columbia University; Columbia University; Columbia University,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","803","817","Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the target program's discrete branching behavior. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly increase the efficiency of the fuzzing process. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 popular real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 previously unknown bugs (including two CVEs) that other fuzzers failed to find in 10 real-world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers over 24 hour runs. Furthermore, NEUZZ also outperformed existing fuzzers on both LAVA-M and DARPA CGC bug datasets.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835342","fuzzing;-neural-program-smoothing;-gradient-guided-mutation","Optimization;Fuzzing;Computer bugs;Artificial neural networks;Smoothing methods;Evolutionary computation","","70","","89","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"The Spyware Used in Intimate Partner Violence","R. Chatterjee; P. Doerfler; H. Orgad; S. Havron; J. Palmer; D. Freed; K. Levy; N. Dell; D. McCoy; T. Ristenpart",Cornell Tech; New York University; Technion; Cornell University; Hunter College; Cornell Tech; Cornell University; Cornell Tech; New York University; Cornell Tech,"2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","441","458","Survivors of intimate partner violence increasingly report that abusers install spyware on devices to track their location, monitor communications, and cause emotional and physical harm. To date there has been only cursory investigation into the spyware used in such intimate partner surveillance (IPS). We provide the first in-depth study of the IPS spyware ecosystem. We design, implement, and evaluate a measurement pipeline that combines web and app store crawling with machine learning to find and label apps that are potentially dangerous in IPS contexts. Ultimately we identify several hundred such IPS-relevant apps. While we find dozens of overt spyware tools, the majority are ""dual-use"" apps - they have a legitimate purpose (e.g., child safety or anti-theft), but are easily and effectively repurposed for spying on a partner. We document that a wealth of online resources are available to educate abusers about exploiting apps for IPS. We also show how some dual-use app developers are encouraging their use in IPS via advertisements, blogs, and customer support services. We analyze existing anti-virus and anti-spyware tools, which universally fail to identify dual-use apps as a threat.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418618","Spyware;Android Spyware;Intimate Partner Violence;Domestic Violence;Dual use Apps;Play Store Crawling;Query Snowballing","IP networks;Spyware;Tools;Google;Monitoring;Safety;Pipelines","","54","","62","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"FP-STALKER: Tracking Browser Fingerprint Evolutions","A. Vastel; P. Laperdrix; W. Rudametkin; R. Rouvoy",Univ. Lille/Inria; INSA/Inria; Univ. Lille/Inria; Univ. Lille/Inria/IUF,"2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","728","741","Browser fingerprinting has emerged as a technique to track users without their consent. Unlike cookies, fingerprinting is a stateless technique that does not store any information on devices, but instead exploits unique combinations of attributes handed over freely by browsers. The uniqueness of fingerprints allows them to be used for identification. However, browser fingerprints change over time and the effectiveness of tracking users over longer durations has not been properly addressed. In this paper, we show that browser fingerprints tend to change frequently-from every few hours to days-due to, for example, software updates or configuration changes. Yet, despite these frequent changes, we show that browser fingerprints can still be linked, thus enabling long-term tracking. FP-STALKER is an approach to link browser fingerprint evolutions. It compares fingerprints to determine if they originate from the same browser. We created two variants of FP-STALKER, a rule-based variant that is faster, and a hybrid variant that exploits machine learning to boost accuracy. To evaluate FP-STALKER, we conduct an empirical study using 98,598 fingerprints we collected from 1, 905 distinct browser instances. We compare our algorithm with the state of the art and show that, on average, we can track browsers for 54.48 days, and 26 % of browsers can be tracked for more than 100 days.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418634","Privacy;browser fingerprinting;browsers","Browsers;Microsoft Windows;Portable document format;Security;Privacy;Fingerprint recognition;Target tracking","","34","","26","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"Illuminant-Based Transformed Spaces for Image Forensics","T. Carvalho; F. A. Faria; H. Pedrini; R. da S. Torres; A. Rocha","RECOD Laboratory, Institute of Computing, University of Campinas, Campinas, Brazil; GIBIS Laboratory, Federal University of São Paulo, São Paulo, Brazil; RECOD Laboratory, Institute of Computing, University of Campinas, Campinas, Brazil; RECOD Laboratory, Institute of Computing, University of Campinas, Campinas, Brazil; RECOD Laboratory, Institute of Computing, University of Campinas, Campinas, Brazil","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","4","720","733","In this paper, we explore transformed spaces, represented by image illuminant maps, to propose a methodology for selecting complementary forms of characterizing visual properties for an effective and automated detection of image forgeries. We combine statistical telltales provided by different image descriptors that explore color, shape, and texture features. We focus on detecting image forgeries containing people and present a method for locating the forgery, specifically, the face of a person in an image. Experiments performed on three different open-access data sets show the potential of the proposed method for pinpointing image forgeries containing people. In the two first data sets (DSO-1 and DSI-1), the proposed method achieved a classification accuracy of 94% and 84%, respectively, a remarkable improvement when compared with the state-of-the-art methods. Finally, when evaluating the third data set comprising questioned images downloaded from the Internet, we also present a detailed analysis of target images.","1556-6021","","10.1109/TIFS.2015.2506548","Coordination for the Improvement of Higher Education Personnel(grant numbers:0214-13-2); Microsoft Research; CAPES DeepEyes Project; São Paulo Research Foundation(grant numbers:2010/05647-4,2010/14910-0,2011/22749-8); Brazilian National Research Council(grant numbers:140916/2012-1,477662/2013-7,307113/2012-4,304352/2012-8); Instituto Federal de Educação, Ciência e Tecnologia do Sudeste de Minas Gerais; University of Campinas; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349174","Digital forensics;splicing detection;illuminant maps;image descriptors;machine learning;diversity measures;Digital forensics;splicing detection;illuminant maps;image descriptors;machine learning;diversity measures","Forgery;Image color analysis;Splicing;Light sources;Lighting;Visualization;Shape","","70","","33","IEEE","8 Dec 2015","","","IEEE","IEEE Journals"
"Camera Model Identification With Unknown Models","Y. Huang; J. Zhang; H. Huang","Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Information Technology, Deakin University, Burwood, VIC, Australia; Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","12","2692","2704","Feature based camera model identification plays an important role for forensics investigations on images. The conventional feature based identification schemes suffer from the problem of unknown models, that is, some images are captured by the camera models previously unknown to the identification system. To address this problem, we propose a new scheme: Source Camera Identification with Unknown models (SCIU). It has the capability of identifying images of the unknown models as well as distinguishing images of the known models. The new SCIU scheme consists of three stages: 1) unknown detection; 2) unknown expansion; and 3) (K+1 )-class classification. Unknown detection applies a k -nearest neighbours method to recognize a few sample images of unknown models from the unlabeled images. Unknown expansion further extends the set of unknown sample images using a self-training strategy. Then, we address a specific (K+1)-class classification, in which the sample images of unknown (1-class) and known models (K-class) are combined to train a classifier. In addition, we develop a parameter optimization method for unknown detection, and investigate the stopping criterion for unknown expansion. The experiments carried out on the Dresden image collection confirm the effectiveness of the proposed SCIU scheme. When unknown models present, the identification accuracy of SCIU is significantly better than the four state-of-art methods: 1) multi-class Support Vector Machine (SVM); 2) binary SVM; 3) combined classification framework; and 4) decision boundary carving.","1556-6021","","10.1109/TIFS.2015.2474836","National Natural Science Foundation of China(grant numbers:61300077); National 863 Programme(grant numbers:2013AA01A212); Basic Research Foundation of Beijing Institute of Technology(grant numbers:20130742004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229324","Camera model identification;unknown models;machine learning;digital forensics;Camera model identification;unknown models;machine learning;digital forensics","Cameras;Training;Support vector machines;Nickel;Feature extraction;Accuracy;Testing","","26","","45","IEEE","28 Aug 2015","","","IEEE","IEEE Journals"
"TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems","B. G. Doan; M. Xue; S. Ma; E. Abbasnejad; D. C. Ranasinghe","School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; Department of Computer Science, Rutgers University, New Brunswick, NJ, USA; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Information Forensics and Security","31 Oct 2022","2022","17","","3816","3830","Deep neural networks (DNNs), regardless of their impressive performance, are vulnerable to attacks from adversarial inputs and, more recently, Trojans to misguide or hijack the decision of the model. We expose the existence of an intriguing class of spatially bounded, physically realizable, adversarial examples— Universal NaTuralistic adversarial paTches—we call TnTs, by exploring the super set of the spatially bounded adversarial example space and the natural input space within generative adversarial networks. Now, an adversary can arm themselves with a patch that is naturalistic, less malicious-looking, physically realizable, highly effective—achieving high attack success rates, and universal. A TnT is universal because any input image captured with a TnT in the scene will: i) misguide a network (untargeted attack); or ii) force the network to make a malicious decision (targeted attack). Interestingly, now, an adversarial patch attacker has the potential to exert a greater level of control—the ability to choose a location independent, natural-looking patch as a trigger in contrast to being constrained to noisy perturbations—an ability is thus far shown to be only possible with Trojan attack methods needing to interfere with the model building processes to embed a backdoor at the risk discovery; but, still realize a patch deployable in the physical world. Through extensive experiments on the large-scale visual classification task, ImageNet with evaluations across its entire validation set of 50,000 images, we demonstrate the realistic threat from TnTs and the robustness of the attack. We show a generalization of the attack to create patches achieving higher attack success rates than existing state-of-the-art methods. Our results show the generalizability of the attack to different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural networks such as WideResnet50, Inception-V3 and VGG-16.","1556-6021","","10.1109/TIFS.2022.3198857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856683","Artificial intelligence;machine learning;adversarial machine learning neural networks;artificial neural networks;convolutional neural networks","Trojan horses;Generative adversarial networks;Task analysis;Perturbation methods;Training;Neural networks;Deep learning","","16","","66","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Certified Robustness to Adversarial Examples with Differential Privacy","M. Lecuyer; V. Atlidakis; R. Geambasu; D. Hsu; S. Jana",Columbia University; Columbia University; Columbia University; Columbia University; Columbia University,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","656","672","Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835364","Adversarial-Examples;Machine-Learning;Security;Defense;Deep-Learning","Robustness;Databases;Measurement;Predictive models;Mathematical model;Standards;Differential privacy","","234","1","71","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Bad Characters: Imperceptible NLP Attacks","N. Boucher; I. Shumailov; R. Anderson; N. Papernot","University of Cambridgem, Computer Science & Technology; University of Cambridge and Vector Institute; University of Cambridge and University of Edinburgh; University of Cambridge and Vector Institute","2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1987","2004","Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection – representing one invisible character, homoglyph, reordering, or deletion – an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833641","adversarial machine learning;NLP;text-based models;text encodings;search engines","Visualization;Toxicology;Systematics;Social networking (online);Perturbation methods;Taxonomy;Natural language processing","","23","","84","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Automatically Detecting Bystanders in Photos to Reduce Privacy Risks","R. Hasan; D. Crandall; M. Fritz; A. Kapadia","Indiana University, Bloomington, USA; Indiana University, Bloomington, USA; CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Germany; Indiana University, Bloomington, USA","2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","318","335","Photographs taken in public places often contain bystanders - people who are not the main subject of a photo. These photos, when shared online, can reach a large number of viewers and potentially undermine the bystanders' privacy. Furthermore, recent developments in computer vision and machine learning can be used by online platforms to identify and track individuals. To combat this problem, researchers have proposed technical solutions that require bystanders to be proactive and use specific devices or applications to broadcast their privacy policy and identifying information to locate them in an image.We explore the prospect of a different approach - identifying bystanders solely based on the visual information present in an image. Through an online user study, we catalog the rationale humans use to classify subjects and bystanders in an image, and systematically validate a set of intuitive concepts (such as intentionally posing for a photo) that can be used to automatically identify bystanders. Using image data, we infer those concepts and then use them to train several classifier models. We extensively evaluate the models and compare them with human raters. On our initial dataset, with a 10-fold cross validation, our best model achieves a mean detection accuracy of 93% for images when human raters have 100% agreement on the class label and 80% when the agreement is only 67%. We validate this model on a completely different dataset and achieve similar results, demonstrating that our model generalizes well.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152778","privacy;computer vision;machine learning;photos;bystanders","Privacy;Cameras;Visualization;Face;Feature extraction;Facial features","","16","","72","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Secret Key Generation for IRS-Assisted Multi-Antenna Systems: A Machine Learning-Based Approach","C. Chen; J. Zhang; T. Lu; M. Sandell; L. Chen","Department of Electrical Engineering and Electronics, University of Liverpool, Liverpool, U.K.; Department of Electrical Engineering and Electronics, University of Liverpool, Liverpool, U.K.; School of Cyber Science and Engineering, Southeast University, Nanjing, China; Bristol Research and Innovation Laboratory, Toshiba Research Europe Ltd., Bristol, U.K.; School of Cyber Science and Engineering, Southeast University, Nanjing, China","IEEE Transactions on Information Forensics and Security","30 Nov 2023","2024","19","","1086","1098","Physical-layer key generation (PKG) based on wireless channels is a lightweight technique to establish secure keys between legitimate communication nodes. Recently, intelligent reflecting surfaces (IRSs) have been leveraged to enhance the performance of PKG in terms of secret key rate (SKR), as it can reconfigure the wireless propagation environment and introduce more channel randomness. In this paper, we investigate an IRS-assisted PKG system, taking into account the channel spatial correlation at both the base station (BS) and the IRS. Based on the considered system model, the closed-form expression of SKR is derived analytically considering correlated eavesdropping channels. Aiming to maximize the SKR, a joint design problem of the BS’s precoding matrix and the IRS’s phase shift vector is formulated. To address this high-dimensional non-convex optimization problem, we propose a novel unsupervised deep neural network (DNN)-based algorithm with a simple structure. Different from most previous works that adopt iterative optimization to solve the problem, the proposed DNN-based algorithm directly obtains the BS precoding and IRS phase shifts as the output of the DNN. Simulation results reveal that the proposed DNN-based algorithm outperforms the benchmark methods with regard to SKR.","1556-6021","","10.1109/TIFS.2023.3331588","National Key Research and Development Program of China(grant numbers:2020YFE0200600); U.K. Engineering and Physical Sciences Research Council(grant numbers:EP/V027697/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10315046","Physical-layer key generation;intelligent reflecting surfaces;deep neural network","Precoding;Optimization;Wireless communication;Transmission line matrix methods;Quantization (signal);Eavesdropping;Downlink","","","","38","IEEE","10 Nov 2023","","","IEEE","IEEE Journals"
"Hear ""No Evil"", See ""Kenansville"": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems","H. Abdullah; M. S. Rahman; W. Garcia; K. Warren; A. S. Yadav; T. Shrimpton; P. Traynor",University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","712","729","Automatic speech recognition and voice identification systems are being deployed in a wide array of applications, from providing control mechanisms to devices lacking traditional interfaces, to the automatic transcription of conversations and authentication of users. Many of these applications have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state of the art systems, with minimal impact on human comprehension. Processing pipelines for modern systems are comprised of signal preprocessing and feature extraction steps, whose output is fed to a machine-learned model. Prior work has focused on the models, using white-box knowledge to tailor model-specific attacks. We focus on the pipeline stages before the models, which (unlike the models) are quite similar across systems. As such, our attacks are black-box, transferable, can be tuned to require zero queries to the target, and demonstrably achieve mistranscription and misidentification rates as high as 100% by modifying only a few frames of audio. We perform a study via Amazon Mechanical Turk demonstrating that there is no statistically significant difference between human perception of regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are generally not perceived by human subjects, but that are crucial for model accuracy.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519472","","Training;Privacy;Perturbation methods;Pipelines;Force;Feature extraction;Robustness","","24","","85","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Prepose: Privacy, Security, and Reliability for Gesture-Based Programming","L. S. Figueiredo; B. Livshits; D. Molnar; M. Veanes",Federal University of Pernambuco; Microsoft Research; Microsoft Research; Microsoft Research,"2016 IEEE Symposium on Security and Privacy (SP)","18 Aug 2016","2016","","","122","137","With the rise of sensors such as the Microsoft Kinect, Leap Motion, and hand motion sensors in phones (i.e., Samsung Galaxy S6), gesture-based interfaces have become practical. Unfortunately, today, to recognize such gestures, applications must have access to depth and video of the user, exposing sensitive data about the user and her environment. Besides these privacy concerns, there are also security threats in sensor-based applications, such as multiple applications registering the same gesture, leading to a conflict (akin to Clickjacking on the web). We address these security and privacy threats with Prepose, a novel domain-specific language (DSL) for easily building gesture recognizers, combined with a system architecture that protects privacy, security, and reliability with untrusted applications. We run Prepose code in a trusted core, and only return specific gesture events to applications. Prepose is specifically designed to enable precise and sound static analysis using SMT solvers, allowing the system to check security and reliability properties before running a gesture recognizer. We demonstrate that Prepose is expressive by creating gestures in three representative domains: physical therapy, tai-chi, and ballet. We further show that runtime gesture matching in Prepose is fast, creating no noticeable lag, as measured on traces from Microsoft Kinect runs. To show that gesture checking at the time of submission to a gesture store is fast, we developed a total of four Z3-based static analyses to test for basic gesture safety and internal validity, to make sure the so-called protected gestures are not overridden, and to check inter-gesture conflicts. Our static analysis scales well in practice: safety checking is under 0.5 seconds per gesture, average validity checking time is only 188ms, lastly, for 97% of the cases, the conflict detection time is below 5 seconds, with only one query taking longer than 15 seconds.","2375-1207","978-1-5090-0824-7","10.1109/SP.2016.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546499","augmented reality;domain-specific language;kinect;security;privacy","Security;Privacy;Reliability;Skeleton;Sensors;Runtime;Safety","","15","1","27","IEEE","18 Aug 2016","","","IEEE","IEEE Conferences"
"SoK: Certified Robustness for Deep Neural Networks","L. Li; T. Xie; B. Li","University of Illinois Urbana-Champaign; Key Laboratory of High Confidence Software Technologies, MoE (Peking University); University of Illinois Urbana-Champaign","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1289","1310","Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as summarize the methodologies for representative algorithms, 2) reveal the characteristics, strengths, limitations, and fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and future directions for certifiably robust approaches for DNNs, and 4) provide an open-sourced unified platform to evaluate 20+ representative certifiably robust approaches.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179303","certified robustness;neural networks;verification","Privacy;Adaptation models;Taxonomy;Artificial neural networks;Benchmark testing;Robustness;Security","","14","","222","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Are anonymity-seekers just like everybody else? An analysis of contributions to Wikipedia from Tor","C. Tran; K. Champion; A. Forte; B. M. Hill; R. Greenstadt","Department of Computer Science & Engineering, New York University, New York, USA; Department of Communication, University of Washington, Seatle, USA; College of Computing & Informatics, Drexel University, Philadelphia, USA; Department of Communication, University of Washington, Seatle, USA; Department of Computer Science & Engineering, New York University, New York, USA","2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","186","202","User-generated content sites routinely block contributions from users of privacy-enhancing proxies like Tor because of a perception that proxies are a source of vandalism, spam, and abuse. Although these blocks might be effective, collateral damage in the form of unrealized valuable contributions from anonymity seekers is invisible. One of the largest and most important user-generated content sites, Wikipedia, has attempted to block contributions from Tor users since as early as 2005. We demonstrate that these blocks have been imperfect and that thousands of attempts to edit on Wikipedia through Tor have been successful. We draw upon several data sources and analytical techniques to measure and describe the history of Tor editing on Wikipedia over time and to compare contributions from Tor users to those from other groups of Wikipedia users. Our analysis suggests that although Tor users who slip through Wikipedia's ban contribute content that is more likely to be reverted and to revert others, their contributions are otherwise similar in quality to those from other unregistered participants and to the initial contributions of registered users.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152664","","Internet;Encyclopedias;Electronic publishing;IP networks;Relays;Peer-to-peer computing","","5","","39","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems","J. Jin; E. McMurtry; B. I. P. Rubinstein; O. Ohrimenko","School of Computing and Information Systems, The University of Melbourne; Department of Computer Science, ETH Zurich; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne","2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","473","488","Differential privacy is a de facto privacy framework that has seen adoption in practice via a number of mature software platforms. Implementation of differentially private (DP) mechanisms has to be done carefully to ensure end-to-end security guarantees. In this paper we study two implementation flaws in the noise generation commonly used in DP systems. First we examine the Gaussian mechanism’s susceptibility to a floating-point representation attack. The premise of this first vulnerability is similar to the one carried out by Mironov in 2011 against the Laplace mechanism. Our experiments show the attack’s success against DP algorithms, including deep learning models trained using differentially-private stochastic gradient descent. In the second part of the paper we study discrete counterparts of the Laplace and Gaussian mechanisms that were previously proposed to alleviate the shortcomings of floating-point representation of real numbers. We show that such implementations unfortunately suffer from another side channel: a novel timing attack. An observer that can measure the time to draw (discrete) Laplace or Gaussian noise can predict the noise magnitude, which can then be used to recover sensitive attributes. This attack invalidates differential privacy guarantees of systems implementing such mechanisms. We demonstrate that several commonly used, state-of-the-art implementations of differential privacy are susceptible to these attacks. We report success rates up to 92.56% for floating point attacks on DP-SGD, and up to 99.65% for end-to-end timing attacks on private sum protected with discrete Laplace. Finally, we evaluate and suggest partial mitigations.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833672","University of Melbourne; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833672","Differential-Privacy;Timing-Side-Channel;Floating-Point-Representation;Gaussian-Mechanisms;Laplace-Mechanisms","Differential privacy;Privacy;Sensitivity;Stochastic processes;Observers;Libraries;Timing","","3","","55","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"RoFL: Robustness of Secure Federated Learning","H. Lycklama; L. Burkhalter; A. Viand; N. Küchler; A. Hithnawi",ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","453","476","Even though recent years have seen many attacks exposing severe vulnerabilities in Federated Learning (FL), a holistic understanding of what enables these attacks and how they can be mitigated effectively is still lacking. In this work, we demystify the inner workings of existing (targeted) attacks. We provide new insights into why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the need for ML algorithms to memorize tail data has significant implications for FL integrity. This phenomenon has largely been studied in the context of privacy; our analysis sheds light on its implications for ML integrity. We show that certain classes of severe attacks can be mitigated effectively by enforcing constraints such as norm bounds on clients’ updates. We investigate how to efficiently incorporate these constraints into secure FL protocols in the single-server setting. Based on this, we propose RoFL, a new secure FL system that extends secure aggregation with privacy-preserving input validation. Specifically, RoFL can enforce constraints such as L2 and L∞ bounds on high-dimensional encrypted model updates.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179400","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179400","federated-learning;secure-aggregation;privacy-preserving-machine-learning","Privacy;Protocols;Federated learning;Scalability;Aggregates;Bandwidth;Tail","","6","","99","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models","R. Zhu; D. Tang; S. Tang; X. Wang; H. Tang",Indiana University Bloomington; Indiana University Bloomington; Indiana University Bloomington; Indiana University Bloomington; Indiana University Bloomington,"2023 IEEE Symposium on Security and Privacy (SP)","11 Dec 2023","2023","","","1","19","The extensive applications of deep neural network (DNN) and its increasingly complicated architecture and supply chain make the risk of backdoor attacks more realistic than ever. In such an attack, the adversary either poisons the training data of a DNN model or manipulates its training process to stealthily inject a covert backdoor task, alongside the primary task, so as to strategically misclassify inputs carrying a trigger. Defending against such an attack, particularly removing the backdoor effect from an infected model, is known to be hard. For this purpose, prior research either requires a recovered trigger, which is hard to come by, or attempts to fine-tune a model on its primary task, which becomes less effective when the clean data is scarce. In this paper, we present a simple yet surprisingly effective technique to induce ""selective amnesia"" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) efficiently (e.g., about 30 times faster than training a model from scratch on the MNIST dataset), with only a small amount of clean data (e.g., with a size of just 0.1% of training data for TrojAI models).","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10351028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10351028","","Training;Analytical models;Toxicology;Supply chains;Training data;Artificial neural networks;Data models","","","","81","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Event-Triggered Interval-Based Anomaly Detection and Attack Identification Methods for an In-Vehicle Network","M. L. Han; B. I. Kwak; H. K. Kim","School of Cybersecurity, Korea University, Seoul, Republic of Korea; School of Cybersecurity, Korea University, Seoul, Republic of Korea; School of Cybersecurity, Korea University, Seoul, Republic of Korea","IEEE Transactions on Information Forensics and Security","19 Apr 2021","2021","16","","2941","2956","Vehicle communication technology has been steadily progressing alongside the convergence of the in-vehicle network (IVN) and wireless communication technology. The communication with various external networks further reinforces the connectivity between the inside and outside of a vehicle. However, this bears risks of malicious packet attacks on computer-assisted mechanical mechanisms that are capable of hijacking the vehicle's functions. The present study proposes a method to detect and identify abnormalities in vehicular networks based on the periodic event-triggered interval of the controller area network (CAN) messages. To this end, we first define four attack scenarios and then extract normal and abnormal driving data corresponding to these scenarios. Next, we analyze the CAN ID's event-triggered interval and measure statistical moments depending on the defined time-window. Finally, we conduct extensive evaluations of the proposed methods' performance by considering different attack scenarios and three types of machine learning models. The results demonstrate that the proposed method can effectively detect an abnormality in the IVN, with up to 99% accuracy. Our results suggest that when tree-based machine learning models are used as the classifier, the proposed method of attack identification can achieve more than 94% accuracy.","1556-6021","","10.1109/TIFS.2021.3069171","Institute for Information and Communications Technology Promotion (Development of Security Primitives for Unmanned Vehicles)(grant numbers:2020-0-00374); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387321","Anomaly detection;attack identification;controller area network;event-triggered interval;in-vehicle network","Anomaly detection;Entropy;Vehicles;Safety;Receivers;Wireless communication;Protocols","","22","","39","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Label Correlation in Deep Learning-Based Side-Channel Analysis","L. Wu; L. Weissbart; M. Krček; H. Li; G. Perin; L. Batina; S. Picek","Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands; Digital Security Group, Radboud University, Nijmegen, EC, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, XE, The Netherlands","IEEE Transactions on Information Forensics and Security","30 Jun 2023","2023","18","","3849","3861","The efficiency of the profiling side-channel analysis can be significantly improved with machine learning techniques. Although powerful, a fundamental machine learning limitation of being data-hungry received little attention in the side-channel community. In practice, the maximum number of leakage traces that evaluators/attackers can obtain is constrained by the scheme requirements or the limited accessibility of the target. Even worse, various countermeasures in modern devices increase the conditions on the profiling size to break the target. This work demonstrates a practical approach to dealing with the lack of profiling traces. Instead of learning from a one-hot encoded label, transferring the labels to their distribution can significantly speed up the convergence of guessing entropy. By studying the relationship between all possible key candidates, we propose a new metric, denoted Label Correlation (LC), to evaluate the generalization ability of the profiling model. We validate LC with two common use cases: early stopping and network architecture search, and the results indicate its superior performance.","1556-6021","","10.1109/TIFS.2023.3287728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155470","Side-channel analysis;profiling analysis;deep learning;label distribution;profiling model fitting","Measurement;Predictive models;Entropy;Training;Security;Indexes;Correlation","","2","","42","IEEE","19 Jun 2023","","","IEEE","IEEE Journals"
"Secure and Private Distributed Source Coding With Private Keys and Decoder Side Information","O. Günlü; R. F. Schaefer; H. Boche; H. V. Poor","Information Coding Division, Linköping University, Linköping, Sweden; Chair of Information Theory and Machine Learning, the BMBF Research Hub 6G-Life, the Cluster of Excellence “Centre for Tactile Internet with Human-in-the-Loop (CeTI),” and the 5G Laboratory Germany, Technische Universität Dresden, Dresden, Germany; Chair of Theoretical Information Technology and the BMBF Research Hub 6G-Life, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Information Forensics and Security","28 Jun 2023","2023","18","","3803","3816","The distributed source coding problem is extended by positing that noisy measurements of a remote source are the correlated random variables that should be reconstructed at another terminal. We consider a secure and private distributed lossy source coding problem with two encoders and one decoder such that (i) all terminals noncausally observe a noisy measurement of the remote source; (ii) a private key is available to each legitimate encoder and all private keys are available to the decoder; (iii) rate-limited noiseless communication links are available between each encoder and the decoder; (iv) the amount of information leakage to an eavesdropper about the correlated random variables is defined as (v) secrecy leakage, and privacy leakage is measured with respect to the remote source; and (vi) two passive attack scenarios are considered, where a strong eavesdropper can access both communication links and a weak eavesdropper can choose only one of the links to access. Inner and outer bounds on the rate regions defined under secrecy, privacy, communication, and distortion constraints are derived for both passive attack scenarios. When one or both sources should be reconstructed reliably, the rate region bounds are simplified.","1556-6021","","10.1109/TIFS.2023.3286285","ELLIIT funding endowed by the Swedish Government; Zenith Research and Leadership Career Development; German Federal Ministry of Education and Research (BMBF) within the National Initiative on 6G Communication Systems through the Research Hub 6G-Life(grant numbers:16KISK001K); BMBF 6G-Life under Grant 16KISK002 and within the National Initiative for Information Theory for Post Quantum Crypto “Quantum Token Theory and Applications—QTOK”(grant numbers:16KISQ037K); U.S. National Science Foundation (NSF)(grant numbers:CCF-1908308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152476","Secure and private distributed source coding;remote source;rate-limited public communication;weak eavesdropper;passive attack","Source coding;Decoding;Noise measurement;Random variables;Satellite broadcasting;Privacy;6G mobile communication","","","","49","IEEE","14 Jun 2023","","","IEEE","IEEE Journals"
"Automatically Dismantling Online Dating Fraud","G. Suarez-Tangil; M. Edwards; C. Peersman; G. Stringhini; A. Rashid; M. Whitty","King’s College London, London, U.K.; University of Bristol, Bristol, U.K.; University of Bristol, Bristol, U.K.; Boston University, Boston, USA; University of Bristol, Bristol, U.K.; The University of Melbourne, Parkville, Australia","IEEE Transactions on Information Forensics and Security","5 Dec 2019","2020","15","","1128","1137","Online romance scams are a prevalent form of mass-marketing fraud in the West, and yet few studies have presented data-driven responses to this problem. In this type of scam, fraudsters craft fake profiles and manually interact with their victims. Because of the characteristics of this type of fraud and how dating sites operate, traditional detection methods (e.g., those used in spam filtering) are ineffective. In this paper, we investigate the archetype of online dating profiles used in this form of fraud, including their use of demographics, profile descriptions, and images, shedding light on both the strategies deployed by scammers to appeal to victims and the traits of victims themselves. Furthermore, in response to the severe financial and psychological harm caused by dating fraud, we develop a system to detect romance scammers on online dating platforms. This paper presents the first fully described system for automatically detecting this fraud. Our aim is to provide an early detection system to stop romance scammers as they create fraudulent profiles or before they engage with potential victims. Previous research has indicated that the victims of romance scams score highly on scales for idealized romantic beliefs. We combine a range of structured, unstructured, and deep-learned features that capture these beliefs in order to build a detection system. Our ensemble machine-learning approach is robust to the omission of profile details and performs at high accuracy (97%) in a hold-out validation set. The system enables development of automated tools for dating site providers and individual users.","1556-6021","","10.1109/TIFS.2019.2930479","Engineering and Physical Sciences Research Council(grant numbers:EP/N028112/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768406","Computer security","Feature extraction;Machine learning;Tools;IP networks;Data mining;Sociology;Statistics","","39","","35","IEEE","22 Jul 2019","","","IEEE","IEEE Journals"
"AReN: A Deep Learning Approach for Sound Event Recognition Using a Brain Inspired Representation","A. Greco; N. Petkov; A. Saggese; M. Vento","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Faculty of Science and Engineering, University of Groningen, CP, The Netherlands; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy","IEEE Transactions on Information Forensics and Security","3 Jul 2020","2020","15","","3610","3624","Audio surveillance is gaining in the last years wide interest. This is due to the large number of situations in which this kind of systems can be used, either alone or combined with video-based algorithms. In this paper we propose a deep learning method to automatically recognize events of interest in the context of audio surveillance (namely screams, broken glasses and gun shots). The audio stream is represented by a gammatonegram image. We propose a 21-layer CNN to which we feed sections of the gammatonegram representation. At the output of this CNN there are units that correspond to the classes. We trained the CNN, called AReN, by taking advantage of a problem-driven data augmentation, which extends the training dataset with gammatonegram images extracted by sounds acquired with different signal to noise ratios. We experimented it with three datasets freely available, namely SESA, MIVIA Audio Events and MIVIA Road Events and we achieved 91.43%, 99.62% and 100% recognition rate, respectively. We compared our method with other state of the art methodologies based both on traditional machine learning methodologies and deep learning. The comparison confirms the effectiveness of the proposed approach, which outperforms the existing methods in terms of recognition rate. We experimentally prove that the proposed network is resilient to the noise, has the capability to significantly reduce the false positive rate and is able to generalize in different scenarios. Furthermore, AReN is able to process 5 audio frames per second on a standard CPU and, consequently, it is suitable for real audio surveillance applications.","1556-6021","","10.1109/TIFS.2020.2994740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093814","audio surveillance;deep learning;CNN;gammatonegram;brain inspired representation","Training;Time-frequency analysis;Machine learning;Spectrogram;Surveillance;Signal to noise ratio;Standards","","35","","49","IEEE","14 May 2020","","","IEEE","IEEE Journals"
"Similarity Distance Learning on SPD Manifold for Writer Independent Offline Signature Verification","E. N. Zois; D. Tsourounis; D. Kalivas","TelSip Research Laboratory, University of West Attica, Egaleo, Greece; Electronics Laboratory, University of Patras, Patras, Greece; TelSip Research Laboratory, University of West Attica, Egaleo, Greece","IEEE Transactions on Information Forensics and Security","5 Dec 2023","2024","19","","1342","1356","Identifying the existence or approval of a human in a number of past, recent and present day activities with the use of a handwritten signature is a captivating biometric challenge. Several engineering branches such as computer vision, pattern recognition and quite recently data-driven machine learning algorithms are combined in a multi-disciplined signature verification framework in order to deliver an equivalent and efficient e-assistance to manually executed duties, which usually demand knowledge and skills. In this work, we propose, for the first time, the use of a learnable Symmetric Positive Definite manifold distance framework in offline signature verification literature in order to build a global writer-independent signature verification classifier. The key building block of the framework relies on the use of regional covariance matrices of handwritten signature images as visual descriptors, which maps them into the Symmetric Positive Definite manifold. The learning and verification protocol explores both blind intra and blind inter transfer learning frameworks with the use of four popular signature datasets of Western and Asian origin. Experiments strongly indicate that the learnable SPD manifold similarity distance can be highly efficient for offline writer independent signature verification.","1556-6021","","10.1109/TIFS.2023.3333681","Special Account for Research Grants (SARG) of the University of West Attica; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10319735","Manifold optimization;symmetric positive definite matrices;spatial pyramid segmentation;writer independent off-line signature verification","Measurement;Manifolds;Covariance matrices;Geometry;Feature extraction;Visualization;Machine learning algorithms","","","","107","IEEE","16 Nov 2023","","","IEEE","IEEE Journals"
"DAPASA: Detecting Android Piggybacked Apps Through Sensitive Subgraph Analysis","M. Fan; J. Liu; W. Wang; H. Li; Z. Tian; T. Liu","MOEKLINNS, Xi’an Jiaotong University, Xi’an, China; MOEKLINNS, Xi’an Jiaotong University, Xi’an, China; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; Department of Computer Science, Union University, Jackson, TN, USA; School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, China; MOEKLINNS, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","8","1772","1785","With the exponential growth of smartphone adoption, malware attacks on smartphones have resulted in serious threats to users, especially those on popular platforms, such as Android. Most Android malware is generated by piggybacking malicious payloads into benign applications (apps), which are called piggybacked apps. In this paper, we propose DAPASA, an approach to detect Android piggybacked apps through sensitive subgraph analysis. Two assumptions are established to reflect the different invocation patterns of sensitive APIs in the injected malicious payloads (rider) of a piggybacked app and in its host app (carrier). With these two assumptions, DAPASA generates a sensitive subgraph (SSG) to profile the most suspicious behavior of an app. Five features are constructed from SSG to depict the invocation patterns. The five features are fed into the machine learning algorithms to detect whether the app is piggybacked or benign. DAPASA is evaluated on a large real-world data set consisting of 2551 piggybacked apps and 44 921 popular benign apps. Extensive evaluation results demonstrate that the proposed approach exhibits an impressive detection performance compared with that of three baseline approaches even with only five numeric features. Furthermore, the proposed approach can complement permission-based approaches and API-based approaches with the combination of our five features from a new perspective of the invocation structure.","1556-6021","","10.1109/TIFS.2017.2687880","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000903); National Natural Science Foundation of China(grant numbers:91418205,61472318,61532015,61532004,61672419,61632015); Fok Ying-Tong Education Foundation(grant numbers:151067); Ministry of Education Innovation Research Team(grant numbers:IRT13035); Fundamental Research Funds for the Central Universities; Shenzhen City Science and Technology Research and Development Fund(grant numbers:JCYJ20150630115257892); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887707","Piggybacked apps;sensitive API;sensitive subgraph;malware detection;static analysis","Malware;Feature extraction;Androids;Humanoid robots;Payloads;Sensitivity;Frequency measurement","","98","","51","IEEE","27 Mar 2017","","","IEEE","IEEE Journals"
"Modeling and Extending the Ensemble Classifier for Steganalysis of Digital Images Using Hypothesis Testing Theory","R. Cogranne; J. Fridrich","Laboratory for System Modelling and Dependability, Charles Dealaunay Institute, Troyes University of Technology, Troyes, France; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","12","2627","2642","The machine learning paradigm currently predominantly used for steganalysis of digital images works on the principle of fusing the decisions of many weak base learners. In this paper, we employ a statistical model of such an ensemble and replace the majority voting rule with a likelihood ratio test. This allows us to train the ensemble to guarantee desired statistical properties, such as the false-alarm probability and the detection power, while preserving the high detection accuracy of original ensemble classifier. It also turns out the proposed test is linear. Moreover, by replacing the conventional total probability of error with an alternative criterion of optimality, the ensemble can be extended to detect messages of an unknown length to address composite hypotheses. Finally, the proposed well-founded statistical formulation allows us to extend the ensemble to multi-class classification with an appropriate criterion of optimality and an optimal associated decision rule. This is useful when a digital image is tested for the presence of secret data hidden by more than one steganographic method. Numerical results on real images show the sharpness of the theoretically established results and the relevance of the proposed methodology.","1556-6021","","10.1109/TIFS.2015.2470220","Conseil Régional Champagne Ardenne; Air Force Office of Scientific Research(grant numbers:FA9950-12-1-0124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210196","Hypothesis testing theory;information hiding;optimal detection;multi-class classification;ensemble classifier;Hypothesis testing theory;information hiding;optimal detection;multi-class classification;ensemble classifier","Training;Testing;Probability;Digital images;Detectors;Feature extraction;Mathematical model","","44","","48","IEEE","19 Aug 2015","","","IEEE","IEEE Journals"
"RF Impairment Model-Based IoT Physical-Layer Identification for Enhanced Domain Generalization","S. Rajendran; Z. Sun","Department of Electrical Engineering, University at Buffalo, Buffalo, NY, USA; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","24 Mar 2022","2022","17","","1285","1299","For small, inexpensive, and power-constrained IoT devices, Radiofrequency fingerprinting (RF-fingerprinting) has emerged as a cost-effective security solution. Robustness and permanence of the RF-fingerprints (RFFs) are major challenges since this solution’s inception. This is due to domain-related complications such as environmental effects and time-varying device-related perturbations. Since data from domains have divergent distributions, blindly plugging in Machine learning algorithms can overfit domain-related residuals rather than the fingerprint. Recent popular methods like blind channel equalization-based solutions only partially solve this problem while adversely affecting the RFF’s user capacity. Our paper presents a solution to overcome the domain generalization of these computationally intensive feature mining methods in a real-world wireless domain while retaining the fingerprints’ richness. We perform a reverse analysis of a typical RFIC and create a parametric RF-impairment distribution model currently missing in the literature. Then, we use this model to tailor a knowledge-based parametric signal processing and conditioning method, which would create an optimum signal representation of the RFF for ML algorithms. Additionally, our method can significantly reduce the dimensionality of the data needed to train the ML algorithms, eliminate noise, and simplify the classifier needed for RF-fingerprinting. We present our results after evaluation using real-world cross-domain experiments under varying domain conditions with COTS IoT microchips (SX1276).","1556-6021","","10.1109/TIFS.2022.3158553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732439","Physical layer security;specific emitter identification;radio frequency fingerprinting;LoRa;Internet of Things;security;chirp spread spectrum","Fingerprint recognition;Feature extraction;Perturbation methods;Robustness;Radio frequency;Internet of Things;Radiofrequency integrated circuits","","27","","43","IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"One Parameter Defense—Defending Against Data Inference Attacks via Differential Privacy","D. Ye; S. Shen; T. Zhu; B. Liu; W. Zhou","Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; Institute of Data Science, City University of Macau, Macau, China","IEEE Transactions on Information Forensics and Security","15 Apr 2022","2022","17","","1466","1480","Machine learning models are vulnerable to data inference attacks, such as membership inference and model inversion attacks. In these types of breaches, an adversary attempts to infer a data record’s membership in a dataset or even reconstruct this data record using a confidence score vector predicted by the target model. However, most existing defense methods only protect against membership inference attacks. Methods that can combat both types of attacks require a new model to be trained, which may not be time-efficient. In this paper, we propose a differentially private defense method that handles both types of attacks in a time-efficient manner by tuning only one parameter, the privacy budget. The central idea is to modify and normalize the confidence score vectors with a differential privacy mechanism which preserves privacy and obscures membership and reconstructed data. Moreover, this method can guarantee the order of scores in the vector to avoid any loss in classification accuracy. The experimental results show the method to be an effective and timely defense against both membership inference and model inversion attacks with no reduction in accuracy.","1556-6021","","10.1109/TIFS.2022.3163591","Australian Research Council (ARC), Australia(grant numbers:DP190100981,DP200100946); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745062","Deep learning;data privacy;differential privacy","Data models;Privacy;Differential privacy;Training;Predictive models;Deep learning;Neural networks","","21","","39","IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"Sequence Data Matching and Beyond: New Privacy-Preserving Primitives Based on Bloom Filters","W. Xue; D. Vatsalan; W. Hu; A. Seneviratne","School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Data61, CSIRO, Eveleigh, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Cyber Security Cooperative Research Centre, Sydney, Australia","IEEE Transactions on Information Forensics and Security","31 Mar 2020","2020","15","","2973","2987","Bloom filter encoding has widely been used as an efficient masking technique for privacy-preserving matching functions. The existing matching techniques, however, are limited to relatively simple types such as string, categorical and signal numerical values. In this paper, we propose a new scheme that significantly extends the class of matching primitives that are based on privacy-preserving Bloom filter mechanism. These primitives include sequence data matching and popular distance-based machine learning algorithms such as KNN and SVM. Our scheme hash-maps a sequence data vector into the Bloom filter space while checking the similarity of the data points efficiently with negligible utility loss by adding a timestamp (bit) for each element in the data represented with its neighboring values. Furthermore, it includes a Laplace-like perturbation method on the constructed Bloom filters to address the weakness of deterministic probability led by encoding techniques. As a result, the proposed work guarantee the private data records are difficult to be discriminated due to collisions and differential privacy. The experimental results on three real-scenario based datasets illustrate that our method can achieve a significantly better trade-off between utility and privacy than the state-of-the-art differential privacy-based method by adding Laplace noise to the data directly.","1556-6021","","10.1109/TIFS.2020.2980835","Cyber Security Research Centre Limited through the Australian Government’s Cooperative Research Centres Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037114","Sequence data matching;encoding;bloom filter;privacy-preserving data publishing;differential privacy","Encoding;Privacy;Differential privacy;Measurement;Australia;Perturbation methods","","20","","69","IEEE","16 Mar 2020","","","IEEE","IEEE Journals"
"What You Submit Is Who You Are: A Multimodal Approach for Deanonymizing Scientific Publications","M. Payer; L. Huang; N. Z. Gong; K. Borgolte; M. Frank","West Lafayette, Purdue University, IN, USA; Datavisor, Inc., Berkeley, CA, USA; University of California, Berkeley, CA, USA; University of California, Santa Barbara, CA, USA; University of California, Berkeley, CA, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","1","200","212","The peer-review system of most academic conferences relies on the anonymity of both the authors and reviewers of submissions. In particular, with respect to the authors, the anonymity requirement is heavily disputed and pros and cons are discussed exclusively on a qualitative level. In this paper, we contribute a quantitative argument to this discussion by showing that it is possible for a machine to reveal the identity of authors of scientific publications with high accuracy. We attack the anonymity of authors using statistical analysis of multiple heterogeneous aspects of a paper, such as its citations, its writing style, and its content. We apply several multilabel, multiclass machine learning methods to model the patterns exhibited in each feature category for individual authors and combine them to a single ensemble classifier to deanonymize authors with high accuracy. To the best of our knowledge, this is the first approach that exploits multiple categories of discriminative features and uses multiple, partially complementing classifiers in a single, focused attack on the anonymity of the authors of an academic publication. We evaluate our author identification framework, deAnon, based on a real-world data set of 3894 papers. From these papers, we target 1405 productive authors that each have at least three publications in our data set. Our approach returns a ranking of probable authors for anonymous papers, an ordering for guessing the authors of a paper. In our experiments, following this ranking, the first guess corresponds to one of the authors of a paper in 39.7% of the cases, and at least one of the authors is among the top 10 guesses in 65.6% of all cases. Thus, deAnon significantly outperforms current state-of-the-art techniques for automatic deanonymization.","1556-6021","","10.1109/TIFS.2014.2368355","National Science Foundation(grant numbers:CCF-0424422); Air Force Office of Scientific Research within the Multidisciplinary University Research Initiative(grant numbers:FA9550-09-1-0539); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949149","Data privacy;text analysis;text mining","Feature extraction;Writing;Training;Data mining;Accuracy;Support vector machines;Portable document format","","19","","35","IEEE","6 Nov 2014","","","IEEE","IEEE Journals"
"TANTRA: Timing-Based Adversarial Network Traffic Reshaping Attack","Y. Sharon; D. Berend; Y. Liu; A. Shabtai; Y. Elovici","Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Nanyang Technological University, Jurong West, Singapore; Nanyang Technological University, Jurong West, Singapore; Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be’er Sheva, Israel","IEEE Transactions on Information Forensics and Security","16 Sep 2022","2022","17","","3225","3237","Network intrusion attacks are a known threat. To detect such attacks, network intrusion detection systems (NIDSs) have been developed and deployed. These systems apply machine learning models to high-dimensional vectors of features extracted from network traffic to detect intrusions. Advances in NIDSs have made it challenging for attackers, who must execute attacks without being detected by these systems. Prior research on bypassing NIDSs has mainly focused on perturbing the features extracted from the attack traffic to fool the detection system, however, this may jeopardize the attack’s functionality. In this work, we present TANTRA, a novel end-to-end Timing-based Adversarial Network Traffic Reshaping Attack that can bypass a variety of NIDSs. Our evasion attack utilizes a long short-term memory (LSTM) deep neural network (DNN) which is trained to learn the time differences between the target network’s benign packets. The trained LSTM is used to set the time differences between the malicious traffic packets (attack), without changing their content, such that they will “behave” like benign network traffic and will not be detected as an intrusion. We evaluate TANTRA on eight common intrusion attacks and three state-of-the-art NIDS systems, achieving an average success rate of 99.99% in network intrusion detection system evasion. We also propose a novel mitigation technique to address this new evasion attack.","1556-6021","","10.1109/TIFS.2022.3201377","European Union’s Horizon 2020 Research and Innovation program(grant numbers:830927); National Research Foundation(grant numbers:NRF2018NCR-NCR005-0001,AISG2-RP-2020-019,NRFI06-2020-0022-0001,NRF2018NCR-NSOE003-0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865980","Network intrusion;adversarial attack;neural networks;deep learning","Feature extraction;Telecommunication traffic;Perturbation methods;Network intrusion detection;Deep learning;Behavioral sciences;Delays","","12","","40","IEEE","24 Aug 2022","","","IEEE","IEEE Journals"
"Non-Invasive Recognition of Poorly Resolved Integrated Circuit Elements","E. Matlin; M. Agrawal; D. Stoker","SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2014","9","3","354","363","We present a non-invasive method for recognition of components in a digital CMOS integrated circuit (IC). We use a confocal infrared laser scanning optical microscope to collect multimodal images through the backside of the IC. Individual modes correspond to passive reflectivity measurements or active measurements, such as light-induced voltage alteration. The modes are registered and stored in a multidimensional data cube. We apply a machine learning algorithm using a binary representation to identify a variety of data structures from transistors to entire logic cells. Because of the compact representation, objects can be detected rapidly. We show that by increasing the number of imaging modes used to develop the descriptor, we can significantly increase recognition accuracy. The approach allows recognition of poorly resolved components, whose primary distinguishing features are below traditional optical resolution limits, and is general enough to be applied to multiple design processes. We believe this represents a significant step toward a fully non-invasive IC reverse engineering system.","1556-6021","","10.1109/TIFS.2013.2297518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6701342","Image processing and computer vision;bag-of-words;ensemble learning;non-invasive reverse engineering;integrated circuits","Integrated circuits;Microscopy;Image resolution;Optical microscopy;Reverse engineering;Logistics","","11","1","34","IEEE","2 Jan 2014","","","IEEE","IEEE Journals"
"Efficient Profiled Attacks on Masking Schemes","L. Lerman; O. Markowitch","Department of Computer Science, Université libre de Bruxelles, Bruxelles, Belgium; Department of Computer Science, Université libre de Bruxelles, Bruxelles, Belgium","IEEE Transactions on Information Forensics and Security","11 Feb 2019","2019","14","6","1445","1454","Side-channel adversaries represent real-world threats against (certified and uncertified) cryptographic devices. Masking schemes represent prevailing countermeasures to reduce the success probabilities of side-channel attacks. However, masking schemes increase the implementation cost in terms of power consumption, clock cycles, and random number generation. Investigation of tools evaluating the degree of resilience of cryptographic devices using masking (against side-channel attacks) represents an important aspect in certification procedures (e.g., common criteria, FIPS 140-2, and EMVco). Several side-channel evaluation techniques exist such as template attacks and machine learning-based attacks. In this paper, we formalize results obtained in side-channel attacks community when targeting masked implementations. We report theoretical as well as practical results of parametric and non-parametric side-channel attacks on masking schemes. The theoretical part reports results based on a simulation of the execution of software devices, while the practical part focuses on actual leakages measured during the execution of a software implementation in three different contexts (that contain different levels of noise).","1556-6021","","10.1109/TIFS.2018.2879295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519754","Side-channel analysis;masking;template attacks;kernel density;random forests","Kernel;Side-channel attacks;Forestry;Estimation;Gaussian distribution","","9","","30","IEEE","1 Nov 2018","","","IEEE","IEEE Journals"
"Systematically Quantifying Cryptanalytic Nonlinearities in Strong PUFs","D. Chatterjee; K. Pratihar; A. Hazra; U. Rührmair; D. Mukhopadhyay","Indian Institute of Technology Kharagpur, Kharagpur, India; Indian Institute of Technology Kharagpur, Kharagpur, India; Indian Institute of Technology Kharagpur, Kharagpur, India; Security in Telecommunications, TU Berlin, Berlin, Germany; Indian Institute of Technology Kharagpur, Kharagpur, India","IEEE Transactions on Information Forensics and Security","4 Dec 2023","2024","19","","1126","1141","Physically Unclonable Functions (PUFs) with large challenge space (also called Strong PUFs) are promoted for usage in authentications and various other cryptographic and security applications. In order to qualify for these cryptographic applications, the Boolean functions realized by PUFs need to possess a high nonlinearity (NL). However, with a large challenge space (usually  $\geq 64$  bits), measuring NL by classical techniques like the Walsh transformation is computationally infeasible. In this paper, we propose the usage of a heuristic-based measure called the non-homomorphicity test which estimates the cryptographic NL of Boolean functions with high accuracy in spite of not needing access to the entire challenge-response set. We also combine our analysis with a technique used in linear cryptanalysis, called Piling-up lemma, to measure the NL of popular PUF compositions. As a demonstration to justify the soundness of the metric, we perform extensive experimentation by first estimating the NL of constituent Arbiter/Bistable Ring PUFs using the non-homomorphicity test, and then applying them to quantify the same for their XOR compositions namely XOR Arbiter PUFs and XOR Bistable Ring PUF. Our findings show that the metric explains the impact of various parameter choices of these PUF compositions on the NL obtained and thus promises to be used as an important objective criterion for future efforts to evaluate PUF designs. While the framework is not representative of the machine learning robustness of PUFs, it can be a useful complementary tool to analyze the cryptanalytic strengths of PUF primitives.","1556-6021","","10.1109/TIFS.2023.3329438","Air Force Office of Scientific Research (AFOSR)(grant numbers:FA9550-21-1-0039); European Union (EU) Project NEUROPULS; Department of Science and Technology (DST), Government of India, IHUB NTIHAC Foundation, C3i Building, Indian Institute of Technology (IIT) Kanpur; Centre on Hardware-Security Entrepreneurship Research and Development, Meity, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304294","Strong PUFs;nonlinearity;cryptanalysis;cryptanalytic attacks;non-homomorphicity tests","Cryptography;Measurement;Boolean functions;Sensitivity;Robustness;Hardware;Computational modeling","","1","","43","IEEE","1 Nov 2023","","","IEEE","IEEE Journals"
"Attention-Based API Locating for Malware Techniques","G. -W. Wong; Y. -T. Huang; Y. -R. Guo; Y. Sun; M. C. Chen","Department of Computer Science and Information Engineering, National Taiwan University, Taipei City, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei City, Taiwan; Academia Sinica, Taipei City, Taiwan; Department of Information Management, National Taiwan University, Taipei City, Taiwan; Academia Sinica, Taipei City, Taiwan","IEEE Transactions on Information Forensics and Security","30 Nov 2023","2024","19","","1199","1212","This paper presents APILI, an innovative approach to behavior-based malware analysis that utilizes deep learning to locate the API calls corresponding to discovered malware techniques in dynamic execution traces. APILI defines multiple attentions between API calls, resources, and techniques, incorporating MITRE ATT&CK framework, adversary tactics, techniques and procedures, through a neural network. We employ fine-tuned BERT for arguments/resources embedding, SVD for technique representation, and several design enhancements, including layer structure and noise addition, to improve the locating performance. To the best of our knowledge, this is the first attempt to locate low-level API calls that correspond to high-level malicious behaviors (that is, techniques). Our evaluation demonstrates that APILI outperforms other traditional and machine learning techniques in both technique discovery and API locating. These results indicate the promising performance of APILI, thus allowing it to reduce the analysis workload.","1556-6021","","10.1109/TIFS.2023.3330337","National Science and Technology Council (NSTC), Taiwan(grant numbers:109-2221-E-001-010-MY3,110-2218-E-001-001-MBK,111-2218-E-001-001-MBK,112-2634-F-001-001-MBK); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309174","Causality tracking;dynamic analysis;malicious behavior discovery;malware analysis;MITRE ATT&CK","Malware;Behavioral sciences;Deep learning;Manuals;Task analysis;Semantics;Codes","","1","","52","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"TEAR: Exploring Temporal Evolution of Adversarial Robustness for Membership Inference Attacks Against Federated Learning","G. Liu; Z. Tian; J. Chen; C. Wang; J. Liu","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Computing Science, Simon Fraser University, Burnaby, Canada","IEEE Transactions on Information Forensics and Security","21 Aug 2023","2023","18","","4996","5010","Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple clients to train a unified model without disclosing their private data. However, susceptibility to membership inference attacks (MIAs) arises due to the natural inclination of FL models to overfit on the training data during the training process, thereby enabling MIAs to exploit the subtle differences in the FL model’s parameters, activations, or predictions between the training and testing data to infer membership information. It is worth noting that most if not all existing MIAs against FL require access to the model’s internal information or modification of the training process, yielding them unlikely to be performed in practice. In this paper, we present with TEAR the first evidence that it is possible for an honest-but-curious federated client to perform MIA against an FL system, by exploring the Temporal Evolution of the Adversarial Robustness between the training and non-training data. We design a novel adversarial example generation method to quantify the target sample’s adversarial robustness, which can be utilized to obtain the membership features to train the inference model in a supervised manner. Extensive experiment results on five realistic datasets demonstrate that TEAR can achieve a strong inference performance compared with two existing MIAs, and is able to escape from the protection of two representative defenses.","1556-6021","","10.1109/TIFS.2023.3303718","National Natural Science Foundation of China(grant numbers:62272183,62171189,62002104,62071192); Key Research and Development Program of Hubei Province(grant numbers:2021BAA026); special fund for Wuhan Yellow Crane Talents (Excellent Young Scholar); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214061","Federated learning;membership inference attack;adversarial robustness;temporal evolution","Training;Data models;Robustness;Predictive models;Computational modeling;Training data;Testing","","","","70","IEEE","9 Aug 2023","","","IEEE","IEEE Journals"
"Privacy-Preserving Multi-User Outsourced Computation for Boolean Circuits","X. Liu; G. Yang; W. Susilo; K. He; R. H. Deng; J. Weng","Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Systems, Singapore Management University, Bras Basah, Singapore; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; School of Cyberspace Security, Dongguan University of Technology, Dongguan, China; School of Computing and Information Systems, Singapore Management University, Bras Basah, Singapore; College of Cyber Security, Jinan University, Guangzhou, China","IEEE Transactions on Information Forensics and Security","11 Aug 2023","2023","18","","4929","4943","With the prevalence of outsourced computation, such as Machine Learning as a Service, protecting the privacy of sensitive data throughout the whole computation is a critical yet challenging task. The problem becomes even more tricky when multiple sources of input and/or multiple recipients of output are involved, who would encrypt/decrypt data using different keys. Considering many computation tasks demand binary operands and operations but there are only outsourced computation constructions for arithmetic calculations, in this paper, the authors propose a privacy-preserving outsourced computation framework for Boolean circuits. The proposed framework can protect sensitive data throughout the whole computation, i.e., input, output and all the intermediate values, ensuring privacy for general outsourced tasks. Moreover, it compresses the ciphertext domain of Liu et al., (2016) and attains secure protocols for four logic gates (AND, OR, NOT, and XOR) which are the basic operations in Boolean circuits. With the proposed framework as a building block, a novel Privacy-preserved (encrypted) Bloom Filter and a Multi-keyword Searchable Encryption scheme under the multi-user setting are presented. Security proof and experimental results show that the proposal is reliable and practical.","1556-6021","","10.1109/TIFS.2023.3301734","National Key Research and Development Program of China(grant numbers:2021ZD0112802); National Natural Science Foundation of China(grant numbers:62272287); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207696","Bitwise operation;Boolean circuit;multi-user;bloom filter;searchable encryption","Protocols;Encryption;Task analysis;Security;Computational modeling;Servers;Outsourcing","","","","52","IEEE","3 Aug 2023","","","IEEE","IEEE Journals"
"Scanner-Hunter: An Effective ICS Scanning Group Identification System","C. Sheng; Y. Yao; L. Zhao; P. Zeng; J. Zhao","State Key Laboratory of Robotics, Shenyang Institute of AutomationKey Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; Army Equipment Department, Shenyang District JunDaiJu Second JunDaiShi, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of AutomationKey Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of AutomationKey Laboratory of Networked Control Systems, Chinese Academy of Sciences, Shenyang, China","IEEE Transactions on Information Forensics and Security","7 Feb 2024","2024","19","","3077","3092","As the precursor of cyber-attacks, the campaigns of scanning groups are able to reflect the attack target and attack trend to a great extent, which provide highly valuable threat intelligence for cyber defenders to understand the current cyber security situation. However, how to identify scanning groups in the context of limited information, especially in the absence of relevant threat intelligence, remains a challenging problem. In this paper, we utilize the honeynet as the unique data source to propose a scanning group identification system, Scanner-Hunter, which focuses on identifying scanning groups targeting ICS devices. To better characterize scanning patterns, a novel traffic representation scheme for scanning traffic is proposed, which is composed of a set of feature vectors to describe all the ICS request packets. On this basis, we propose a novel self-expanding multi-class classification (SEMCC) model and the IP prefix judgment, which are deliberately integrated to cope with sophisticated scanning groups. Take the Modbus protocol as an example, we implement a prototype of Scanner-Hunter, and use six years of real-world honeynet datasets to evaluate its performance. The experimental results illustrate its effectiveness and superior performance compared with some popular machine learning methods and existing SOTA scanning group identification methods. In addition, Scanner-Hunter is further leveraged to investigate the group distribution and maliciousness of 506 unknown scanners, and some suspicious attack groups with APT characteristics are analyzed. Furthermore, accurate scanning group information will contribute to revealing potential attack organizations and supporting decision making to prevent or interrupt cyber-attacks in time.","1556-6021","","10.1109/TIFS.2024.3359002","Special Research Assistant Program of Chinese Academy of Sciences;; China Postdoctoral Science Foundation(grant numbers:2023M743701); Natural Science Fund of Liaoning Province(grant numbers:2023-BS-028,2023JH2/101300202); 2022 General Project of Basic Research Program of Shenyang Institute of Automation, Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415218","ICS;scanning group;scanning pattern;multi-class classification;IP prefix","Integrated circuits;IP networks;Cyberattack;Behavioral sciences;Telescopes;Analytical models;Reconnaissance","","","","53","IEEE","26 Jan 2024","","","IEEE","IEEE Journals"
"Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers","Y. Guo; Y. Zhao; S. Hou; C. Wang; X. Jia","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Information Forensics and Security","21 Nov 2023","2024","19","","708","721","Machine unlearning as a fundamental requirement in Machine-Learning-as-a-Service (MLaaS) has been extensively studied with increasing concerns about data privacy. It requires MLaaS providers should delete training data upon user requests. Unfortunately, none of the existing studies can efficiently achieve machine unlearning validation while preserving the retraining efficiency and the service quality after data deletion. Besides, how to craft the validation scheme to prevent providers from spoofing validation by forging proofs remains under-explored. In this paper, we introduce a backdoor-assisted validation scheme for machine unlearning. The proposed design is built from the ingenious combination of backdoor triggers and incremental learning to assist users in verifying proofs of machine unlearning without compromising performance and service quality. We propose to embed invisible markers based on backdoor triggers into privacy-sensitive data to prevent MLaaS providers from distinguishing poisoned data for validation spoofing. Users can use prediction results to determine whether providers comply with data deletion requests. Besides, we incorporate our validation scheme into an efficient incremental learning approach via our index structure to further facilitate the performance of retraining after data deletion. Evaluation results on real-world datasets confirm the efficiency and effectiveness of our proposed verifiable machine unlearning scheme.","1556-6021","","10.1109/TIFS.2023.3328269","National Natural Science Foundation of China(grant numbers:62102035,62206022); Research Grants Council of Hong Kong(grant numbers:N_CityU139/21,C2004-21G,R1012-21,R6021-20F,CityU 11213920 (GRF)); Fundamental Research Funds for the Central Universities(grant numbers:2021NTST31); National Key Research and Development Program of China(grant numbers:2022ZD0115901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298847","Machine unlearning;ML-as-a-service;backdoor attacks;incremental learning","Data models;Training;Computational modeling;Data privacy;Training data;Servers;Deep learning","","","","41","IEEE","27 Oct 2023","","","IEEE","IEEE Journals"
"Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection","D. Li; Q. Li","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Information Forensics and Security","17 Jul 2020","2020","15","","3886","3900","Malware remains a big threat to cyber security, calling for machine learning based malware detection. While promising, such detectors are known to be vulnerable to evasion attacks. Ensemble learning typically facilitates countermeasures, while attackers can leverage this technique to improve attack effectiveness as well. This motivates us to investigate which kind of robustness the ensemble defense or effectiveness the ensemble attack can achieve, particularly when they combat with each other. We thus propose a new attack approach, named mixture of attacks, by rendering attackers capable of multiple generative methods and multiple manipulation sets, to perturb a malware example without ruining its malicious functionality. This naturally leads to a new instantiation of adversarial training, which is further geared to enhancing the ensemble of deep neural networks. We evaluate defenses using Android malware detectors against 26 different attacks upon two practical datasets. Experimental results show that the new adversarial training significantly enhances the robustness of deep neural networks against a wide range of attacks, ensemble methods promote the robustness when base classifiers are robust enough, and yet ensemble attacks can evade the enhanced malware detectors effectively, even notably downgrading the VirusTotal service.","1556-6021","","10.1109/TIFS.2020.3003571","Jiangsu Province Key Research and Development Programs: Social Development Project(grant numbers:BE2017739); China’s National Key Research and Development Program of the 2020 Industrial Internet Innovation and Development Project(grant numbers:2020YFB1804604,2020YFB1804600); Fundamental Research Fund for the Central Universities(grant numbers:30918012204); China Scholarship Council(grant numbers:201706840123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121297","Adversarial Machine Learning;Deep Neural Networks;Ensemble;Adversarial Malware Detection","Malware;Training;Robustness;Detectors;Neural networks;Computer crime;Feature extraction","","77","","63","IEEE","19 Jun 2020","","","IEEE","IEEE Journals"
"Optimized Privacy-Preserving CNN Inference With Fully Homomorphic Encryption","D. Kim; C. Guyot","College of AI Convergence, Dongguk University, Seoul, South Korea; Western Digital Research, Milpitas, CA, USA","IEEE Transactions on Information Forensics and Security","12 Apr 2023","2023","18","","2175","2187","Inference of machine learning models with data privacy guarantees has been widely studied as privacy concerns are getting growing attention from the community. Among others, secure inference based on Fully Homomorphic Encryption (FHE) has proven its utility by providing stringent data privacy at sometimes affordable cost. Still, previous work was restricted to shallow and narrow neural networks and simple tasks due to the high computational cost incurred from FHE. In this paper, we propose a more efficient way of evaluating convolutions with FHE, where the cost remains constant regardless of the kernel size, resulting in 12– $46\times $  timing improvement on various kernel sizes. Combining our methods with FHE bootstrapping, we achieve at least 18.9% (and 48.1%) timing reduction in homomorphic evaluation of 20-layer CNN classifiers (and a part of it) on CIFAR10/100 (and ImageNet, respectively) datasets. Furthermore, in consideration of our methods being effective for evaluating CNNs with intensive convolutional operations and exploring such CNNs, we achieve at least  $5\times $  faster inference on CIFAR10/100 with FHE than the prior works having the same or less accuracy.","1556-6021","","10.1109/TIFS.2023.3263631","Dongguk University Research Fund of 2022(grant numbers:S-2022-G0001-00070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089847","Privacy-preserving machine learning;fully homomorphic encryption;convolutional neural network","Convolution;Costs;Kernel;Data privacy;Low latency communication;Convolutional neural networks;Homomorphic encryption","","7","","49","IEEE","31 Mar 2023","","","IEEE","IEEE Journals"
"Adversarial RL-Based IDS for Evolving Data Environment in 6LoWPAN","A. M. Pasikhani; J. A. Clark; P. Gope","Department of Computer Science, The University of Sheffield, Sheffield, U.K.; Department of Computer Science, The University of Sheffield, Sheffield, U.K.; Department of Computer Science, The University of Sheffield, Sheffield, U.K.","IEEE Transactions on Information Forensics and Security","31 Oct 2022","2022","17","","3831","3846","Low-power and Lossy Networks (LLNs) comprise nodes characterised by constrained computational power, memory, and energy resources. The LLN nodes empower ubiquitous connections amongst numerous devices (e.g. temperature, humidity, and turbidity sensors, together with motors, valves and other actuators) to sense, control and store properties of their environments. They are often deployed in hostile, unattended, and unfavourable conditions. Securing them often becomes very challenging. The extent of interconnected LLN devices poses a series of routing threats (e.g. wormhole, grayhole, DIO suppression, and increase rank attacks). Consequently, an efficient and effective intrusion detection system (IDS) is of utmost importance in identifying anomalous activities in the IPv6 over Low-powered Wireless Personal Area Networks (6LoWPAN). This article proposes a robust Adversarial Reinforcement Learning (ARL) framework to generate efficient IDSs for evolving data environments. The integration of ARL and incremental machine-learning facilitates the generation of resource-efficient and robust IDS detectors. We demonstrate in particular how such an approach, leveraging notions of ‘concept drift’ detection and adaptation, can handle inevitable changes in the environment, giving the IDS best chances of detecting attacks in the current profile. The range of routing attacks considered is the most comprehensive to date. For the first time, Black-box and Grey-box ML-based adversaries aiming to destabilise the 6LoWPAN are distinguished and addressed.","1556-6021","","10.1109/TIFS.2022.3214099","Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/V039156/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916285","Intrusion detection system;RPL attacks;6LoWPAN;adversarial reinforcement learning;incremental machine learning;concept-drift detection","Routing;Temperature sensors;Intrusion detection;Radio frequency;Internet;Data models;Wireless personal area networks","","5","","53","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"Privacy-Preserving Deep Learning via Additively Homomorphic Encryption","L. T. Phong; Y. Aono; T. Hayashi; L. Wang; S. Moriai","National Institute of Information and Communications Technology, Tokyo, Japan; National Institute of Information and Communications Technology, Tokyo, Japan; Kobe University, Kobe, Japan; National Institute of Information and Communications Technology, Tokyo, Japan; National Institute of Information and Communications Technology, Tokyo, Japan","IEEE Transactions on Information Forensics and Security","30 Jan 2018","2018","13","5","1333","1345","We present a privacy-preserving deep learning system in which many learning participants perform neural network-based deep learning over a combined dataset of all, without revealing the participants' local data to a central server. To that end, we revisit the previous work by Shokri and Shmatikov (ACM CCS 2015) and show that, with their method, local data information may be leaked to an honest-but-curious server. We then fix that problem by building an enhanced system with the following properties: 1) no information is leaked to the server and 2) accuracy is kept intact, compared with that of the ordinary deep learning system also over the combined dataset. Our system bridges deep learning and cryptography: we utilize asynchronous stochastic gradient descent as applied to neural networks, in combination with additively homomorphic encryption. We show that our usage of encryption adds tolerable overhead to the ordinary deep learning system.","1556-6021","","10.1109/TIFS.2017.2787987","Japan Science and Technology Agency CREST(grant numbers:JPMJCR168A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241854","Privacy;deep learning;neural network;additively homomorphic encryption;LWE-based encryption;Paillier encryption","Servers;Machine learning;Encryption;Neural networks;Privacy","","624","","28","IEEE","29 Dec 2017","","","IEEE","IEEE Journals"
"Deep Residual Network for Steganalysis of Digital Images","M. Boroumand; M. Chen; J. Fridrich","Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA","IEEE Transactions on Information Forensics and Security","22 Jan 2019","2019","14","5","1181","1193","Steganography detectors built as deep convolutional neural networks have firmly established themselves as superior to the previous detection paradigm - classifiers based on rich media models. Existing network architectures, however, still contain elements designed by hand, such as fixed or constrained convolutional kernels, heuristic initialization of kernels, the thresholded linear unit that mimics truncation in rich models, quantization of feature maps, and awareness of JPEG phase. In this work, we describe a deep residual architecture designed to minimize the use of heuristics and externally enforced elements that is universal in the sense that it provides state-of-the-art detection accuracy for both spatial-domain and JPEG steganography. The key part of the proposed architecture is a significantly expanded front part of the detector that “computes noise residuals” in which pooling has been disabled to prevent suppression of the stego signal. Extensive experiments show the superior performance of this network with a significant improvement, especially in the JPEG domain. Further performance boost is observed by supplying the selection channel as a second channel.","1556-6021","","10.1109/TIFS.2018.2871749","National Science Foundation(grant numbers:1561446); Air Force Office of Scientific Research(grant numbers:FA9950-12-1-0124); Defense Advanced Research Projects Agency(grant numbers:FA8750-16-2-0173); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470101","Steganography;steganalysis;convolutional neural network;deep residual network;selection channel;SRNet","Detectors;Transform coding;Kernel;Machine learning;Feature extraction;Training;Convolution","","483","","68","IEEE","23 Sep 2018","","","IEEE","IEEE Journals"
"AnomalyNet: An Anomaly Detection Network for Video Surveillance","J. T. Zhou; J. Du; H. Zhu; X. Peng; Y. Liu; R. S. M. Goh","Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, 138632; College of Computer Science, Sichuan University, Chengdu, China; Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore","IEEE Transactions on Information Forensics and Security","11 Jun 2019","2019","14","10","2537","2550","Sparse coding-based anomaly detection has shown promising performance, of which the keys are feature learning, sparse representation, and dictionary learning. In this paper, we propose a new neural network for anomaly detection (termed AnomalyNet) by deeply achieving feature learning, sparse representation, and dictionary learning in three joint neural processing blocks. Specifically, to learn better features, we design a motion fusion block accompanied by a feature transfer block to enjoy the advantages of eliminating noisy background, capturing motion, and alleviating data deficiency. Furthermore, to address some disadvantages (e.g., nonadaptive updating) of the existing sparse coding optimizers and embrace the merits of neural network (e.g., parallel computing), we design a novel recurrent neural network to learn sparse representation and dictionary by proposing an adaptive iterative hard-thresholding algorithm (adaptive ISTA) and reformulating the adaptive ISTA as a new long short-term memory (LSTM). To the best of our knowledge, this could be one of the first works to bridge the$\ell _{1}$ -solver and LSTM and may provide novel insight into understanding LSTM and model-based optimization (or named differentiable programming), as well as sparse coding-based anomaly detection. Extensive experiments show the state-of-the-art performance of our method in the abnormal events detection task.","1556-6021","","10.1109/TIFS.2019.2900907","Singapore government’s Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engineering domain) through the Programmatic Grant(grant numbers:A1687b0033); Fundamental Research Funds for the Central Universities(grant numbers:YJ201748); NFSC(grant numbers:61806135,61876211); NFSC for Distinguished Young Scholar(grant numbers:61625204); Key Program of NFSC(grant numbers:61836006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649753","Video surveillance;anomaly detection;recurrent neural network based sparsity learning","Anomaly detection;Encoding;Feature extraction;Neural networks;Optimization;Task analysis;Machine learning","","214","","73","IEEE","22 Feb 2019","","","IEEE","IEEE Journals"
"Deep Representation-Based Feature Extraction and Recovering for Finger-Vein Verification","H. Qin; M. A. El-Yacoubi","Chongqing Technology and Business University, Chongqing, China; SAMOVAR, Telecom SudParis, CNRS, University Paris-Saclay, Paris, France","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","8","1816","1829","Finger-vein biometrics has been extensively investigated for personal verification. Despite recent advances in finger-vein verification, current solutions completely depend on domain knowledge and still lack the robustness to extract finger-vein features from raw images. This paper proposes a deep learning model to extract and recover vein features using limited a priori knowledge. First, based on a combination of the known state-of-the-art handcrafted finger-vein image segmentation techniques, we automatically identify two regions: a clear region with high separability between finger-vein patterns and background, and an ambiguous region with low separability between them. The first is associated with pixels on which all the above-mentioned segmentation techniques assign the same segmentation label (either foreground or background), while the second corresponds to all the remaining pixels. This scheme is used to automatically discard the ambiguous region and to label the pixels of the clear region as foreground or background. A training data set is constructed based on the patches centered on the labeled pixels. Second, a convolutional neural network (CNN) is trained on the resulting data set to predict the probability of each pixel of being foreground (i.e., vein pixel), given a patch centered on it. The CNN learns what a finger-vein pattern is by learning the difference between vein patterns and background ones. The pixels in any region of a test image can then be classified effectively. Third, we propose another new and original contribution by developing and investigating a fully convolutional network to recover missing finger-vein patterns in the segmented image. The experimental results on two public finger-vein databases show a significant improvement in terms of finger-vein verification accuracy.","1556-6021","","10.1109/TIFS.2017.2689724","Direction générale des Entreprises of the Ministère de l’économie, de l’industrie et du numérique(grant numbers:ITEA3 IDEA4SWIFT 12028); National Natural Science Foundation of China(grant numbers:61402063); Natural Science Foundation Project of Chongqing(grant numbers:cstc2013kjrc-qnrc40013); Scientific Research Foundation of Chongqing Technology and Business University(grant numbers:1352019,2013-56-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890487","Hand biometrics;finger-vein verification;deep learning;convolutional neural network;convolutional autoencoder;representation learning","Veins;Feature extraction;Image segmentation;Neural networks;Iris recognition;Machine learning","","162","","41","IEEE","30 Mar 2017","","","IEEE","IEEE Journals"
"Large-Scale JPEG Image Steganalysis Using Hybrid Deep-Learning Framework","J. Zeng; S. Tan; B. Li; J. Huang","College of Information Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","30 Jan 2018","2018","13","5","1200","1214","Adoption of deep learning in image steganalysis is still in its initial stage. In this paper, we propose a generic hybrid deep-learning framework for JPEG steganalysis incorporating the domain knowledge behind rich steganalytic models. Our proposed framework involves two main stages. The first stage is hand-crafted, corresponding to the convolution phase and the quantization and truncation phase of the rich models. The second stage is a compound deep-neural network containing multiple deep subnets, in which the model parameters are learned in the training procedure. We provided experimental evidence and theoretical reflections to argue that the introduction of threshold quantizers, though disabling the gradient-descent-based learning of the bottom convolution phase, is indeed cost-effective. We have conducted extensive experiments on a large-scale data set extracted from ImageNet. The primary data set used in our experiments contains 500 000 cover images, while our largest data set contains five million cover images. Our experiments show that the integration of quantization and truncation into deep-learning steganalyzers do boost the detection performance by a clear margin. Furthermore, we demonstrate that our framework is insensitive to JPEG blocking artifact alterations, and the learned model can be easily transferred to a different attacking target and even a different data set. These properties are of critical importance in practical applications.","1556-6021","","10.1109/TIFS.2017.2779446","NSFC(grant numbers:61772349,U1636202,61402295,61572329,61702340); Guangdong NSF(grant numbers:2014A030313557); Shenzhen R&D Program(grant numbers:JCYJ20160328144421330); Alibaba Group through Alibaba Innovative Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8125774","Hybrid deep-learning framework;CNN network;steganalysis;steganography","Transform coding;Quantization (signal);Neurons;Convolution;Machine learning;Backpropagation;Training","","131","","37","IEEE","4 Dec 2017","","","IEEE","IEEE Journals"
"Learning Generalized Deep Feature Representation for Face Anti-Spoofing","H. Li; P. He; S. Wang; A. Rocha; X. Jiang; A. C. Kot","Nanyang Technological University, Singapore; Shanghai Jiao Tong University, Shanghai, China; City University of Hong Kong, Hong Kong; University of Campinas, Campinas, Brazil; Shanghai Jiao Tong University, Shanghai, China; Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","14 May 2018","2018","13","10","2639","2652","In this paper, we propose a novel framework leveraging the advantages of the representational ability of deep learning and domain generalization for face spoofing detection. In particular, the generalized deep feature representation is achieved by taking both spatial and temporal information into consideration, and a 3D convolutional neural network architecture tailored for the spatial-temporal input is proposed. The network is first initialized by training with augmented facial samples based on cross-entropy loss and further enhanced with a specifically designed generalization loss, which coherently serves as the regularization term. The training samples from different domains can seamlessly work together for learning the generalized feature representation by manipulating their feature distribution distances. We evaluate the proposed framework with different experimental setups using various databases. Experimental results indicate that our method can learn more discriminative and generalized information compared with the state-of-the-art methods.","1556-6021","","10.1109/TIFS.2018.2825949","National Research Foundation, Singapore; Infocomm Media Development Authority, Singapore; Tan Chin Tuan Foundation; São Paulo Research Foundation, Fapesp, DéjàVu(grant numbers:2017/12646-3); Coordination for the Improvement of Higher Level Education Personnel, CAPES (DeepEyes Grant); China Scholarship Council for the Scholarship(grant numbers:[2016] 3100 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8335313","Face spoofing;deep learning;3D CNN;domain generalization","Face;Three-dimensional displays;Distortion;Feature extraction;Machine learning;Cameras;Data mining","","129","","70","IEEE","11 Apr 2018","","","IEEE","IEEE Journals"
"Deep Feature Fusion for Iris and Periocular Biometrics on Mobile Devices","Q. Zhang; H. Li; Z. Sun; T. Tan","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","30 May 2018","2018","13","11","2897","2912","The quality of iris images on mobile devices is significantly degraded due to hardware limitations and less constrained environments. Traditional iris recognition methods cannot achieve high identification rate using these low-quality images. To enhance the performance of mobile identification, we develop a deep feature fusion network that exploits the complementary information presented in iris and periocular regions. The proposed method first applies maxout units into the convolutional neural networks (CNNs) to generate a compact representation for each modality and then fuses the discriminative features of two modalities through a weighted concatenation. The parameters of convolutional filters and fusion weights are simultaneously learned to optimize the joint representation of iris and periocular biometrics. To promote the iris recognition research on mobile devices under near-infrared (NIR) illumination, we publicly release the CASIA-Iris-Mobile-V1.0 database, which in total includes 11 000 NIR iris images of both eyes from 630 Asians. It is the largest NIR mobile iris database as far as we know. On the newly built CASIA-Iris-M1-S3 data set, the proposed method achieves 0.60% equal error rate and 2.32% false non-match rate at false match rate =10-5, which are obviously better than unimodal biometrics as well as traditional fusion methods. Moreover, the proposed model requires much fewer storage spaces and computational resources than general CNNs.","1556-6021","","10.1109/TIFS.2018.2833033","National Natural Science Foundation of China(grant numbers:61427811,61573360); National Key Research and Development Program of China(grant numbers:2017YFB0801900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356122","Iris recognition;periocular recognition;deep feature fusion;adaptive weights;mobile devices","Iris recognition;Mobile handsets;Databases;Lighting;Machine learning;Sensors","","106","","65","IEEE","8 May 2018","","","IEEE","IEEE Journals"
"A Siamese CNN for Image Steganalysis","W. You; H. Zhang; X. Zhao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","11 Aug 2020","2021","16","","291","306","Image steganalysis is a technique for detecting data hidden in images. Recent research has shown the powerful capabilities of using convolutional neural networks (CNN) for image steganalysis. However, due to the particularity of steganographic signals, there are still few reliable CNN-based methods for applying steganalysis to images of arbitrary size. In this paper, we address this issue by exploring the possibility of exploiting a network for steganalyzing images of varying sizes without retraining its parameters. On the assumption that natural image noise is similar between different image sub-regions, we propose an end-to-end, deep learning, novel solution for distinguishing steganography images from normal images that provides satisfying performance. The proposed network first takes the image as the input, then identifies the relationships between the noise of different image sub-regions, and, finally, outputs the resulting classification based upon them. Our algorithm adopts a Siamese, CNN-based architecture, which consists of two symmetrical subnets with shared parameters, and contains three phases: preprocessing, feature extraction, and fusion/classification. To validate the network, we generated datasets composed of steganography images with multiple sizes and their corresponding normal images sourced from BOSSbase 1.01 and ALASKA #2. Experimental results produced by the data generated by various methods show that our proposed network is well-generalized and robust.","1556-6021","","10.1109/TIFS.2020.3013204","National Natural Science Foundation of China(grant numbers:61972390,61802393,U1736214); National Basic Research Program of China (973 Program)(grant numbers:2019QY0701); Climbing Program of the Institute of Information Engineering of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153041","Steganalysis;deep learning;Siamese convolutional neural network","Feature extraction;Machine learning;Distortion;Detectors;Convolutional neural networks;Software","","97","","63","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks","H. Proença; J. C. Neves","Department of Computer Science, IT: Instituto de Telecomunicações, University of Beira Interior, Covilhã, Portugal; Department of Computer Science, IT: Instituto de Telecomunicações, University of Beira Interior, Covilhã, Portugal","IEEE Transactions on Information Forensics and Security","3 Jan 2018","2018","13","4","888","896","This paper is based on a disruptive hypothesis for periocular biometrics-in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recognizer's response is exclusively based on the information from the surroundings of the eye. As a major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of “multi-class” artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).","1556-6021","","10.1109/TIFS.2017.2771230","FCT Project(grant numbers:UID/EEA/50008/2013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101565","Soft biometrics;visual surveillance;homeland security","Iris recognition;Convolution;Eyelids;Feature extraction;Machine learning","","92","","27","IEEE","9 Nov 2017","","","IEEE","IEEE Journals"
"Learning Deep Off-the-Person Heart Biometrics Representations","E. J. da Silva Luz; G. J. P. Moreira; L. S. Oliveira; W. R. Schwartz; D. Menotti","Computing Department, Federal University of Ouro Preto, Ouro Preto, Brazil; Computing Department, Federal University of Ouro Preto, Ouro Preto, Brazil; Department of Informatics, Federal University of Paraná, Curitiba, Brazil; Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil; Department of Informatics, Federal University of Paraná, Curitiba, Brazil","IEEE Transactions on Information Forensics and Security","30 Jan 2018","2018","13","5","1258","1270","Since the beginning of the new millennium, the electrocardiogram (ECG) has been studied as a biometric trait for security systems and other applications. Recently, with devices such as smartphones and tablets, the acquisition of ECG signal in the off-the-person category has made this biometric signal suitable for real scenarios. In this paper, we introduce the usage of deep learning techniques, specifically convolutional networks, for extracting useful representation for heart biometrics recognition. Particularly, we investigate the learning of feature representations for heart biometrics through two sources: on the raw heartbeat signal and on the heartbeat spectrogram. We also introduce heartbeat data augmentation techniques, which are very important to generalization in the context of deep learning approaches. Using the same experimental setup for six methods in the literature, we show that our proposal achieves state-of-the-art results in the two off-the-person publicly available databases.","1556-6021","","10.1109/TIFS.2017.2784362","UFOP, UFPR, UFMG, FAPEMIG(grant numbers:APQ-02825-14,PPM-00540-17,APQ-00567-14); CAPES; CNPq(grant numbers:307010/2014-7,311053/2016-5,428333/2016-8); IBM Ph.D. fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8219706","Electrocardiogram;biometric systems;deep learning;off-the-person category","Electrocardiography;Biometrics (access control);Databases;Heart beat;Machine learning;Feature extraction","","79","","37","IEEE","18 Dec 2017","","","IEEE","IEEE Journals"
"Research on Deep Learning Techniques in Breaking Text-Based Captchas and Designing Image-Based Captcha","M. Tang; H. Gao; Y. Zhang; Y. Liu; P. Zhang; P. Wang","Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","8 May 2018","2018","13","10","2522","2537","The ability of hackers to infiltrate computer systems using computer attack programs and bots led to the development of Captchas or Completely Automated Public Turing Tests to Tell Computers and Humans Apart. The text Captcha is the most popular Captcha scheme given its ease of construction and user friendliness. However, the next generation of hackers and programmers has decreased the expected security of these mechanisms, leaving websites open to attack. Text Captchas are still widely used, because it is believed that the attack speeds are slow, typically two to five seconds per image, and this is not seen as a critical threat. In this paper, we introduce a simple, generic, and fast attack on text Captchas that effectively challenges that supposition. With deep learning techniques, our attack demonstrates a high success rate in breaking the Roman-character-based text Captchas deployed by the top 50 most popular international websites and three Chinese Captchas that use a larger character set. These targeted schemes cover almost all existing resistance mechanisms, demonstrating that our attack techniques are also applicable to other existing Captchas. Does this work then spell the beginning of the end for text-based Captcha? We believe so. A novel image-based Captcha named Style Area Captcha (SACaptcha) is proposed in this paper, which is based on semantic information understanding, pixel-level segmentation, and deep learning techniques. Having demonstrated that text Captchas are no longer secure, we hope that our proposal shows promise in the development of image-based Captchas using deep learning techniques.","1556-6021","","10.1109/TIFS.2018.2821096","National Natural Science Foundation of China(grant numbers:61472311); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327894","Captcha;text-based;security;deep learning;convolutional neural network;image-based","CAPTCHAs;Machine learning;Resistance;Image segmentation;Character recognition;Computer security","","63","","60","IEEE","29 Mar 2018","","","IEEE","IEEE Journals"
"Improving Periocular Recognition by Explicit Attention to Critical Regions in Deep Neural Network","Z. Zhao; A. Kumar","Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Transactions on Information Forensics and Security","5 Jun 2018","2018","13","12","2937","2952","Periocular recognition has been emerging as an effective biometric identification approach, especially under less constrained environments where face and/or iris recognition is not applicable. This paper proposes a new deep learning-based architecture for robust and more accurate periocular recognition which incorporates attention model to emphasize important regions in the periocular images. The new architecture adopts multi-glance mechanism, in which part of the intermediate components are configured to incorporate emphasis on important semantical regions, i.e., eyebrow and eye, within a periocular image. By focusing on these regions, the deep convolutional neural network is able to learn additional discriminative features, which in turn improves the recognition capability of the whole model. The superior performance of our method strongly suggests that eyebrow and eye regions are important for periocular recognition, and deserve special attention during the deep feature learning process. This paper also presents a customized verification-oriented loss function, which is shown to provide higher discriminating power than conventional contrastive/triplet loss functions. Extensive experiments on six publicly available databases are performed to evaluate the proposed approach. The reproducible experimental results indicate that our approach significantly outperforms several state-of-the-art methods for the periocular recognition.","1556-6021","","10.1109/TIFS.2018.2833018","General Research Fund from the Hong Kong Research Grant Council(grant numbers:15206814 (PolyU 152068/14E)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8353874","Periocular recognition;deep learning;attention model;region of interest","Convolution;Iris recognition;Machine learning;Eyebrows;Feature extraction;Visualization;Training","","63","","48","IEEE","3 May 2018","","","IEEE","IEEE Journals"
"Matching Software-Generated Sketches to Face Photographs With a Very Deep CNN, Morphed Faces, and Transfer Learning","C. Galea; R. A. Farrugia","Department of Communications and Computer Engineering, University of Malta, Msida, Malta; Department of Communications and Computer Engineering, University of Malta, Msida, Malta","IEEE Transactions on Information Forensics and Security","7 Feb 2018","2018","13","6","1421","1431","Sketches obtained from eyewitness descriptions of criminals have proven to be useful in apprehending criminals, particularly when there is a lack of evidence. Automated methods to identify subjects depicted in sketches have been proposed in the literature, but their performance is still unsatisfactory when using software-generated sketches and when tested using extensive galleries with a large amount of subjects. Despite the success of deep learning in several applications including face recognition, little work has been done in applying it for face photograph-sketch recognition. This is mainly a consequence of the need to ensure robust training of deep networks by using a large number of images, yet limited quantities are publicly available. Moreover, most algorithms have not been designed to operate on software-generated face composite sketches which are used by numerous law enforcement agencies worldwide. This paper aims to tackle these issues with the following contributions: 1) a very deep convolutional neural network is utilised to determine the identity of a subject in a composite sketch by comparing it to face photographs and is trained by applying transfer learning to a state-of-the-art model pretrained for face photograph recognition; 2) a 3-D morphable model is used to synthesise both photographs and sketches to augment the available training data, an approach that is shown to significantly aid performance; and 3) the UoM-SGFS database is extended to contain twice the number of subjects, now having 1200 sketches of 600 subjects. An extensive evaluation of popular and state-of-the-art algorithms is also performed due to the lack of such information in the literature, where it is demonstrated that the proposed approach comprehensively outperforms state-of-the-art methods on all publicly available composite sketch datasets.","1556-6021","","10.1109/TIFS.2017.2788002","Malta Government Scholarship Scheme; NVIDIA Corporation through the donation of a Titan X Pascal GPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241712","Deep learning;convolutional neural network;software-generated composite sketches;face photos;morphological model;augmentation;database","Face;Face recognition;Databases;Algorithm design and analysis;Feature extraction;Machine learning","","54","","53","IEEE","29 Dec 2017","","","IEEE","IEEE Journals"
"DRL-FAS: A Novel Framework Based on Deep Reinforcement Learning for Face Anti-Spoofing","R. Cai; H. Li; S. Wang; C. Chen; A. C. Kot","Department of Electrical and Electrics Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electrics Engineering, Nanyang Technological University, Singapore; Department of Computer Science, City University of Hong Kong, Hong Kong; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China; Department of Electrical and Electrics Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","7 Oct 2020","2021","16","","937","951","Inspired by the philosophy employed by human beings to determine whether a presented face example is genuine or not, i.e., to glance at the example globally first and then carefully observe the local regions to gain more discriminative information, for the face anti-spoofing problem, we propose a novel framework based on the Convolutional Neural Network (CNN) and the Recurrent Neural Network (RNN). In particular, we model the behavior of exploring face-spoofing-related information from image sub-patches by leveraging deep reinforcement learning. We further introduce a recurrent mechanism to learn representations of local information sequentially from the explored sub-patches with an RNN. Finally, for the classification purpose, we fuse the local information with the global one, which can be learned from the original input image through a CNN. Moreover, we conduct extensive experiments, including ablation study and visualization analysis, to evaluate our proposed framework on various public databases. The experiment results show that our method can generally achieve state-of-the-art performance among all scenarios, demonstrating its effectiveness.","1556-6021","","10.1109/TIFS.2020.3026553","Wallenberg-NTU Presidential Postdoctoral Fellowship; NTU-PKU Joint Research Institute; Ng Teng Fong Charitable Foundation; Science and Technology Foundation of Guangzhou Huangpu Development District(grant numbers:2017GH22,201902010028); Sino-Singapore International Joint Research Institute(grant numbers:206-A017023,206-A018001); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20180305124550725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205636","Face anti-spoofing;deep learning;reinforcement learning","Feature extraction;Faces;Cameras;Machine learning;Learning (artificial intelligence);Support vector machines;Recurrent neural networks","","50","","47","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"Image to Video Person Re-Identification by Learning Heterogeneous Dictionary Pair With Feature Projection Matrix","X. Zhu; X. -Y. Jing; X. You; W. Zuo; S. Shan; W. -S. Zheng","State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; CAS, Institute of Computing Technology, Beijing, China; School of Data and Computer Science, Sun Yat-sen University, China","IEEE Transactions on Information Forensics and Security","18 Dec 2017","2018","13","3","717","732","Person re-identification plays an important role in video surveillance and forensics applications. In many cases, person re-identification needs to be conducted between image and video clip, e.g., re-identifying a suspect from large quantities of pedestrian videos given a single image of the suspect. We call re-identification in this scenario as image to video person reidentification (IVPR). In practice, image and video are usually represented with different features, and there usually exist large variations between frames within each video. These factors make matching between image and video become a very challenging task. In this paper, we propose a joint feature projection matrix and heterogeneous dictionary pair learning (PHDL) approach for IVPR. Specifically, the PHDL jointly learns an intra-video projection matrix and a pair of heterogeneous image and video dictionaries. With the learned projection matrix, the influence caused by the variations within each video on the matching can be reduced. With the learned dictionary pair, the heterogeneous image and video features can be transformed into coding coefficients with the same dimension, such that the matching can be conducted by using the coding coefficients. Furthermore, to ensure that the obtained coding coefficients own favorable discriminability, the PHDL designs a point-to-set coefficient discriminant term. To make better use of the complementary spatial-temporal and visual appearance information contained in pedestrian video data, we further propose a multi-view PHDL approach, which can fuse different video information effectively in the dictionary learning process. Experiments on four publicly available person sequence data sets demonstrate the effectiveness of the proposed approaches.","1556-6021","","10.1109/TIFS.2017.2765524","National Key Research and Development Program of China(grant numbers:2017YFB0202001); National Nature Science Foundation of China(grant numbers:61671182,61672208,61772220,41571417,U1404618,61272273,61572375,61233011,91418202,61472178,61373038,61672392); National Basic Research 973 Program of China(grant numbers:2014CB340702); National Key Technology Research and Development Program of the Ministry of Science and Technology of China(grant numbers:2015BAK36B00,2015BAK01B06); Key Science and Technology of Shenzhen(grant numbers:CXZZ20150814155434903); Key Program for International S&T Cooperation Projects of China(grant numbers:2016YFE0121200); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170900); Scientific Research Staring Foundation for Introduced Talents in NJUPT under NUPTSF(grant numbers:NY217009); Science and Technology Program in Henan Province(grant numbers:1721102410064,172102210186); Research Foundation of Henan University(grant numbers:2015YBZR024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8078250","Person re-identification;image to video person re-identification;heterogeneous dictionary pair learning;feature projection matrix;multi-view learning","Machine learning;Feature extraction;Dictionaries;Visualization;Electronic mail;Legged locomotion;Measurement","","49","","73","IEEE","23 Oct 2017","","","IEEE","IEEE Journals"
"DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification","S. Zhang; R. He; Z. Sun; T. Tan","Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","18 Dec 2017","2018","13","3","637","647","MeshFace photos have been widely used in many Chinese business organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i.e., blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered ID images and the ground-truth clear ID images and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace data sets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper.","1556-6021","","10.1109/TIFS.2017.2763119","National Natural Science Foundation of China(grant numbers:61622310,61473289); Beijing Municipal Science and Technology Commission(grant numbers:Z161100000216144); State Key Development Program(grant numbers:2016YFB1001001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067496","MeshFace;face verification;blind inpainting;deep learning;DeMeshNet;spatial transformer","Face;Feature extraction;Face recognition;Training;Facial features;Machine learning","","49","","59","IEEE","13 Oct 2017","","","IEEE","IEEE Journals"
"A CNN-Based Framework for Comparison of Contactless to Contact-Based Fingerprints","C. Lin; A. Kumar","Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Transactions on Information Forensics and Security","27 Aug 2018","2019","14","3","662","676","Accurate comparison of contactless 2-D fingerprint images with contact-based fingerprints is critical for the success of emerging contactless 2-D fingerprint technologies, which offer more hygienic and deformation-free acquisition of fingerprint features. Convolutional neural networks (CNNs) have shown remarkable capabilities in biometrics recognition. However, there has been almost nil attempt to match fingerprint images using CNN-based approaches. This paper develops a CNN-based framework to accurately match contactless and contact-based fingerprint images. Our framework first trains a multi-Siamese CNN using fingerprint minutiae, respective ridge map and specific region of ridge map. This network is used to generate deep fingerprint representation using a distance-aware loss function. Deep fingerprint representations generated in such multi-Siamese network are concatenated for more accurate cross comparison. The proposed approach for cross-fingerprint comparison is evaluated on two publicly available databases containing contactless 2-D fingerprints and respective contact-based fingerprints. Our experiments presented in this paper consistently achieve outperforming results over several popular deep learning architectures and over contactless to contact-based fingerprints comparison methods in the literature.","1556-6021","","10.1109/TIFS.2018.2854765","General Research Fund from Hong Kong Research Grant Council(grant numbers:PolyU152192/17E,PolyU 5169/13E); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409476","Contactless and contact-based fingerprint;sensor interoperability;multi-Siamese CNN","Sensors;Image sensors;Image recognition;Interoperability;Fingerprint recognition;Databases;Machine learning","","48","","57","IEEE","10 Jul 2018","","","IEEE","IEEE Journals"
"Face Verification via Learned Representation on Feature-Rich Video Frames","G. Goswami; M. Vatsa; R. Singh","Indraprastha Institute of Information Technology Delhi, Delhi, India; Indraprastha Institute of Information Technology Delhi, Delhi, India; Indraprastha Institute of Information Technology Delhi, Delhi, India","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","7","1686","1698","Abundance and availability of video capture devices, such as mobile phones and surveillance cameras, have instigated research in video face recognition, which is highly pertinent in law enforcement applications. While the current approaches have reported high accuracies at equal error rates, performance at lower false accept rates requires significant improvement. In this paper, we propose a novel face verification algorithm, which starts with selecting feature-rich frames from a video sequence using discrete wavelet transform and entropy computation. Frame selection is followed by representation learning-based feature extraction, where three contributions are presented: 1) deep learning architecture, which is a combination of stacked denoising sparse autoencoder (SDAE) and deep Boltzmann machine (DBM); 2) formulation for joint representation in an autoencoder; and 3) updating the loss function of DBM by including sparse and low rank regularization. Finally, a multilayer neural network is used as the classifier to obtain the verification decision. The results are demonstrated on two publicly available databases, YouTube Faces and Point and Shoot Challenge. Experimental analysis suggests that: 1) the proposed feature-richness-based frame selection offers noticeable and consistent performance improvement compared with frontal only frames, random frames, or frame selection using perceptual no-reference image quality measures and 2) joint feature learning in SDAE and sparse and low rank regularization in DBM helps in improving face verification performance. On the benchmark Point and Shoot Challenge database, the algorithm yields the verification accuracy of over 97% at 1% false accept rate whereas, on the YouTube Faces database, over 95% verification accuracy is observed at equal error rate.","1556-6021","","10.1109/TIFS.2017.2668221","MEITY, India, NVIDIA GPU grant, and Infosys CAI, IIIT-Delhi; IBM Ph.D. fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850956","Deep learning;autoencoder;deep Boltzmann machine;face recognition;frame selection","Face;Face recognition;Databases;Feature extraction;YouTube;Machine learning;Neural networks","","42","","50","IEEE","13 Feb 2017","","","IEEE","IEEE Journals"
"Detection of Double Compressed AMR Audio Using Stacked Autoencoder","D. Luo; R. Yang; B. Li; J. Huang","College of Information Engineering, Shenzhen University, Shenzhen, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","2","432","444","The adaptive multi-rate (AMR) audio codec adopted by many portable recording devices is widely used in speech compression. The use of AMR speech recordings as evidence in court is growing. Nowadays, it is easy to tamper with digital speech recordings, which makes audio forensics increasingly important. The detection of double compressed audio is one of the key issues in audio forensics. In this paper, we propose a framework for detecting double compressed AMR audio based on the stacked autoencoder (SAE) network and the universal background model-Gaussian mixture model (UBM-GMM). Instead of hand-crafted features, we used the SAE to learn the optimal features automatically from the audio waveforms. Audio frames are used as network input and the last hidden layer's output constitutes the features of a single frame. For an audio clip with many frames, the features of all the frames are aggregated and classified by UBM-GMM. Experimental results show that our method is effective in distinguishing single/double compressed AMR audio and outperforms the existing methods by achieving a detection accuracy of 98% on the TIMIT database. Exhaustive experiments demonstrate the effectiveness and robustness of the proposed method.","1556-6021","","10.1109/TIFS.2016.2622012","Natural Science Foundation of China(grant numbers:61332012,61572329,61602318); Natural Science Foundation of Guangdong Province(grant numbers:2014A030313557); Shenzhen Research and Development Program(grant numbers:GJHZ20140418191518323,JCYJ20160328144421330); Beijing Key Laboratory of Advanced Information Science and Network Technology(grant numbers:XDXX1602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707437","Adaptive multi-mate;double compressed AMR;audio forensics;stacked autoencoder","Feature extraction;Forensics;Codecs;Speech;Machine learning;History;Digital audio players","","33","","36","IEEE","26 Oct 2016","","","IEEE","IEEE Journals"
"Conditional Variational Auto-Encoder and Extreme Value Theory Aided Two-Stage Learning Approach for Intelligent Fine-Grained Known/Unknown Intrusion Detection","J. Yang; X. Chen; S. Chen; X. Jiang; X. Tan","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Information Forensics and Security","1 Jul 2021","2021","16","","3538","3553","Promptly discovering unknown network attacks is critical for reducing the risk of major loss imposed on organizations and information infrastructure. This paper aims at developing an intelligent intrusion detection system capable of classifying known attacks as well as inferring unknown ones. To achieve this, we formulate the problem of fine-grained known/unknown intrusion detection as a two-stage minimization problem, where the first stage is to seek a score measure for minimizing the empirical risk of misclassifying the known attacks, while the second stage is to find another score measure for minimizing the identification risk of inferring unknown attacks. The hierarchical nature of problem formulation allows us to employ the class conditioned auto-encoders to construct a hierarchical intrusion detection framework. Since the reconstruction errors of unknown attacks are generally higher than that of the known attacks, we further employ extreme value theory in the second stage to model the distribution of reconstruction errors for differentiating known/unknown attack. To further reduce the false positive rate, we add a benign clustering module for learning the multimodal distribution of benign traffic. We conduct an experiment on two widely used datasets for assessing intrusion detection. The results show that the proposed method improves the detection rate of unknown attacks while keeping a low false positive rate.","1556-6021","","10.1109/TIFS.2021.3083422","National Key Research and Development Program of China(grant numbers:2018YFF01012200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439944","Intrusion detection;artificial intelligence;auto-encoder;extreme value theory","Intrusion detection;Feature extraction;Machine learning;Support vector machines;Anomaly detection;Training;Deep learning","","30","","39","IEEE","24 May 2021","","","IEEE","IEEE Journals"
"Sparse Coding for N-Gram Feature Extraction and Training for File Fragment Classification","F. Wang; T. -T. Quach; J. Wheeler; J. B. Aimone; C. D. James","Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA","IEEE Transactions on Information Forensics and Security","14 May 2018","2018","13","10","2553","2562","File fragment classification is an important step in the task of file carving in digital forensics. In file carving, files must be reconstructed based on their content as a result of their fragmented storage on disk or in memory. Existing methods for classification of file fragments typically use hand-engineered features, such as byte histograms or entropy measures. In this paper, we propose an approach using sparse coding that enables automated feature extraction. Sparse coding, or sparse dictionary learning, is an unsupervised learning algorithm, and is capable of extracting features based simply on how well those features can be used to reconstruct the original data. With respect to file fragments, we learn sparse dictionaries for n-grams, continuous sequences of bytes, of different sizes. These dictionaries may then be used to estimate n-gram frequencies for a given file fragment, but for significantly larger n-gram sizes than are typically found in existing methods which suffer from combinatorial explosion. To demonstrate the capability of our sparse coding approach, we used the resulting features to train standard classifiers, such as support vector machines over multiple file types. Experimentally, we achieved significantly better classification results with respect to existing methods, especially when the features were used in supplement to existing hand-engineered features.","1556-6021","","10.1109/TIFS.2018.2823697","Sandia National Laboratories’ Laboratory Directed Research and Development (LDRD) Program; Hardware Acceleration of Adaptive Neural Algorithms Grand Challenge Project; National Technology and Engineering Solutions of Sandia LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration(grant numbers:DE-NA0003525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8331868","File fragment classification;file carving;automated feature extraction;sparse coding;dictionary learning;unsupervised learning;n-gram;support vector machine","Encoding;Feature extraction;Dictionaries;Support vector machines;Training;Machine learning;Data mining","","28","","24","IEEE","5 Apr 2018","","","IEEE","IEEE Journals"
"Enhanced PeerHunter: Detecting Peer-to-Peer Botnets Through Network-Flow Level Community Behavior Analysis","D. Zhuang; J. M. Chang","Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Information Forensics and Security","11 Feb 2019","2019","14","6","1485","1500","Peer-to-peer (P2P) botnets have become one of the major threats in network security for serving as the fundamental infrastructure for various cyber-crimes. More challenges are involved in the problem of detecting P2P botnets, despite a few work claimed to detect centralized botnets effectively. We propose an enhanced PeerHunter, a network-flow level community behavior analysis based system, to detect P2P botnets. Our system starts from a P2P network flow detection component. Then, it uses “mutual contacts” to cluster bots into communities. Finally, it uses network-flow level community behavior analysis to detect potential botnets. In the experimental evaluation, we propose two evasion attacks, where we assume the adversaries know our techniques in advance and attempt to evade our system by making the P2P bots mimic the behavior of legitimate P2P applications. Our results showed that enhanced PeerHunter can obtain high detection rate with few false positives, and high robustness against the proposed attacks.","1556-6021","","10.1109/TIFS.2018.2881657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8536452","P2P botnet;intrusion detection;network security;community detection","Botnet;Peer-to-peer computing;Payloads;Cryptography;Feature extraction;Indexes;Machine learning algorithms","","26","","35","IEEE","15 Nov 2018","","","IEEE","IEEE Journals"
"Topology Preserving Structural Matching for Automatic Partial Face Recognition","Y. Duan; J. Lu; J. Feng; J. Zhou","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","27 Mar 2018","2018","13","7","1823","1837","In this paper, we propose a topology preserving graph matching (TPGM) method for partial face recognition. Most existing face recognition methods extract features from holistic facial images. However, faces in real-world unconstrained environments may be occluded by objects or other faces, which cannot provide the whole face images for description. Keypoint-based partial face recognition methods such as multi-keypoint descriptor with Gabor ternary pattern and robust point set matching match the local keypoints for partial face recognition. However, they simply measure the nodewise similarity without higher order geometric graph information, which are susceptible to noises. To address this, our TPGM method estimates a non-rigid transformation encoding the second-order geometric structure of the graph, so that more accurate and robust correspondence can be computed with the topological information. In order to exploit higher order topological information, we propose a topology preserving structural matching method to construct a higher order structure for each face and estimate the transformation. Experimental results on four widely used face data sets demonstrate that our method outperforms most existing state-of-the-art face recognition methods.","1556-6021","","10.1109/TIFS.2018.2804919","National Natural Science Foundation of China(grant numbers:U1713214,61672306,61572271,61527808); National 1000 Young Talents Plan Program; National Basic Research Program of China(grant numbers:2014CB349304); Shenzhen Fundamental Research Fund (Subject Arrangement)(grant numbers:JCYJ20170412170438636); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288854","Partial face recognition;keypoint extraction;structural matching","Face;Face recognition;Robustness;Topology;Feature extraction;Encoding;Machine learning","","23","","90","IEEE","12 Feb 2018","","","IEEE","IEEE Journals"
"A Game-Theoretic Analysis of Adversarial Classification","L. Dritsoula; P. Loiseau; J. Musacchio","University of California, Santa Cruz, Santa Cruz, CA, USA; MPI-SWS, Saarbrücken, Germany; Department of Technology Management, University of California, Santa Cruz, Santa Cruz, CA, USA","IEEE Transactions on Information Forensics and Security","26 Sep 2017","2017","12","12","3094","3109","Attack detection is usually approached as a classification problem. However, standard classification tools often perform poorly, because an adaptive attacker can shape his attacks in response to the algorithm. This has led to the recent interest in developing methods for adversarial classification, but to the best of our knowledge, there have been a very few prior studies that take into account the attacker's tradeoff between adapting to the classifier being used against him with his desire to maintain the efficacy of his attack. Including this effect is a key to derive solutions that perform well in practice. In this investigation, we model the interaction as a game between a defender who chooses a classifier to distinguish between attacks and normal behavior based on a set of observed features and an attacker who chooses his attack features (class 1 data). Normal behavior (class 0 data) is random and exogenous. The attacker's objective balances the benefit from attacks and the cost of being detected while the defender's objective balances the benefit of a correct attack detection and the cost of false alarm. We provide an efficient algorithm to compute all Nash equilibria and a compact characterization of the possible forms of a Nash equilibrium that reveals intuitive messages on how to perform classification in the presence of an attacker. We also explore qualitatively and quantitatively the impact of the non-attacker and underlying parameters on the equilibrium strategies.","1556-6021","","10.1109/TIFS.2017.2718494","AFOSR(grant numbers:FA9550-09-1-0049); NSF(grant numbers:CNS-0910711,CNS-0953884); Alexander von Humboldt Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954621","Adversarial classification;game theory;security;Nash equilibrium;threshold strategies;randomization","Games;Security;Nash equilibrium;Machine learning algorithms;Standards;Computational modeling","","23","","57","IEEE","21 Jun 2017","","","IEEE","IEEE Journals"
"Synergistic Generic Learning for Face Recognition From a Contaminated Single Sample per Person","M. Pang; Y. -M. Cheung; B. Wang; J. Lou","Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Department of Computer Science, Hong Kong Baptist University, Hong Kong","IEEE Transactions on Information Forensics and Security","11 Sep 2019","2020","15","","195","209","Single sample per person face recognition (SSPP FR), i.e., identifying a person (i.e., data subject) with a single face image only for training, has several attractive potential applications, but it is still a challenging problem. Existing generic learning methods usually leverage prototype plus variation (P+V) model for SSPP FR provided that face samples in the biometric enrolment database are variation-free and thus can be treated as the prototypes of data subjects. However, this condition is not satisfied when these samples are contaminated by nuisance facial variations in the wild, such as varied expressions, poor lightings, and disguises (e.g., wearing scarf). We call this new and practical problem SSPP FR with a  ${c}$ ontaminated biometric  ${e}$ nrolment database (SSPP-ce FR). Subsequently, a challenging issue will be raised on estimating proper prototypes from the contaminated enrolment samples in SSPP-ce FR. Moreover, the generated variation dictionary also needs to be enhanced because it is simply based on the subtraction of average face from the samples of the same data subject in the generic set, thus containing individual characteristics that can hardly be shared by other data subjects. To address these two issues, we propose a novel synergistic generic learning (SGL) method to study the SSPP-ce FR problem. Compared with the existing generic learning methods, SGL develops a new “learned P + learned V” model to identify new query samples. Specifically, it learns better prototypes for the contaminated samples in the biometric enrolment database by preserving their more discriminative subject-specific portions and learns a representative variation dictionary by extracting the less discriminative intra-subject variants from an auxiliary generic set. The experiments on various benchmark face datasets demonstrate the effectiveness of the proposed SGL method.","1556-6021","","10.1109/TIFS.2019.2919950","National Natural Science Foundation of China(grant numbers:61672444,61272366); SZSTI(grant numbers:JCYJ20160531194006833); Faculty Research Grant of Hong Kong Baptist University(grant numbers:FRG2/17-18/082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726380","Single sample per person;generic learning;prototype learning;variation dictionary learning","Databases;Prototypes;Learning systems;Face;Dictionaries;Biological system modeling;Machine learning","","19","","63","IEEE","30 May 2019","","","IEEE","IEEE Journals"
"Soft Semantic Representation for Cross-Domain Face Recognition","C. Peng; N. Wang; J. Li; X. Gao","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Video and Image Processing System Laboratory, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","11 Aug 2020","2021","16","","346","360","The problem of cross-domain face recognition aims to identify facial images obtained across different domains, which attracts increasing attentions because of its wide applications on law-enforcement identification and camera surveillance. The problem is challenging due to the huge domain discrepancy. Despite great progress achieved in recent years, existing algorithms usually fail to fully exploit the semantic information for identifying cross-domain faces, which could be a strong clue for recognition. In this article, we propose an effective algorithm for cross-domain face recognition by exploiting semantic information integrated with deep convolutional neural networks (CNN). We first introduce a soft face parsing algorithm where the boundaries of facial components are measured as probabilistic values. By taking the original face image as the guidance to improve face parsing result, each pixel may belong partially to the facial component to avoid inaccurate segmentation around component boundaries. We then propose a hierarchical soft semantic representation framework for cross-domain face recognition. Both the soft semantic level and contour level deep features obtained via CNN are computed and combined together, which could fully exploit the identical semantic clue among cross-domain faces. We provide extensive experiments to demonstrate that the proposed soft semantic representation algorithm performs superior against state-of-the-art methods.","1556-6021","","10.1109/TIFS.2020.3013209","National Basic Research Program of China (973 Program)(grant numbers:2018AAA0103202); National Natural Science Foundation of China(grant numbers:61806152,61922066,61876142,61671339,61772402,U1605252,61976166); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JM-289); Key Research and Development Program of Shaanxi(grant numbers:2020ZDLGY08-08); China Post-Doctoral Science Foundation(grant numbers:2018M631124,2019T120880); National High-Level Talents Special Support Program of China(grant numbers:CS31117200001); Fundamental Research Funds for the Central Universities(grant numbers:JB191502,JB190117); China 111 Project(grant numbers:B16037); Xidian University Intellifusion Joint Innovation Laboratory of Artificial Intelligence; Innovation Fund of Xidian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153025","Cross-domain face recognition;semantic representation;face parsing","Face;Face recognition;Semantics;Task analysis;Feature extraction;Generative adversarial networks;Machine learning","","18","","93","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"An Attack-Resilient Architecture for the Internet of Things","H. M. J. Almohri; L. T. Watson; D. Evans","Department of Computer Science, Kuwait University, Safat, Kuwait; Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, USA; Department of Computer Science, University of Virginia, Charlottesville, USA","IEEE Transactions on Information Forensics and Security","17 Jul 2020","2020","15","","3940","3954","With current IoT architectures, once a single device in a network is compromised, it can be used to disrupt the behavior of other devices on the same network. Even though system administrators can secure critical devices in the network using best practices and state-of-the-art technology, a single vulnerable device can undermine the security of the entire network. The goal of this work is to limit the ability of an attacker to exploit a vulnerable device on an IoT network and fabricate deceitful messages to co-opt other devices. The approach is to limit attackers by using device proxies that are used to retransmit and control network communications. We present an architecture that prevents deceitful messages generated by compromised devices from affecting the rest of the network. The design assumes a centralized and trustworthy machine that can observe the behavior of all devices on the network. The central machine collects application layer data, as opposed to low-level network traffic, from each IoT device. The collected data is used to train models that capture the normal behavior of each individual IoT device. The normal behavioral data is then used to monitor the IoT devices and detect anomalous behavior. This paper reports on our experiments using both a binary classifier and a density-based clustering algorithm to model benign IoT device behavior with a realistic test-bed, designed to capture normal behavior in an IoT-monitored environment. Results from the IoT testbed show that both the classifier and the clustering algorithms are promising and encourage the use of application-level data for detecting compromised IoT devices.","1556-6021","","10.1109/TIFS.2020.2994777","Kuwait University(grant numbers:RQ01/18); National Science Foundation SaTC Program(grant numbers:1804603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093828","Internet of Things;intrusion detection;unsupervised learning;network security","Data models;Internet of Things;Machine learning;Analytical models;Anomaly detection;Access control","","10","","51","IEEE","14 May 2020","","","IEEE","IEEE Journals"
"Selective Domain-Invariant Feature Alignment Network for Face Anti-Spoofing","L. Zhou; J. Luo; X. Gao; W. Li; B. Lei; J. Leng","Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, China Three Gorges University, Yichang, China; Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Information Forensics and Security","6 Dec 2021","2021","16","","5352","5365","One primary challenge in face anti-spoofing refers to suffering a sharp performance drop in cross-domain scenes, where training and testing images are collected from different datasets. Recent methods have achieved promising results by aligning the features of all images among the available source domains. However, due to significant distribution discrepancies among non-face regions of all images, it is challenging to capture domain-invariant features for these regions. In this paper, we propose a novel Selective Domain-invariant Feature Alignment Network (SDFANet) for cross-domain face anti-spoofing, which aims to seek common feature representations by fully exploring the generalization of different regions of images. Different from previous works that align the whole features directly, the proposed SDFANet leverages multiple domain discriminators with the same architecture to balance the generalization of different regions of the all images. Specifically, we firstly design a multi-grained feature alignment network composed of a local-region and global-image alignment subnetworks to learn more generalized feature space for real faces. Besides, the domain adapter module, which aims to alleviate the large domain discrepancy with the help of the domain attention strategy, is adopted to facilitate the learning of our multi-grained feature alignment network. In addition, a multi-scale attention fusion module is designed in our feature generator to refine the different levels of features effectively. Experimental results show that the proposed SDFANet can greatly improve the generalization ability of face anti-spoofing, and that is superior to the existing methods.","1556-6021","","10.1109/TIFS.2021.3125603","Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJZD-K 201900601,KJQN-202100627); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyjmsxmX0461); National Natural Science Foundation of China(grant numbers:62102057,62036007,62050175); Hubei Key Laboratory of Intelligent Vision-Based Monitoring for Hydroelectric Engineering(grant numbers:2020SDSJ01); Construction fund for Hubei Key Laboratory of Intelligent Vision-Based Monitoring for Hydroelectric Engineering(grant numbers:2019ZYYD007); Graduate Scientific Research and Innovation Foundation of Chongqing(grant numbers:CYS21303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600829","Cross-domain face anti-spoofing;feature alignment network;domain adapter;adversarial learning","Face recognition;Adversarial machine learning;Feature extraction;Generators;Task analysis;Deep learning","","9","","63","IEEE","4 Nov 2021","","","IEEE","IEEE Journals"
"Semi-Supervised Face Frontalization in the Wild","Z. Zhang; R. Liang; X. Chen; X. Xu; G. Hu; W. Zuo; E. R. Hancock","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; Anyvision Group, Belfast, U.K.; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computer Science, University of York, York, U.K.","IEEE Transactions on Information Forensics and Security","7 Oct 2020","2021","16","","909","922","Synthesizing a frontal view face from a single nonfrontal image, i.e. face frontalization, is a task of practical importance in a wide range of facial image analysis applications. However, to train the frontalization model in a supervised manner, most existing face frontalization methods rely on the availability of nonfrontal-frontal face pairs (typically from the Multi-PIE dataset) captured in a constrained environment. Such approaches, in return, limit the generalizability of their application to unconstrained scenarios. Unfortunately, although a large amount of in-the-wild face datasets are available, they cannot easily be utilized for face frontalization training since the nonfrontal and frontal facial images are not paired. To train a frontalization network which generalizes well to both constrained and unconstrained environments, we propose a semi-supervised learning framework which effectively uses both (labeled) indoor and (unlabeled) outdoor faces. Specifically, to achieve this goal, this article presents a Cycle-Consistent Face Frontalization Generative Adversarial Network (CCFF-GAN) which consists of both (1) the supervised and (2) the unsupervised components. For (1), we use the indoor paired (labeled) data to learn a roughly accurate frontalization network which may not generalize well to outdoor (in-the-wild) scenarios. For (2), to cope with the generalization issue, the unsupervised part uses the unpaired (unlabeled) images under the perceptual cycle consistency constraint in the semantic feature space to generalize the network from controlled (indoor) to uncontrolled (outdoor) environment. Extensive experiments demonstrate the effectiveness of the proposed method in comparison with the state-of-the-art face frontalization methods, especially under the in-the-wild scenarios.","1556-6021","","10.1109/TIFS.2020.3025412","Research Funds of State Grid Shaanxi Electric Power Company and State Grid Shaanxi Information and Telecommunication Company(grant numbers:SGSNXT00GCJS1900134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201316","Face frontalization;face synthesis;face recognition","Faces;Training;Face recognition;Three-dimensional displays;Generative adversarial networks;Machine learning;Solid modeling","","8","","48","IEEE","21 Sep 2020","","","IEEE","IEEE Journals"
"Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval","X. Yuan; Z. Zhang; X. Wang; L. Wu","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Swansea University, Swansea, U.K.","IEEE Transactions on Information Forensics and Security","1 Aug 2023","2023","18","","4681","4694","Deep hashing has been intensively studied and successfully applied in large-scale image retrieval systems due to its efficiency and effectiveness. Recent studies have recognized that the existence of adversarial examples poses a security threat to deep hashing models, that is, adversarial vulnerability. Notably, it is challenging to efficiently distill reliable semantic representatives for deep hashing to guide adversarial learning, and thereby it hinders the enhancement of adversarial robustness of deep hashing-based retrieval models. Moreover, current researches on adversarial training for deep hashing are hard to be formalized into a unified minimax structure. In this paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the adversarial robustness of deep hashing models. Specifically, we conceive a discriminative mainstay features learning (DMFL) scheme to construct semantic representatives for guiding adversarial learning in deep hashing. Particularly, our DMFL with the strict theoretical guarantee is adaptively optimized in a discriminative learning manner, where both discriminative and semantic properties are jointly considered. Moreover, adversarial examples are fabricated by maximizing the Hamming distance between the hash codes of adversarial samples and mainstay features, the efficacy of which is validated in the adversarial attack trials. Further, we, for the first time, formulate the formalized adversarial training of deep hashing into a unified minimax optimization under the guidance of the generated mainstay codes. Extensive experiments on benchmark datasets show superb attack performance against the state-of-the-art algorithms, meanwhile, the proposed adversarial training can effectively eliminate adversarial perturbations for trustworthy deep hashing-based retrieval. Our code is available at https://github.com/xandery-geek/SAAT.","1556-6021","","10.1109/TIFS.2023.3297791","Shenzhen Science and Technology Program(grant numbers:RCYX20221008092852077); National Natural Science Foundation of China(grant numbers:62002085); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515010057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189878","Adversarial attack;adversarial training;trustworthy deep hashing;similarity retrieval","Codes;Training;Semantics;Adversarial machine learning;Task analysis;Robustness;Perturbation methods","","2","","50","IEEE","21 Jul 2023","","","IEEE","IEEE Journals"
"Asymptotic Nash Equilibrium for the M-Ary Sequential Adversarial Hypothesis Testing Game","J. Pan; Y. Li; V. Y. F. Tan","Department of Electrical and Computer Engineering (ECE), National University of Singapore (NUS), Queenstown, Singapore; Department of Electrical and Computer Engineering (ECE) and the Center of Quantum Technologies (CQT), National University of Singapore, Queenstown, Singapore; Department of Mathematics and the Department of Electrical and Computer Engineering, National University of Singapore, Queenstown, Singapore","IEEE Transactions on Information Forensics and Security","30 Dec 2022","2023","18","","831","845","In this paper, we consider a novel  $M$ -ary sequential hypothesis testing problem in which an adversary is present and perturbs the distributions of the samples before the decision maker observes them. This problem is formulated as a sequential adversarial hypothesis testing game played between the decision maker and the adversary. This game is a zero-sum and strategic one. We assume the adversary is active under all hypotheses and knows the underlying distribution of observed samples. We adopt this framework as it is the worst-case scenario from the perspective of the decision maker. The goal of the decision maker is to minimize the expectation of the stopping time to ensure that the test is as efficient as possible; the adversary’s goal is, instead, to maximize the stopping time. We derive a pair of strategies under which the asymptotic Nash equilibrium of the game is attained. We also consider the case in which the adversary is not aware of the underlying hypothesis and hence is constrained to apply the same strategy regardless of which hypothesis is in effect. Numerical results corroborate our theoretical findings.","1556-6021","","10.1109/TIFS.2022.3231095","Singapore National Research Foundation (NRF) Fellowship(grant numbers:A-0005077-01-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994736","Game theory;Nash equilibrium;M-ary sequential hypothesis testing;adversary","Testing;Games;Nash equilibrium;Training;Perturbation methods;Markov processes;Machine learning algorithms","","1","","32","IEEE","20 Dec 2022","","","IEEE","IEEE Journals"
"Dual-Adversarial Representation Disentanglement for Visible Infrared Person Re-Identification","Z. Wei; X. Yang; N. Wang; X. Gao","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Information Forensics and Security","5 Jan 2024","2024","19","","2186","2200","Heterogeneous pedestrian images are captured by visible and infrared cameras with different spectrums, which play an important role in night-time video surveillance. However, visible infrared person re-identification (VI-REID) is still a challenging problem due to the considerable cross-modality discrepancies. To extract modality-invariant features which are discriminative for the person identity, recent studies are inclined to regard modality-specific features as noise and discard them. Actually, the modality-specific characteristics containing background and color information are indispensable for learning modality-shared features. In this paper, we propose a novel Dual-Adversarial Representation Disentanglement (DARD) model to separate modality-specific features from tangled pedestrian representations and effectively learn the robust modality-invariant representations. Specifically, our method employs dual-adversarial learning, incorporating image-level channel exchange and feature-level magnitude change to introduce variations in modality-specific representations. This deliberate perturbation raises the learning difficulty for the model to learn modality-shared features. Simultaneously, to control the changing scope of modality-specific features, bi-constrained noise alleviation is introduced during adversarial learning, keeping the balance of feature generation and adversary. The proposed dual-adversarial learning methodology enhances the robustness against cross-modality visual discrepancy and strengthens the discriminative power of the learned modality-shared representations without introducing additional network parameters. This improvement further elevates the retrieval performance of VI-REID. Extensive experiments with insightful analysis on two cross-modality re-identification datasets verify the effectiveness and superiority of the proposed DARD method.","1556-6021","","10.1109/TIFS.2023.3344289","National Natural Science Foundation of China(grant numbers:62372348,62036007,U22A2096,U21A20514); Shaanxi Outstanding Youth Science Fund Project(grant numbers:2023-JC-JQ-53); Technology Innovation Leading Program of Shaanxi(grant numbers:2022QFY01-15); Fundamental Research Funds for the Central Universities(grant numbers:QTZX23042); Open Research Projects of Zhejiang Laboratory(grant numbers:2021KG0AB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10364832","Representation disentanglement;adversarial learning;visible infrared person re-identification","Feature extraction;Adversarial machine learning;Image color analysis;Cameras;Transformers;Task analysis;Pedestrians","","","","76","IEEE","18 Dec 2023","","","IEEE","IEEE Journals"
"A Credential Usage Study: Flow-Aware Leakage Detection in Open-Source Projects","R. Han; H. Gong; S. Ma; J. Li; C. Xu; E. Bertino; S. Nepal; Z. Ma; J. Ma","School of Cyber Engineering, Xidian University, Xi’an, China; Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; School of Engineering and Information System, University of New South Wales, Sydney, NSW, Australia; Zhiyuan College, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, Faculty of Engineering and IT, The University of Sydney, Darlington, NSW, Australia; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Commonwealth Scientific and Industrial Research, Sydney, NSW, Australia; School of Cyber Engineering, Xidian University, Xi’an, China; School of Cyber Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","21 Nov 2023","2024","19","","722","734","Authentication and cryptography are critical security functions and, thus, are very often included as part of code. These functions require using credentials, such as passwords, security tokens, and cryptographic keys. However, developers often incorrectly implement/use credentials in their code because of a lack of secure coding skills. This paper analyzes open-source projects concerning the correct use of security credentials. We developed a semantic-rich, language-independent analysis approach for analyzing many projects automatically. We implemented a detection tool, SEAGULL, to automatically check open-source projects based on string literal and code structure information. Instead of analyzing the entire project code, which might result in path explosion when constructing data and control dependencies, SEAGULL pinpoints all literal constants to identify credential candidates and then analyzes the code snippets correlated to these candidates. SEAGULL accurately identifies the leaked credentials by obtaining semantic and syntax information about the code. We applied SEAGULL to 377 open-source projects. SEAGULL successfully reported 19 real-world credential leakages out of those projects. Our analysis shows that some developers protected or erased the credentials in the current project versions, but previously used credentials can still be extracted from the project’s historical versions. Although the implementations of credential leakages seem to be fixed in the current projects, attackers could successfully log into accounts if developers keep using the same credentials as before. Additionally, we found that such credential leakages still affect some projects. By exploiting leaked credentials, attackers can log into particular accounts.","1556-6021","","10.1109/TIFS.2023.3326985","National Natural Science Foundation of China (Key Program)(grant numbers:62232013); Major Research Plan of the National Natural Science Foundation of China(grant numbers:92267204,92167203); Natural Science Basis Research Plan in Shaanxi Province of China(grant numbers:2022JM-338); Fundamental Research Funds for the Central Universities(grant numbers:XJSJ23185); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292698","Credential leakage;bug detection;static code analysis","Codes;Source coding;Passwords;Authentication;Semantics;Java;Machine learning","","","","56","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Cloak and Swagger: Understanding Data Sensitivity through the Lens of User Anonymity","S. T. Peddinti; A. Korolova; E. Bursztein; G. Sampemane","Polytechnic School of Engineering, New York University, Brooklyn, NY; Google, Amphitheatre Parkway, CA; Google, Amphitheatre Parkway, CA; Google, Amphitheatre Parkway, CA","2014 IEEE Symposium on Security and Privacy","20 Nov 2014","2014","","","493","508","Most of what we understand about data sensitivity is through user self-report (e.g., surveys), this paper is the first to use behavioral data to determine content sensitivity, via the clues that users give as to what information they consider private or sensitive through their use of privacy enhancing product features. We perform a large-scale analysis of user anonymity choices during their activity on Quora, a popular question-and-answer site. We identify categories of questions for which users are more likely to exercise anonymity and explore several machine learning approaches towards predicting whether a particular answer will be written anonymously. Our findings validate the viability of the proposed approach towards an automatic assessment of data sensitivity, show that data sensitivity is a nuanced measure that should be viewed on a continuum rather than as a binary concept, and advance the idea that machine learning over behavioral data can be effectively used in order to develop product features that can help keep users safe.","2375-1207","978-1-4799-4686-0","10.1109/SP.2014.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6956583","","Context;Sensitivity;Privacy;Data privacy;Facebook;Crawlers;Search engines","","14","5","64","IEEE","20 Nov 2014","","","IEEE","IEEE Conferences"
"Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy","W. Ruan; M. Xu; W. Fang; L. Wang; L. Wang; W. Han","Laboratory of Data Analytics and Security, Fudan University; Laboratory of Data Analytics and Security, Fudan University; Ant Group; Ant Group; Ant Group; Laboratory of Data Analytics and Security, Fudan University","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1926","1943","Secure multi-party computation-based machine learning, referred to as multi-party learning (MPL for short), has become an important technology to utilize data from multiple parties with privacy preservation. While MPL provides rigorous security guarantees for the computation process, the models trained by MPL are still vulnerable to attacks that solely depend on access to the models. Differential privacy could help to defend against such attacks. However, the accuracy loss brought by differential privacy and the huge communication overhead of secure multi-party computation protocols make it highly challenging to balance the 3-way trade-off between privacy, efficiency, and accuracy.In this paper, we are motivated to resolve the above issue by proposing a solution, referred to as PEA (Private, Efficient, Accurate), which consists of a secure differentially private stochastic gradient descent (DPSGD for short) protocol and two optimization methods. First, we propose a secure DPSGD protocol to enforce DPSGD, which is a popular differentially private machine learning algorithm, in secret sharing-based MPL frameworks. Second, to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, we propose two optimization methods for the training process of MPL: (1) the data-independent feature extraction method, which aims to simplify the trained model structure; (2) the local data-based global model initialization method, which aims to speed up the convergence of the model training. We implement PEA in two open-source MPL frameworks: TF-Encrypted and Queqiao. The experimental results on various datasets demonstrate the efficiency and effectiveness of PEA. E.g. when ϵ = 2, we can train a differentially private classification model with an accuracy of 88% for CIFAR-10 within 7 minutes under the LAN setting. This result significantly outperforms the one from CryptGPU, one state-of-the-art MPL framework: it costs more than 16 hours to train a non-private deep neural network model on CIFAR-10 with the same accuracy.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179422","Secure Multi-party Computation;Multi-Party Learning;Differential Privacy;Privacy Computing","Multiprotocol label switching;Training;Differential privacy;Privacy;Computational modeling;Optimization methods;Stochastic processes","","2","","82","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Stealthy Backdoors as Compression Artifacts","Y. Tian; F. Suya; F. Xu; D. Evans","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, University of Virginia, Charlottesville, VA, USA","IEEE Transactions on Information Forensics and Security","6 Apr 2022","2022","17","","1372","1387","Model compression is a widely-used approach for reducing the size of deep learning models without much accuracy loss, enabling resource-hungry models to be compressed for use on resource-constrained devices. In this paper, we study the risk that model compression could provide an opportunity for adversaries to inject stealthy backdoors. In a backdoor attack on a machine learning model, an adversary produces a model that performs well on normal inputs but outputs targeted misclassifications on inputs containing a small trigger pattern. We design stealthy backdoor attacks such that the full-sized model released by adversaries appears to be free from backdoors (even when tested using state-of-the-art techniques), but when the model is compressed it exhibits a highly effective backdoor. We show this can be done for two common model compression techniques—model pruning and model quantization—even in settings where the adversary has limited knowledge of how the particular compression will be done. Our findings demonstrate the importance of performing security tests on the models that will actually be deployed not in their precompressed version. Our implementation is available at https://github.com/yulongtzzz/Stealthy-Backdoors-as-Compression-Artifacts.","1556-6021","","10.1109/TIFS.2022.3160359","National Key Research and Development Program of China(grant numbers:2021YFB3100300); National Natural Science Foundation of China (NSFC)(grant numbers:61872180); National Science Foundation (NSF) SaTC Program (Center for Trustworthy Machine Learning)(grant numbers:1804603); Jiangsu “Shuang-Chuang” Program; Jiangsu “Six-Talent-Peaks” Program; Program B for Outstanding Ph.D. Candidate of Nanjing University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737144","Deep learning;neural network compression;backdoor attack","Computational modeling;Quantization (signal);Security;Numerical models;Deep learning;Data models;Training","","6","","61","IEEE","16 Mar 2022","","","IEEE","IEEE Journals"
"Analog Secret Sharing With Applications to Private Distributed Learning","M. Soleymani; H. Mahdavifar; A. S. Avestimehr","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA","IEEE Transactions on Information Forensics and Security","18 May 2022","2022","17","","1893","1904","We consider the critical problems of distributed computing and learning over data while keeping it private from the computational servers. The state-of-the-art approaches to this problem rely on quantizing the data into a finite field, so that the cryptographic approaches for secure multiparty computing can then be employed. These approaches, however, can result in substantial accuracy losses due to fixed-point representation of the data and computation overflows. To address these critical issues, we propose a novel algorithm to solve the privacy-preserving distributed computing problem when data is in the analog domain, e.g., the field of real/complex numbers. We characterize the privacy of the data from both information-theoretic and cryptographic perspectives, while establishing a connection between the two notions in the analog domain. More specifically, the well-known connection between the distinguishing security (DS) and the mutual information security (MIS) metrics is extended from the discrete domain to the analog domain. This is then utilized to bound the amount of information about the data leaked to the servers in our protocol, in terms of the DS metric, using well-known results on the capacity of single-input multiple-output (SIMO) channel with correlated noise. It is shown how the proposed framework can be adopted to do computation tasks when data is represented using floating-point numbers. We then show that this leads to a fundamental trade-off between the privacy level of data and accuracy of the result. By extending the setup to distributed learning, we show how to train a machine learning model using the proposed algorithm while keeping the data as well as the trained model private. Then numerical results are shown for experiments on several datasets. Furthermore, experimental advantages are shown comparing to fixed-point implementations over finite fields.","1556-6021","","10.1109/TIFS.2022.3173417","Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR001117C0053,FA8750-19-2-1005); Army Research Office (ARO)(grant numbers:W911NF1810400); NSF(grant numbers:CCF-1703575,CCF-1763673,CCF-1763348,CCF-1909771,CCF-1941633); NSF/Intel Partnership on Machine Learning for Wireless Networking Systems (MLWINS)(grant numbers:2002874); Office of Naval Research (ONR)(grant numbers:N00014-16-1-2189); Cisco; Intel/Avast/Borsetta via the PrivateAI institute; Cisco Research Gift and Contract; Qualcomm and Intel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773066","Analog secret sharing;privacy-preserving computing;distributed learning","Servers;Protocols;Symbols;Distributed databases;Cryptography;Data privacy;Measurement","","4","","52","IEEE","11 May 2022","","","IEEE","IEEE Journals"
"Is Private Learning Possible with Instance Encoding?","N. Carlini; S. Deng; S. Garg; S. Jha; S. Mahloujifar; M. Mahmoody; A. Thakurta; F. Tramèr",Google; Columbia University; UC Berkeley and NTT Research; University of Wisconsin; Princeton University; University of Virginia; Google; Stanford University,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","410","427","A private machine learning algorithm hides as much as possible about its training data while still preserving accuracy. In this work, we study whether a non-private learning algorithm can be made private by relying on an instance-encoding mechanism that modifies the training inputs before feeding them to a normal learner. We formalize both the notion of instance encoding and its privacy by providing two attack models. We first prove impossibility results for achieving a (stronger) model. Next, we demonstrate practical attacks in the second (weaker) attack model on InstaHide, a recent proposal by Huang, Song, Li and Arora [ICML’20] that aims to use instance encoding for privacy.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519450","","Training;Privacy;Machine learning algorithms;Computational modeling;Neural networks;Training data;Encoding","","11","","49","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"D-DAE: Defense-Penetrating Model Extraction Attacks","Y. Chen; R. Guan; X. Gong; J. Dong; M. Xue","School of Computer Science, Wuhan University, China; School of Mathematics and Statistics, Wuhan University, China; School of Computer Science, Wuhan University, China; School of Cyber Science and Engineering, Wuhan University, China; School of Computer Science, Wuhan University, China","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","382","399","Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179406","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179406","Model extraction attacks;disruption-based defenses;meta-learning","Training;Privacy;Closed box;Machine learning;Feature extraction;Image restoration;Security","","4","","46","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"ShadowNet: A Secure and Efficient On-device Model Inference System for Convolutional Neural Networks","Z. Sun; R. Sun; C. Liu; A. R. Chowdhury; L. Lu; S. Jha","Google; Florida International University; Northeastern University; University of California, San Diego; Northeastern University; University of Wisconsin-Madison","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1596","1612","With the increased usage of AI accelerators on mobile and edge devices, on-device machine learning (ML) is gaining popularity. Thousands of proprietary ML models are being deployed today on billions of untrusted devices. This raises serious security concerns about model privacy. However, protecting model privacy without losing access to the untrusted AI accelerators is a challenging problem. In this paper, we present a novel on-device model inference system, ShadowNet. ShadowNet protects the model privacy with Trusted Execution Environment (TEE) while securely outsourcing the heavy linear layers of the model to the untrusted hardware accelerators. ShadowNet achieves this by transforming the weights of the linear layers before outsourcing them and restoring the results inside the TEE. The non-linear layers are also kept secure inside the TEE. ShadowNet’s design ensures efficient transformation of the weights and the subsequent restoration of the results. We build a ShadowNet prototype based on TensorFlow Lite and evaluate it on five popular CNNs, namely, MobileNet, ResNet-44, MiniVGG, ResNet-404, and YOLOv4-tiny. Our evaluation shows that ShadowNet achieves strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179382","Secure;Model-Inference;CNN;On-device-ML","Privacy;Prototypes;AI accelerators;Machine learning;Outsourcing;Security;Convolutional neural networks","","4","","75","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Silph: A Framework for Scalable and Accurate Generation of Hybrid MPC Protocols","E. Chen; J. Zhu; A. Ozdemir; R. S. Wahby; F. Brown; W. Zheng",Carnegie Mellon University; Carnegie Mellon University; Stanford University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","848","863","Many applications in finance and healthcare need access to data from multiple organizations. While these organizations can benefit from computing on their joint datasets, they often cannot share data with each other due to regulatory constraints and business competition. One way mutually distrusting parties can collaborate without sharing their data in the clear is to use secure multiparty computation (MPC). However, MPC’s performance presents a serious obstacle for adoption as it is difficult for users who lack expertise in advanced cryptography to optimize. In this paper, we present Silph, a framework that can automatically compile a program written in a high-level language to an optimized, hybrid MPC protocol that mixes multiple MPC primitives securely and efficiently. Compared to prior works, our compilation speed is improved by up to 30000×. On various database analytics and machine learning workloads, the MPC protocols generated by Silph match or outperform prior work by up to 3.6×.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179397","","Protocols;Program processors;Databases;Information sharing;Finance;Organizations;Machine learning","","3","","75","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"SoK: Cryptographic Neural-Network Computation","L. K. L. Ng; S. S. M. Chow",Georgia Institute of Technology; The Chinese University of Hong Kong,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","497","514","We studied 53 privacy-preserving neural-network papers in 2016-2022 based on cryptography (without trusted processors or differential privacy), 16 of which only use homomorphic encryption, 19 use secure computation for inference, and 18 use non-colluding servers (among which 12 support training), solving a wide variety of research problems. We dissect their cryptographic techniques and ""love-hate relationships"" with machine learning alongside a genealogy highlighting noteworthy developments. We also re-evaluate the state of the art under WAN. We hope this can serve as a go-to guide connecting different experts in related fields.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179483","","Wide area networks;Training;Privacy;Differential privacy;Program processors;Machine learning;Cryptography","","2","","114","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective","X. Zhang; H. Ye; Z. Huang; X. Ye; Y. Cao; Y. Zhang; M. Yang","Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China; Johns Hopkins University, Baltimore, USA; Fudan University, Shanghai, China; Fudan University, Shanghai, China","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","934","950","Face Verification Systems (FVSes) are more and more deployed by real-world mobile applications (apps) to verify a human’s claimed identity. One popular type of FVSes is called cross-side FVS (XFVS), which splits the FVS functionality into two sides: one at a mobile phone to take pictures or videos and the other at a trusted server for verification. Prior works have studied the security of XFVSes from the machine learning perspective, i.e., whether the learning models used by XFVSes are robust to adversarial attacks. However, the security of other parts of XFVSes, especially the design and implementation of the verification procedure used by XFVSes, is not well understood.In this paper, we conduct the first measurement study on the security of real-world XFVSes used by popular mobile apps from a system perspective. More specifically, we design and implement a semi-automated system, called XFVSChecker, to detect XFVSes in mobile apps and then inspect their compliance with four security properties. Our evaluation reveals that most of existing XFVS apps, including those with billions of downloads, are vulnerable to at least one of four types of attacks. These attacks require only easily available attack prerequisites, such as one photo of the victim, to pose significant security risks, including complete account takeover, identity fraud and financial loss. Our findings result in 14 Chinese National Vulnerability Database (CNVD) IDs and one of them, particularly CNVD-2021-86899, is awarded the most valuable vulnerability in 2021 among all the reported vulnerabilities to CNVD.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179474","Research and Development; National Natural Science Foundation of China; Shanghai Rising-Star Program; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179474","Mobile-Security;Face-Verification;System-Perspective","Privacy;Machine learning;Mobile applications;Internet;Fraud;Security;Servers","","1","","87","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"FLUTE: Fast and Secure Lookup Table Evaluations","A. Brüggemann; R. Hundt; T. Schneider; A. Suresh; H. Yalame","Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","515","533","The concept of using Lookup Tables (LUTs) instead of Boolean circuits is well-known and been widely applied in a variety of applications, including FPGAs, image processing, and database management systems. In cryptography, using such LUTs instead of conventional gates like AND and XOR results in more compact circuits and has been shown to substantially improve online performance when evaluated with secure multi-party computation. Several recent works on secure floating-point computations and privacy-preserving machine learning inference rely heavily on existing LUT techniques. However, they suffer from either large overhead in the setup phase or subpar online performance.We propose FLUTE, a novel protocol for secure LUT evaluation with good setup and online performance. In a two-party setting, we show that FLUTE matches or even outperforms the online performance of all prior approaches, while being competitive in terms of overall performance with the best prior LUT protocols. In addition, we provide an open-source implementation of FLUTE written in the Rust programming language, and implementations of the Boolean secure two-party computation protocols of ABY2.0 and silent OT. We find that FLUTE outperforms the state of the art by two orders of magnitude in the online phase while retaining similar overall communication.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179345","Lookup-Tables;2PC;ABY2.0;Privacy;Boolean;LUTs;MPC","Privacy;Computer languages;Protocols;Machine learning;Logic gates;Multi-party computation;Database systems","","1","","89","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Human Identity and Gender Recognition From Gait Sequences With Arbitrary Walking Directions","J. Lu; G. Wang; P. Moulin","Advanced Digital Sciences Center, Singapore; Nanyang Technological University, School of Electrical and Electronic Engineering, Singapore; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA","IEEE Transactions on Information Forensics and Security","16 Dec 2013","2014","9","1","51","61","We investigate the problem of human identity and gender recognition from gait sequences with arbitrary walking directions. Most current approaches make the unrealistic assumption that persons walk along a fixed direction or a pre-defined path. Given a gait sequence collected from arbitrary walking directions, we first obtain human silhouettes by background subtraction and cluster them into several clusters. For each cluster, we compute the cluster-based averaged gait image as features. Then, we propose a sparse reconstruction based metric learning method to learn a distance metric to minimize the intra-class sparse reconstruction errors and maximize the inter-class sparse reconstruction errors simultaneously, so that discriminative information can be exploited for recognition. The experimental results show the efficacy of our approach.","1556-6021","","10.1109/TIFS.2013.2291969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671367","Human gait analysis;identity recognition;gender recognition;metric learning;sparse reconstruction","Measurement;Legged locomotion;Image reconstruction;Gait recognition;Feature extraction;Databases;Training","","157","","54","IEEE","20 Nov 2013","","","IEEE","IEEE Journals"
"COTD: Reference-Free Hardware Trojan Detection and Recovery Based on Controllability and Observability in Gate-Level Netlist","H. Salmani","EECS Department, Howard University, Washington, DC, USA","IEEE Transactions on Information Forensics and Security","21 Nov 2016","2017","12","2","338","350","This paper presents a novel hardware Trojan detection technique in gate-level netlist based on the controllability and observability analyses. Using an unsupervised clustering analysis, the paper shows that the controllability and observability characteristics of Trojan gates present significant inter-cluster distance from those of genuine gates in a Trojan-inserted circuit, such that Trojan gates are easily distinguishable. The proposed technique does not require any golden model and can be easily integrated into the current integrated circuit design flow. Furthermore, it performs a static analysis and does not require any test pattern application for Trojan activation either partially or fully. In addition, the timing complexity of the proposed technique is an order of the number of signals in a circuit. Moreover, the proposed technique makes it possible to fully restore an inserted Trojan and to isolate its trigger and payload circuits. The technique has been applied on various types of Trojans, and all Trojans are successfully detected with 0 false positive and negative rates in less than 14 s in the worst case.","1556-6021","","10.1109/TIFS.2016.2613842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577739","Hardware security;hardware Trojans;controllability;observability;unsupervised clustering","Trojan horses;Hardware;Logic gates;Observability;Controllability;Payloads;Wires","","146","","44","IEEE","27 Sep 2016","","","IEEE","IEEE Journals"
"A Method of Few-Shot Network Intrusion Detection Based on Meta-Learning Framework","C. Xu; J. Shen; X. Du","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","3 Jul 2020","2020","15","","3540","3552","Conventional intrusion detection systems based on supervised learning techniques require a large number of samples for training, while in some scenarios, such as zero-day attacks, security agencies can only intercept a limited number of shots of malicious samples. Therefore, there is a need for few-shot detection. In this paper, a detection method based on a meta-learning framework is proposed for this purpose. The proposed method can be used to distinguish and compare a pair of network traffic samples as a basic task of learning, including a normal unaffected sample and a malicious one. To accomplish this task, we design a deep neural network (DNN) named FC-Net, which mainly comprises two parts: feature extraction network and comparison network. FC-Net learns a pair of feature maps for classification from a pair of network traffic samples, then compares the obtained feature maps, and finally determines whether the pair of samples belongs to the same type. To evaluate the proposed detection method, we construct two datasets for few-shot network intrusion detection based on real network traffic data sources, using a specifically developed approach. The experimental results indicate that the proposed detection method is universal and is not limited to specific datasets or attack types. Training and testing on the same datasets demonstrate that the proposed method can achieve the average detection rate up to 98.88%. The outcome of training on one dataset and testing on the other one confirms that the proposed method can achieve better performance. In a few-shot scenario, malicious samples in an untrained dataset can be detected successfully, and the average detection rate is up to 99.62%.","1556-6021","","10.1109/TIFS.2020.2991876","National Natural Science Foundation of China(grant numbers:61471314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9083983","Network security;intrusion detection system;meta-learning;few-shot learning;deep learning","Task analysis;Feature extraction;Network intrusion detection;Training;Knowledge engineering","","120","","42","IEEE","1 May 2020","","","IEEE","IEEE Journals"
"NetSpam: A Network-Based Spam Detection Framework for Reviews in Online Social Media","S. Shehnepoor; M. Salehi; R. Farahbakhsh; N. Crespi","University of Tehran, Tehran, Iran; University of Tehran, Tehran, Iran; Institut Mines-Telecom, Telecom Sud-Paris, Paris, France; Institut Mines-Telecom, Telecom Sud-Paris, Paris, France","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","7","1585","1595","Nowadays, a big part of people rely on available content in social media in their decisions (e.g., reviews and feedback on a topic or product). The possibility that anybody can leave a review provides a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research, and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this paper, we propose a novel framework, named NetSpam, which utilizes spam features for modeling review data sets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features helps us to obtain better results in terms of different metrics experimented on real-world review data sets from Yelp and Amazon Web sites. The results show that NetSpam outperforms the existing methods and among four categories of features, including review-behavioral, user-behavioral, review-linguistic, and user-linguistic, the first type of features performs better than the other categories.","1556-6021","","10.1109/TIFS.2017.2675361","Iran National Science Foundation(grant numbers:94017889); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865975","Social media;social network;spammer;spam review;fake review;heterogeneous information networks","Feature extraction;Business;Social network services;Time complexity;Pragmatics;Metadata;Telecommunications","","108","","37","IEEE","1 Mar 2017","","","IEEE","IEEE Journals"
"Gabor Ordinal Measures for Face Recognition","Z. Chai; Z. Sun; H. Méndez-Vázquez; R. He; T. Tan","Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Advanced Technologies Application Center, Havana, Cuba; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","IEEE Transactions on Information Forensics and Security","16 Dec 2013","2014","9","1","14","26","Great progress has been achieved in face recognition in the last three decades. However, it is still challenging to characterize the identity related features in face images. This paper proposes a novel facial feature extraction method named Gabor ordinal measures (GOM), which integrates the distinctiveness of Gabor features and the robustness of ordinal measures as a promising solution to jointly handle inter-person similarity and intra-person variations in face images. In the proposal, different kinds of ordinal measures are derived from magnitude, phase, real, and imaginary components of Gabor images, respectively, and then are jointly encoded as visual primitives in local regions. The statistical distributions of these visual primitives in face image blocks are concatenated into a feature vector and linear discriminant analysis is further used to obtain a compact and discriminative feature representation. Finally, a two-stage cascade learning method and a greedy block selection method are used to train a strong classifier for face recognition. Extensive experiments on publicly available face image databases, such as FERET, AR, and large scale FRGC v2.0, demonstrate state-of-the-art face recognition performance of GOM.","1556-6021","","10.1109/TIFS.2013.2290064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657757","Biometrics;face recognition;feature extraction;ordinal measures;Gabor filters","Face;Face recognition;Feature extraction;Robustness;Visualization;Noise","","99","","52","IEEE","7 Nov 2013","","","IEEE","IEEE Journals"
"Joint Feature Learning for Face Recognition","J. Lu; V. E. Liong; G. Wang; P. Moulin","Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","7","1371","1383","This paper presents a new joint feature learning (JFL) approach to automatically learn feature representation from raw pixels for face recognition. Unlike many existing face recognition systems, where conventional feature descriptors, such as local binary patterns and Gabor features, are used for face representation, we propose an unsupervised feature learning method to learn hierarchical feature representation. Since different face regions have different physical characteristics, we propose to use different feature dictionaries to represent them, and to learn multiple yet related feature projection matrices for these regions simultaneously. Hence position-specific discriminative information can be exploited for face representation. Having learned these feature projections for different face regions, we perform spatial pooling for face patches within each region to enhance the representative power of the learned features. Moreover, we stack our JFL model into a deep architecture to exploit hierarchical information for feature representation and further improve the recognition performance. Experimental results on five widely used face data sets show the effectiveness of our proposed approach.","1556-6021","","10.1109/TIFS.2015.2408431","Human Cyber Security Systems Program within the Advanced Digital Sciences Center, Singapore, through the Agency for Science, Technology and Research, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053922","Face recognition;feature learning;joint learning;deep learning.;Face recognition;feature learning;joint learning;deep learning","Feature extraction;Face recognition;Learning systems;Deep learning;Dictionaries","","92","","70","IEEE","3 Mar 2015","","","IEEE","IEEE Journals"
"Efficient Intrusion Detection With Bloom Filtering in Controller Area Networks","B. Groza; P. -S. Murvay","Faculty of Automatics and Computers, Politehnica University of Timişoara, Timişoara, Romania; Faculty of Automatics and Computers, Politehnica University of Timişoara, Timişoara, Romania","IEEE Transactions on Information Forensics and Security","7 Nov 2018","2019","14","4","1037","1051","Due to its cost efficiency, the controller area network (CAN) is still the most wide-spread in-vehicle bus, and the numerous reported attacks demonstrate the urgency in designing new security solutions for CAN. In this paper, we propose an intrusion detection mechanism that takes advantage of Bloom filtering to test frame periodicity based on message identifiers and parts of the data-field which facilitates detection of potential replay or modification attacks. This proves to be an effective approach since most of the traffic from in-vehicle buses is cyclic in nature and the format of the data-field is fixed due to rigid signal allocation. Bloom filters provide an efficient time-memory tradeoff which is beneficial for the constrained resources of automotive grade controllers. We test the correctness of our approach and obtain good results on an industry-standard CANoe-based simulation for a J1939 commercial-vehicle bus and also on CAN with flexible data-rate traces obtained from a real-world high-end vehicle. The proposed filtering mechanism is straightforward to adapt for any other time-triggered in-vehicle bus, e.g., FlexRay, since it is built on time-driven characteristics.","1556-6021","","10.1109/TIFS.2018.2869351","Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii(grant numbers:PN-II-RU-TE-2014-4-1501 (2015–2017)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457262","Intrusion detection;microcontrollers;network security","Intrusion detection;Proposals;Entropy;Standards;Resource management;Automotive engineering","","80","","46","IEEE","11 Sep 2018","","","IEEE","IEEE Journals"
"Multiple Account Identity Deception Detection in Social Media Using Nonverbal Behavior","M. Tsikerdekis; S. Zeadally","College of Communication and Information, University of Kentucky, Lexington, KY, USA; College of Communication and Information, University of Kentucky, Lexington, KY, USA","IEEE Transactions on Information Forensics and Security","11 Jul 2014","2014","9","8","1311","1321","Identity deception has become an increasingly important issue in the social media environment. The case of blocked users initiating new accounts, often called sockpuppetry, is widely known and past efforts, which have attempted to detect such users, have been primarily based on verbal behavior (e.g., using profile data or lexical features in text). Although these methods yield a high detection accuracy rate, they are computationally inefficient for the social media environment, which often involves databases with large volumes of data. To date, little attention has been paid to detecting online deception using nonverbal behavior. We present a detection method based on nonverbal behavior for identity deception, which can be applied to many types of social media. Using Wikipedia as an experimental case, we demonstrate that our proposed method results in high detection accuracy over previous methods proposed while being computationally efficient for the social media environment. We also demonstrate the potential of nonverbal behavior data that exists in social media and how designers and developers can leverage such nonverbal information in detecting deception to safeguard their online communities.","1556-6021","","10.1109/TIFS.2014.2332820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6843931","Algorithm;deception;identity;performance;social media","Encyclopedias;Internet;Electronic publishing;Media;Databases;Accuracy","","79","","36","IEEE","25 Jun 2014","","","IEEE","IEEE Journals"
"Deep Face Representations for Differential Morphing Attack Detection","U. Scherhag; C. Rathgeb; J. Merkle; C. Busch","da/sec—Biometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; da/sec—Biometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; Security Networks AG, Essen, Germany; da/sec—Biometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany","IEEE Transactions on Information Forensics and Security","3 Jul 2020","2020","15","","3625","3639","The vulnerability of facial recognition systems to face morphing attacks is well known. Many different approaches for morphing attack detection (MAD) have been proposed in the scientific literature. However, the MAD algorithms proposed so far have mostly been trained and tested on datasets whose distributions of image characteristics are either very limited (e.g., only created with a single morphing tool) or rather unrealistic (e.g., no print-scan transformation). As a consequence, these methods easily overfit on certain image types and the results presented cannot be expected to apply to real-world scenarios. For example, the results of the latest NIST FRVT MORPH show that the majority of submitted MAD algorithms lacks robustness and performance when considering unseen and challenging datasets. In this work, subsets of the FERET and FRGCv2 face databases are used to create a realistic database for training and testing of MAD algorithms, containing a large number of ICAO-compliant bona fide facial images, corresponding unconstrained probe images, and morphed images created with four different face morphing tools. Furthermore, multiple post-processings are applied on the reference images, e.g., print-scan and JPEG2000 compression. On this database, previously proposed differential morphing algorithms are evaluated and compared. In addition, the application of deep face representations for differential MAD algorithms is investigated. It is shown that algorithms based on deep face representations can achieve very high detection performance (less than 3% D-EER) and robustness with respect to various post-processings. Finally, the limitations of the developed methods are analyzed.","1556-6021","","10.1109/TIFS.2020.2994750","German Federal Ministry of Education and Research; Hessen State Ministry for Higher Education, Research and the Arts within their joint support of the ATHENE (National Research Center for Applied Cybersecurity); Federal Office of Information Security (BSI) through the FACETRUST Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093905","Biometrics;face recognition;morphing attacks;morphing attack detection;differential attack detection;deep face representation","Face;Databases;Probes;Face recognition;Feature extraction;Neural networks;Forensics","","75","","56","CCBY","14 May 2020","","","IEEE","IEEE Journals"
"A Performance-Sensitive Malware Detection System Using Deep Learning on Mobile Devices","R. Feng; S. Chen; X. Xie; G. Meng; S. -W. Lin; Y. Liu","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","11 Dec 2020","2021","16","","1563","1578","Currently, Android malware detection is mostly performed on server side against the increasing number of malware. Powerful computing resource provides more exhaustive protection for app markets than maintaining detection by a single user. However, apart from the applications (apps) provided by the official market (i.e., Google Play Store), apps from unofficial markets and third-party resources are always causing serious security threats to end-users. Meanwhile, it is a time-consuming task if the app is downloaded first and then uploaded to the server side for detection, because the network transmission has a lot of overhead. In addition, the uploading process also suffers from the security threats of attackers. Consequently, a last line of defense on mobile devices is necessary and much-needed. In this paper, we propose an effective Android malware detection system, MobiTive, leveraging customized deep neural networks to provide a real-time and responsive detection environment on mobile devices. MobiTive is a pre-installed solution rather than an app scanning and monitoring engine using after installation, which is more practical and secure. Although a deep learning-based approach can be maintained on server side efficiently for malware detection, original deep learning models cannot be directly deployed and executed on mobile devices due to various performance limitations, such as computation power, memory size, and energy. Therefore, we evaluate and investigate the following key points: (1) the performance of different feature extraction methods based on source code or binary code; (2) the performance of different feature type selections for deep learning on mobile devices; (3) the detection accuracy of different deep neural networks on mobile devices; (4) the real-time detection performance and accuracy on different mobile devices; (5) the potential based on the evolution trend of mobile devices' specifications; and finally we further propose a practical solution (MobiTive) to detect Android malware on mobile devices.","1556-6021","","10.1109/TIFS.2020.3025436","Singapore Ministry of Education Academic Research Fund Tier 1(grant numbers:2018-T1-002-069); National Research Foundation, Prime Ministers Office, Singapore through its National Cybersecurity Research and Development Program(grant numbers:RF2018 NCR-NCR005-0001); Singapore National Research Foundation through NCR(grant numbers:NSOE003-0001); NRF Investigatorship(grant numbers:NRFI06-2020-0022); National Research Foundation, Prime Ministers Office, Singapore through NCR(grant numbers:NRF2018NCR-NSOE004-0001); National Natural Science Foundation of China(grant numbers:61902395); NVIDIA AI Tech Center (NVAITC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204665","Android malware;malware detection;deep neural network;mobile platform;performance","Malware;Androids;Humanoid robots;Feature extraction;Mobile handsets;Performance evaluation;Security","","65","","87","IEEE","23 Sep 2020","","","IEEE","IEEE Journals"
"Radio Frequency Fingerprint Identification for Narrowband Systems, Modelling and Classification","J. Zhang; R. Woods; M. Sandell; M. Valkama; A. Marshall; J. Cavallaro","Department of Electrical Engineering and Electronics, University of Liverpool, Liverpool, U.K.; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Bristol Research and Innovation Laboratory, Toshiba Research Europe Ltd., Bristol, U.K.; Department of Electrical Engineering, Tampere University, Tampere, Finland; Department of Electrical Engineering and Electronics, University of Liverpool, Liverpool, U.K.; Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA","IEEE Transactions on Information Forensics and Security","17 Aug 2021","2021","16","","3974","3987","Device authentication is essential for securing Internet of things. Radio frequency fingerprint identification (RFFI) is an emerging technique that exploits intrinsic and unique hardware impairments as the device identifier. The existing RFFI literature focuses on experimental exploration but comprehensive modelling is missing. This paper systematically models impairments of transmitter and receiver in narrowband systems and carries out extensive experiments and simulations to evaluate their effects on RFFI. The modelled impairments include oscillator imperfections, imbalance of inphase (I) and quadrature (Q) branches of mixers and power amplifier (PA) nonlinearity. We then propose a convolutional neural network-based RFFI protocol. We carry out experimental measurements over three months and demonstrate that oscillator imperfections are not suitable for RFFI due to their unpredictable time variation caused by temperature change. Our simulation results show that our protocol can classify 50 and 200 devices with uniformly and randomly distributed IQ imbalances and PA nonlinearities with high accuracy, namely 99% and 89%, respectively. We also show that the RFFI has some tolerance on different receiver imbalances during training and classification. Specifically, the accuracy is shown to degrade less than 20% when the residual receiver's gain and phase imbalances are small. Based on the experimental and simulation results, we made recommendations for designing a robust RFFI protocol, namely compensate carrier frequency offset and calibrate IQ imbalances of receivers.","1556-6021","","10.1109/TIFS.2021.3088008","U.K. Royal Society Research Grants(grant numbers:ID RGS/R1/191241); National Key Research and Development Program of China(grant numbers:2020YFE0200600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9450821","Device authentication;radio frequency fingerprint identification;RF impairment;narrowband;convolutional neural network","Receivers;Training;Protocols;Radio frequency;Narrowband;Radio transmitters;Internet of Things","","65","","53","Crown","10 Jun 2021","","","IEEE","IEEE Journals"
"Statistical Application Fingerprinting for DDoS Attack Mitigation","M. E. Ahmed; S. Ullah; H. Kim","Data61, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Department of Computer Science and Engineering, Kyung Hee University, Yongin, South Korea; Department of Software, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Information Forensics and Security","11 Feb 2019","2019","14","6","1471","1484","Because of the dynamic nature of network traffic patterns, such as new traffic application arrivals or flash events, it is becoming increasingly difficult for conventional anomaly detection systems to separate various applications based on their traffic patterns. In this paper, by leveraging transport layer packet-level and flow-level features, new structures called application fingerprints are generated, which express such features in a compact and efficient manner. Based on the generated fingerprints, we propose a novel traffic classification framework. The proposed system generates profiles of normal applications using a multi-modal probability distribution. The proposed classification framework is then extended to detect distributed denial of service attacks from the collected statistical information at flow level. To demonstrate the feasibility of the proposed system, we evaluate its performance using five real-world traffic datasets. The experiment results show that the proposed method is capable of achieving an accuracy of over 97%, whereas the misclassification rate is only 2.5%.","1556-6021","","10.1109/TIFS.2018.2879616","Iran Telecommunication Research Center(grant numbers:IITP-2018-2015-0-00403); Ministry of Science and ICT, South Korea(grant numbers:2016-0-00078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8522048","Traffic classification;anomaly detection;fingerprinting traffic applications;nonparametric clustering;DDoS attack detection","Computer crime;Feature extraction;Anomaly detection;Network security;Pattern classification;Distributed denial-of-service attack;Statistical analysis","","55","","41","IEEE","4 Nov 2018","","","IEEE","IEEE Journals"
"Self-Configurable Cyber-Physical Intrusion Detection for Smart Homes Using Reinforcement Learning","R. Heartfield; G. Loukas; A. Bezemskij; E. Panaousis","School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.; School of Computing and Mathematical Sciences, University of Greenwich, London, U.K.","IEEE Transactions on Information Forensics and Security","28 Dec 2020","2021","16","","1720","1735","The modern Internet of Things (IoT)-based smart home is a challenging environment to secure: devices change, new vulnerabilities are discovered and often remain unpatched, and different users interact with their devices differently and have different cyber risk attitudes. A security breach's impact is not limited to cyberspace, as it can also affect or be facilitated in physical space, for example, via voice. In this environment, intrusion detection cannot rely solely on static models that remain the same over time and are the same for all users. We present MAGPIE, the first smart home intrusion detection system that is able to autonomously adjust the decision function of its underlying anomaly classification models to a smart home's changing conditions (e.g., new devices, new automation rules and user interaction with them). The method achieves this goal by applying a novel probabilistic cluster-based reward mechanism to non-stationary multi-armed bandit reinforcement learning. MAGPIE rewards the sets of hyperparameters of its underlying isolation forest unsupervised anomaly classifiers based on the cluster silhouette scores of their output. Experimental evaluation in a real household shows that MAGPIE exhibits high accuracy because of two further innovations: it takes into account both cyber and physical sources of data; and it detects human presence to utilise models that exhibit the highest accuracy in each case. MAGPIE is available in open-source format, together with its evaluation datasets, so it can benefit from future advances in unsupervised and reinforcement learning and be able to be enriched with further sources of data as smart home environments and attacks evolve.","1556-6021","","10.1109/TIFS.2020.3042049","European Coordinated Research on Long-term Challenges in Information and Communication Sciences and Technologies ERA-NET (CHIST-ERA), under Project COCOON; EPSRC(grant numbers:EP/P016448/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277640","Intrusion detection system;cyber-physical attacks;smart home;reinforcement learning","Smart homes;IP networks;Intrusion detection;Hidden Markov models;Reinforcement learning;Wireless fidelity;Monitoring","","52","","45","IEEE","2 Dec 2020","","","IEEE","IEEE Journals"
"Domain Adaptive Person Re-Identification via Camera Style Generation and Label Propagation","C. -X. Ren; B. Liang; P. Ge; Y. Zhai; Z. Lei","Intelligent Data Center, School of Mathematics, Sun Yat-sen University, Guangzhou, China; Intelligent Data Center, School of Mathematics, Sun Yat-sen University, Guangzhou, China; Intelligent Data Center, School of Mathematics, Sun Yat-sen University, Guangzhou, China; Intelligent Data Center, School of Mathematics, Sun Yat-sen University, Guangzhou, China; National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","11 Dec 2019","2020","15","","1290","1302","Unsupervised domain adaptation in person re-identification resorts to labeled source data to promote the model training on target domain, facing the dilemmas caused by large domain shift and large camera variations. The non-overlapping labels challenge that the source domain and the target domain have entirely different persons further increases the re-identification difficulty. In this paper, we propose a novel algorithm to narrow such domain gaps. We derive a camera style adaptation framework to learn the style-based mappings between different camera views, from the target domain to the source domain, and then we can transfer the identity-based distribution from the source domain to the target domain on the camera level. Target camera variations can be captured by the style adaptation method, thus, the re-identification model trained on the target domain can learn target camera-invariant features better. It indicates that the style translator approximates an appropriate metric space for improving feature matching. To overcome the non-overlapping labels challenge and guide the person re-identification model to narrow the gap further, an efficient and effective soft-labeling method is proposed to mine the intrinsic local structure of the target domain through building the connection between GAN-translated source domain and the target domain. Experiment results conducted on real benchmark datasets indicate that our method gets state-of-the-art results.","1556-6021","","10.1109/TIFS.2019.2939750","National Natural Science Foundation of China(grant numbers:61976229,61906046,61572536,11631015,U1611265); Science and Technology Program of Guangzhou(grant numbers:201804010248); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825844","Person re-identification;domain adaptation;distribution discrepancy;camera style transfer;soft-labeling","Cameras;Adaptation models;Generators;Training;Generative adversarial networks;Task analysis;Labeling","","39","","44","IEEE","5 Sep 2019","","","IEEE","IEEE Journals"
"We Can Track You if You Take the Metro: Tracking Metro Riders Using Accelerometers on Smartphones","J. Hua; Z. Shen; S. Zhong","State Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","2","286","297","Motion sensors, especially accelerometers, on smartphones have been discovered to be a powerful side channel for spying on users' privacy. In this paper, we reveal a new accelerometer-based side-channel attack which is particularly serious: malware on smartphones can easily exploit the accelerometers to trace metro riders stealthily. We first address the challenge to automatically filter out metro-related data from a mass of miscellaneous accelerometer readings, and then propose a basic attack which leverages an ensemble interval classifier built from supervised learning to infer the riding trajectory of the user. As the supervised learning requires the attacker to collect labeled training data for each station interval, this attack confronts the scalability problem in big cities with a huge metro network. We thus further present an improved attack using semi-supervised learning, which only requires the attacker to collect labeled data for a very small number of distinctive station intervals. We conduct real experiments on a large self-built dataset, which contains more than 120 h of data collected from six metro lines of three major cities. The results show that the inferring accuracy could reach 89% and 94% if the user takes the metro for four and six stations, respectively. We finally discuss possible countermeasures against the proposed attack.","1556-6021","","10.1109/TIFS.2016.2611489","National Natural Science Foundation of China(grant numbers:NSFC-61321491,NSFC-61425024,NSFC-61300235,NSFC-61402223); Jiangsu Province Double Innovation Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572080","Accelerometers;location privacy;metro;dataset","Accelerometers;Smart phones;Sensors;Malware;Supervised learning;Urban areas;Feature extraction","","32","","25","IEEE","20 Sep 2016","","","IEEE","IEEE Journals"
"Stealthy MTD Against Unsupervised Learning-Based Blind FDI Attacks in Power Systems","M. Higgins; F. Teng; T. Parisini","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Information Forensics and Security","11 Nov 2020","2021","16","","1275","1287","This paper examines how moving target defenses (MTD) implemented in power systems can be countered by unsupervised learning-based false data injection (FDI) attack and how MTD can be combined with physical watermarking to enhance the system resilience. A novel intelligent attack, which incorporates dimensionality reduction and density-based spatial clustering, is developed and shown to be effective in maintaining stealth in the presence of traditional MTD strategies. In resisting this new type of attack, a novel implementation of MTD combining with physical watermarking is proposed by adding Gaussian watermark into physical plant parameters to drive detection of traditional and intelligent FDI attacks, while remaining hidden to the attackers and limiting the impact on system operation and stability.","1556-6021","","10.1109/TIFS.2020.3027148","ESRC(grant numbers:ES/T000112/1); EPSRC Centre for Doctoral Training in Future Power Networks and Smart Grids(grant numbers:EP/L015471/1); European Union’s Horizon 2020 research and innovation programme(grant numbers:739551 (KIOS CoE)); Italian Ministry for Research in the framework of the 2017 Program for Research Projects of National Interest (PRIN)(grant numbers:2017YKXYXJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207760","Cybersecurity;false data injection attacks;power systems state estimation;moving target defense;physical watermarking","Topology;State estimation;Power measurement;Watermarking;Meters;Power system stability","","28","","38","CCBY","28 Sep 2020","","","IEEE","IEEE Journals"
"Wiretap Code Design by Neural Network Autoencoders","K. -L. Besser; P. -H. Lin; C. R. Janda; E. A. Jorswieck","Communications Lab, Technische Universität Dresden, Dresden, Germany; Communications Lab, Technische Universität Dresden, Dresden, Germany; Communications Lab, Technische Universität Dresden, Dresden, Germany; Communications Lab, Technische Universität Dresden, Dresden, Germany","IEEE Transactions on Information Forensics and Security","11 May 2020","2020","15","","3374","3386","In industrial machine type communications, an increasing number of wireless devices communicate under reliability, latency, and confidentiality constraints, simultaneously. From information theory, it is known that wiretap codes can asymptotically achieve reliability (vanishing block error rate (BLER) at the legitimate receiver Bob) while also achieving secrecy (vanishing information leakage (IL) to an eavesdropper Eve). However, under finite block length, there exists a tradeoff between the BLER at Bob and the IL at Eve. In this work, we propose a flexible wiretap code design for degraded Gaussian wiretap channels under finite block length, which can change the operating point on the Pareto boundary of the tradeoff between BLER and IL given specific code parameters. To attain this goal, we formulate a multi-objective programming problem, which takes the BLER at Bob and the IL at Eve into account. During training, we approximate the BLER by the mean square error and the IL by schemes based on Jensen's inequality and the Taylor expansion and then solve the optimization problem by neural network autoencoders. Simulation results show that the proposed scheme can find codes outperforming polar wiretap codes (PWC) with respect to both BLER and IL simultaneously. We show that the codes found by the autoencoders could be implemented with real modulation schemes with only small losses in performance.","1556-6021","","10.1109/TIFS.2019.2945619","Deutsche Forschungsgemeinschaft (DFG)(grant numbers:JO801/23-1,LI 2886/2-1); Bundesministerium für Bildung und Forschung (BMBF) FastCloud and FastSecure(grant numbers:03ZZ0517A,03ZZ0522A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859269","Physical layer security;wiretap codes;deep learning;autoencoders","Reliability;Computational modeling;Decoding;Artificial neural networks;Mutual information;Neurons","","28","","51","IEEE","4 Oct 2019","","","IEEE","IEEE Journals"
"Metrics Towards Measuring Cyber Agility","J. D. Mireles; E. Ficke; J. -H. Cho; P. Hurley; S. Xu","Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; U.S. Air Force Research Laboratory, Rome, NY, USA; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Information Forensics and Security","31 Jul 2019","2019","14","12","3217","3232","In cyberspace, evolutionary strategies are commonly used by both attackers and defenders. For example, an attacker's strategy often changes over the course of time, as new vulnerabilities are discovered and/or mitigated. Similarly, a defender's strategy changes over time. These changes may or may not be in direct response to a change in the opponent's strategy. In any case, it is important to have a set of quantitative metrics to characterize and understand the effectiveness of attackers' and defenders' evolutionary strategies, which reflect their cyber agility. Despite its clear importance, few systematic metrics have been developed to quantify the cyber agility of attackers and defenders. In this paper, we propose the first metric framework for measuring cyber agility in terms of the effectiveness of the dynamic evolution of cyber attacks and defenses. The proposed framework is generic and applicable to transform any relevant, quantitative, and/or conventional static security metrics (e.g., false positives and false negatives) into dynamic metrics to capture dynamics of system behaviors. In order to validate the usefulness of the proposed framework, we conduct case studies on measuring the evolution of cyber attacks and defenses using two real-world datasets. We discuss the limitations of the current work and identify future research directions.","1556-6021","","10.1109/TIFS.2019.2912551","U.S. Department of Defense; Army Research Office(grant numbers:W911NF-17-1-0566); Army Research Laboratory(grant numbers:W911NF-17-2-0127); National Science Foundation(grant numbers:1814825); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695107","Security metrics;agility metrics;cyber agility;cyber maneuverability;measurements;attack;defense","Time measurement;Cyberattack;Systematics;Current measurement","","26","","54","IEEE","22 Apr 2019","","","IEEE","IEEE Journals"
"Pilot Contamination Attack Detection for 5G MmWave Grant-Free IoT Networks","N. Wang; W. Li; A. Alipour-Fanid; L. Jiao; M. Dabaghchian; K. Zeng","Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA; Department of Computer Science, Morgan State University, Baltimore, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, USA","IEEE Transactions on Information Forensics and Security","14 Sep 2020","2021","16","","658","670","Grant-free random access is an emerging technology for providing massive connectivity for 5G massive machine-type communications (mMTC), where non-orthogonal pilot sequences are used to simultaneously detect active users and estimate channels. However, grant-free 5G IoT networks are vulnerable to pilot contamination attacks (PCA), where the attacker can send the same pilots as legitimate IoT users to harm the active user detection and channel estimation. To defend against this attack, in this article, we propose a physical-layer countermeasure based on the channel virtual representation (CVR). CVR can emphasize the unique characteristics of mmWave channels that are sensitive to the location of the sender. This can be utilized to counter PCA no matter if the attacker's pilots are superimposed to that of the victim or not. Based on this observation, to achieve an efficient PCA detection, a single-hidden-layer multiple measurement (SHMM) Siamese network is employed. This solution tackles the challenges of channel randomness and massive connectivity in mMTC IoT networks, and supports small sample learning. Simulation results evaluate and confirm the effectiveness of the proposed detection scheme under various scenarios. The detection accuracy can approach 99% with 128 antennas at the receiver and reach above 95% even with only 50 training samples.","1556-6021","","10.1109/TIFS.2020.3017932","Commonwealth Cyber Initiative (CCI) and its Northern Virginia (NOVA) Node, an investment in the advancement of cyber R&D, innovation and workforce development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171328","Pilot contamination attacks;channel virtual representation;grant-free multiple access;5G IoT networks","Principal component analysis;Internet of Things;5G mobile communication;Partial transmit sequences;Channel estimation;Training;Massive MIMO","","25","","28","IEEE","19 Aug 2020","","","IEEE","IEEE Journals"
"Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures","Z. Yan; J. Wu; G. Li; S. Li; M. Guizani","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Qatar University, Doha, Qatar","IEEE Transactions on Information Forensics and Security","19 Oct 2021","2021","16","","4827","4842","Semi-Supervised Learning (SSL) is a powerful derivative for humans to discover the hidden knowledge, and will be a great substitute for data taggers. Although the availability of unlabeled data rises up a huge passion to SSL, the untrustness of unlabeled data leads to many unknown security risks. In this paper, we first identify an insidious backdoor threat of SSL where unlabeled training data are poisoned by backdoor methods migrated from supervised settings. Then, to further exploit this threat, a Deep Neural Backdoor (DeNeB) scheme is proposed, which requires less data poisoning budgets and produces stronger backdoor effectiveness. By poisoning a fraction of our unlabeled training data, the DeNeB achieves the illegal manipulation on the trained model without modifying the training process. Finally, an efficient detection-and-purification defense (DePuD) framework is proposed to thwart the proposed scheme. In DePuD, we construct a deep detector to locate trigger patterns in the unlabeled training data, and perform secured SSL training with purified unlabeled data where the detected trigger patterns are obfuscated. Extensive experiments based on benchmark datasets are performed to demonstrate the huge threatening of DeNeB and the effectiveness of DePuD. To the best of our knowledge, this is the first work to achieve the backdoor and its defense in semi-supervised learning.","1556-6021","","10.1109/TIFS.2021.3116431","National Natural Science Foundation of China(grant numbers:61971283,U20B2048,61972255); Shanghai Sailing Program(grant numbers:21YF1421700); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551983","Semi-supervised learning;backdoor;unlabeled data;detection-and-purification","Data models;Training data;Semisupervised learning;Supervised learning;Distributed databases;Adaptation models;Training","","25","","41","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"Detecting Anomalous Behavior in VoIP Systems: A Discrete Event System Modeling","D. Golait; N. Hubballi","IIT Indore, Indore, India; IIT Indore, Indore, India","IEEE Transactions on Information Forensics and Security","18 Jan 2017","2017","12","3","730","745","Session initiation protocol (SIP) is an application layer protocol used for signaling purposes to manage voice over IP connections. SIP being a text-based protocol is vulnerable to a range of denial of service (DoS) attacks. These DoS attacks can render the SIP servers/SIP proxy servers unusable by depleting memory and CPU time. In this paper, we consider two types of DoS attacks, namely, flooding attacks and coordinated attacks for detection. Flooding attacks affect both stateless and stateful SIP servers while coordinated attacks affect stateful SIP servers. We model the SIP operation as discrete event system (DES) and design a new state transition machine, which we name as probabilistic counting deterministic timed automata (PCDTA) to describe the behavior of SIP operations. We also identify different types of anomalies that can occur in a DES model, which appear in the form of illegal transitions, violating timing constraints, and appear in number which is otherwise not seen. Subsequently, we map various DoS attacks in SIP to a type of anomaly in DES. PCDTA can learn probabilities of various transitions and timings delay from a set of nonmalicious training sequences. A trained PCDTA can detect anomalies, and hence various DoS attacks in SIP. We perform a thorough experiment with computer simulated SIP traffic and report the detection performance of PCDTA on various attacks generated through custom scripts.","1556-6021","","10.1109/TIFS.2016.2632071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755836","Communication system security;Computer security;Network security","Servers;Computer crime;Discrete-event systems;Registers;Protocols;Authentication;Floods","","22","","41","IEEE","23 Nov 2016","","","IEEE","IEEE Journals"
"A New Multimodal Approach for Password Strength Estimation—Part I: Theory and Algorithms","J. Galbally; I. Coisel; I. Sanchez","Joint Research Center, European Commission, Ispra, Italy; Joint Research Center, European Commission, Ispra, Italy; Joint Research Center, European Commission, Ispra, Italy","IEEE Transactions on Information Forensics and Security","29 Aug 2017","2017","12","12","2829","2844","After more than two decades of research in the field of password strength estimation, one clear conclusion may be drawn: no password strength metric by itself is better than all other metrics for every possible password. Building upon this certainty and also taking advantage of the knowledge gained in the area of information fusion, in this paper, we propose a novel multimodal strength metric that combines several imperfect individual metrics to benefit from their strong points in order to overcome many of their weaknesses. The final multimodal metric comprises different modules based both on heuristics and statistics, which, after their fusion, succeed to provide in real time a realistic and reliable feedback regarding the “guessability” of passwords. The validation protocol and the test results are presented and discussed in a companion paper.","1556-6021","","10.1109/TIFS.2016.2636092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776908","Password security;strength metrics;information fusion;multimodality;Markov chains;password policies;privacy","Measurement;Estimation;Security;Markov processes;Forensics;LinkedIn;Probabilistic logic","","20","","80","EU","7 Dec 2016","","","IEEE","IEEE Journals"
"Detection of Message Injection Attacks Onto the CAN Bus Using Similarities of Successive Messages-Sequence Graphs","M. Jedh; L. Ben Othmane; N. Ahmed; B. Bhargava","Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Air Force Research Laboratory, Rome, NY, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Information Forensics and Security","30 Aug 2021","2021","16","","4133","4146","The smart features of modern cars are enabled by a number of Electronic Control Units (ECUs) components that communicate through an in-vehicle network, known as Controller Area Network (CAN) bus. The fundamental challenge is the security of the communication link where an attacker can inject messages (e.g., increase the speed) that may impact the safety of the driver. Most of existing practical IDS solutions rely on the knowledge of the identity of the ECUs, which is proprietary information. This paper proposes a message injection attack detection solution that is independent of the IDs of the ECUs. First, we represent the sequencing of the messages in a given time-interval as a direct graph and compute the similarities of the successive graphs using the cosine similarity and Pearson correlation. Then, we apply threshold, change point detection, and Long Short-Term Memory (LSTM)-Recurrent Neural Network (RNN) to detect and predict malicious message injections into the CAN bus. The evaluation of the methods using a dataset collected from a moving vehicle under malicious RPM and speed reading message injections show a detection accuracy of 97.32% and detection speed of 2.5 milliseconds when using a threshold method. The performance metrics makes the IDS suitable for real-time control mechanisms for vehicle resiliency to cyber-attacks.","1556-6021","","10.1109/TIFS.2021.3098162","Iowa State University’s Regents Innovation Fund (RIF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490207","Industry applications;security;information security;intrusion detection;intelligent transportation systems;transportation;vehicle;mathematics;algorithms;detection algorithms","Security;Automobiles;Connected vehicles;Protocols;Safety;Universal Serial Bus;Time-frequency analysis","","20","","63","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Information Leakage-Aware Computer-Aided Cyber-Physical Manufacturing","S. R. Chhetri; S. Faezi; M. A. Al Faruque","Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Information Forensics and Security","2 May 2018","2018","13","9","2333","2344","Cyber-physical additive manufacturing systems consist of tight integration of cyber and physical domains. This union, however, induces new cross-domain vulnerabilities that pose unique security challenges. One of these challenges is preventing confidentiality breach, caused by physical-to-cyber domain attacks. In this form of attack, attackers utilize the side-channels (such as acoustics, power, electromagnetic emissions, and so on) in the physical-domain to estimate and steal cyber-domain data (such as G/M-codes). Since these emissions depend on the physical structure of the system, one way to minimize the information leakage is to modify the physical-domain. However, this process can be costly due to added hardware modification. Instead, we propose a novel methodology that allows the cyber-domain tools [such as computer aided-manufacturing (CAM)] to be aware of the existing information leakage. Then, we propose to change either machine process or product design parameters in the cyber-domain to minimize the information leakage. Our methodology aids the existing cyber-domain and physical-domain security solution by utilizing the cross-domain relationship. We have implemented our methodology in a fused-deposition modeling-based Cartesian additive manufacturing system. Our methodology achieves reduction of mutual information by 24.94% in acoustic side-channel, 32.91% in power side-channel, 32.29% in magnetic side-channel, and 55.65% in vibration side-channel. As a case study, to help understand the implication of mutual information drop, we have also presented the calculation of success rate and the reconstruction of the 3D object based on an attack model. For the given attack model, our leakage-aware CAM tool decreases the success rate of an attacker by 8.74% and obstructs the reconstruction of finer geometry details.","1556-6021","","10.1109/TIFS.2018.2818659","NSF CPS(grant numbers:CNS-1546993); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322219","Cyber-physical systems;confidentiality;security;information leakage;manufacturing","Three-dimensional displays;Tools;Three-dimensional printing;Solid modeling;Mutual information;Security;Geometry","","18","","41","IEEE","22 Mar 2018","","","IEEE","IEEE Journals"
"Bandwidth Scanning When Facing Interference Attacks Aimed at Reducing Spectrum Opportunities","A. Garnaev; W. Trappe","Wireless Information Network Laboratory, Rutgers University, North Brunswick, NJ, USA; Wireless Information Network Laboratory, Rutgers University, North Brunswick, NJ, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","8","1916","1930","Unutilized spectra, i.e., spectrum holes, are opportunities that may be used for communication or adapting other services that use radio frequency (RF). Such opportunities can also represent an adversarial target, if his objective is to block the RF system from using such opportunities opened by spectrum holes. In this paper, we explore the challenge of finding spectrum holes in an adversarial environment. First, by means of a simple model, we show that an adversary’s attack designed to close spectrum holes can be more harmful for the spectrum holes than just random jamming. This calls for designing a scanning strategy to detect such an attack. Second, by applying a game-theoretical model, we design the optimal scanning strategy to detect such attacks. In particular, we show the efficiency of such a scanning strategy compared with uninformed random scanning. This efficiency is achieved by focusing scanning efforts on the bands that will be more likely under attack, and neglecting less promising bands. Beyond the benefits, though, such a strategy has also drawbacks since, if the adversary has a different objective, such as sneaking usage of the spectrum, he can sneak usage undetected by using the bands neglected by such specially tuned scanning. To deal with this problem, third, we suggest to combine this strategy with a strategy that maximizes detection probability in a learning algorithm that updates the beliefs about the adversary’s objective. The convergence of the combined algorithm is proven.","1556-6021","","10.1109/TIFS.2017.2694766","National Science Foundation(grant numbers:ECCS-1247864); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7902178","Intrusion detection;wireless networks;Bayes methods","Jamming;Sensors;Games;Interference;Wireless communication;Resource management;Bandwidth","","15","","38","IEEE","17 Apr 2017","","","IEEE","IEEE Journals"
"Near-Optimal and Practical Jamming-Resistant Energy-Efficient Cognitive Radio Communications","P. Zhou; Q. Wang; W. Wang; Y. Hu; D. Wu","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; The State Key Laboratory of Software Engineering, School of Computer Science, Wuhan University, Wuhan, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Information Forensics and Security","22 Aug 2017","2017","12","11","2807","2822","This paper studies the problem of jamming-resistant spectrum aggregation and access (SAA) for energy-efficiency (EE) cognitive radio communications. We consider various jamming behaviors, where jammers may attack all available channels with arbitrarily changing strategies over time, attack a subset of the channels at certain time slots, or have different intelligence, i.e., oblivious or adaptive adversary, and so on. Without any priori knowledge about the channels and jammers, it is very challenging to design an efficient and practical jamming-resistant SAA algorithm to reach the optimal EE goal. In this paper, we utilize the advanced martingale concentration inequalities in an multi-armed bandits-based online learning framework to facilitate the optimal detection of various jamming behaviors. We first define a novel EE model for discontiguous orthogonal frequency division multiplexing to facilitate scalable SAA over distributed spectrum pools in practice. Then, the jamming-resistant dynamic channel access problem is formulated as a regret minimization problem. Meanwhile, an online stochastic gradient descent with bandit feedback procedure is adopted to allocate the transmit power. The proposed algorithm can autonomously detect the environmental features and find a near-optimal solution in each attacking scenario. Our algorithm is implemented with low complexity and with multiple users under some practical jamming scenarios. Extensive numerical studies show that under some practical jamming scenarios, our algorithm has an EE improvement of 45.3% over a fixed learning period, and an improvement of 82.5% in terms of learning duration compared with existing approaches.","1556-6021","","10.1109/TIFS.2017.2721931","National Natural Science Foundation of China(grant numbers:61401169,61571396,61373167,61502191,61502190,U1636219); National High Technology Research and Development Program of China (863 Program)(grant numbers:2015AA016004); US National Science Foundation(grant numbers:CNS-1318948); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR17F010001); Fundamental Research Funds for the Central Universities(grant numbers:2017KFYXJJ065); Hubei Provincial Natural Science Foundation of China(grant numbers:2016CFB226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962233","Jamming;spectrum aggregation and access;energy efficiency;cognitive radio and multi-armed bandits","Jamming;Stochastic processes;Sensors;Communication system security;Cognitive radio;Games;Numerical models","","15","","34","IEEE","29 Jun 2017","","","IEEE","IEEE Journals"
"Safe Exploration in Wireless Security: A Safe Reinforcement Learning Algorithm With Hierarchical Structure","X. Lu; L. Xiao; G. Niu; X. Ji; Q. Wang","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Automation, Tsinghua University, Beijing, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","21 Feb 2022","2022","17","","732","743","Most safe reinforcement learning (RL) algorithms depend on the accurate reward that is rarely available in wireless security applications and suffer from severe performance degradation for the learning agents that have to choose the policy from a large action set. In this paper, we propose a safe RL algorithm, which uses a policy priority-based hierarchical structure to divide each policy into sub-policies with different selection priorities and thus compresses the action set. By applying inter-agent transfer learning to initialize the learning parameters, this algorithm accelerates the initial exploration of the optimal policy. Based on a security criterion that evaluates the risk value, the sub-policy distribution formulation avoids the dangerous sub-policies that cause learning failure such as severe network security problems in wireless security applications, e.g., Internet services interruption. We also propose a deep safe RL and design four deep neural networks in each sub-policy selection to further improve the learning efficiency for the learning agents that support four convolutional neural networks (CNNs): The Q-network evaluates the long-term expected reward of each sub-policy under the current state, and the E-network evaluates the long-term risk value. The target Q and E-networks update the learning parameters of the corresponding CNN to improve the policy exploration stability. As a case study, our proposed safe RL algorithms are implemented in the anti-jamming communication of unmanned aerial vehicles (UAVs) to select the frequency channel and transmit power to the ground node. Experimental results show that our proposed schemes significantly improve the UAV communication performance, save the UAV energy and increase the reward compared with the benchmark against jamming.","1556-6021","","10.1109/TIFS.2022.3149396","Natural Science Foundation of China(grant numbers:U21A20444,61971366,U20B2049,61822207); Fundamental Research Funds for the Central Universities(grant numbers:2042021gf0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705557","Reinforcement learning;deep learning;safe exploration;wireless security;jamming attacks","Security;Jamming;Communication system security;Optimization;Q-learning;Complexity theory;Bit error rate","","14","","33","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Identity Deception Prevention Using Common Contribution Network Data","M. Tsikerdekis","College of Communication and Information, University of Kentucky, Lexington, KY, USA","IEEE Transactions on Information Forensics and Security","3 Nov 2016","2017","12","1","188","199","Identity deception in social media applications has negatively impacted online communities and it is likely to increase as the social media user population grows. The ease of generating new accounts on social media has exacerbated the issue. Many previous studies have been posited that focused on both verbal, non-verbal, and network data produced by users in an attempt to detect identity deception. However, although these methods produced a high accuracy, they are mainly reactive to the issue of identity deception. This paper proposes a proactive approach that leverages social network data and it is focused on identity deception prevention for online sub-communities, communities that exist within larger communities (e.g., Facebook groups or Subreddits). The method can be applied to various types of social media applications and produces high accuracy in identifying deceptive accounts at the time of attempted entry to a sub-community. Performance results as well as limitations for the method are presented. A discussion follows on the identification of possible implications of this paper for social media applications and future directions on deception prevention are proposed.","1556-6021","","10.1109/TIFS.2016.2607697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563451","Identity;deception;prevention;network;data","Media;Context;Forgery;Security;Facebook;Complexity theory","","14","","38","IEEE","8 Sep 2016","","","IEEE","IEEE Journals"
"An Image-Based Approach to Detection of Fake Coins","L. Liu; Y. Lu; C. Y. Suen","Engineering Research Center of Optical Instrument and System, Ministry of Education, Shanghai Key Lab of Modern Optical System, University of Shanghai for Science and Technology, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Centre for Pattern Recognition and Machine Intelligence, Concordia University, Montreal, QC, Canada","IEEE Transactions on Information Forensics and Security","19 May 2017","2017","12","5","1227","1239","We propose a new approach to detect fake coins using their images in this paper. A coin image is represented in the dissimilarity space, which is a vector space constructed by comparing the image with a set of prototypes. Each dimension measures the dissimilarity between the image under consideration and a prototype. In order to obtain the dissimilarity between two coin images, the local keypoints on each image are detected and described. Based on the characteristics of the coin, the matched keypoints between the two images can be identified in an efficient manner. A post-processing procedure is further proposed to remove mismatched keypoints. Due to the limited number of fake coins in real life, one-class learning is conducted for fake coin detection, so only genuine coins are needed to train the classifier. Extensive experiments have been carried out to evaluate the proposed approach on different data sets. The impressive results have demonstrated its validity and effectiveness.","1556-6021","","10.1109/TIFS.2017.2656478","National Natural Science Foundation of China(grant numbers:61603256); Hujiang Foundation of China(grant numbers:C14002); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828126","Fake coins;fake coin detection;coin image representation;dissimilarity space;one-class learning","Prototypes;Image representation;Image recognition;Support vector machines;Detectors;Shape;Patents","","14","","47","IEEE","20 Jan 2017","","","IEEE","IEEE Journals"
"BCAuth: Physical Layer Enhanced Authentication and Attack Tracing for Backscatter Communications","P. Wang; Z. Yan; K. Zeng","State Key Laboratory of ISN, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of ISN, School of Cyber Engineering, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, the Department of Cyber Security Engineering, and the Department of Computer Science, George Mason University, Fairfax, VA, USA","IEEE Transactions on Information Forensics and Security","15 Aug 2022","2022","17","","2818","2834","Backscatter communication (BC) enables ultra-low-power communications and allows devices to harvest energy simultaneously. But its practical deployment faces severe security threats caused by its nature of openness and broadcast. Authenticating backscatter devices (BDs) is treated as the first line of defense. However, complex cryptographic approaches are not desirable due to the limited computation capability of BDs. Existing physical layer authentication schemes cannot effectively support BD mobility, multiple attacker identification and attacker location tracing in an integrated way. To tackle these problems, this paper proposes BCAuth, a multi-stage authentication and attack tracing scheme based on the physical spatial information of BDs to realize enhanced BD authentication security for both static and mobile BDs. After initial authentication based on BD identity with its position information registration, preemptive authentication and re-authentication are performed according to spatial correlation of backscattered signal source locations associated with the BD. By exploiting clustering-based analysis on spacial information, BCAuth is capable of determining the number of attackers and localizing their positions. In addition, we propose a reciprocal channel-based method for BD re-authentication with better authentication performance than the clustering-based method for mobile BDs when the BDs is able to measure received signal strength (RSS), which also enables mutual authentication. We theoretically analyze BCAuth security and conduct extensive numerical simulations with various settings to show its desirable performance.","1556-6021","","10.1109/TIFS.2022.3195407","National Natural Science Foundation of China(grant numbers:62072351); Academy of Finland(grant numbers:345072,308087,335262,350464); Open Research Project of Zhejiang Laboratory(grant numbers:2021PD0AB01); Higher Education Discipline Innovation Project(grant numbers:B16037); U.S. National Science of Foundation through Networking Technology and Systems (NeTS) Program(grant numbers:2131507); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845476","Backscatter communication;physical layer security;device authentication;attack detection;positioning","Authentication;Security;Physical layer;Feature extraction;Servers;RF signals;Backscatter","","13","","62","CCBY","1 Aug 2022","","","IEEE","IEEE Journals"
"Multiple Sensitive Values-Oriented Personalized Privacy Preservation Based on Randomized Response","H. Song; T. Luo; X. Wang; J. Li","Beijing Laboratory of Advanced Information Networks, and the Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications (BUPT), Beijing, China; Beijing Laboratory of Advanced Information Networks, and the Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications (BUPT), Beijing, China; Beijing Laboratory of Advanced Information Networks, and the Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications (BUPT), Beijing, China; Beijing Laboratory of Advanced Information Networks, and the Beijing Key Laboratory of Network System Architecture and Convergence, Beijing University of Posts and Telecommunications (BUPT), Beijing, China","IEEE Transactions on Information Forensics and Security","4 Feb 2020","2020","15","","2209","2224","In the case where the private data is not equally important, personalized local privacy preservation based on randomized response (RR) is studied in the collection of sensitive data. So far, the existing RR mechanisms for multiple discrete private sources, which are termed as conventional randomized response (CRR) mechanisms, focus on a universal approach that exerts the same amount of privacy preservation for all sensitive values, without catering for their concrete privacy requirements. An immediate consequence is that they may be offering insufficient protection to a subset of data contributors with relatively higher privacy requirements, while applying excessive privacy control to another subset with relatively lower privacy requirements. Motivated by this, a novel perturbation framework, which is termed as personalized randomized response (PRR) mechanism, is proposed to achieve personalized privacy preservation (Personalized-PP) by designing the statistical privatization mechanism for multiple sensitive values. The proposed PRR technique introduces the weights for different sensitive values according to their sensitivity, and then introduces the weights into the decision of PRR by considering the concrete requirements for privacy, and thus, attains a higher data utility with respect to the quality of statistics while guaranteeing Personalized-PP. The estimate error of the private distribution is used to measure the quality of statistics for the two RR mechanisms. Theoretical study shows that the estimate error of PRR mechanism is smaller than that of the CRR mechanism for a certain same subjective privacy leakage degree. In particular, simulation results reveal the circumstances where CRR mechanism fails to provide Personalized-PP, and then establish the superiority of PRR mechanism.","1556-6021","","10.1109/TIFS.2019.2959911","National Basic Research Program of China (973 Program)(grant numbers:2016YFF0201003); National Natural Science Foundation of China(grant numbers:61571065); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933081","Randomized response;weight of the sensitive value;data quality;personalized privacy preservation;subjective privacy leakage degree","Data privacy;Privacy;Analytical models;Privatization;Sensitivity;Data models;Perturbation methods","","12","","42","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"MCTSteg: A Monte Carlo Tree Search-Based Reinforcement Learning Framework for Universal Non-Additive Steganography","X. Mo; S. Tan; B. Li; J. Huang","College of Information Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","2 Sep 2021","2021","16","","4306","4320","Recent research has shown that non-additive image steganographic frameworks effectively improve security performance through adjusting distortion distribution. However, as far as we know, all of the existing non-additive proposals are based on handcrafted policies, and can only be applied to a specific image domain, which heavily prevent non-additive steganography from releasing its full potentiality. In this paper, we propose an automatic non-additive steganographic distortion learning framework called MCTSteg to remove the above restrictions. Guided by the reinforcement learning paradigm, we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental model to build MCTSteg. MCTS makes sequential decisions to adjust distortion distribution without human intervention. Our proposed environmental model is used to obtain feedbacks from each decision. Due to its self-learning characteristic and domain-independent reward function, MCTSteg has become the first reported universal non-additive steganographic framework which can work in both spatial and JPEG domains. Extensive experimental results show that MCTSteg can effectively withstand the detection of both hand-crafted feature-based and deep-learning-based steganalyzers. In both spatial and JPEG domains, the security performance of MCTSteg steadily outperforms the state of the art by a clear margin under different scenarios.","1556-6021","","10.1109/TIFS.2021.3104140","Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2019B010139003); National Natural Science Foundation of China(grant numbers:61772349,U19B2022,61872244); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B151502001); Shenzhen Research and Development Program(grant numbers:JCYJ20200109105008228); Alibaba Group through Alibaba Innovative Research (AIR) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511491","Steganography;steganalysis;Monte Carlo tree search;reinforcement learning","Distortion;Reinforcement learning;Security;Cost function;Transform coding;Monte Carlo methods;Media","","12","","53","IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Amplitude-Varying Perturbation for Balancing Privacy and Utility in Federated Learning","X. Yuan; W. Ni; M. Ding; K. Wei; J. Li; H. V. Poor","Data61, CSIRO, Marsfield, NSW, Australia; Data61, CSIRO, Marsfield, NSW, Australia; Data61, CSIRO, Marsfield, NSW, Australia; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Information Forensics and Security","27 Mar 2023","2023","18","","1884","1897","While preserving the privacy of federated learning (FL), differential privacy (DP) inevitably degrades the utility (i.e., accuracy) of FL due to model perturbations caused by DP noise added to model updates. Existing studies have considered exclusively noise with persistent root-mean-square amplitude and overlooked an opportunity of adjusting the amplitudes to alleviate the adverse effects of the noise. This paper presents a new DP perturbation mechanism with a time-varying noise amplitude to protect the privacy of FL and retain the capability of adjusting the learning performance. Specifically, we propose a geometric series form for the noise amplitude and reveal analytically the dependence of the series on the number of global aggregations and the  $(\epsilon,\delta)$ -DP requirement. We derive an online refinement of the series to prevent FL from premature convergence resulting from excessive perturbation noise. Another important aspect is an upper bound developed for the loss function of a multi-layer perceptron (MLP) trained by FL running the new DP mechanism. Accordingly, the optimal number of global aggregations is obtained, balancing the learning and privacy. Extensive experiments are conducted using MLP, supporting vector machine, and convolutional neural network models on four public datasets. The contribution of the new DP mechanism to the convergence and accuracy of privacy-preserving FL is corroborated, compared to the state-of-the-art Gaussian noise mechanism with a persistent noise amplitude.","1556-6021","","10.1109/TIFS.2023.3258255","National Natural Science Foundation of China(grant numbers:61872184); Fundamental Research Funds for the Central Universities(grant numbers:30921013104); Future Network Grant of Provincial Education Board in Jiangsu; U.S National Science Foundation(grant numbers:CCF-1908308,CNS-2128448); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073536","Federated learning;differential privacy;time-varying noise variance;convergence analysis","Privacy;Perturbation methods;Servers;Convergence;Training;Analytical models;Upper bound","","12","","39","IEEE","16 Mar 2023","","","IEEE","IEEE Journals"
"Joint Differential Game and Double Deep Q-Networks for Suppressing Malware Spread in Industrial Internet of Things","S. Shen; L. Xie; Y. Zhang; G. Wu; H. Zhang; S. Yu","School of Information Engineering, Huzhou University, Huzhou, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science and Technology, Donghua University, Shanghai, China; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Information Forensics and Security","4 Sep 2023","2023","18","","5302","5315","Industrial Internet of Things (IIoT), which has the capability of perception, monitoring, communication and decision-making, has already exposed more security problems that are easy to be invaded by malware because of many simple edge devices that help smart factories, smart cities and smart homes. In this paper, a two-layer malware spread−patch model  $IIPV$  is proposed based on a hybrid patches distribution method according to the simple edge equipments and limited central computer resources of IIoT. The spread process of malware in IIoT was deeply analyzed using differential game and a differential game model was established. Then optimization theory was further used to solve the optimization problem extracted by introducing subjective effort parameters to obtain the optimal control strategies of devices for malware and patches. In addition, we combined the deep reinforcement learning algorithm into the model  $IIPV$  to design a new algorithm  $DDQN-PV$  suitable for suppressing the spread of malware in IIoT during the experiments. Finally, the effectiveness of model  $IIPV$  and algorithm  $DDQN-PV$  are verified by numerous comparative experiments.","1556-6021","","10.1109/TIFS.2023.3307956","Humanities and Social Sciences Planning Foundation of Ministry of Education of China(grant numbers:22YJAZH090); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LZ22F020002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10227347","Malware;patches;Industrial Internet of Things;differential game;double deep Q-networks","Malware;Industrial Internet of Things;Differential games;Security;Optimal control;Computational modeling;Bandwidth","","11","","39","IEEE","23 Aug 2023","","","IEEE","IEEE Journals"
"PhishSim: Aiding Phishing Website Detection With a Feature-Free Tool","R. W. Purwanto; A. Pal; A. Blair; S. Jha","School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW, Australia; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW, Australia; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW, Australia; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW, Australia","IEEE Transactions on Information Forensics and Security","28 Apr 2022","2022","17","","1497","1512","In this paper, we propose a feature-free method for detecting phishing websites using the Normalized Compression Distance (NCD), a parameter-free similarity measure which computes the similarity of two websites by compressing them, thus eliminating the need to perform any feature extraction. It also removes any dependence on a specific set of website features. This method examines the HTML of webpages and computes their similarity with known phishing websites, in order to classify them. We use the Furthest Point First algorithm to perform phishing prototype extractions, in order to select instances that are representative of a cluster of phishing webpages. We also introduce the use of an incremental learning algorithm as a framework for continuous and adaptive detection without extracting new features when concept drift occurs. On a large dataset, our proposed method significantly outperforms previous methods in detecting phishing websites, with an AUC score of 98.68%, a high true positive rate (TPR) of around 90%, while maintaining a low false positive rate (FPR) of 0.58%. Our approach uses prototypes, eliminating the need to retain long term data in the future, and is feasible to deploy in real systems with a processing time of roughly 0.3 seconds.","1556-6021","","10.1109/TIFS.2022.3164212","Australian Government’s Cooperative Research Centres Programme; University of New South Wales (UNSW) University International Postgraduate Award (UIPA) Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745933","Phishing detection;webpage;incremental learning;feature-free methods","Phishing;Feature extraction;Visualization;Prototypes;Electronic mail;Blocklists;Semantics","","10","","41","CCBYNCND","1 Apr 2022","","","IEEE","IEEE Journals"
"Matrix-Regularized One-Class Multiple Kernel Learning for Unseen Face Presentation Attack Detection","S. R. Arashloo","Department of Computer Engineering, Faculty of Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Information Forensics and Security","1 Oct 2021","2021","16","","4635","4647","The functionality of face biometric systems is severely challenged by presentation attacks (PA’s), and especially those attacks that have not been available during the training phase of a PA detection (PAD) subsystem. Among other alternatives, the one-class classification (OCC) paradigm is an applicable strategy that has been observed to provide good generalisation against unseen attacks. Following an OCC approach for the unseen face PAD from RGB images, this work advocates a matrix-regularised multiple kernel learning algorithm to make use of several sources of information each constituting a different view of the face PAD problem. In particular, drawing on the one-class null Fisher classification principle, we characterise different deep CNN representations as kernels and propose a multiple kernel learning (MKL) algorithm subject to an ( $r,p$ )-norm ( $1\leq r,p$ ) matrix regularisation constraint. The propose MKL algorithm is formulated as a saddle point Lagrangian optimisation task for which we present an effective optimisation algorithm with guaranteed convergence. An evaluation of the proposed one-class MKL algorithm on both general object images in an OCC setting as well as on different face PAD datasets in an unseen zero-shot attack detection setting illustrates the merits of the proposed method compared to other one-class multiple kernel and deep end-to-end CNN-based methods.","1556-6021","","10.1109/TIFS.2021.3111766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535490","Unseen face presentation attack detection;one-class Fisher null projection;multiple kernel learning;matrix regularisation;zero-shot learning","Kernel;Faces;Face recognition;Training;Task analysis;Optimization;Support vector machines","","10","","100","IEEE","10 Sep 2021","","","IEEE","IEEE Journals"
"Personalized Federated Learning With Differential Privacy and Convergence Guarantee","K. Wei; J. Li; C. Ma; M. Ding; W. Chen; J. Wu; M. Tao; H. V. Poor","School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; Zhejiang Laboratory, Hangzhou, China; Data61, CSIRO, Sydney, NSW, Australia; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Intelligent Information Processing and the School of Computer Science, Fudan University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Information Forensics and Security","31 Jul 2023","2023","18","","4488","4503","Personalized federated learning (PFL), as a novel federated learning (FL) paradigm, is capable of generating personalized models for heterogenous clients. Combined with a meta-learning mechanism, PFL can further improve the convergence performance with few-shot training. However, meta-learning based PFL has two stages of gradient descent in each local training round, therefore posing a more serious challenge in information leakage. In this paper, we propose a differential privacy (DP) based PFL (DP-PFL) framework and analyze its convergence performance. Specifically, we first design a privacy budget allocation scheme for inner and outer update stages based on the Rényi DP composition theory. Then, we develop two convergence bounds for the proposed DP-PFL framework under convex and non-convex loss function assumptions, respectively. Our developed convergence bounds reveal that 1) there is an optimal size of the DP-PFL model that can achieve the best convergence performance for a given privacy level, and 2) there is an optimal tradeoff among the number of communication rounds, convergence performance and privacy budget. Evaluations on various real-life datasets demonstrate that our theoretical results are consistent with experimental results. The derived theoretical results can guide the design of various DP-PFL algorithms with configurable tradeoff requirements on the convergence performance and privacy levels.","1556-6021","","10.1109/TIFS.2023.3293417","National Natural Science Foundation of China(grant numbers:62002170,62071296); Fundamental Research Funds for the Central Universities(grant numbers:30921013104); Future Network Grant of Provincial Education Board in Jiangsu; Youth Foundation Project through the Zhejiang Laboratory(grant numbers:K2023PD0AA01); National Key Project(grant numbers:2020YFB1807700); Shanghai(grant numbers:22JC1404000,20JC1416502,PKX2021-D02); U.S. National Science Foundation(grant numbers:CNS-2128448); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177379","Federated learning;meta-learning;differential privacy;convergence analysis","Convergence;Training;Privacy;Adaptation models;Metalearning;Servers;Computational modeling","","9","","41","IEEE","10 Jul 2023","","","IEEE","IEEE Journals"
"Lightweight Privacy-Preserving GAN Framework for Model Training and Image Synthesis","Y. Yang; K. Mu; R. H. Deng","College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computing and Information Systems, Singapore Management University, Singapore","IEEE Transactions on Information Forensics and Security","23 Mar 2022","2022","17","","1083","1098","Generative adversarial network (GAN) has excellent performance for data generation and is widely used in image synthesis. Outsourcing GAN to cloud platform is a popular way to save local computation resources and improve the efficiency, but it still faces the privacy leakage concerns: (1) the sensitive information of the training dataset may be disclosed in the cloud; (2) the trained model may reveal the privacy of training samples since it extracts the characteristics from the data. In this paper, we propose a lightweight privacy-preserving GAN framework (LP-GAN) for model training and image synthesis based on secret sharing scheme. Specifically, we design a series of efficient secure interactive protocols for different layers (convolution, batch normalization, ReLU, Sigmoid) of neural network (NN) used in GAN. Our protocols are scalable to build secure training or inference tasks for NN-based applications. We utilize edge computing to reduce the latency and all the protocols are executed on two edge servers collaboratively. Compared with the existing schemes, the proposed solution greatly improves efficiency, reduces communication overhead, and guarantees the privacy. We prove the correctness and security of LP-GAN by theoretical analysis. Extensive experiments on different real-world datasets demonstrate the effectiveness, accuracy, and efficiency of our scheme.","1556-6021","","10.1109/TIFS.2022.3156818","National Natural Science Foundation of China(grant numbers:61872091); Singapore National Research Foundation(grant numbers:NRF2018NCR-NSOE004-0001); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729111","Privacy-preserving;generative adversarial network;secret sharing;secure computation;deep learning","Protocols;Generative adversarial networks;Training;Cryptography;Computational modeling;Image synthesis;Privacy","","8","","50","IEEE","4 Mar 2022","","","IEEE","IEEE Journals"
"Sample Space Dimensionality Refinement for Symmetrical Object Detection","Y. -F. Liu; J. -M. Guo; C. -H. Hsia; S. -Y. Su; H. Lee","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, Chinese Culture University, Taipei, Taiwan; Institute of Electrical and Control Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA","IEEE Transactions on Information Forensics and Security","16 Oct 2014","2014","9","11","1953","1961","Formerly, dimensionality reduction techniques are effective ways for extracting statistical significance of features from their original dimensions. However, the dimensionality reduction also induces an additional complexity burden which may encumber the real efficiency. In this paper, a technique is proposed for the reduction of the dimension of samples rather than the features in the former schemes, and it is able to additionally reduce the computational complexity of the applied systems during the reduction process. This method effectively reduces the redundancies of a sample, in particular for those objects which possess partially symmetric property, such as human face, pedestrian, and license plate. As demonstrated in the experiments, based upon the premises of faster speeds in training and detection by a factor of 4.06 and 1.24, respectively, similar accuracies to the ones without considering the proposed method are achieved. The performance verifies that the proposed technique can offer competitive practical values in pattern recognition related fields.","1556-6021","","10.1109/TIFS.2014.2355495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894132","Sample refinement;dimension reduction;data reduction;face detection;pedestrian detection","Feature extraction;Training;Standards;Face;Pattern recognition;Licenses;Complexity theory","","7","","23","IEEE","8 Sep 2014","","","IEEE","IEEE Journals"
"Attacking Recommender Systems With Plausible Profile","X. Zhang; J. Chen; R. Zhang; C. Wang; L. Liu","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Transportation Internet of Things, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","19 Oct 2021","2021","16","","4788","4800","Recommender systems (RS) have become an essential component of web services due to their excellent performance. Despite their great success, RS have proved to be vulnerable to data poisoning attacks, which inject well-crafted fake profiles into RS, so that the target items can be maliciously recommended. In this paper, we first reveal that existing poisoning attacks in RS can be detected effortlessly, as the features of the generated fake profiles cannot be inconsistent with those of normal profiles all the time. We further propose RecUP, a poisoning attack in RS that can generate plausible profiles whose features stay almost the same as the normal ones, based on Generative Adversarial Networks (GAN). To tailor GAN for poisoning in RS, we develop HRGAN and devise a loss function to guide the training of the generator, along with a masking operation with selected potentially powerful profiles, so that the final generated profiles can perform malicious recommendations as expected. Evaluations against various defense methods using three real-world datasets show that, RecUP can generate the most plausible profiles while maintaining comparable attacking performance compared with state-of-the-art attacks.","1556-6021","","10.1109/TIFS.2021.3117078","National Natural Science Foundation of China(grant numbers:61872416,U20A20181,52031009,62002104,62071192,62171189); Fundamental Research Funds for the Central Universities of China(grant numbers:2019kfyXJJS017); Special Fund for Wuhan Yellow Crane Talents (Excellent Young Scholar); National Science Foundation CISE(grant numbers:2038029,2026945,1564097); IBM Faculty Award; Cisco Grant on Edge Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555630","Recommender systems;shilling attack;generative adversarial network","Feature extraction;Generative adversarial networks;Generators;Correlation;Wavelength division multiplexing;Recommender systems;Training","","7","","55","IEEE","1 Oct 2021","","","IEEE","IEEE Journals"
"Towards Privacy-Preserving Spatial Distribution Crowdsensing: A Game Theoretic Approach","Y. Ren; X. Li; Y. Miao; B. Luo; J. Weng; K. -K. R. Choo; R. H. Deng","State Key Laboratory of Integrated Services Networks and the School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks and the School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks and the School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks and the School of Cyber Engineering, Xidian University, Xi’an, China; College of Cyber Security, Jinan University, Guangzhou, China; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; School of Information Systems, Singapore Management University, Singapore","IEEE Transactions on Information Forensics and Security","3 Mar 2022","2022","17","","804","818","Acquiring the spatial distribution of users in mobile crowdsensing (MCS) brings many benefits to users (e.g., avoiding crowded areas during the COVID-19 pandemic). Although the leakage of users’ location privacy has received a lot of research attention, existing works still ignore the rationality of users, resulting that users may not obtain satisfactory spatial distribution even if they provide true location information. To solve the problem, we employ game theory with incomplete information to model the interactions among users and seek an equilibrium state through learning approaches of the game. Specifically, we first model the service as a game in the satisfaction form and define the equilibrium for this service. Then, we design a LEFS algorithm for the privacy strategy learning of users when their satisfaction expectations are fixed, and further design LSRE that allows users to have dynamic satisfaction expectations. We theoretically analyze the convergence conditions and characteristics of the proposed algorithms, along with the privacy protection level obtained by our solution. We conduct extensive experiments to show the superiority and various performances of our proposal, which illustrates that our proposal can get more than 85% advantage in terms of the sensing distribution availability compared to the traditional spatial cloaking based solutions.","1556-6021","","10.1109/TIFS.2022.3152409","National Natural Science Foundation of China(grant numbers:62125205,U1708262,U1736203,62072361); Key Research and Development Program of Shaanxi(grant numbers:2021ZDLGY05-04); Fundamental Research Funds for the Central Universities(grant numbers:JB211505); Cloud Technology Endowed Professorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715112","Mobile crowdsensing;spatial distribution;location privacy;game theory;satisfaction form","Privacy;Graphical models;Distribution functions;Sensors;Games;Differential privacy;Servers","","6","","43","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Label-Only Model Inversion Attacks: Attack With the Least Information","T. Zhu; D. Ye; S. Zhou; B. Liu; W. Zhou","School of Computer Science, China University of Geosciences, Wuhan, China; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; School of Data Science, City University of Macau, Macau, China","IEEE Transactions on Information Forensics and Security","5 Jan 2023","2023","18","","991","1005","In a model inversion attack, an adversary attempts to reconstruct the training data records of a target model using only the model’s output. In launching a contemporary model inversion attack, the strategies discussed are generally based on either predicted confidence score vectors, i.e., black-box attacks, or the parameters of a target model, i.e., white-box attacks. However, in the real world, model owners usually only give out the predicted labels; the confidence score vectors and model parameters are hidden as a defense mechanism to prevent such attacks. Unfortunately, we have found a model inversion method that can reconstruct representative samples of the target model’s training data based only on the output labels. We believe this attack requires the least information to succeed and, therefore, has the best applicability. The key idea is to exploit the error rate of the target model to compute the median distance from a set of data records to the decision boundary of the target model. The distance is then used to generate confidence score vectors which are adopted to train an attack model to reconstruct the representative samples. The experimental results show that highly recognizable representative samples can be reconstructed with far less information than existing methods.","1556-6021","","10.1109/TIFS.2022.3233190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003239","Model inversion;deep learning and label-only attacks","Data models;Image reconstruction;Computational modeling;Predictive models;Flight recording;Electronic equipment;Training","","5","","61","IEEE","29 Dec 2022","","","IEEE","IEEE Journals"
"Reinforcement Learning Enabled Intelligent Energy Attack in Green IoT Networks","L. Li; Y. Luo; J. Yang; L. Pu","Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA; Department of Electronics and Communication Engineering, Mississippi State University, Starkville, MS, USA; Department of Electronics and Communication Engineering, Mississippi State University, Starkville, MS, USA; Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA","IEEE Transactions on Information Forensics and Security","21 Feb 2022","2022","17","","644","658","In this paper, we study a new security issue brought by the renewable energy feature in green Internet of Things (IoT) network. We define a new attack method, called the malicious energy attack, where the attacker can charge specific nodes to manipulate routing paths. By intelligently selecting the victim nodes, the attacker can “encourage” most of the data traffic into passing through a compromised node and harm the information security. The performance of the energy attack depends on the charging strategies. We develop two reinforcement-learning enabled algorithms, namely, Q- learning enabled intelligent energy attack (Q-IEA) and Policy Gradient enabled intelligent energy attack (PG-IEA). Through interacting with the network environment, the attacker can intelligently take attack actions without knowing the private information of the IoT network. This can greatly enhance the adaptability of the attacker to different network settings. Simulation results verify that the proposed IEA methods can considerably increase the amount of traffic traveling through the compromised node. Compared with the network without attack, an additional 53.3% data traffic is lured to the compromised node, which is more than 4 times higher than the performance of Random Attack.","1556-6021","","10.1109/TIFS.2022.3149148","Division of Computer and Network Systems(grant numbers:2051356); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703346","Green IoT networks;security;malicious energy attack;reinforcement learning","Routing;Green products;Batteries;Radio frequency;Energy harvesting;Security;Routing protocols","","5","","34","IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"Detection of Spoofing Attacks in Aeronautical Ad-Hoc Networks Using Deep Autoencoders","T. M. Hoang; T. van Chien; T. van Luong; S. Chatzinotas; B. Ottersten; L. Hanzo","School of Electronics and Computer Science, University of Southampton, Southampton, U.K; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg, Luxembourg; Faculty of Computer Science, Phenikaa University, Hanoi, Vietnam; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg, Luxembourg; School of Electronics and Computer Science, University of Southampton, Southampton, U.K","IEEE Transactions on Information Forensics and Security","16 Mar 2022","2022","17","","1010","1023","We consider an aeronautical ad-hoc network relying on aeroplanes operating in the presence of a spoofer. The aggregated signal received by the terrestrial base station is considered as “clean” or “normal”, if the legitimate aeroplanes transmit their signals and there is no spoofing attack. By contrast, the received signal is considered as “spurious” or “abnormal” in the face of a spoofing signal. An autoencoder (AE) is trained to learn the characteristics/features from a training dataset, which contains only normal samples associated with no spoofing attacks. The AE takes original samples as its input samples and reconstructs them at its output. Based on the trained AE, we define the detection thresholds of our spoofing discovery algorithm. To be more specific, contrasting the output of the AE against its input will provide us with a measure of geometric waveform similarity/dissimilarity in terms of the peaks of curves. To quantify the similarity between unknown testing samples and the given training samples (including normal samples), we first propose a so-called deviation-based algorithm. Furthermore, we estimate the angle of arrival (AoA) from each legitimate aeroplane and propose a so-called AoA-based algorithm. Then based on a sophisticated amalgamation of these two algorithms, we form our final detection algorithm for distinguishing the spurious abnormal samples from normal samples under a strict testing condition. In conclusion, our numerical results show that the AE improves the trade-off between the correct spoofing detection rate and the false alarm rate as long as the detection thresholds are carefully selected.","1556-6021","","10.1109/TIFS.2022.3155970","European Research Council (ERC) Grant (AGNOSTIC Actively enhanced cognition-based framework for design of complex systems) at the University of Luxembourg(grant numbers:742648); Engineering and Physical Sciences Research Council(grant numbers:EP/P034284/1,EP/P003990/1 (COALESCE)); European Research Council’s Advanced Fellow Grant QuantCom(grant numbers:789028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724185","PHY authentication;spoofing detection;AANET;deep learning;autoencoder","Authentication;Testing;Training;Receiving antennas;Antenna arrays;Security;Wireless communication","","5","","34","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"An Exploit Kits Detection Approach Based on HTTP Message Graph","Y. Qin; W. Wang; S. Zhang; K. Chen","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","3 Jun 2021","2021","16","","3387","3400","The exploit kits (EKs) are used by attackers to distribute malware automatically and silently. Existing approaches to EKs detection usually need to perform dynamic analysis on the content contained in the network traffic, which requires dumping all the network traffic and thus causes high detection overhead. Although some approaches detect EKs based on static analysis, they usually fail to restore the complete attack path because of the obstruction set by the attackers. In this paper, we propose an approach that can detect EKs based on only information extracted by static analysis. Our method builds a graph for web sessions and extracts features from the graph to perform EKs detection. The built graph catches important structural characteristics of the interaction during EK attacks that were not revealed in existing methods, with which EKs can be detected with high accuracy. The experiments show that our method works well in both the ground-truth datasets and the latest practical cases. Our method can also identify the malicious websites concealed in EKs, which can further improve the efficiency of analysis.","1556-6021","","10.1109/TIFS.2021.3080082","National Natural Science Foundation of China(grant numbers:61672543,61772559); NSFC(grant numbers:U1836211); Beijing Natural Science Foundation(grant numbers:JQ18011); Youth Innovation Promotion Association, CAS; Beijing Academy of Artificial Intelligence (BAAI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9430543","Exploit kits;drive-by download;static analysis;HTTP headers;network traffic analysis","Feature extraction;Malware;Browsers;Uniform resource locators;Static analysis;Monitoring;Databases","","4","","58","IEEE","13 May 2021","","","IEEE","IEEE Journals"
"Advanced Joint Bayesian Method for Face Verification","Y. Liang; X. Ding; J. -H. Xue","State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Statistical Science, University College London, London, U.K.","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","2","346","354","Generative Bayesian models have recently become the most promising framework in classifier design for face verification. However, we report in this paper that the joint Bayesian method, a successful classifier in this framework, suffers performance degradation due to its underuse of the expectation-maximization algorithm in its training phase. To rectify the underuse, we propose a new method termed advanced joint Bayesian (AJB). AJB has a good convergence property and achieves a higher verification rate than both the Joint Bayesian method and other state-of-the-art classifiers on the labeled faces in the wild face database.","1556-6021","","10.1109/TIFS.2014.2375552","National Basic Research Program (973 Program) of China(grant numbers:2013CB329403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971119","Face verification;generative Bayesian models;expectation-maximization (EM);model training;Face verification;generative Bayesian models;expectation-maximization (EM);model training","Bayes methods;Joints;Face;Training;Estimation;Mathematical model;Standards","","4","","24","IEEE","2 Dec 2014","","","IEEE","IEEE Journals"
"Relative Privacy Threats and Learning From Anonymized Data","M. Boreale; F. Corradi; C. Viscardi","Dipartimento di Statistica, Informatica, Applicazioni (DiSIA), Università di Firenze, Florence, Italy; Dipartimento di Statistica, Informatica, Applicazioni (DiSIA), Università di Firenze, Florence, Italy; Dipartimento di Statistica, Informatica, Applicazioni (DiSIA), Università di Firenze, Florence, Italy","IEEE Transactions on Information Forensics and Security","16 Dec 2019","2020","15","","1379","1393","We consider group-based anonymization schemes, a popular approach to data publishing. This approach aims at protecting privacy of the individuals involved in a dataset, by releasing an obfuscated version of the original data, where the exact correspondence between individuals and attribute values is hidden. When publishing data about individuals, one must typically balance the learner's utility against the risk posed by an attacker, potentially targeting individuals in the dataset. Accordingly, we propose a unified Bayesian model of group-based schemes and a related MCMC methodology to learn the population parameters from an anonymized table. This allows one to analyze the risk for any individual in the dataset to be linked to a specific sensitive value, when the attacker knows the individual's nonsensitive attributes, beyond what is implied for the general population. We call this relative threat analysis. Finally, we illustrate the results obtained with the proposed methodology on a real-world dataset.","1556-6021","","10.1109/TIFS.2019.2937640","Dipartimenti Eccellenti 2018–2022 Fund of Italian Ministry of Education, University and Research Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813073","Privacy;anonymization;k-anonymity;mcmc methods","Data privacy;Privacy;Sociology;Statistics;Heart;Publishing;Diseases","","3","","50","IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Prototype-Guided Autoencoder for OCT-Based Fingerprint Presentation Attack Detection","Y. -P. Liu; W. Zuo; R. Liang; H. Sun; Z. Li","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","IEEE Transactions on Information Forensics and Security","12 Jun 2023","2023","18","","3461","3475","Anti-spoofing ability is vital for fingerprint identification systems. Conventional fingerprint scanning devices can only obtain information from the fingertip surfaces, and their performance is susceptible to skin conditions and presentation attacks (PAs). However, optical coherence tomography (OCT) can scan subcutaneous tissue and obtain 3D fingerprint structures, naturally enhancing its PA detection (PAD) ability from the perspective of hardware. Existing unsupervised PAD methods are based on image reconstruction. However, the reconstruction error is easily affected by OCT noise and the rich details of OCT images. Therefore we propose feature-based reconstruction to alleviate this problem, called the prototype-guided autoencoder. The model consists of a memory module and a denoising autoencoder without the requirement of PA fingerprints. As only bona fide fingerprints are available during the training phase, the memory module contains the prototype features of the bona fide fingerprints. During the inference phase, as the prototype memory module is frozen, the reconstructed representation of the bona fide input is close to the bona fide fingerprint features. Calculating the distance between the original features and the prototype reconstructed representation of the sample can achieve PAD. To obtain a better decision making boundary, we propose a representation consistency constraint, which reduces the bona fide representation reconstruction distance closer, so that it is easier to differentiate between fingerprints and PAs.","1556-6021","","10.1109/TIFS.2023.3282386","Natural Science Foundation of China(grant numbers:62076220); Zhejiang Provincial Natural Science Foundation(grant numbers:LY22F030018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143222","Presentation attack detection;optical coherence tomography;autoencoder;prototype","Fingerprint recognition;Feature extraction;Image reconstruction;Prototypes;Memory modules;Training;Noise reduction","","2","","51","IEEE","2 Jun 2023","","","IEEE","IEEE Journals"
"A Differentially Private Framework for Deep Learning With Convexified Loss Functions","Z. Lu; H. J. Asghar; M. A. Kaafar; D. Webb; P. Dickinson","Department of Computing, Macquarie University, Sydney, NSW, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia; Cyber and Electronic Warfare Division, Defence Science and Technology Group, Edinburgh, SA, Australia; Cyber and Electronic Warfare Division, Defence Science and Technology Group, Edinburgh, SA, Australia","IEEE Transactions on Information Forensics and Security","17 Jun 2022","2022","17","","2151","2165","Differential privacy (DP) has been applied in deep learning for preserving privacy of the underlying training sets. Existing DP practice falls into three categories—objective perturbation (injecting DP noise into the objective function), gradient perturbation (injecting DP noise into the process of gradient descent) and output perturbation (injecting DP noise into the trained neural networks, scaled by the global sensitivity of the trained model parameters). They suffer from three main problems. First, conditions on objective functions limit objective perturbation in general deep learning tasks. Second, gradient perturbation does not achieve a satisfactory privacy-utility trade-off due to over-injected noise in each epoch. Third, high utility of the output perturbation method is not guaranteed because of the loose upper bound on the global sensitivity of the trained model parameters as the noise scale parameter. To address these problems, we analyse a tighter upper bound on the global sensitivity of the model parameters. Under a black-box setting, based on this global sensitivity, to control the overall noise injection, we propose a novel output perturbation framework by injecting DP noise into a randomly sampled neuron (via the exponential mechanism) at the output layer of a baseline non-private neural network trained with a convexified loss function. We empirically compare the privacy-utility trade-off, measured by accuracy loss to baseline non-private models and the privacy leakage against black-box membership inference (MI) attacks, between our framework and the open-source differentially private stochastic gradient descent (DP-SGD) approaches on six commonly used real-world datasets. The experimental evaluations show that, when the baseline models have observable privacy leakage under MI attacks, our framework achieves a better privacy-utility trade-off than existing DP-SGD implementations, given an overall privacy budget  $\epsilon \leq 1$  for a large number of queries.","1556-6021","","10.1109/TIFS.2022.3169911","Macquarie University Cyber Security Hub, in partnership with the Defence Science and Technology Group and Data61-CSIRO, through the Next Generation Technologies Fund; Australasian Leadership Computing Grants Scheme, with computational resources provided by NCI Australia, an NCRIS enabled capability; Australian Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762326","Differential privacy;membership inference attack;neural networks;convexified loss function","Perturbation methods;Sensitivity;Training;Deep learning;Upper bound;Task analysis;Privacy","","2","","32","IEEE","22 Apr 2022","","","IEEE","IEEE Journals"
"Secure Metric Learning via Differential Pairwise Privacy","J. Li; Y. Pan; Y. Sui; I. W. Tsang","Centre for Artificial Intelligence (CAI), University of Technology Sydney, Ultimo, Australia; Centre for Artificial Intelligence (CAI), University of Technology Sydney, Ultimo, Australia; Centre for Artificial Intelligence (CAI), University of Technology Sydney, Ultimo, Australia; Centre for Artificial Intelligence (CAI), University of Technology Sydney, Ultimo, Australia","IEEE Transactions on Information Forensics and Security","3 Jul 2020","2020","15","","3640","3652","Distance Metric Learning (DML) has drawn much attention over the last two decades. A number of previous works have shown that it performs well in measuring the similarities of individuals given a set of correctly labeled pairwise data by domain experts. These important and precisely-labeled pairwise data are often highly sensitive in real world (e.g., patients similarity). This paper studies, for the first time, how pairwise information can be leaked to attackers during distance metric learning, and develops differential pairwise privacy (DPP), generalizing the definition of standard differential privacy, for secure metric learning. Unlike traditional differential privacy which only applies to independent samples, thus cannot be used for pairwise data, DPP successfully deals with this problem by reformulating the worst case. Specifically, given the pairwise data, we reveal all the involved correlations among pairs in the constructed undirected graph. DPP is then formalized that defines what kind of DML algorithm is private to preserve pairwise data. After that, a case study employing the contrastive loss is exhibited to clarify the details of implementing a DPP- DML algorithm. Particularly, the sensitivity reduction technique is proposed to enhance the utility of the output distance metric. Experiments both on a toy dataset and benchmarks demonstrate that the proposed scheme achieves pairwise data privacy without compromising the output performance much (Accuracy declines less than 0.01 throughout all benchmark datasets when the privacy budget is set at 4).","1556-6021","","10.1109/TIFS.2020.2997183","Australian Research Council(grant numbers:DE170101081,DP180100106,DP200101328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099300","Pairwise data;differential privacy;metric learning;graph;gradient perturbation","Measurement;Privacy;Perturbation methods;Correlation;Diseases","","2","","64","IEEE","25 May 2020","","","IEEE","IEEE Journals"
"Dynamic Spectrum Anti-Jamming Access With Fast Convergence: A Labeled Deep Reinforcement Learning Approach","Y. Li; Y. Xu; G. Li; Y. Gong; X. Liu; H. Wang; W. Li","College of Communications Engineering, Army Engineering University of PLA, Nanjing, China; College of Communications Engineering, Army Engineering University of PLA, Nanjing, China; College of Communications Engineering, Army Engineering University of PLA, Nanjing, China; College of Communications Engineering, Army Engineering University of PLA, Nanjing, China; College of Information Science and Engineering, Guilin University of Technology, Guilin, China; Field Engineering College, Army Engineering University of PLA, Nanjing, China; College of Communications Engineering, Army Engineering University of PLA, Nanjing, China","IEEE Transactions on Information Forensics and Security","6 Sep 2023","2023","18","","5447","5458","The primary objective of anti-jamming techniques is to ensure that the transmitted data arrives at the intended receiver without being disturbed or jammed with by any jamming signal or other hostile activities to ensuring the security of the communication system. Deep reinforcement learning (DRL) has been extensively utilized in solving the dynamic spectrum anti-jamming problem. However, most of existing DRL-based algorithms require lots of training time, which fails to adapt the fast-channging jamming environment. Our objective is to find a practical and fast-convergence anti-jamming learning solution. To achieve this, we redesign the DRL algorithm in the following two ways. First, we split the cycle of reinforcement learning into two parts: applying process and training process. Second, we use soft labels instead of rewards which bring more information. We further theoretically show that the information gain can help our proposed algorithm converge faster. Moreover, we also show that our labeled DRL algorithm is better than the idealized DRL-based scheme which can obtain the same information as the soft labels. Simulation results demonstrate that compared with existing DRL-based algorithms, our proposed algorithm reduces the number of iterations by up to 90%.","1556-6021","","10.1109/TIFS.2023.3307950","Jiangsu Province Natural Science Foundation(grant numbers:BK20200580); National Natural Science Fund of China(grant numbers:U22B2002,62101595,62071488,62071135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10227374","Anti-jamming;deep reinforcement learning;dynamic spectrum access;fast convergence","Jamming;Convergence;Heuristic algorithms;Training;Reinforcement learning;Time-frequency analysis;Deep learning","","1","","48","IEEE","23 Aug 2023","","","IEEE","IEEE Journals"
"Collaborative Honeypot Defense in UAV Networks: A Learning-Based Game Approach","Y. Wang; Z. Su; A. Benslimane; Q. Xu; M. Dai; R. Li","School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Cyber Science and Engineering, Xi’an Jiaotong University, Xi’an, China; Laboratory of Computer Sciences, Avignon University, Avignon, France; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Electrical and Computer Engineering, Kanazawa University, Kanazawa, Japan","IEEE Transactions on Information Forensics and Security","1 Jan 2024","2024","19","","1963","1978","The proliferation of unmanned aerial vehicles (UAVs) opens up new opportunities for on-demand service provision anywhere and anytime, but also exposes UAVs to a variety of cyber threats. Low/medium interaction honeypots offer a promising lightweight defense for actively protecting mobile Internet of things, particularly UAV networks. While previous research has primarily focused on honeypot system design and attack pattern recognition, the incentive issue for motivating UAVs’ participation (e.g., sharing trapped attack data in honeypots) to collaboratively resist distributed and sophisticated attacks remains unexplored. This paper proposes a novel game-theoretical collaborative defense approach to address optimal, fair, and feasible incentive design, in the presence of network dynamics and UAVs’ multi-dimensional private information (e.g., valid defense data (VDD) volume, communication delay, and UAV cost). Specifically, we first develop a honeypot game between UAVs and the network operator under both partial and complete information asymmetry scenarios. The optimal VDD-reward contract design problem with partial information asymmetry is then solved using a contract-theoretic approach that ensures budget feasibility, truthfulness, fairness, and computational efficiency. In addition, under complete information asymmetry, we devise a distributed reinforcement learning algorithm to dynamically design optimal contracts for distinct types of UAVs in the time-varying UAV network. Extensive simulations demonstrate that the proposed scheme can motivate UAV’s cooperation in VDD sharing and improve defensive effectiveness, compared with conventional schemes.","1556-6021","","10.1109/TIFS.2023.3318942","NSFC(grant numbers:62302387,U22A2029,U20A20175,62001278,62273223); Postdoctoral Innovative Talent Support Program of China(grant numbers:BX20230282); Natural Science Foundation of Shanghai(grant numbers:20ZR1420700); Shanghai Sailing Program(grant numbers:20YF1413600); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10273619","Unmanned aerial vehicle (UAV);mobile honeypot;collaborative defense;game;reinforcement learning","Autonomous aerial vehicles;Games;Costs;Contracts;Collaboration;Delays;Heuristic algorithms","","1","","40","IEEE","6 Oct 2023","","","IEEE","IEEE Journals"
"Good Learning, Bad Performance: A Novel Attack Against RL-Based Congestion Control Systems","Z. Yang; J. Cao; Z. Liu; X. Zhang; K. Sun; Q. Li","Department of Computer Science and Technology, Institute of Network Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Institute of Network Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Institute of Network Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Institute of Network Science and Technology, Tsinghua University, Beijing, China; Department of Information Sciences and Technology, George Mason University, Fairfax, VA, USA; Department of Computer Science and Technology, Institute of Network Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Information Forensics and Security","15 Mar 2022","2022","17","","1069","1082","Reinforcement Learning (RL) has been applied to solve decision-making problems in computer network designs, especially in TCP congestion control. As RL-based congestion control methods enable powerful learning abilities, it achieves competitive performance and adaptiveness advantages over the traditional methods. However, RL-based systems suffer from adversarial attacks that generate perturbations to significantly degrade the performance. In this paper, we conduct a comprehensive study of adversarial attacks against RL-based congestion control systems. Unlike the state-of-the-art adversarial attacks on images where an attacker can easily obtain the input states to introduce perturbations, the attacker cannot directly obtain the input states in congestion control settings that are only available to the agents. It is challenging to add effective perturbations without knowing the input states for RL-based congestion control models. To solve the challenge, we develop an adversarial attack to estimate states of the target agent, craft adversarial perturbations, and apply the generated perturbations in an automated fashion. We evaluate how our adversarial attack affects the target agent’s decision-making process. Our experiments illustrate that our attack can effectively reduce about 50% average throughput while increasing more than 36x latency and 45% packet loss rate.","1556-6021","","10.1109/TIFS.2022.3154244","NSFC(grant numbers:62132011); Beijing National Research Center for Information Science and Technology (BNRist)(grant numbers:BNR2020RC01013); Shuimu Tsinghua Scholar Program; U.S. NAVY(grant numbers:N00014-20-1-2407,N00014-18-2893); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722881","Reinforcement learning;adversarial attack;congestion control","Perturbation methods;Control systems;Receivers;Packet loss;Throughput;Sun;Protocols","","1","","43","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"Long-Term Privacy-Preserving Aggregation With User-Dynamics for Federated Learning","Z. Liu; H. -Y. Lin; Y. Liu","Huawei International, Kwai Chung, Singapore; Huawei International, Kwai Chung, Singapore; Huawei International, Kwai Chung, Singapore","IEEE Transactions on Information Forensics and Security","27 Apr 2023","2023","18","","2398","2412","Privacy-preserving aggregation protocol is an essential building block in privacy-enhanced federated learning (FL), which enables the server to obtain the sum of users’ locally trained models while keeping local training data private. However, most of the work on privacy-preserving aggregation provides privacy guarantees for only one communication round in FL. In fact, as FL usually involves long-term training, i.e., multiple rounds, it may lead to more information leakages due to the dynamic user participation over rounds. In this connection, we propose a long-term privacy-preserving aggregation (LTPA) protocol providing both single-round and multi-round privacy guarantees. Specifically, we first introduce our batch-partitioning-dropping-updating (BPDU) strategy that enables any user-dynamic FL system to provide multi-round privacy guarantees. Then we present our LTPA construction which integrates our proposed BPDU strategy with the state-of-the-art privacy-preserving aggregation protocol. Furthermore, we investigate the impact of LTPA parameter settings on the trade-off between privacy guarantee, protocol efficiency, and FL convergence performance from both theoretical and experimental perspectives. Experimental results show that LTPA provides similar complexity to that of the state-of-the-art, i.e., an additional cost of around only 1.04X for a 100,000-user FL system, with an additional long-term privacy guarantee.","1556-6021","","10.1109/TIFS.2023.3266919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102279","Long-term privacy;user-dynamics;privacy-preserving aggregation;federated learning","Privacy;Protocols;Servers;Convergence;Computational modeling;Complexity theory;Training","","1","","53","IEEE","13 Apr 2023","","","IEEE","IEEE Journals"
"ResNeXt+: Attention Mechanisms Based on ResNeXt for Malware Detection and Classification","Y. He; X. Kang; Q. Yan; E. Li","Guangdong Key Laboratory of Information Security Technology, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Information Security Technology, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science, Bridgewater State University, Bridgewater, MA, USA","IEEE Transactions on Information Forensics and Security","30 Nov 2023","2024","19","","1142","1155","Malware detection and classification are crucial for protecting digital devices and information systems. Accurate identification of malware enables researchers and incident responders to take prompt measures against malware and mitigate its damage. With the development of attention mechanisms in the field of computer vision, attention mechanism-based malware detection techniques are also rapidly evolving. The essence of the attention mechanism is to focus on the information of interest and suppress the useless information. In this paper, we develop different plug-and-play attention mechanisms based on the ResNeXt tagging model, where the designed model is trained to focus on the malware features by capturing the malware image channel perception field of view and is also able to provide more helpful and flexible information than other methods. We have named this designed neural network ResNeXt+, and its core modules are built with different plug-and-play attention mechanisms. Extensive experimental results show that ResNeXt+ is effective and efficient in malware detection and classification with high classification accuracy. The proposed methods outperform the state-of-the-art techniques with seven benchmark datasets. Cross-dataset experiments conducted on the Windows and Android datasets, with an accuracy of 90.64% on cross-dataset detection of the android. Ablation experiments are also conducted on seven datasets, which demonstrate that attention mechanisms can improve malware detection and classification accuracy.","1556-6021","","10.1109/TIFS.2023.3328431","NSFC(grant numbers:62072484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299708","Malware detection and classification;attention mechanism;ResNeXt+;cross-dataset;ablation experiments","Malware;Feature extraction;Deep learning;Mathematical models;Convolution;Computational modeling;Neural networks","","1","","63","IEEE","30 Oct 2023","","","IEEE","IEEE Journals"
"Distributed Differential Privacy via Shuffling Versus Aggregation: A Curious Study","Y. Wei; J. Jia; Y. Wu; C. Hu; C. Dong; Z. Liu; X. Chen; Y. Peng; S. Wang","College of Cyber Science and the College of Computer Science, Key Laboratory of Data and Intelligent System Security, Ministry of Education, Nankai University, Tianjin, China; College of Cyber Science and the College of Computer Science, Key Laboratory of Data and Intelligent System Security, Ministry of Education, Nankai University, Tianjin, China; College of Cyber Science and the College of Computer Science, Key Laboratory of Data and Intelligent System Security, Ministry of Education, Nankai University, Tianjin, China; School of Cyberspace Security and the School of Cryptology, Hainan University, Haikou, China; Institute of Artificial Intelligence, Guangzhou University, Guangzhou, China; College of Cyber Science and the College of Computer Science, Key Laboratory of Data and Intelligent System Security, Ministry of Education, Nankai University, Tianjin, China; School of Cyber Engineering, Xidian University, Xi’an, China; Institute of Artificial Intelligence, Guangzhou University, Guangzhou, China; Institute of Artificial Intelligence, Guangzhou University, Guangzhou, China","IEEE Transactions on Information Forensics and Security","12 Jan 2024","2024","19","","2501","2516","How to achieve distributed differential privacy (DP) without a trusted central party is of great interest in both theory and practice. Recently, the shuffle model has attracted much attention. Unlike the local DP model in which the users send randomized data directly to the data collector/analyzer, in the shuffle model an intermediate untrusted shuffler is introduced to randomly permute the data, which have already been randomized by the users, before they reach the analyzer. The most appealing aspect is that while shuffling does not explicitly add more noise to the data, it can make privacy better. The privacy amplification effect in consequence means the users need to add less noise to the data than in the local DP model, but can achieve the same level of differential privacy. Thus, protocols in the shuffle model can provide better accuracy than those in the local DP model. What looks interesting to us is that the architecture of the shuffle model is similar to private aggregation, which has been studied for more than a decade. In private aggregation, locally randomized user data are aggregated by an intermediate untrusted aggregator. Thus, our question is whether aggregation also exhibits some sort of privacy amplification effect? And if so, how good is this “aggregation model” in comparison with the shuffle model. We conducted the first comparative study between the two, covering privacy amplification, functionalities, protocol accuracy, and practicality. The results as yet suggest that the new shuffle model does not have obvious advantages over the old aggregation model. On the contrary, protocols in the aggregation model outperform those in the shuffle model, sometimes significantly, in many aspects.","1556-6021","","10.1109/TIFS.2024.3351474","National Natural Science Foundation of China(grant numbers:62072132,62261160651); National Key Research and Development Program of China(grant numbers:2020YFB1005700); Engineering and Physical Sciences Research Council of U.K.(grant numbers:EP/M013561/2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387292","Differential privacy;shuffle model;aggregation model","Protocols;Data models;Differential privacy;Privacy;Analytical models;Sensitivity;Computational modeling","","","","45","IEEE","9 Jan 2024","","","IEEE","IEEE Journals"
"AgrAmplifier: Defending Federated Learning Against Poisoning Attacks Through Local Update Amplification","Z. Gong; L. Shen; Y. Zhang; L. Y. Zhang; J. Wang; G. Bai; Y. Xiang","School of Information and Communication Technology, Griffith University, Southport, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; School of Information and Communication Technology, Griffith University, Southport, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia","IEEE Transactions on Information Forensics and Security","7 Dec 2023","2024","19","","1241","1250","The collaborative nature of federated learning (FL) poses a major threat in the form of manipulation of local training data and local updates, known as the Byzantine poisoning attack. To address this issue, many Byzantine-robust aggregation rules (AGRs) have been proposed to filter out or moderate suspicious local updates uploaded by Byzantine participants. This paper introduces a novel approach called AGRAMPLIFIER, aiming to simultaneously improve robustness, fidelity, and efficiency of the existing AGRs. The core idea of AGRAMPLIFIER is to amplify the “morality” of local updates by identifying the most repressive features of each gradient update, which provides a clearer distinction between malicious and benign updates, consequently improving the detection effect. To achieve this objective, two approaches, namely AGRMP and AGRXAI, are proposed. AGRMP organizes local updates into patches and extracts the largest value from each patch, while AGRXAI leverages explainable AI methods to extract the gradient of the most activated features. By equipping AGRAMPLIFIER with the existing Byzantine-robust mechanisms, we successfully enhance the model robustness, maintaining its fidelity and improving overall efficiency. AGRAMPLIFIER is universally compatible with the existing Byzantine-robust mechanisms. The paper demonstrates its effectiveness by integrating it with all mainstream AGR mechanisms. Extensive evaluations conducted on seven datasets from diverse domains against seven representative poisoning attacks consistently show enhancements in robustness, fidelity, and efficiency, with average gains of 40.08%, 39.18%, and 10.68%, respectively.","1556-6021","","10.1109/TIFS.2023.3333555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10319733","Federated learning;Byzantine-robust aggregation;poisoning attack;explainable AI","Robustness;Predictive models;Training;Servers;Feature extraction;Computational modeling;Data models","","","","45","IEEE","16 Nov 2023","","","IEEE","IEEE Journals"
"Manipulating Pre-Trained Encoder for Targeted Poisoning Attacks in Contrastive Learning","J. Chen; Y. Gao; G. Liu; A. M. Abdelmoniem; C. Wang","Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Information Forensics and Security","12 Jan 2024","2024","19","","2412","2424","In recent years, contrastive learning has become very powerful for representation learning using large-scale unlabeled data, by involving pre-trained encoders to fine-tune downstream classifiers. However, the latest research indicates that contrastive learning can potentially suffer from the risks of data poisoning attacks, where the attacker injects maliciously crafted poisoned samples into the unlabeled pre-training data. To step forward, in this paper, we present a more stealthy poisoning attack dubbed PA-CL to directly poison the pre-trained encoder, such that the downstream classifier’s behavior on a single target instance to the attacker-desired class can be manipulated without affecting the overall downstream classification performance. We observe that a high similarity exists between the feature representation generated by the poisoned pre-trained encoder for the target sample and samples from the attacker-desired class. This leads to the downstream classifier misclassifying the target sample with the attacker-desired class. Therefore, we formulate our attack as an optimization problem, and design two novel loss functions, namely, the target effectiveness loss to effectively poison the pre-trained encoder, and the model utility loss to maintain the downstream classification performance. Experimental results on four real-world datasets demonstrate that the attack success rate of the proposed attack is 40% higher on average than that of the three baseline attacks, and the fluctuation of the downstream classifier’s prediction accuracy is within 5%.","1556-6021","","10.1109/TIFS.2024.3350389","National Natural Science Foundation of China(grant numbers:62272183,62171189,62002104,62071192); Key Research and Development Program of Hubei Province(grant numbers:2023BAB074); Special Fund for Wuhan Artificial Intelligence Innovation(grant numbers:2022010702040061); Special Fund for Wuhan Yellow Crane Talents (Excellent Young Scholar); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10381885","Targeted poisoning attack;contrastive learning;poisoned pre-trained encoder","Task analysis;Toxicology;Testing;Training;Feature extraction;Behavioral sciences;Pipelines","","","","56","IEEE","5 Jan 2024","","","IEEE","IEEE Journals"
"Publicly Verifiable Homomorphic Secret Sharing for Polynomial Evaluation","X. Chen; L. F. Zhang","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Information Forensics and Security","1 Aug 2023","2023","18","","4609","4624","There are two main security concerns in outsourcing computations. One is how to protect the privacy of the outsourced data, and the other is how to ensure the correctness of the outsourced computations. Homomorphic secret sharing (HSS) schemes allow a client to store a set of private data on two servers and then offload a computation on the data to servers. Such schemes ensure that each individual server learns no information about the data. While HSS schemes that allow the client to use a secret key to verify the correctness of the computation results exist and relieve both security concerns, the current literature lacks a publicly verifiable HSS scheme for polynomial evaluations. In this paper, we consider a two-server publicly verifiable HSS (PVHSS) model, where any third party can use a public key to perform verifications. We propose both a basic construction and an improved construction of PVHSS for evaluating polynomials. Our PVHSS ensures that no single server is able to learn any information about the outsourced data or persuade the verifier to accept a wrong result. We also implement the proposed scheme. For polynomials of degree  $\leq 20$ , our experiments show that: 1) the proposed PVHSS is  $2\times $ - $144\times $  faster than the existing non-verifiable or privately verifiable HSS on the server-side; 2) the proposed PVHSS is friendly to resource-restricted clients and takes less than  $\rm 27ms$  to reconstruct and verify the results.","1556-6021","","10.1109/TIFS.2023.3298258","Natural Science Foundation of Shanghai(grant numbers:21ZR1443000); National Natural Science Foundation of China(grant numbers:61602304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190747","Homomorphic secret sharing;public verification;polynomial evaluation","Servers;Encoding;Actuators;Public key;Data privacy;Control systems;Computational modeling","","","","58","IEEE","24 Jul 2023","","","IEEE","IEEE Journals"
"Unsupervised NIR-VIS Face Recognition via Homogeneous-to-Heterogeneous Learning and Residual-Invariant Enhancement","Y. Yang; W. Hu; H. Hu","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Electrical and Electronic Engineering (EEE), Nanyang Technological University, Jurong West, Singapore; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Information Forensics and Security","3 Jan 2024","2024","19","","2112","2126","Near-Infrared and Visible light (NIR-VIS) face recognition methods have achieved remarkable success in the fields of security surveillance, criminal investigation, and multimedia information retrieval. But the existing methods heavily rely on carefully annotated labels, leading to expensive manual labelling consumption and deployment flexibility. This motivates us to design unsupervised methods to address NIR-VIS recognition without relying on label information. To this end, we propose a novel homogeneous-to-HEterogeneous learning and Residual-invariant Enhancement (HERE) network for Unsupervised NIR-VIS Heterogeneous Face Recognition (NIR-VIS-UHFR). As the name suggests, the optimization of HERE follow a ”homogeneous-to-heterogeneous learning” strategy to fully explore complementary and common semantic information across different modalities. During the homogeneous learning phase, Modality-Adversarial Contrastive Learning (MACL) leverages the collaboration of modality contrastive learning and adversarial learning. On the one hand, MACL learns compact and discriminative intra-modal representations for NIR and VIS data, respectively. On the other hand, MACL guarantees that NIR-VIS data conform to the common feature distribution in a shared feature space, effectively reducing modal differences even in the absence of identity information between modalities. In the heterogeneous learning phase, K-reciprocal-Encoding-based Cross-modal Labeling (KECL) is introduced as robust pseudo label estimation to fully explore cross-modal relationships and group cross-modal features into clusters. With the pseudo labels provided by KECL, Refined cross-modal Contrastive Learning (RCL) is developed with modality-invariant averaging initialization and dynamic focus weighting strategies to extract modality-invariant features. Finally, Residual-invariant Representations Enhancement (RRE) mines partial features under the cross-modal face for robust matching. Compared to supervised methods, our unsupervised HERE demonstrates comparable performance on multiple datasets, greater scalability and practicality in deployment by reducing data acquisition requirements and costs.","1556-6021","","10.1109/TIFS.2023.3346176","National Natural Science Foundation of China(grant numbers:62076262,61673402,61273270,60802069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371345","NIR-VIS face recognition;unsupervised learning;contrastive learning;residual-invariant enhancement","Face recognition;Feature extraction;Task analysis;Labeling;Semantics;Unsupervised learning;Faces","","","","70","IEEE","22 Dec 2023","","","IEEE","IEEE Journals"
"Subject-Level Membership Inference Attack via Data Augmentation and Model Discrepancy","Y. Liu; P. Jiang; L. Zhu","School of Cyberspace Science and Technology and the School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Information Forensics and Security","6 Oct 2023","2023","18","","5848","5859","Federated learning (FL) models are vulnerable to membership inference attacks (MIAs), and the requirement of individual privacy motivates the protection of subjects where the individual data is distributed across multiple users in the cross-silo FL setting. In this paper, we propose a subject-level membership inference attack based on data augmentation and model discrepancy. It can effectively infer whether the data distribution of the target subject has been sampled and used for training by specific federated users, even if other users (also) may sample from the same subject and use it as part of their training set. Specifically, the adversary uses a generative adversarial network (GAN) to perform data augmentation on a small amount of priori federation-associated information known in advance. Subsequently, the adversary merges two different outputs from the global and tested user models using an optimal feature construction method. We simulate a controlled federation configuration and conduct extensive experiments on real datasets that include both image and categorical data. Results show that the area under the curve (AUC) is improved by 12.6% to 16.8% compared to the classical membership inference attack. This is at the expense of the test accuracy of the data augmented with GAN, which is at most 3.5% lower than the real test data. We also explore the degree of privacy leakage between overfitted models and well-generalized models in the cross-silo FL setting and conclude experimentally that the former is more likely to leak individual privacy with a subject-level degradation rate of up to 0.43. Finally, we present two possible defense mechanisms to attenuate this newly discovered privacy risk.","1556-6021","","10.1109/TIFS.2023.3318950","NSFC(grant numbers:62272038,U2241213,62172025); National Key Research and Development Program of China(grant numbers:2022YFB2702903); Beijing Natural Science Foundation(grant numbers:M23018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262058","Federated learning;subject-level membership inference attacks;privacy degradation;generative adversarial networks","Data models;Training;Data privacy;Privacy;Distributed databases;Degradation;Data augmentation","","","","38","IEEE","25 Sep 2023","","","IEEE","IEEE Journals"
"A Manifold Consistency Interpolation Method of Poisoning Attacks Against Semi-Supervised Model","X. Wang; X. Wang; M. He; M. Zhang; Z. Zhang","School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Information Forensics and Security","31 Aug 2023","2023","18","","5272","5285","Semi-Supervised Learning (SSL) is an influential derivative that allows humans to uncover invisible knowledge, potentially substituting it for extensive labeling data. Despite the optimism generated by the availability of unlabeled data, its potential unreliability can result in numerous unknown security risks. Assailants may covertly contaminate data, leading to potentially catastrophic and unpredictable outcomes. We investigate poisoning attacks in triangular manifolds to understand how SSL models defend against attacks resulting from small perturbations. By inserting tiny amounts of artificially modified samples totaling 2% of the entire training set, we can deceive classification models into altering the prediction results of arbitrary categories. In addition, considering that the poisoned data in practical scenarios belong to a minority sample attack, which is typically only about 0.1%-2% of the total data, we employed outlier detection to examine all inserted instances and discovered that it could bypass discovery. Our poisoning strategy can work across multiple datasets, models, and application domains of images and network traffic. Experimental results prove that our proposed method is effective on at least seven semi-supervised models. The declining ratio of model detection accuracy of autoencoder with confidence (ConAE) is 52.55% at the lowest cost. Another persuasive result is that our model poisoning exceeds the state-of-the-art methods in the image domain. The extended conclusion corroborates that the more accurate classification models do not have a corresponding improvement in their ability to resist interference, which also provides a new standard for testing model robustness.","1556-6021","","10.1109/TIFS.2023.3268882","National Natural Science Foundation of China(grant numbers:62227805,62071056,30603020403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105919","Data poisons;manifolds assuming;offline network flows;interpolation consistency;semi-supervised learning","Data models;Manifolds;Training;Interpolation;Predictive models;Supervised learning;Analytical models","","","","42","IEEE","20 Apr 2023","","","IEEE","IEEE Journals"
"Dual Consistency-Constrained Learning for Unsupervised Visible-Infrared Person Re-Identification","B. Yang; J. Chen; C. Chen; M. Ye","National Engineering Research Center for Multimedia Software, School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","27 Dec 2023","2024","19","","1767","1779","Unsupervised visible-infrared person re-identification (US-VI-ReID) aims at learning a cross-modality matching model under unsupervised conditions, which is an extremely important task for practical nighttime surveillance to retrieve a specific identity. Previous advanced US-VI-ReID works mainly focus on associating the positive cross-modality identities to optimize the feature extractor by off-line manners, inevitably resulting in error accumulation of incorrect off-line cross-modality associations in each training epoch due to the intra-modality and inter-modality discrepancies. They ignore the direct cross-modality feature interaction in the training process, i.e., the on-line representation learning and updating. Worse still, existing interaction methods are also susceptible to inter-modality differences, leading to unreliable heterogeneous neighborhood learning. To address the above issues, we propose a dual consistency-constrained learning framework (DCCL) simultaneously incorporating off-line cross-modality label refinement and on-line feature interaction learning. The basic idea is that the relations between cross-modality instance-instance and instance-identity should be consistent. More specifically, DCCL constructs an instance memory, an identity memory, and a domain memory for each modality. At the beginning of each training epoch, DCCL explores the off-line consistency of cross-modality instance-instance and instance-identity similarities to refine the reliable cross-modality identities. During the training, DCCL finds credible homogeneous and heterogeneous neighborhoods with on-line consistency between query-instance similarity and query-instance domain probability similarities for feature interaction in one batch, enhancing the robustness against intra-modality and inter-modality variations. Extensive experiments validate that our method significantly outperforms existing works, and even surpasses some supervised counterparts. The source code is available at https://github.com/yangbincv/DCCL.","1556-6021","","10.1109/TIFS.2023.3341392","National Natural Science Foundation of China(grant numbers:62071338,62176188,62306215); Key Research and Development Program of Hubei Province(grant numbers:2021BAA187,2022BCA009); Special Fund of Hubei Luojia Laboratory(grant numbers:220100015); China Postdoctoral Science Foundation(grant numbers:2023M732698,2023TQ0249); Interdisciplinary Innovative Talents Foundation from Renmin Hospital of Wuhan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10352335","Person re-identification;visible-infrared;unsupervised learning;cross-modality","Training;Cameras;Feature extraction;Data mining;Surveillance;Annotations;Task analysis","","","","66","IEEE","8 Dec 2023","","","IEEE","IEEE Journals"
"Breaking the Anonymity of Ethereum Mixing Services Using Graph Feature Learning","H. Du; Z. Che; M. Shen; L. Zhu; J. Hu","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Engineering and IT, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Information Forensics and Security","21 Nov 2023","2024","19","","616","631","With the property of helping users further enhance the anonymity of transactions, mixing services in blockchain have gained wide popularity in recent years. However, the strong untraceability offered by mixing services has led to the abuse of them by criminals for money laundering and committing fraud. These illegal actions pose significant threats to the blockchain ecosystem and financial order. In this paper, we focus on the problem of correlating the addresses of mixing transactions in Tornado Cash, a widely-used mixing service on Ethereum. We propose a graph neural network framework named MixBroker, which aims to break the anonymity of Tornado Cash by correlate mixing addresses from the perspective of node-pair link prediction. Specifically, we construct a Mixing Interaction Graph (MIG) using raw Ethereum mixing transaction data that can be used for subsequent analysis. To better represent the properties of mixing account nodes, we extract features from account nodes in the MIG from multiple perspectives. Furthermore, we design a GNN-based link prediction mechanism to serve as the backbone of MixBroker. This mechanism captures the interconnected nature of nodes within the MIG and calculates the probability of correlation between account nodes through node embeddings. In addition, to solve the problem of lacking ground-truth, we collect a large number of real mixing transactions of Ethereum in Tornado Cash and construct a ground-truth dataset by combining the principles of Ethereum Name Service (ENS). We conduct extensive experiments on the datasets, and the results demonstrate that MixBroker has a superior performance over other state-of-the-art methods on the address correlation problem in Ethereum mixing transactions.","1556-6021","","10.1109/TIFS.2023.3326984","National Key Research and Development Program of China(grant numbers:2020YFB1006100); China National Funds for Excellent Young Scientists(grant numbers:62222201); Beijing Nova Program(grant numbers:Z201100006820006,20220484174); NSFC Project(grant numbers:61972039); Beijing Natural Science Foundation(grant numbers:M23020,L222098,7232041); ARC Discovery(grant numbers:DP190103660,DP200103207); ARC Linkage(grant numbers:LP180100663); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292691","Blockchain;ethereum;mixing services;deep learning;graph neural networks","Tornadoes;Correlation;Feature extraction;Receivers;Blockchains;Topology;Representation learning","","","","40","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Fingerprint Liveness Detection Using Convolutional Neural Networks","R. F. Nogueira; R. de Alencar Lotufo; R. Campos Machado","Department of Computer Science, New York University, New York, NY, USA; Department of Electrical and Computer Engineering, University of Campinas, Campinas, Brazil; Center for Information Technology Renato Archer, Campinas, Brazil","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","6","1206","1213","With the growing use of biometric authentication systems in the recent years, spoof fingerprint detection has become increasingly important. In this paper, we use convolutional neural networks (CNNs) for fingerprint liveness detection. Our system is evaluated on the data sets used in the liveness detection competition of the years 2009, 2011, and 2013, which comprises almost 50 000 real and fake fingerprints images. We compare four different models: two CNNs pretrained on natural images and fine-tuned with the fingerprint images, CNN with random weights, and a classical local binary pattern approach. We show that pretrained CNNs can yield the state-of-the-art results with no need for architecture or hyperparameter selection. Data set augmentation is used to increase the classifiers performance, not only for deep architectures but also for shallow ones. We also report good accuracy on very small training sets (400 samples) using these large pretrained networks. Our best model achieves an overall rate of 97.1% of correctly classified samples-a relative improvement of 16% in test error when compared with the best previously published results. This model won the first prize in the fingerprint liveness detection competition 2015 with an overall accuracy of 95.5%.","1556-6021","","10.1109/TIFS.2016.2520880","Conselho Nacional de Desenvolvimento Científico e Tecnológico(grant numbers:311228/2014-3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390065","Fingerprint recognition;machine learning;supervised learning;neural networks","Feature extraction;Support vector machines;Principal component analysis;Kernel;Convolution;Fingerprint recognition;Neural networks","","261","2","45","IEEE","22 Jan 2016","","","IEEE","IEEE Journals"
"Detecting Android Malware Leveraging Text Semantics of Network Flows","S. Wang; Q. Yan; Z. Chen; B. Yang; C. Zhao; M. Conti","Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, Jinan, China; Department of Computer Science and Engineering, University of Nebraska–Lincoln, Lincoln, NE, USA; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, Jinan, China; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, Jinan, China; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, University of Jinan, Jinan, China; Department of Mathematics, University of Padova, Padua, Italy","IEEE Transactions on Information Forensics and Security","30 Jan 2018","2018","13","5","1096","1109","The emergence of malicious apps poses a serious threat to the Android platform. Most types of mobile malware rely on network interface to coordinate operations, steal users' private information, and launch attack activities. In this paper, we propose an effective and automatic malware detection method using the text semantics of network traffic. In particular, we consider each HTTP flow generated by mobile apps as a text document, which can be processed by natural language processing to extract text-level features. Then, we use the text semantic features of network traffic to develop an effective malware detection model. In an evaluation using 31 706 benign flows and 5258 malicious flows, our method outperforms the existing approaches, and gets an accuracy of 99.15%. We also conduct experiments to verify that the method is effective in detecting newly discovered malware, and requires only a few samples to achieve a good detection result. When the detection model is applied to the real environment to detect unknown applications in the wild, the experimental results show that our method performs significantly better than other popular anti-virus scanners with a detection rate of 54.81%. Our method also reveals certain malware types that can avoid the detection of anti-virus scanners. In addition, we design a detection system on encrypted traffic for bring-your-own-device enterprise network, home network, and 3G/4G mobile network. The detection model is integrated into the system to discover suspicious network behaviors.","1556-6021","","10.1109/TIFS.2017.2771228","National Natural Science Foundation of China(grant numbers:61672262,61573166,61472164,61572230,61702218,61702216); Natural Science Foundation of Shandong Province(grant numbers:ZR2014JL042,ZR2012FM010); Shandong Provincial Key R&D Program(grant numbers:2016GGX101001); CERNET Next Generation Internet Technology Innovation Project(grant numbers:NGII20160404); NSF(grant numbers:CNS-1566388); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101555","Malware detection;HTTP flow analysis;text semantics;machine learning","Malware;Mobile communication;Feature extraction;Mobile computing;Semantics;Mobile handsets;Protocols","","99","","42","IEEE","9 Nov 2017","","","IEEE","IEEE Journals"
"VeriFL: Communication-Efficient and Fast Verifiable Aggregation for Federated Learning","X. Guo; Z. Liu; J. Li; J. Gao; B. Hou; C. Dong; T. Baker","College of Cyber Science, the College of Computer Science, and the Tianjin Key Laboratory of Network and Data Security Technology, Nankai University, Tianjin, China; College of Cyber Science, the College of Computer Science, and the Tianjin Key Laboratory of Network and Data Security Technology, Nankai University, Tianjin, China; School of Computer Science, Guangzhou University, Guangzhou, China; College of Cyber Science, the College of Computer Science, and the Tianjin Key Laboratory of Network and Data Security Technology, Nankai University, Tianjin, China; College of Cyber Science, the College of Computer Science, and the Tianjin Key Laboratory of Network and Data Security Technology, Nankai University, Tianjin, China; School of Computing, Newcastle University, Newcastle upon Tyne, U.K.; Department of Computer Science, College of Computing and Informatics, University of Sharjah, Sharjah, United Arab Emirates","IEEE Transactions on Information Forensics and Security","31 Dec 2020","2021","16","","1736","1751","Federated learning (FL) enables a large number of clients to collaboratively train a global model through sharing their gradients in each synchronized epoch of local training. However, a centralized server used to aggregate these gradients can be compromised and forge the result in order to violate privacy or launch other attacks, which incurs the need to verify the integrity of aggregation. In this work, we explore how to design communication-efficient and fast verifiable aggregation in FL. We propose VeriFL, a verifiable aggregation protocol, with O(N) (dimension-independent) communication and O(N+ d) computation for verification in each epoch, where N is the number of clients and d is the dimension of gradient vectors. Since d can be large in some real-world FL applications (e.g., 100K), our dimension-independent communication is especially desirable for clients with limited bandwidth and high-dimensional gradients. In addition, the proposed protocol can be used in the FL setting where secure aggregation is needed or there is a subset of clients dropping out of protocol execution. Experimental results indicate that our protocol is efficient in these settings.","1556-6021","","10.1109/TIFS.2020.3043139","National Natural Science Foundation of China(grant numbers:62032012); National Key Research and Development Program of China(grant numbers:2020YFB1005700); CNKLSTISS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285303","Federated learning;verifiable aggregation;linearly homomorphic hash;commitment;machine learning","Training;Privacy;Protocols;Bandwidth;Collaborative work;Synchronization;Servers","","91","","58","IEEE","7 Dec 2020","","","IEEE","IEEE Journals"
"Random Partitioning Forest for Point-Wise and Collective Anomaly Detection—Application to Network Intrusion Detection","P. -F. Marteau","IRISA UMR CNRS 6074, Université Bretagne Sud, Campus de Tohannic, Vannes, France","IEEE Transactions on Information Forensics and Security","4 Feb 2021","2021","16","","2157","2172","In this paper, we propose DiFF-RF, an ensemble approach composed of random partitioning binary trees to detect point-wise and collective (as well as contextual) anomalies. Thanks to a distance-based paradigm used at the leaves of the trees, this semi-supervised approach solves a drawback that has been identified in the isolation forest (IF) algorithm. Moreover, taking into account the frequencies of visits in the leaves of the random trees allows to significantly improve the performance of DiFF-RF when considering the presence of collective anomalies. DiFF-RF is fairly easy to train, and good performance can be obtained by using a simple semi-supervised procedure to setup the extra hyper-parameter that is introduced. We first evaluate DiFF-RF on a synthetic data set to i) verify that the limitation of the IF algorithm is overcome, ii) demonstrate how collective anomalies are actually detected and iii) to analyze the effect of the meta-parameters it involves. We assess the DiFF-RF algorithm on a large set of datasets from the UCI repository, as well as four benchmarks related to network intrusion detection applications. Our experiments show that DiFF-RF almost systematically outperforms the IF algorithm and one of its extended variant, but also challenges the one-class SVM baseline, deep learning variational auto-encoder and ensemble of auto-encoder architectures. Finally, DiFF-RF is computationally efficient and can be easily parallelized on multi-core architectures.","1556-6021","","10.1109/TIFS.2021.3050605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319404","Random forest;machine learning;semi-supervised learning;anomaly detection;intrusion detection;NIDS","Vegetation;Anomaly detection;Forestry;Entropy;Complexity theory;Support vector machines;Partitioning algorithms","","42","","42","IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"Selective Audio Adversarial Example in Evasion Attack on Speech Recognition System","H. Kwon; Y. Kim; H. Yoon; D. Choi","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Electrical Engineering, Korea Military Academy, Seoul, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Medical Information, Kongju National University, Gongju, South Korea","IEEE Transactions on Information Forensics and Security","24 Sep 2019","2020","15","","526","538","Deep neural networks (DNNs) are widely used for image recognition, speech recognition, and other pattern analysis tasks. Despite the success of DNNs, these systems can be exploited by what is termed adversarial examples. An adversarial example, in which a small distortion is added to the input data, can be designed to be misclassified by the DNN while remaining undetected by humans or other systems. Such adversarial examples have been studied mainly in the image domain. Recently, however, studies on adversarial examples have been expanding into the voice domain. For example, when an adversarial example is applied to enemy wiretapping devices (victim classifiers) in a military environment, the enemy device will misinterpret the intended message. In such scenarios, it is necessary that friendly wiretapping devices (protected classifiers) should not be deceived. Therefore, the selective adversarial example concept can be useful in mixed situations, defined as situations in which there is both a classifier to be protected and a classifier to be attacked. In this paper, we propose a selective audio adversarial example with minimum distortion that will be misclassified as the target phrase by a victim classifier but correctly classified as the original phrase by a protected classifier. To generate such examples, a transformation is carried out to minimize the probability of incorrect classification by the protected classifier and that of correct classification by the victim classifier. We conducted experiments targeting the state-of-the-art DeepSpeech voice recognition model using Mozilla Common Voice datasets and the Tensorflow library. They showed that the proposed method can generate a selective audio adversarial example with a 91.67% attack success rate and 85.67% protected classifier accuracy.","1556-6021","","10.1109/TIFS.2019.2925452","National Research Foundation of Korea; National Research Foundation of Korea(grant numbers:2017R1A2B4006026); Institute for Information and communications Technology Promotion; Korean Government (MSIT) (2016-0-00173, Security Technologies for Financial Fraud Prevention on Fintech); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747397","Deep neural network (DNN);recursive neural network (RNN);adversarial example;machine learning;speech recognition","Hidden Markov models;Distortion;Mel frequency cepstral coefficient;Speech recognition;Image recognition;Distortion measurement;Neural networks","","40","","49","IEEE","27 Jun 2019","","","IEEE","IEEE Journals"
"Familial Clustering for Weakly-Labeled Android Malware Using Hybrid Representation Learning","Y. Zhang; Y. Sui; S. Pan; Z. Zheng; B. Ning; I. Tsang; W. Zhou","Centre for Artificial Intelligence and the School of Computer Science, University of Technology Sydney (UTS), Sydney, Australia; Centre for Artificial Intelligence and the School of Computer Science, University of Technology Sydney (UTS), Sydney, Australia; Faculty of Information Technology, Monash University, Clayton, Australia; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Centre for Artificial Intelligence and the School of Computer Science, University of Technology Sydney (UTS), Sydney, Australia; Centre for Artificial Intelligence and the School of Computer Science, University of Technology Sydney (UTS), Sydney, Australia; School of Computer Science, University of Technology Sydney (UTS), Sydney, Australia","IEEE Transactions on Information Forensics and Security","25 May 2020","2020","15","","3401","3414","Labeling malware or malware clustering is important for identifying new security threats, triaging and building reference datasets. The state-of-the-art Android malware clustering approaches rely heavily on the raw labels from commercial AntiVirus (AV) vendors, which causes misclustering for a substantial number of weakly-labeled malware due to the inconsistent, incomplete and overly generic labels reported by these closed-source AV engines, whose capabilities vary greatly and whose internal mechanisms are opaque (i.e., intermediate detection results are unavailable for clustering). The raw labels are thus often used as the only important source of information for clustering. To address the limitations of the existing approaches, this paper presents Andre, a new ANDroid Hybrid REpresentation Learning approach to clustering weakly-labeled Android malware by preserving heterogeneous information from multiple sources (including the results of static code analysis, the meta-information of an app, and the raw-labels of the AV vendors) to jointly learn a hybrid representation for accurate clustering. The learned representation is then fed into our outlier-aware clustering to partition the weakly-labeled malware into known and unknown families. The malware whose malicious behaviours are close to those of the existing families on the network, are further classified using a three-layer Deep Neural Network (DNN). The unknown malware are clustered using a standard density-based clustering algorithm. We have evaluated our approach using 5,416 ground-truth malware from Drebin and 9,000 malware from VirusShare (uploaded between Mar. 2017 and Feb. 2018), consisting of 3324 weakly-labeled malware. The evaluation shows that Andre effectively clusters weakly-labeled malware which cannot be clustered by the state-of-the-art approaches, while achieving comparable accuracy with those approaches for clustering ground-truth samples.","1556-6021","","10.1109/TIFS.2019.2947861","Australian Research(grant numbers:DE170101081,DP180102828,LP150100671,DP180100106); National Natural Science Foundation of China(grant numbers:61772055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8871171","Android malware;malware clustering;machine learning;neural network","Malware;Labeling;Engines;Neural networks;Standards;Clustering algorithms;Smart phones","","40","","56","IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"Android Malware Detection via (Somewhat) Robust Irreversible Feature Transformations","Q. Han; V. S. Subrahmanian; Y. Xiong","Department of Computer Science, Institute for Security Technology and Society, Dartmouth College, Hanover, USA; Department of Computer Science, Institute for Security Technology and Society, Dartmouth College, Hanover, USA; Department of Computer Science, Institute for Security Technology and Society, Dartmouth College, Hanover, USA","IEEE Transactions on Information Forensics and Security","26 Jun 2020","2020","15","","3511","3525","As the most widely used OS on earth, Android is heavily targeted by malicious hackers. Though much work has been done on detecting Android malware, hackers are becoming increasingly adept at evading ML classifiers. We develop  $\textsf {FARM}$ , a Feature transformation based AndRoid Malware detector.  $\textsf {FARM}$  takes well-known features for Android malware detection and introduces three new types of feature transformations that transform these features irreversibly into a new feature domain. We first test  $\textsf {FARM}$  on 6 Android classification problems separating goodware and “other malware” from 3 classes of malware: rooting malware, spyware, and banking trojans. We show that  $\textsf {FARM}$  beats standard baselines when no attacks occur. Though we cannot guess all possible attacks that an adversary might use, we propose three realistic attacks on  $\textsf {FARM}$  and show that  $\textsf {FARM}$  is very robust to these attacks in all classification problems. Additionally,  $\textsf {FARM}$  has automatically identified two malware samples which were not previously classified as rooting malware by any of the 61 anti-viruses on VirusTotal. These samples were reported to Google’s Android Security Team who subsequently confirmed our findings.","1556-6021","","10.1109/TIFS.2020.2975932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013026","Android;machine learning;feature transformation;malware detection;spyware;Banking Trojans;rooting malware","Malware;Androids;Humanoid robots;Feature extraction;Robustness;Spyware;Banking","","28","","42","IEEE","26 Feb 2020","","","IEEE","IEEE Journals"
"FiFTy: Large-Scale File Fragment Type Identification Using Convolutional Neural Networks","G. Mittal; P. Korus; N. Memon","Center for Cybersecurity, New York University Tandon School of Engineering, Brooklyn Borough, USA; Center for Cybersecurity, New York University Tandon School of Engineering, Brooklyn Borough, USA; Center for Cybersecurity, New York University Tandon School of Engineering, Brooklyn Borough, USA","IEEE Transactions on Information Forensics and Security","29 Jul 2020","2021","16","","28","41","We present FiFTy, a modern file-type identification tool for memory forensics and data carving. In contrast to previous approaches based on hand-crafted features, we design a compact neural network architecture, which uses a trainable embedding space. Our approach dispenses with the explicit feature extraction which has been a bottleneck in legacy systems. We evaluate the proposed method on a novel dataset with 75 file-types – the most diverse and balanced dataset reported to date. FiFTy consistently outperforms all baselines in terms of speed, accuracy and individual misclassification rates. We achieved an average accuracy of 77.5% with processing speed of  $\approx 38$  sec/GB, which is better and more than an order of magnitude faster than the previous state-of-the-art tool - Sceadan (69% at 9 min/GB). Our tool and the corresponding dataset is open-source.","1556-6021","","10.1109/TIFS.2020.3004266","New York University (NYU) Abu Dhabi, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122499","File-type classification;memory forensics;carving;machine learning;convolutional neural network","Feature extraction;Entropy;Transform coding;Tools;Artificial neural networks;Complexity theory;Computer architecture","","20","","35","IEEE","22 Jun 2020","","","IEEE","IEEE Journals"
"Exploring Feature Coupling and Model Coupling for Image Source Identification","Y. Huang; L. Cao; J. Zhang; L. Pan; Y. Liu","Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Information Forensics and Security","6 Jun 2018","2018","13","12","3108","3121","Recently, there has been great interest in feature-based image source identification. Previous statistical learning-based methods usually regarded the identification process as a classification problem. They assumed the dependence of features and the dependence of models. However, the two assumptions are usually problematic because of the genuine coupling of features and models. To address the issues, in this paper, we propose a novel image source identification scheme. For the feature coupling, a coupled feature representation is adopted to analyze the coupled interaction among features. The coupling relations among features and their powers are measured with Pearson’s correlations and integrated in a Taylor-like expansion manner. Regarding model coupling, a new coupled probability representation is developed. The model coupling relationships are characterized with conditional probabilities induced by the confusion matrix and then combined with the law of total probability. The experiments carried out on the Dresden image collection confirm the effectiveness of the proposed scheme. Via mining the feature coupling and model coupling, the identification accuracy can be significantly improved.","1556-6021","","10.1109/TIFS.2018.2838079","National Natural Science Foundation of China(grant numbers:61300077); Beijing Natural Science Foundation(grant numbers:4172054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361041","Image source identification;feature coupling;model coupling;machine learning;digital forensics","Couplings;Cameras;Feature extraction;Correlation;Image color analysis;Hardware;Support vector machines","","17","","48","IEEE","18 May 2018","","","IEEE","IEEE Journals"
"Secure Detection of Image Manipulation by Means of Random Feature Selection","Z. Chen; B. Tondi; X. Li; R. Ni; Y. Zhao; M. Barni","Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy","IEEE Transactions on Information Forensics and Security","4 Jun 2019","2019","14","9","2454","2469","We address the problem of data-driven image manipulation detection in the presence of an attacker with limited knowledge about the detector. Specifically, we assume that the attacker knows the architecture of the detector, the training data, and the class of features V the detector can rely on. In order to get an advantage in his race of arms with the attacker, the analyst designs the detector by relying on a subset of features chosen at random in V. Given its ignorance about the exact feature set, the adversary attacks a version of the detector based on the entire feature set. In this way, the effectiveness of the attack diminishes since there is no guarantee that attacking a detector working in the full feature space will result in a successful attack against the reduced-feature detector. We theoretically prove that, thanks to random feature selection, the security of the detector significantly increases at the expense of a negligible loss of performance in the absence of attacks. We also provide an experimental validation of the proposed procedure by focusing on the detection of two specific kinds of image manipulations, namely adaptive histogram equalization and median filtering. The experiments confirm the gain in security at the expense of a negligible loss of performance in the absence of attacks.","1556-6021","","10.1109/TIFS.2019.2901826","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800404); National Natural Science Foundation of China(grant numbers:61572052,61672090,U1736213); Fundamental Research Funds for the Central Universities(grant numbers:2018JBZ001); Key Research and Development of Hebei Province of China(grant numbers:17210332); Defense Advanced Research Projects Agency(grant numbers:FA8750-16-2-0173); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653427","Adversarial signal processing;adversarial machine learning;image manipulation detection;feature selection;multimedia forensics and counter-forensics;secure classification;randomization-based adversarial detection","Detectors;Feature extraction;Security;Image forensics;Tools;Information science","","16","","50","IEEE","26 Feb 2019","","","IEEE","IEEE Journals"
"DPPG: A Dynamic Password Policy Generation System","S. Yang; S. Ji; R. Beyah","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Information Forensics and Security","18 Dec 2017","2018","13","3","545","558","To keep password users from creating simple and common passwords, major websites and applications provide a password-strength measure, namely a password checker. While critical requirements for a password checker to be stringent have prevailed in the study of password security, we show that regardless of the stringency, such static checkers can leak information and actually help the adversary enhance the performance of their attacks. To address this weakness, we propose and devise the Dynamic Password Policy Generator, namely DPPG, to be an effective and usable alternative to the existing password strength checker. DPPG aims to enforce an evenly-distributed password space and generate dynamic policies for users to create passwords that are diverse and that contribute to the overall security of the password database. Since DPPG is modular and can function with different underlying metrics for policy generation, we further introduce a diversity-based password security metric that evaluates the security of a password database in terms of password space and distribution. The metric is useful as a countermeasure to well-crafted offline cracking algorithms and theoretically illustrates why DPPG works well.","1556-6021","","10.1109/TIFS.2017.2737971","Provincial Key Research and Development Program of Zhejiang, China(grant numbers:2016C01G2010916); Fundamental Research Funds for the Central Universities; Alibaba-Zhejiang University Joint Research Institute for Frontier Technologies(grant numbers:XT622017000118); CCF-Tencent Open Research Fund(grant numbers:AGR20160109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006310","Information security;authentication;computer security;network security;natural language processing;machine learning","Training;Security;Measurement;LinkedIn;Databases;Training data;Testing","","11","","33","IEEE","9 Aug 2017","","","IEEE","IEEE Journals"
"Minipatch: Undermining DNN-Based Website Fingerprinting With Adversarial Patches","D. Li; Y. Zhu; M. Chen; J. Wang","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","IEEE Transactions on Information Forensics and Security","6 Jul 2022","2022","17","","2437","2451","Website Fingerprinting (WF) enables a local passive attacker to infer which website a user is visiting over an encrypted connection. Classifiers utilizing deep neural networks (DNNs) automatically extract reliable features and have achieved up to 98% accuracy even against Tor. Since DNNs are known to be vulnerable to adversarial examples, several recent studies have exploited adversarial perturbations to defeat WF attacks. These defenses, however, require a high bandwidth overhead that typically exceeds 20% of the original traffic, prohibiting them from real-world deployment. Moreover, many studies on WF defense have been criticized for unrealistic assumptions such as full access to the target model and operating on the entire website trace. In this paper, we leverage adversarial patches—a special type of adversarial example that perturbs only local parts of the input—to control the overhead and enable black-box perturbation. In particular, we propose a new WF defense called Minipatch that injects extremely few dummy packets in real-time traffic to evade the attacker’s classifier. Experimental results demonstrate that Minipatch provides over 97% protection success rate with less than 5% bandwidth overhead, much lower than existing defenses. Moreover, we show that our adversarial patches remain effective in challenging settings, e.g., where dummy packets are injected only on the client-side and where perturbations are applied almost two months later. Finally, we also analyze several potential countermeasures and suggest ways to preserve perturbation effectiveness during deployment.","1556-6021","","10.1109/TIFS.2022.3186743","National Key Research and Development Program of China(grant numbers:2019QY1300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9808155","Traffic analysis;deep neural networks;adversarial machine learning;adversarial example","Perturbation methods;Real-time systems;Optimization;Neural networks;Deep learning;Bandwidth;Training","","6","","49","IEEE","27 Jun 2022","","","IEEE","IEEE Journals"
"Efficient Profiled Side-Channel Analysis of Masked Implementations, Extended","O. Bronchain; F. Durvaux; L. Masure; F. -X. Standaert","Crypto Group, ICTEAM Institute, Université Catholique de Louvain, Louvain-la-Neuve, Belgium; Crypto Group, ICTEAM Institute, Université Catholique de Louvain, Louvain-la-Neuve, Belgium; Crypto Group, ICTEAM Institute, Université Catholique de Louvain, Louvain-la-Neuve, Belgium; Crypto Group, ICTEAM Institute, Université Catholique de Louvain, Louvain-la-Neuve, Belgium","IEEE Transactions on Information Forensics and Security","9 Feb 2022","2022","17","","574","584","We extend the study of efficient profiled attacks on masking schemes initiated by Lerman and Markowitch (TIFS, 2019) in different directions. First, we study both the profiling complexity and the online attack complexity of different profiled distinguishers. Second, we extend the range of the noise levels of their experiments, in order to cover (higher-noise) contexts where masking is effective. Third, we further contextualize the investigated distinguishers (e.g., in terms of adversarial capabilities and a priori assumptions on the leakage probability density function). Finally, we complete the list of distinguishers considered in this previous work and add expectation-maximization, soft analytical side-channel attacks and multi-layer perceptrons in our comparisons. Our results allow shedding an interesting new light on the respective strengths and weaknesses of these different statistical tools, both in the context of a side-channel security evaluation and for concrete attacks. In particular, they confirm the experimental relevance of evaluation shortcuts leveraging the masking randomness during profiling, in order to speed up the evaluation process.","1556-6021","","10.1109/TIFS.2022.3144871","European Commission; Walloon Region through the European Research Council (ERC) Project SWORD(grant numbers:724725); Fonds européen de développement régional (FEDER) Project USERMedia(grant numbers:501907-379156); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9686688","Side-channel analysis;masking countermeasure;profiled attacks;machine learning;security evaluations","Complexity theory;Measurement;Side-channel attacks;Estimation;Noise level;Encoding;Signal to noise ratio","","6","","65","IEEE","19 Jan 2022","","","IEEE","IEEE Journals"
"Perturbation Inactivation Based Adversarial Defense for Face Recognition","M. Ren; Y. Zhu; Y. Wang; Z. Sun","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Postgraduate Department, China Academy of Railway Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","25 Aug 2022","2022","17","","2947","2962","Deep learning-based face recognition models are vulnerable to adversarial attacks. To curb these attacks, most defense methods aim to improve the robustness of recognition models against adversarial perturbations. However, the generalization capacities of these methods are quite limited. In practice, they are still vulnerable to unseen adversarial attacks. Deep learning models are fairly robust to general perturbations, such as Gaussian noises. A straightforward approach is to inactivate the adversarial perturbations so that they can be easily handled as general perturbations. In this paper, a plug-and-play adversarial defense method, named perturbation inactivation (PIN), is proposed to inactivate adversarial perturbations for adversarial defense. We discover that the perturbations in different subspaces have different influences on the recognition model. There should be a subspace, called the immune space, in which the perturbations have fewer adverse impacts on the recognition model than in other subspaces. Hence, our method estimates the immune space and inactivates the adversarial perturbations by restricting them to this subspace. The proposed method can be generalized to unseen adversarial perturbations since it does not rely on a specific kind of adversarial attack method. This approach not only outperforms several state-of-the-art adversarial defense methods but also demonstrates a superior generalization capacity through exhaustive experiments. Moreover, the proposed method can be successfully applied to four commercial APIs without additional training, indicating that it can be easily generalized to existing face recognition systems.","1556-6021","","10.1109/TIFS.2022.3195384","National Natural Science Foundation of China(grant numbers:U1836217,62006225,61622310,62071468); Strategic Priority Research Program of Chinese Academy of Sciences through the CAAI-Huawei Mindspore Open Fund(grant numbers:XDA27040700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845464","Adversarial machine learning;deep learning;graph neural network;face recognition","Face recognition;Perturbation methods;Robustness;Immune system;Principal component analysis;Deep learning;Training","","5","","73","IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"A2S2-GNN: Rigging GNN-Based Social Status by Adversarial Attacks in Signed Social Networks","X. Yin; W. Lin; K. Sun; C. Wei; Y. Chen","School of Information Science and Technology, Northwest University, Xi’an, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Kexin Sun is now with BYD Company, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","7 Dec 2022","2023","18","","206","220","Social status, the social influence of a user, plays an important role in many real-world applications, e.g., trust relations and information propagation in a social network. In this paper, we reveal the possibility of falsifying social status through adversarial attacks in graph neural networks (GNNs). Different from neural networks in the visual or speech domain, GNNs take the attributes of nodes and edges in a graph as features. To cater to the characteristics of GNNs, $\vphantom {_{\int }}$  we design a new paradigm of adversarial example attack, named  $A^{2} S^{2}$ - GNN ( $\mathbf {GNN}$ -based  $\mathbf {A}$ dversarial  $\mathbf {A}$ ttacks on  $\mathbf {S}$ ocial  $\mathbf {S}$ tatus), aiming at manipulating the social status of a target node in social networks. The key idea is to establish relationships or break relationships between a set of compromised nodes and the target node. More specifically, we consider a signed directed graph representing complicated positive/negative asymmetric relationships between nodes. We design an efficient adversarial attack algorithm to determine the minimum set of signed links that should be created or deleted to reach the attack objective. We conduct extensive experiments on baseline datasets. Compared with the benchmark algorithms,  $A^{2} S^{2}$ - GNN can effectively promote or vilify the social status of the target node up to 89.36% and 192.38%, respectively, while keeping the modification to the social network to the minimum. Furthermore, the experimental results on six status evaluating algorithms verify the transferability of our proposed attack algorithm.","1556-6021","","10.1109/TIFS.2022.3219342","National Natural Science Foundation of China(grant numbers:61872295); Shaanxi Natural Science Foundation(grant numbers:2020JM-416); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9936655","Social computing;adversarial machine learning;white-box attack;graph neural networks","Social networking (online);Directed graphs;Visualization;Sun;Perturbation methods;Network topology;Internet","","4","","43","IEEE","3 Nov 2022","","","IEEE","IEEE Journals"
"Logit Margin Matters: Improving Transferable Targeted Adversarial Attack by Logit Calibration","J. Weng; Z. Luo; S. Li; N. Sebe; Z. Zhong","Department of Artificial Intelligence, Xiamen University, Xiamen, China; Department of Artificial Intelligence, Xiamen University, Xiamen, China; Department of Artificial Intelligence, Xiamen University, Xiamen, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Information Forensics and Security","15 Jun 2023","2023","18","","3561","3574","Previous works have extensively studied the transferability of adversarial samples in untargeted black-box scenarios. However, it still remains challenging to craft targeted adversarial examples with higher transferability than non-targeted ones. Recent studies reveal that the traditional Cross-Entropy (CE) loss function is insufficient to learn transferable targeted adversarial examples due to the issue of vanishing gradient. In this work, we provide a comprehensive investigation of the CE loss function and find that the logit margin between the targeted and untargeted classes will quickly obtain saturation in CE, which largely limits the transferability. Therefore, in this paper, we devote to the goal of continually increasing the logit margin along the optimization to deal with the saturation issue and propose two simple and effective logit calibration methods, which are achieved by downscaling the logits with a temperature factor and an adaptive margin, respectively. Both of them can effectively encourage optimization to produce a larger logit margin and lead to higher transferability. Besides, we show that minimizing the cosine distance between the adversarial examples and the classifier weights of the target class can further improve the transferability, which is benefited from downscaling logits via L2-normalization. Experiments conducted on the ImageNet dataset validate the effectiveness of the proposed methods, which outperform the state-of-the-art methods in black-box targeted attacks. The source code is available at Link.","1556-6021","","10.1109/TIFS.2023.3284649","National Natural Science Foundation of China(grant numbers:62276221); Natural Science Foundation of Fujian Province of China(grant numbers:2022J01002); EU H2020 Project AI4Media(grant numbers:951911); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147340","Adversarial machine learning;convolutional neural networks;targeted adversarial examples;logit calibration","Closed box;Training;Optimization;Calibration;Perturbation methods;Neural networks;Residual neural networks","","3","","53","IEEE","9 Jun 2023","","","IEEE","IEEE Journals"
"Fast Locally Optimal Detection of Targeted Universal Adversarial Perturbations","A. Goel; P. Moulin","Department of Electrical and Computer Engineering, University of Illinois at Urbana–Champaign, Champaign, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana–Champaign, Champaign, IL, USA","IEEE Transactions on Information Forensics and Security","17 May 2022","2022","17","","1757","1770","This paper proposes a locally-optimal generalized likelihood ratio test (LO-GLRT) for detecting targeted attacks on a classifier, where the attacks add a norm-bounded targeted universal adversarial perturbation (UAP) to the classifier’s input. The paper includes both an analysis of the test as well as its empirical evaluation. The analysis provides an expression for the approximate lower bound of the detection probability, and the empirical evaluation shows this approximation to be similar to the actual detection probability. Since the LO-GLRT requires the score function of the input distribution, which is usually unknown in practice, we study the LO-GLRT for a learned surrogate input distribution. Specifically, we use a Gaussian distribution over the input subvectors as the surrogate distribution, for its mathematical tractability and computational efficiency. We evaluate the detector for several popular image classifiers and datasets, and compare the statistical and computational performance with the perturbation rectifying network (PRN) detector, another successful approach for detecting the UAPs. The LO-GLRT outperforms the PRN detector on both counts, with a running time at least 100 times lower than that of the PRN detector.","1556-6021","","10.1109/TIFS.2022.3169922","US National Science Foundation(grant numbers:CCF-1527388); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762886","Universal adversarial perturbations;locally optimal detection;deep neural networks;image classification;adversarial machine learning","Perturbation methods;Detectors;Training;Robustness;Random variables;Principal component analysis;Support vector machines","","3","","63","CCBY","25 Apr 2022","","","IEEE","IEEE Journals"
"pvCNN: Privacy-Preserving and Verifiable Convolutional Neural Network Testing","J. Weng; J. Weng; G. Tang; A. Yang; M. Li; J. -N. Liu","College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; Pazhou Laboratory, Guangzhou, China","IEEE Transactions on Information Forensics and Security","13 Apr 2023","2023","18","","2218","2233","We propose a new approach for privacy-preserving and verifiable convolutional neural network (CNN) testing in a distrustful multi-stakeholder environment. The approach is aimed to enable that a CNN model developer convinces a user of the truthful CNN performance over non-public data from multiple testers, while respecting model and data privacy. To balance the security and efficiency issues, we appropriately integrate three tools with the CNN testing, including collaborative inference, homomorphic encryption (HE) and zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK). We start with strategically partitioning a CNN model into a private part kept locally by the model developer, and a public part outsourced to an outside server. Then, the private part runs over the HE-protected test data sent by a tester, and transmits its outputs to the public part for accomplishing subsequent computations of the CNN testing. Second, the correctness of the above CNN testing is enforced by generating zk-SNARK based proofs, with an emphasis on optimizing proving overhead for two-dimensional (2-D) convolution operations, since the operations dominate the performance bottleneck during generating proofs. We specifically present a new quadratic matrix program (QMP)-based arithmetic circuit with a single multiplication gate for expressing 2-D convolution operations between multiple filters and inputs in a batch manner. Third, we aggregate multiple proofs with respect to a same CNN model but different testers’ test data (i.e., different statements) into one proof, and ensure that the validity of the aggregated proof implies the validity of the original multiple proofs. Lastly, our experimental results demonstrate that our QMP-based zk-SNARK performs nearly  $13.9\times $  faster than the existing quadratic arithmetic program (QAP)-based zk-SNARK in proving time, and  $17.6\times $  faster in Setup time, for high-dimension matrix multiplication. Besides, the limitation on handling a bounded number of multiplications of QAP-based zk-SNARK is relieved.","1556-6021","","10.1109/TIFS.2023.3262932","National Natural Science Foundation of China(grant numbers:61825203); National Key Research and Development Plan of China(grant numbers:2020YFB1005600); Major Program of Guangdong Basic and Applied Research Project(grant numbers:2019B030302008); Guangdong Provincial Science and Technology Project(grant numbers:2021A0505030033); National Joint Engineering Research Center of Network Security Detection and Protection Technology; Guangdong Key Laboratory of Data Security and Privacy Preserving; Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101090004,2020B0101360001); National Key Research and Development Program of China(grant numbers:2021ZD0112802); National Natural Science Foundation of China(grant numbers:62072215); National Natural Science Foundation of China(grant numbers:62102166,62032025); Guangdong Provincial Science and Technology Project(grant numbers:2020A1515111175); Science and Technology Major Project of Tibet Autonomous Region(grant numbers:XZ202201ZD0006G); National Natural Science Foundation of China(grant numbers:62102165); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086653","Machine learning;convolutional neural networks (CNNs);zero knowledge proof;data privacy","Convolutional neural networks;Testing;Data models;Manganese;Computational modeling;Closed box;Face recognition","","2","","57","IEEE","29 Mar 2023","","","IEEE","IEEE Journals"
"Dictionary Attacks on Speaker Verification","M. Marras; P. Korus; A. Jain; N. Memon","Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy; Tandon School of Engineering, New York University, Brooklyn, NY, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA","IEEE Transactions on Information Forensics and Security","26 Dec 2022","2023","18","","773","788","In this paper, we propose dictionary attacks against speaker verification-a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.","1556-6021","","10.1109/TIFS.2022.3229583","National Science Foundation(grant numbers:1956200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987527","Authentication;biometrics (access control);speaker recognition;adversarial machine learning;impersonation attacks","Dictionaries;Psychoacoustic models;Statistics;Sociology;Optimization;Fingerprint recognition;Perturbation methods","","2","","64","IEEE","15 Dec 2022","","","IEEE","IEEE Journals"
"SAFELearning: Secure Aggregation in Federated Learning With Backdoor Detectability","Z. Zhang; J. Li; S. Yu; C. Makaya","Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; HP Inc., Palo Alto, CA, USA","IEEE Transactions on Information Forensics and Security","8 Jun 2023","2023","18","","3289","3304","For model privacy, local model parameters in federated learning shall be obfuscated before sent to the remote aggregator. This technique is referred to as secure aggregation. However, secure aggregation makes model poisoning attacks such as backdooring more convenient given that existing anomaly detection methods mostly require access to plaintext local models. This paper proposes a new federated learning technique SAFELearning to support backdoor detection for secure aggregation. We achieve this through two new primitives -oblivious random grouping (ORG) and partial parameter disclosure (PPD). ORG partitions participants into one-time random subgroups with group configurations oblivious to participants; PPD allows secure partial disclosure of aggregated subgroup models for anomaly detection without leaking individual model privacy. ORG is based on our construction of several new primitives including tree-based random subgroup generation, oblivious secure aggregation, and randomized Diffie-Hellman key exchange. ORG can thwart colluding attackers from knowing each other’s group membership assignment with non-negligible advantage than random guess. Backdoor attacks are detected based on statistical distributions of the subgroup aggregated parameters of the learning iterations. SAFELearning can significantly reduce backdoor model accuracy without jeopardizing the main task accuracy under common backdoor strategies. Extensive experiments show SAFELearning is robust against malicious and faulty participants, whilst being more efficient than the state-of-art secure aggregation protocol in terms of both communication and computation costs.","1556-6021","","10.1109/TIFS.2023.3280032","NSF(grant numbers:ECCS#1923739); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136231","Federated learning;secure aggregation;backdoor attack;machine learning","Federated learning;Data models;Computational modeling;Privacy;Servers;Cryptography;Protocols","","1","","51","IEEE","25 May 2023","","","IEEE","IEEE Journals"
"PLCPrint: Fingerprinting Memory Attacks in Programmable Logic Controllers","M. M. Cook; A. K. Marnerides; D. Pezaros","School of Computing Science, University of Glasgow, Glasgow, U.K.; School of Computing Science, University of Glasgow, Glasgow, U.K.; School of Computing Science, University of Glasgow, Glasgow, U.K.","IEEE Transactions on Information Forensics and Security","12 Jun 2023","2023","18","","3376","3387","Programmable Logic Controllers (PLCs) constitute the functioning basis of Industrial Control Systems (ICS) and hence are often a focal point for attackers to exploit. Previous attacks have seen PLC memory maliciously altered in order to disrupt the underlying physical process. Different types of memory attack can cause a similar impact on the PLC’s operation and result in indistinguishable physical manifestations. Consequently, delays in triaging attacks through digital forensic practices can induce significant financial loss, physical damage to the infrastructure, and degradation of safety. In this work, we propose PLCPrint, a novel vendor-independent fingerprinting approach that utilises PLC memory artefacts to perform detection and classification of memory attacks. PLCPrint uses PLC memory register mapping, a novel method exploiting the relationship between PLC registers and memory artefacts including the PLC application code. Through this, registers are assigned a Mapping Condition (MC) to indicate how they exist within the PLC memory artefacts. We evaluate the performance of PLCPrint over realistic emulations conducted at a real testbed emulating water filtration and distribution. Through PLCPrint we depict how MC deviations are utilised within supervised learning schemes such as to adequately classify PLC memory attacks with high accuracy performance. In general, we demonstrate that PLCPrint fills the gap in the context of attack technique triaging since this has been a missing element within current ICS forensics schemes.","1556-6021","","10.1109/TIFS.2023.3277688","Engineering and Physical Sciences Research Council (EPSRC) Industrial Cooperative Awards in Science & Technology (CASE) Award(grant numbers:EP/R511936/1); Defence Science and Technology Laboratory (Dstl) under the Defence and Security Accelerator (DASA) Contract Purchase Agreement (CPA)(grant numbers:PA0000000223); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130481","Anomaly classification;anomaly detection;cyber-security;digital forensics;industrial control systems;machine learning;programmable logic controllers","Registers;Fingerprint recognition;Codes;Integrated circuit modeling;Anomaly detection;Actuators;Digital forensics","","1","","32","IEEE","22 May 2023","","","IEEE","IEEE Journals"
"UCG: A Universal Cross-Domain Generator for Transferable Adversarial Examples","Z. Li; W. Wang; J. Li; K. Chen; S. Zhang","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, China","IEEE Transactions on Information Forensics and Security","5 Feb 2024","2024","19","","3023","3037","Generating transferable adversarial examples is a challenging issue in adversarial attacks. Existing works on transferable adversarial examples generation mainly focus on models with similar architectures and trained on the same data domain. However, in practice, information such as the model architecture type and training data domain is unlikely to be revealed in deployed models. In this work, we introduce the Universal Cross-domain Generator (UCG), a pioneering framework for transferable adversarial examples that is the first to simultaneously address both cross-domain and cross-architecture challenges in adversarial attacks. The design of UCG is mainly inspired by two key observations. First, there exists some commonality in attention regions even when the structures of models are different. Second, there exists prevalent instability of intermediate-feature maps across cross-domain models. We accordingly design an attention transfer mechanism and a roughness abatement mechanism to enhance the cross-architecture and cross-domain transferability of the generated adversarial examples. Moreover, we propose an integrated transformation processing technique to improve the transferability of the generated adversarial examples under different transformations. Experimental results demonstrate that, compared with state-of-the-art solutions, UCG improves the average transferable attack success rate by 14.6%, 7.8%, and 7.9% in the cross-architecture task (convolutional neural networks (CNNs) to vision transformers (ViTs)), coarse-grained cross-domain tasks, and fine-grained cross-domain tasks, respectively.","1556-6021","","10.1109/TIFS.2024.3352913","National Key Research and Development Program of China(grant numbers:2023YFB3106900); National Science Foundation of China(grant numbers:62272486,62372473,92270204); Hunan Provincial Natural Science Foundation of China(grant numbers:2023JJ30702); Beijing Natural Science Foundation(grant numbers:M22004); Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10388391","Adversarial machine learning;cross-architecture;cross-domain;generator;transferable adversarial examples","Generators;Data models;Iterative methods;Task analysis;Adaptation models;Training data;Perturbation methods","","1","","61","IEEE","11 Jan 2024","","","IEEE","IEEE Journals"
"Byzantines Can Also Learn From History: Fall of Centered Clipping in Federated Learning","K. Özfatura; E. Özfatura; A. Küpçü; D. Gunduz","KUIS AI Center, Koç University, İIstanbul, Turkey; IPC Laboratory, Imperial College London, London, U.K; Department of Computer Engineering, Koç University, İIstanbul, Turkey; IPC Laboratory, Imperial College London, London, U.K","IEEE Transactions on Information Forensics and Security","3 Jan 2024","2024","19","","2010","2022","The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients’ models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this work, we first expose vulnerabilities of the CC framework, and introduce a novel attack strategy that can circumvent the defences of CC and other robust aggregators and reduce their test accuracy up to %33 on best-case scenarios in image classification tasks. Then, we propose a new robust and fast defence mechanism that is effective against the proposed and other existing Byzantine attacks.","1556-6021","","10.1109/TIFS.2023.3345171","TÜBİITAK; Scientific and Technological Research Council of Turkey(grant numbers:119E088); UK Research and Innovation (UKRI) for the Project “AIR” (European Research Council (ERC)-Consolidator Grant)(grant numbers:EP/X030806/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366296","Federated learning;adversarial machine learning;deep learning","Task analysis;Robustness;Federated learning;Security;Training;Aggregates;Taxonomy","","","","58","IEEE","19 Dec 2023","","","IEEE","IEEE Journals"
"Watch Your Speed: Injecting Malicious Voice Commands via Time-Scale Modification","X. Ji; Q. Jiang; C. Li; Z. Shi; W. Xu","College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","13 Feb 2024","2024","19","","3366","3379","Existing adversarial example (AE) attacks against automatic speech recognition (ASR) systems focus on adding deliberate noises to input audio. In this paper, we propose a new attack that purely speeds up or slows down original audio instead of adding perturbations, and we call it Time-Scale Modification Adversarial Example (TSMAE). By investigating the impact of speed variation on 100, 000 pieces of audio clips, we found that misrecognition manifests in three categories: delete, substitution, and insertion. These are the accumulated results caused by the misrecognition of both the acoustic and language models inside an ASR system. Despite the challenges, i.e., ASR systems are typically black-box and reveal no gradient information, we managed to launch one-segment untargeted and targeted TSMAE attacks based on particle swarm optimization algorithms. Our untargeted attacks only require modifying the speed of one segment (e.g., 20 ms), and our targeted attacks can generate meaningful yet benign audio to cause an ASR system to output a malicious output, e.g., “open the door”. We validate the feasibility of TSMAE on two open-source ASR models (e.g., DeepSpeech and Sphinx) and four commercial ones (e.g., IBM, Google, Baidu, and iFLYTEK). Results show that our untargeted attack can successfully attack all 6 ASR models with one segment modification, and our targeted attack is robust to various factors, such as model versions and speech sources. Finally, both attacks can bypass existing open-source defense methods, and our insights call attention to the defense’s focus from coping with perturbation to emerging adversarial example attacks.","1556-6021","","10.1109/TIFS.2024.3352394","National Natural Science Foundation of China (NSFC)(grant numbers:61925109,62071428,62222114); Fundamental Research Funds for the Central Universities(grant numbers:226-2022-00223); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10387471","Speech recognition;time-scale modification;adversarial machine learning","Acoustics;Perturbation methods;Media;Feature extraction;Closed box;Automatic speech recognition;Web and internet services","","","","40","IEEE","10 Jan 2024","","","IEEE","IEEE Journals"
"Exploring Uncharted Waters of Website Fingerprinting","I. Karunanayake; J. Jiang; N. Ahmed; S. K. Jha","Institute for Cybersecurity (IFCYBER), University of New South Wales (UNSW), Sydney, NSW, Australia; Institute for Cybersecurity (IFCYBER), University of New South Wales (UNSW), Sydney, NSW, Australia; Institute for Cybersecurity (IFCYBER), University of New South Wales (UNSW), Sydney, NSW, Australia; Institute for Cybersecurity (IFCYBER), University of New South Wales (UNSW), Sydney, NSW, Australia","IEEE Transactions on Information Forensics and Security","22 Dec 2023","2024","19","","1840","1854","Amidst the rapid technological advancements of today, privacy and anonymity are facing increasing threats. Tor, one of the most widely used anonymity networks, enables users to browse the Internet without their activities being tracked. Extensive research has been conducted on both attacking and defending the anonymity of Tor users. Website Fingerprinting (WF) is one of the popular de-anonymisation techniques employed against Tor users. This paper presents two novel WF techniques based on Graph Neural Networks (GNNs) to explore two relatively understudied avenues of WF: the fingerprintability of Decentralised Applications (DApps) and the impact of reload traffic on WF. Due to the lack of publicly available datasets for DApp traffic and reload traffic suitable for WF, we collected five new datasets for our experiments. Our findings reveal that GNN-based techniques surpass the performance of state-of-the-art WF techniques when reload traffic is used. Meanwhile, certain high-performing state-of-the-art techniques exhibit a significant reduction in accuracy, more than 40%, when reload traffic is used instead of homepage traffic. Additionally, we identify that DApps are less susceptible to fingerprinting than conventional websites, leading to a 25% decrease in accuracy in some state-of-the-art WF techniques. While confirming prior research findings that GNN-based techniques can outperform existing techniques when accessing DApps via Chrome, we further demonstrate that using Tor to access DApps makes them even more difficult to fingerprint. Finally, we expect our datasets, four of which lack publicly available alternatives, will prove invaluable for future research.","1556-6021","","10.1109/TIFS.2023.3342607","Cyber Security Research Centre Limited (CSCRC) whose activities are partially funded by the Australian Government’s Cooperative Research Centres Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356094","Website fingerprinting;traffic analysis;Tor;anonymity;de-anonymization attacks;Graph Neural Networks (GNNs);decentralized applications;communication system traffic;overlay networks;dark web;machine learning algorithms;web services;fingerprint recognition","Fingerprint recognition;Decentralized applications;Browsers;Support vector machines;Social networking (online);Monitoring;Heuristic algorithms","","","","24","IEEE","13 Dec 2023","","","IEEE","IEEE Journals"
"A New Zero Knowledge Argument for General Circuits and Its Application","H. Duan; L. Xiang; X. Wang; P. Chu; C. Zhou","John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China; John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China; John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China; Student Innovation Center, Shanghai Jiao Tong University, Shanghai, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","4 Jul 2023","2023","18","","3906","3920","Verifying the correctness of computation without revealing the input is a critical issue intensively studied in real-world applications. The recent surge of zero knowledge arguments has been focusing on its efficiency and practicality. Among them, GKR-based arguments have received wide attention and become the foundation of many zero-knowledge proof protocols. However, GKR-based protocols are restricted to layered arithmetic circuits. We propose Terrace, a new, efficient zero-knowledge argument system for general circuits, based on GKR. By dynamically patching cross-layer claims to the original circuit for verification instead of verifying those claims separately, Terrace is able to reduce the total circuit size and thus enjoys a logarithmic factor less verification time and proof size. Terrace is further extended to include the verification of non-arithmetic operations by rewriting those claims in the multilinear extension form. Experimental results demonstrate that Terrace enjoys a competitive performance on efficiency, and shows great promise in enabling low-cost verification of neural networks.","1556-6021","","10.1109/TIFS.2023.3288454","NSF China(grant numbers:62272306,62032020,62136006); Shanghai Natural Science Foundation(grant numbers:21JC1403800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164203","Zero knowledge proofs;machine learning;neural networks;general circuits","Logic gates;Protocols;Cross layer design;Arithmetic;Indexes;Technological innovation;Costs","","","","32","IEEE","27 Jun 2023","","","IEEE","IEEE Journals"
"HODA: Hardness-Oriented Detection of Model Extraction Attacks","A. M. Sadeghzadeh; A. M. Sobhanian; F. Dehghan; R. Jalili","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Information Forensics and Security","7 Dec 2023","2024","19","","1429","1439","Model extraction attacks exploit the target model’s prediction API to create a surrogate model, allowing the adversary to steal or reconnoiter the functionality of the target model in the black-box setting. Several recent studies have shown that a data-limited adversaries with no or limited access to the samples from the target model’s training data distribution, can employ synthesized or semantically similar samples to conduct model extraction attacks. In this paper, we introduce the concept of hardness degree to characterize sample difficulty based on the concept of learning speed. The hardness degree of a sample depends on the epoch number at which the predicted label for that sample converges. We investigate the hardness degree of samples and demonstrate that the hardness degree histogram of a data-limited adversary’s sample sequence is differs significantly from that of benign users’ sample sequences. We propose Hardness-Oriented Detection Approach (HODA) to detect the sample sequences of model extraction attacks. Our results indicate that HODA can effectively detect model extraction attack sequences with a high success rate, using only 100 monitored samples. It outperforms all previously proposed methods for model extraction detection.","1556-6021","","10.1109/TIFS.2023.3320609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10266376","Model extraction;model stealing;adversarial machine learning;hardness of samples;adversarial examples","Training;Computational modeling;Predictive models;Data models;Closed box;Training data;Histograms","","","","42","IEEE","28 Sep 2023","","","IEEE","IEEE Journals"
"Machine Unlearning","L. Bourtoule; V. Chandrasekaran; C. A. Choquette-Choo; H. Jia; A. Travers; B. Zhang; D. Lie; N. Papernot",University of Toronto; University of Wisconsin-Madison; University of Toronto; University of Toronto; University of Toronto; University of Toronto; University of Toronto; University of Toronto,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","141","159","Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00019","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519428","","Training;Data privacy;Privacy;Limiting;Transfer learning;Training data;Stochastic processes","","122","","61","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Neutaint: Efficient Dynamic Taint Analysis with Neural Networks","D. She; Y. Chen; A. Shah; B. Ray; S. Jana",Columbia University; Columbia University; Columbia University; Columbia University; Columbia University,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","1527","1543","Dynamic taint analysis (DTA) is widely used by various applications to track information flow during runtime execution. Existing DTA techniques use rule-based taint-propagation, which is neither accurate (i.e., high false positive rate) nor efficient (i.e., large runtime overhead). It is hard to specify taint rules for each operation while covering all corner cases correctly. Moreover, the overtaint and undertaint errors can accumulate during the propagation of taint information across multiple operations. Finally, rule-based propagation requires each operation to be inspected before applying the appropriate rules resulting in prohibitive performance overhead on large real-world applications.In this work, we propose Neutaint, a novel end-to-end approach to track information flow using neural program embeddings. The neural program embeddings model the target's programs computations taking place between taint sources and sinks, which automatically learns the information flow by observing a diverse set of execution traces. To perform lightweight and precise information flow analysis, we utilize saliency maps to reason about most influential sources for different sinks. Neutaint constructs two saliency maps, a popular machine learning approach to influence analysis, to summarize both coarse-grained and fine-grained information flow in the neural program embeddings.We compare Neutaint with 3 state-of-the-art dynamic taint analysis tools. The evaluation results show that Neutaint can achieve 68% accuracy, on average, which is 10% improvement while reducing 40× runtime overhead over the second-best taint tool Libdft on 6 real world programs. Neutaint also achieves 61% more edge coverage when used for taint-guided fuzzing indicating the effectiveness of the identified influential bytes. We also evaluate Neutaint's ability to detect real world software attacks. The results show that Neutaint can successfully detect different types of vulnerabilities including buffer/heap/integer overflows, division by zero, etc. Lastly, Neutaint can detect 98.7% of total flows, the highest among all taint analysis tools.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152790","","Tools;Fuzzing;Runtime;Neural networks;Security;Performance analysis;Task analysis","","20","","64","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Pyramid: Enhancing Selectivity in Big Data Protection with Count Featurization","M. Lecuyer; R. Spahn; R. Geambasu; T. -K. Huang; S. Sen","Columbia University, New York, NY, US; Columbia University, New York, NY, US; Columbia University; Microsoft Research; Microsoft Research","2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","78","95","Protecting vast quantities of data poses a daunting challenge for the growing number of organizations that collect, stockpile, and monetize it. The ability to distinguish data that is actually needed from data collected ""just in case"" would help these organizations to limit the latter's exposure to attack. A natural approach might be to monitor data use and retain only the working-set of in-use data in accessible storage, unused data can be evicted to a highly protected store. However, many of today's big data applications rely on machine learning (ML) workloads that are periodically retrained by accessing, and thus exposing to attack, the entire data store. Training set minimization methods, such as count featurization, are often used to limit the data needed to train ML workloads to improve performance or scalability. We present Pyramid, a limited-exposure data management system that builds upon count featurization to enhance data protection. As such, Pyramid uniquely introduces both the idea and proof-of-concept for leveraging training set minimization methods to instill rigor and selectivity into big data management. We integrated Pyramid into Spark Velox, a framework for ML-based targeting and personalization. We evaluate it on three applications and show that Pyramid approaches state-of-the-art models while training on less than 1% of the raw data.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958572","","Training;Data models;Big Data;Data protection;Organizations;Lakes","","5","","68","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"WTAGRAPH: Web Tracking and Advertising Detection using Graph Neural Networks","Z. Yang; W. Pei; M. Chen; C. Yue",Colorado School of Mines; Colorado School of Mines; Appen; Colorado School of Mines,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1540","1557","Web tracking and advertising (WTA) nowadays are ubiquitously performed on the web, continuously compromising users’ privacy. Existing defense solutions, such as widely deployed blocking tools based on filter lists and alternative machine learning based solutions proposed in prior research, have limitations in terms of accuracy and effectiveness. In this work, we propose WTAGRAPH, a web tracking and advertising detection framework based on Graph Neural Networks (GNNs). We first construct an attributed homogenous multi-graph (AHMG) that represents HTTP network traffic, and formulate web tracking and advertising detection as a task of GNN-based edge representation learning and classification in AHMG. We then design four components in WTAGRAPH so that it can (1) collect HTTP network traffic, DOM, and JavaScript data, (2) construct AHMG and extract corresponding edge and node features, (3) build a GNN model for edge representation learning and WTA detection in the transductive learning setting, and (4) use a pre-trained GNN model for WTA detection in the inductive learning setting. We evaluate WTAGRAPH on a dataset collected from Alexa Top 10K websites, and show that WTAGRAPH can effectively detect WTA requests in both transductive and inductive learning settings. Manual verification results indicate that WTAGRAPH can detect new WTA requests that are missed by filter lists and recognize non-WTA requests that are mistakenly labeled by filter lists. Our ablation analysis, evasion evaluation, and real-time evaluation show that WTAGRAPH can have a competitive performance with flexible deployment options in practice.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833670","web tracking and advertising;privacy;graph neural networks","Representation learning;Privacy;Image edge detection;Telecommunication traffic;Manuals;Information filters;Graph neural networks","","5","","64","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"ELSA: Secure Aggregation for Federated Learning with Malicious Actors","M. Rathee; C. Shen; S. Wagh; R. A. Popa","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1961","1979","Federated learning (FL) is an increasingly popular approach for machine learning (ML) in cases where the training dataset is highly distributed. Clients perform local training on their datasets and the updates are then aggregated into the global model. Existing protocols for aggregation are either inefficient, or don’t consider the case of malicious actors in the system. This is a major barrier in making FL an ideal solution for privacy-sensitive ML applications. We present Elsa, a secure aggregation protocol for FL, which breaks this barrier - it is efficient and addresses the existence of malicious actors at the core of its design. Similar to prior work on Prio and Prio+, Elsa provides a novel secure aggregation protocol built out of distributed trust across two servers that keeps individual client updates private as long as one server is honest, defends against malicious clients, and is efficient end-to-end. Compared to prior works, the distinguishing theme in Elsa is that instead of the servers generating cryptographic correlations interactively, the clients act as untrusted dealers of these correlations without compromising the protocol’s security. This leads to a much faster protocol while also achieving stronger security at that efficiency compared to prior work. We introduce new techniques that retain privacy even when a server is malicious at a small added cost of 7-25% in runtime with negligible increase in communication over the case of semi-honest server. Our work improves end-to-end runtime over prior work with similar security guarantees by big margins - single-aggregator RoFL by up to 305x (for the models we consider), and distributed trust Prio by up to 8x.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179468","Secure-Federated-Learning;Secure-Aggregation;Malicious-Privacy","Training;Privacy;Protocols;Runtime;Correlation;Costs;Federated learning","","4","","116","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"XFL: Naming Functions in Binaries with Extreme Multi-label Learning","J. Patrick-Evans; M. Dannehl; J. Kinder","Research Institute CODE, Bundeswehr University, Munich, Germany; Research Institute CODE, Bundeswehr University, Munich, Germany; Research Institute CODE, Bundeswehr University, Munich, Germany","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","2375","2390","Reverse engineers benefit from the presence of identifiers such as function names in a binary, but usually these are removed for release. Training a machine learning model to predict function names automatically is promising but fundamentally hard: unlike words in natural language, most function names occur only once. In this paper, we address this problem by introducing eXtreme Function Labeling (XFL), an extreme multi-label learning approach to selecting appropriate labels for binary functions. XFL splits function names into tokens, treating each as an informative label akin to the problem of tagging texts in natural language. We relate the semantics of binary code to labels through Dexter, a novel function embedding that combines static analysis-based features with local context from the call graph and global context from the entire binary. We demonstrate that XFL/Dexter outperforms the state of the art in function labeling on a dataset of 10,047 binaries from the Debian project, achieving a precision of 83.5%. We also study combinations of XFL with alternative binary embeddings from the literature and show that Dexter consistently performs best for this task. As a result, we demonstrate that binary function labeling can be effectively phrased in terms of multi-label learning, and that binary function embeddings benefit from including explicit semantic features.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179439","Binary-Analysis;Reverse-Engineering;Representation-Learning;Extreme-Multi-label-Learning","Training;Semantics;Natural languages;XML;Binary codes;Static analysis;Predictive models","","1","","69","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"MPCAuth: Multi-factor Authentication for Distributed-trust Systems","S. Tan; W. Chen; R. Deng; R. A. Popa","University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","829","847","Systems with distributed trust have attracted growing research attention and seen increasing industry adoptions. In these systems, critical secrets are distributed across N servers, and computations are performed privately using secure multi-party computation (SMPC). Authentication for these distributed-trust systems faces two challenges. The first challenge is ease-of-use. Namely, how can an authentication protocol maintain its user experience without sacrificing security? To avoid a central point of attack, a client needs to authenticate to each server separately. However, this would require the client to authenticate N times for each authentication factor, which greatly hampers usability. The second challenge is privacy, as the client’s sensitive profiles are now exposed to all N servers under different trust domains, which creates N times the attack surface for the profile data.We present MPCAuth, a multi-factor authentication system for distributed-trust applications that address both challenges. Our system enables a client to authenticate to N servers independently with the work of only one authentication. In addition, our system is profile hiding, meaning that the client’s authentication profiles such as her email username, phone number, passwords, and biometric features are not revealed unless all servers are compromised. We propose secure and practical protocols for an array of widely adopted authentication factors, including email passcodes, SMS messages, U2F, security questions/passwords, and biometrics. Our system finds practical applications in the space of cryptocurrency custody and collaborative machine learning, and benefits future adoptions of distributed-trust applications.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179481","secure-multi-party-computation;multi-factor-authentication;distributed-trust-systems","Privacy;Protocols;Multi-factor authentication;Biometrics (access control);Authentication;User experience;Electronic mail","","","","90","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Surveylance: Automatically Detecting Online Survey Scams","A. Kharraz; W. Robertson; E. Kirda",University of Illinois Urbana-Champaign; Northeastern University; Northeastern University,"2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","70","86","Online surveys are a popular mechanism for performing market research in exchange for monetary compensation. Unfortunately, fraudulent survey websites are similarly rising in popularity among cyber-criminals as a means for executing social engineering attacks. In addition to the sizable population of users that participate in online surveys as a secondary revenue stream, unsuspecting users who search the web for free content or access codes to commercial software can also be exposed to survey scams. This occurs through redirection to websites that ask the user to complete a survey in order to receive the promised content or a reward. In this paper, we present SURVEYLANCE, the first system that automatically identifies survey scams using machine learning techniques. Our evaluation demonstrates that SURVEYLANCE works well in practice by identifying 8,623 unique websites involved in online survey attacks. We show that SURVEYLANCE is suitable for assisting human analysts in survey scam detection at scale. Our work also provides the first systematic analysis of the survey scam ecosystem by investigating the capabilities of these services, mapping all the parties involved in the ecosystem, and quantifying the consequences to users that are exposed to these services. Our analysis reveals that a large number of survey scams are easily reachable through the Alexa top 30K websites, and expose users to a wide range of security issues including identity fraud, deceptive advertisements, potentially unwanted programs (PUPs), malicious extensions, and malware.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418597","Malware;Web Security;Social Engineering Attacks;Machine learning","Logic gates;Security;Malware;Ecosystems;Biological system modeling;Companies","","24","","54","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"SecFloat: Accurate Floating-Point meets Secure 2-Party Computation","D. Rathee; A. Bhattacharya; R. Sharma; D. Gupta; N. Chandran; A. Rastogi",UC Berkeley; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","576","595","We build a library SecFloat for secure 2-party computation (2PC) of 32-bit single-precision floating-point operations and math functions. The existing functionalities used in cryptographic works are imprecise and the precise functionalities used in standard libraries are not crypto-friendly, i.e., they use operations that are cheap on CPUs but have exorbitant cost in 2PC. SecFloat bridges this gap with its novel crypto-friendly precise functionalities. Compared to the prior cryptographic libraries, SecFloat is up to six orders of magnitude more precise and up to two orders of magnitude more efficient. Furthermore, against a precise 2PC baseline, SecFloat is three orders of magnitude more efficient. The high precision of SecFloat leads to the first accurate implementation of secure inference. All prior works on secure inference of deep neural networks rely on ad hoc float-to-fixed converters. We evaluate a model where the fixed-point approximations used in privacy-preserving machine learning completely fail and floating-point is necessary. Thus, emphasizing the need for libraries like SecFloat.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833697","secure-2-party-computation;floating-point;privacy-preserving-machine-learning;math-libraries;secure-inference;privacy-preserving-proximity-testing;secure-multiparty-computation","Deep learning;Bridges;Privacy;Costs;Computational modeling;Neural networks;Libraries","","10","","74","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Employing Program Semantics for Malware Detection","S. Naval; V. Laxmi; M. Rajarajan; M. S. Gaur; M. Conti","Department of Computer Science and Engineering, Malaviya National Institute of Technology at Jaipur, Jaipur, India; Department of Computer Science and Engineering, Malaviya National Institute of Technology at Jaipur, Jaipur, India; Department of Security Engineering, City University London, London, U.K.; Department of Computer Science and Engineering, Malaviya National Institute of Technology at Jaipur, Jaipur, India; Department of Mathematics, University of Padua, Padua, Italy","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","12","2591","2604","In recent years, malware has emerged as a critical security threat. In addition, malware authors continue to embed numerous anti-detection features to evade the existing malware detection approaches. Against this advanced class of malicious programs, dynamic behavior-based malware detection approaches outperform the traditional signature-based approaches by neutralizing the effects of obfuscation and morphing techniques. The majority of dynamic behavior detectors rely on system-calls to model the infection and propagation dynamics of malware. However, these approaches do not account an important anti-detection feature of modern malware, i.e., systemcall injection attack. This attack allows the malicious binaries to inject irrelevant and independent system-calls during the program execution thus modifying the execution sequences defeating the existing system-call-based detection. To address this problem, we propose an evasion-proof solution that is not vulnerable to system-call injection attacks. Our proposed approach characterizes program semantics using asymptotic equipartition property (AEP) mainly applied in information theoretic domain. The AEP allows us to extract information-rich call sequences that are further quantified to detect the malicious binaries. Furthermore, the proposed detection model is less vulnerable to call-injection attacks as the discriminating components are not directly visible to malware authors. We run a thorough set of experiments to evaluate our solution and compare it with the existing system-call-based malware detection techniques. The results demonstrate that the proposed solution is effective in identifying real malware instances.","1556-6021","","10.1109/TIFS.2015.2469253","Marie Curie Fellowship funded by the European Commission under the agreement PCIG11GA2012321980; EU-India REACH Project ICI+/2014/342896; TENACE PRIN Project 20103P34XC funded by the Italian MIUR; “Project Tackling Mobile Malware with Innovative Machine Learning Techniques” funded by the University of Padua; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7206585","Malware;Malware Detection;System-calls;Semantically-relevant paths;System-call injection attacks;Malware;malware detection;system-calls;semantically-relevant paths;system-call injection attacks","Malware;Feature extraction;Semantics;Silicon;Tin;Forensics","","79","","56","IEEE","17 Aug 2015","","","IEEE","IEEE Journals"
"Security Vulnerabilities and Countermeasures for Target Localization in Bio-NanoThings Communication Networks","A. Giaretta; S. Balasubramaniam; M. Conti","Department of Mathematics, University of Padua, Padua, Italy; Nano Communication Centre, Tampere University of Technology, Tampere, Finland; Department of Mathematics, University of Padua, Padua, Italy","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","4","665","676","The emergence of molecular communication has provided an avenue for developing biological nanonetworks. Synthetic biology is a platform that enables reprogramming cells, which we refer to as Bio-NanoThings, that can be assembled to create nanonetworks. In this paper, we focus on specific Bio-NanoThings, i.e, bacteria, where engineering their ability to emit or sense molecules can result in functionalities, such as cooperative target localization. Although this opens opportunities, e.g., for novel healthcare applications of the future, this can also lead to new problems, such as a new form of bioterrorism. In this paper, we investigate the disruptions that malicious Bio-NanoThings (M-BNTs) can create for molecular nanonetworks. In particular, we introduce two types of attacks: 1) blackhole and 2) sentry attacks. In blackhole attack M-BNTs emit attractant chemicals to draw-in the legitimate Bio-NanoThings (L-BNTs) from searching for their target, while in the sentry attack, the M-BNTs emit repellents to disperse the L-BNTs from reaching their target. We also present a countermeasure that L-BNTs can take to be resilient to the attacks, where we consider two forms of decision processes that includes Bayes’ rule as well as a simple threshold approach. We run a thorough set of simulations to assess the effectiveness of the proposed attacks as well as the proposed countermeasure. Our results show that the attacks can significantly hinder the regular behavior of Bio-NanoThings, while the countermeasures are effective for protecting against such attacks.","1556-6021","","10.1109/TIFS.2015.2505632","Academy of Finland FiDiPro (Finnish Distinguished Professor) program, for the Project “Nanocommunication Networks,” 2012-2016; Finnish Academy Research Fellow program(grant numbers:284531); TENACE PRIN Project 20103P34XC funded by the Italian MIUR; Project “Tackling Mobile Malware with Innovative Machine Learning Techniques” funded by the University of Padua; Marie Curie Fellowship through the European Commission(grant numbers:PCIG11-GA-2012-321980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347397","Molecular Communication;Internet of Nano Things;Security;Bioterrorism;Molecular communication;Internet of Nano Things;security;bioterrorism","Nanobioscience;Microorganisms;Bioterrorism;Biological system modeling;Molecular communication;Chemicals","","41","","37","IEEE","4 Dec 2015","","","IEEE","IEEE Journals"
"Efficient Sorting of Homomorphic Encrypted Data With k-Way Sorting Network","S. Hong; S. Kim; J. Choi; Y. Lee; J. H. Cheon","Department of Mathematical Sciences, Seoul National University, Seoul, South Korea; Department of Mathematical Sciences, Seoul National University, Seoul, South Korea; Department of Mathematics, Sogang University, Seoul, South Korea; Department of Industrial Engineering, ITM Division, SeoulTech, Seoul, South Korea; CryptoLab, Seoul National University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","2 Sep 2021","2021","16","","4389","4404","In this study, we propose an efficient sorting method for encrypted data using fully homomorphic encryption (FHE). The proposed method extends the existing 2-way sorting method by applying the k-way sorting network for any prime k to reduce the depth in terms of comparison operation from O(log22 n) to O(klogk2 n), thereby improving performance for k slightly larger than 2, such as k=5. We apply this method to approximate FHE which is widely used due to its efficiency of homomorphic arithmetic operations. In order to build up the k-way sorting network, the k-sorter, which sorts k-numbers with a minimal comparison depth, is used as a building block. The approximate homomorphic comparison, which is the only type of comparison working on approximate FHE, cannot be used for the construction of the k-sorter as it is because the result of the comparison is not binary, unlike the comparison in conventional bit-wise FHEs. To overcome this problem, we propose an efficient k-sorter construction utilizing the features of approximate homomorphic comparison. Also, we propose an efficient construction of a k-way sorting network using cryptographic SIMD operations. To use the proposed method most efficiently, we propose an estimation formula that finds the appropriate k that is expected to reduce the total time cost when the parameters of the approximating comparisons and the performance of the operations provided by the approximate FHE are given. We also show the implementation results of the proposed method, and it shows that sorting 56 = 15625 data using 5-way sorting network can be about 23.3% faster than sorting 214 = 16384 data using 2-way.","1556-6021","","10.1109/TIFS.2021.3106167","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korean Government through Ministry of Science and ICT (MSIT) (Development and Library Implementation of Fully Homomorphic Machine Learning Algorithms Supporting Neural Network Learning over Encrypted Data)(grant numbers:2020-0-00840); National Research Foundation of Korea (NRF); Korean Government through MSIT(grant numbers:2019R1A2C4069769,2021R1F1A1045403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520302","Approximate homomorphic encryption;sorting network;parallel algorithm","Sorting;Approximation algorithms;Encryption;Tools;Servers;Libraries;Estimation","","11","","41","IEEE","20 Aug 2021","","","IEEE","IEEE Journals"
"On Privacy of Dynamical Systems: An Optimal Probabilistic Mapping Approach","C. Murguia; I. Shames; F. Farokhi; D. Nešić; H. V. Poor","Department of Mechanical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands; Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC, Australia; Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC, Australia; Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC, Australia; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Information Forensics and Security","9 Mar 2021","2021","16","","2608","2620","We address the problem of maximizing privacy of stochastic dynamical systems whose state information is released through quantized sensor data. In particular, we consider the setting where information about the system state is obtained using noisy sensor measurements. This data is quantized and transmitted to a (possibly untrustworthy) remote station through a public/unsecured communication network. We aim at keeping (part of) the state of the system private; however, because the network (and/or the remote station) might be unsecure, adversaries might have access to sensor data, which can be used to estimate the system state. To prevent such adversaries from obtaining an accurate state estimate, before transmission, we randomize quantized sensor data using additive random vectors, and send the corrupted data to the remote station instead. We design the joint probability distribution of these additive vectors (over a time window) to minimize the mutual information (our privacy metric) between some linear function of the system state (a desired private output) and the randomized sensor data for a desired level of distortion-how different quantized sensor measurements and distorted data are allowed to be. We pose the problem of synthesising the joint probability distribution of the additive vectors as a convex program subject to linear constraints. Simulation experiments are presented to illustrate our privacy scheme.","1556-6021","","10.1109/TIFS.2021.3055022","Australian Research Council (ARC)(grant numbers:DP170104099); NATO Science for Peace and Security (SPS) Programme(grant numbers:SPS.SFP G5479); Schmidt Data-X Grant from the Princeton Center for Statistics and Machine Learning; U.S. National Science Foundation(grant numbers:CCF-1908308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345708","Privacy;dynamical systems;quantization;mutual information","Privacy;Data privacy;Additives;Mutual information;Measurement;Probability distribution;Distortion measurement","","10","","57","IEEE","3 Feb 2021","","","IEEE","IEEE Journals"
"Modeling Enlargement Attacks Against UWB Distance Bounding Protocols","A. Compagno; M. Conti; A. A. D’Amico; G. Dini; P. Perazzo; L. Taponecco","Department of Computer Science, Sapienza University of Rome, Rome, Italy; Department of Mathematics, University of Padua, Padua, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","7","1565","1577","Distance bounding protocols make it possible to determine a trusted upper bound on the distance between two devices. Their key property is to resist reduction attacks, i.e., attacks aimed at reducing the distance measured by the protocol. Recently, researchers have also focused on enlargement attacks, aimed at enlarging the measured distance. Providing security against such attacks is important for secure positioning techniques. The contribution of this paper is to provide a probabilistic model for the success of an enlargement attack against a distance bounding protocol realized with the IEEE 802.15.4a ultra-wideband standard. The model captures several variables, such as the propagation environment, the signal-to-noise ratio, and the time-of-arrival estimation algorithm. We focus on non-coherent receivers, which can be used in low-cost low-power applications. We validate our model by comparison with physical-layer simulations and goodness-of-fit tests. The results show that our probabilistic model is sufficiently realistic to replace physical-layer simulations. Our model can be used to evaluate the security of the ranging/positioning solutions that can be subject to enlargement attacks. We expect that it will significantly facilitate future research on secure ranging and secure positioning.","1556-6021","","10.1109/TIFS.2016.2541613","EU TagItSmart! project(grant numbers:H2020-ICT30-2015-688061); EU-India REACH project(grant numbers:ICI+/2014/342-896); Italian MIURPRIN TENACE project(grant numbers:20103P34XC); “Analisi di dati sensoriali: dai sensori tradizionali ai sensori sociali” funded by “Progetti di Ricerca di Ateneo - PRA 2016” of the University of Pisa; “Tackling Mobile Malware with Innovative Machine Learning Techniques” funded by the University of Padua; Marie Curie Fellowship funded by the European Commission(grant numbers:PCIG11-GA-2012-321980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432025","Security;ranging;distance bounding;enlargement attacks;ultrawideband","Protocols;IEEE 802.15 Standard;Time of arrival estimation;Probabilistic logic;Mathematical model;Receivers;Delays","","10","5","22","IEEE","11 Mar 2016","","","IEEE","IEEE Journals"
"BAS-VAS: A Novel Secure Protocol for Value Added Service Delivery to Mobile Devices","N. Saxena; M. Conti; K. -K. R. Choo; N. S. Chaudhari","School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.; Department of Mathematics, University of Padua, Padova, Italy; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, USA; Discipline of Computer Science and Engineering, Indian Institute of Technology, Indore, Indore, India","IEEE Transactions on Information Forensics and Security","20 Dec 2019","2020","15","","1470","1485","Mobile operators offer a wide range of value-added services (VAS) to their subscribers (i.e., mobile users), which in turn generates around 15% of the telecommunication industry revenue. However, simultaneous VAS requests from a large number of mobile devices to a single server or a cluster in an internet-of-things (IoT) environment could result in an inefficient system, if these requests are handled one at a time as the present traditional cellular network scenario is. This will not only slow down the server's efficiency but also adversely impacts the performance of the network. The current (insecure) practice of transmitting user identity in plaintext also results in traceability. In this paper, we introduce the first known protocol designed to efficiently handle multiple VAS requests at one time, as well as ensuring the secure delivery of the services to a large number of requesting mobile users. The proposed batch verification protocol (BAS-VAS) is capable of authenticating multiple simultaneous requests received by a large number of mobile users. We demonstrate that the protocol preserves user privacy over the network. The provider's servers ensure the privacy of the requested service's priority by performing sorting over encrypted integer data. The simulation results also demonstrate that the proposed protocol is lightweight and efficient in terms of communication and computation overheads, protocol execution time, and batch and re-batch verification delay. Specifically, we perform batch and re-batch verification (after detecting and removing malicious requests from the batch) for multiple requests in order to improve the overall efficiency of the system, as well as discussing time, space and cost complexity analysis, along with the security proof of our protocol using Proverif.","1556-6021","","10.1109/TIFS.2019.2940908","European Commission(grant numbers:PCIG11-GA-2012-321980); TENACE PRIN(grant numbers:20103P34XC); Italian MIUR, and by the Project “Tackling Mobile Malware with Innovative Machine Learning Techniques”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839535","Authentication;batch verification;mobile user;privacy preservation;value added service","Protocols;Authentication;Servers;Mobile handsets;Cryptography","","4","","55","IEEE","16 Sep 2019","","","IEEE","IEEE Journals"
"m-Eligibility With Minimum Counterfeits and Deletions for Privacy Protection in Continuous Data Publishing","A. T. Nicolau; J. Parra-Arnau; J. Forné; E. Pallarés","Department of Network Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Department of Network Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Department of Network Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain; Department of Network Engineering, Universitat Politècnica de Catalunya, Barcelona, Spain","IEEE Transactions on Information Forensics and Security","2 Feb 2024","2024","19","","2854","2864","Continuous data publishing consists in the republication of updating microdata. The most relevant syntactic notions in continuous data publishing are based on m-invariance. This notion enforces that no user can be distinguished among, at least,  $m-1$  other users, each with distinct secret data. To achieve m-invariance, the existing methods must first alter the dataset to satisfy a property called m-eligibility. Essentially, a dataset can be made m-invariant if and only if it satisfies the m-eligibility constraint. Although guaranteeing the m-eligibility property is a crucial step, no theoretical study of the best strategies to achieve it has been carried out. This paper performs such a study by giving strategies and demonstrating their optimality under two approaches: insertion of counterfeit tuples and partial publication. The empirical evaluation of our proposal shows a significant reduction on the number of modifications needed to enforce m-eligbility of up to 41% with respect to the literature.","1556-6021","","10.1109/TIFS.2024.3354557","Spanish Government through the Project “Enhancing Communication Protocols with Machine Learning while Protecting Sensitive Data (COMPROMISE)” funded by MCIN/AEI/10.13039/501100011033(grant numbers:PID2020-113795RB-C31); Project “Anonymization Technology for AI-Based Analytics of Mobility Data (MOBILYTICS)” funded by MCIN/AEI/10.13039/501100004837(grant numbers:TED2021-129782B-I00); European Union “NextGenerationEU”/Plan de Recuperación, Transformación y Resiliencia (PRTR), funded by Generalitat de Catalunya, under Agència de Gestió d’Ajuts Universitaris i de Recerca (AGAUR)(grant numbers:2021 SGR 01413); Spanish Ministry of Science and Innovation and the European Union—NextGenerationEU/PRTR through the “Ramón y Cajal” Fellowship(grant numbers:RYC2021-034256-I); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400504","m-invariance;m-eligibility;syntactic privacy;data privacy;dynamic data","Syntactics;Publishing;Proposals;Privacy;Differential privacy;Semantics;Heuristic algorithms","","","","23","CCBY","15 Jan 2024","","","IEEE","IEEE Journals"
"Privacy Risks of General-Purpose Language Models","X. Pan; M. Zhang; S. Ji; M. Yang",Fudan University; Fudan University; Zhejiang University; Fudan University,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","1314","1331","Recently, a new paradigm of building general-purpose language models (e.g., Google’s Bert and OpenAI’s GPT-2) in Natural Language Processing (NLP) for text feature extraction, a standard procedure in NLP systems that converts texts to vectors (i.e., embeddings) for downstream modeling, has arisen and starts to find its application in various downstream NLP tasks and real world systems (e.g., Google’s search engine [6]). To obtain general-purpose text embeddings, these language models have highly complicated architectures with millions of learnable parameters and are usually pretrained on billions of sentences before being utilized. As is widely recognized, such a practice indeed improves the state-of-the-art performance of many downstream NLP tasks. However, the improved utility is not for free. We find the text embeddings from general-purpose language models would capture much sensitive information from the plain text. Once being accessed by the adversary, the embeddings can be reverse-engineered to disclose sensitive information of the victims for further harassment. Although such a privacy risk can impose a real threat to the future leverage of these promising NLP tools, there are neither published attacks nor systematic evaluations by far for the mainstream industry-level language models. To bridge this gap, we present the first systematic study on the privacy risks of 8 state-of-the-art language models with 4 diverse case studies. By constructing 2 novel attack classes, our study demonstrates the aforementioned privacy risks do exist and can impose practical threats to the application of general-purpose language models on sensitive data covering identity, genome, healthcare and location. For example, we show the adversary with nearly no prior knowledge can achieve about 75% accuracy when inferring the precise disease site from Bert embeddings of patients’ medical descriptions. As possible countermeasures, we propose 4 different defenses (via rounding, differential privacy, adversarial training and subspace projection) to obfuscate the unprotected embeddings for mitigation purpose. With extensive evaluations, we also provide a preliminary analysis on the utility-privacy trade-off brought by each defense, which we hope may foster future mitigation researches.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152761","","Privacy;Natural language processing;Google;Bioinformatics;Machine learning;Genomics;Training","","30","","82","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"Property Inference from Poisoning","S. Mahloujifar; E. Ghosh; M. Chase",Princeton University; Microsoft Research; Microsoft Research,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1120","1137","Property inference attacks consider an adversary who has access to a trained ML model and tries to extract some global statistics of the training data. In this work, we study property inference in scenarios where the adversary can maliciously control a part of the training data (poisoning data) with the goal of increasing the leakage. Previous works on poisoning attacks focused on trying to decrease the accuracy of models. Here, for the first time, we study poisoning attacks where the goal of the adversary is to increase the information leakage of the model. We show that poisoning attacks can boost the information leakage significantly and should be considered as a stronger threat model in sensitive applications where some of the data sources may be malicious.We theoretically prove that our attack can always succeed as long as the learning algorithm used has good generalization properties. Then we experimentally evaluate our on different datasets (Census dataset, Enron email dataset, MNIST and CelebA), properties (that are present in the training data as features, that are not present as features, and properties that are uncorrelated with the rest of the training data or classification task) and model architectures (including Resnet-18 and Resnet-50). We were able to achieve high attack accuracy with relatively low poisoning rate, namely, 2–3% poisoning in most of our experiments. We also evaluated our attacks on models trained with DP and we show that even with very small values for $\epsilon$, the attack is still quite successful1.1Code is available at https://github.com/smahloujifar/PropertyInferenceFromPoisoning.git","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833623","Poisoning-attacks;privacy;property-inference-attacks","Privacy;Soft sensors;Training data;Machine learning;Data models;Security;Data mining","","15","","32","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Telepath: A Minecraft-based Covert Communication System","Z. Sun; V. Shmatikov",Cornell Tech; Cornell Tech,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","2223","2237","Covert, censorship-resistant communication in the presence of nation-state adversaries requires unobservable channels whose operation is difficult to detect via network-traffic analysis. Traffic substitution, i.e., replacing data transmitted by a ""cover"" application with covert content, takes advantage of already-existing encrypted channels to produce traffic that is statistically indistinguishable from the traffic of the cover application and thus difficult to censor.Online games are a promising platform for building circumvention channels due to their popularity in many censored regions. We show, however, that previously proposed traffic substitution methods cannot be directly applied to games. Their traces, even if statistically similar to game traces, may violate game-specific invariants and are thus easy to detect because they could not have been generated by an actual gameplay.We explain how to identify non-disruptive content whose substitution does not result in client-server inconsistencies and use these ideas to design and implement Telepath, a covert communication system that uses Minecraft as the platform. Telepath takes advantage of (1) Minecraft’s encrypted client-server channel, (2) decentralized architecture that enables individual users to run their own servers, and (3) popularity of ""mods"" that add functionality to Minecraft clients and servers. Telepath runs a Minecraft game but substitutes non-disruptive in-game messages with covert content, without changing the game’s interaction with the network manager.We measure performance of Telepath for Web browsing and audio streaming, and show that network traffic generated by Telepath resists statistical traffic analysis that aims to distinguish it from popular Minecraft bots.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179335","Censorship-circumvention;Traffic-analysis;Online-games;Minecraft","Resistance;Privacy;Games;Resists;Telecommunication traffic;Machine learning;Censorship","","1","","45","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"CrypTFlow: Secure TensorFlow Inference","N. Kumar; M. Rathee; N. Chandran; D. Gupta; A. Rastogi; R. Sharma",Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research,"2020 IEEE Symposium on Security and Privacy (SP)","30 Jul 2020","2020","","","336","353","We present CrypTFlow, a first of its kind system that converts TensorFlow inference code into Secure Multi-party Computation (MPC) protocols at the push of a button. To do this, we build three components. Our first component, Athos, is an end-to-end compiler from TensorFlow to a variety of semihonest MPC protocols. The second component, Porthos, is an improved semi-honest 3-party protocol that provides significant speedups for TensorFlow like applications. Finally, to provide malicious secure MPC protocols, our third component, Aramis, is a novel technique that uses hardware with integrity guarantees to convert any semi-honest MPC protocol into an MPC protocol that provides malicious security. The malicious security of the protocols output by Aramis relies on integrity of the hardware and semi-honest security of MPC. Moreover, our system matches the inference accuracy of plaintext TensorFlow.We experimentally demonstrate the power of our system by showing the secure inference of real-world neural networks such as ResNet50 and DenseNet121 over the ImageNet dataset with running times of about 30 seconds for semi-honest security and under two minutes for malicious security. Prior work in the area of secure inference has been limited to semi-honest security of small networks over tiny datasets such as MNIST or CIFAR. Even on MNIST/CIFAR, CrypTFlow outperforms prior work.","2375-1207","978-1-7281-3497-0","10.1109/SP40000.2020.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152660","","Protocols;Cryptography;Hardware;Task analysis;Benchmark testing;Logistics","","89","","85","IEEE","30 Jul 2020","","","IEEE","IEEE Conferences"
"SoK: The Challenges, Pitfalls, and Perils of Using Hardware Performance Counters for Security","S. Das; J. Werner; M. Antonakakis; M. Polychronakis; F. Monrose",University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill; Georgia Institute of Technology; Stony Brook University; University of North Carolina at Chapel Hill,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","20","38","Hardware Performance Counters (HPCs) have been available in processors for more than a decade. These counters can be used to monitor and measure events that occur at the CPU level. Modern processors provide hundreds of hardware events that can be monitored, and with each new processor architecture more are added. Yet, there has been little in the way of systematic studies on how performance counters can best be utilized to accurately monitor events in real-world settings. Especially when it comes to the use of HPCs for security applications, measurement imprecisions or incorrect assumptions regarding the measured values can undermine the offered protection. To shed light on this issue, we embarked on a year-long effort to (i) study the best practices for obtaining accurate measurement of events using performance counters, (ii) understand the challenges and pitfalls of using HPCs in various settings, and (iii) explore ways to obtain consistent and accurate measurements across different settings and architectures. Additionally, we then empirically evaluated the way HPCs have been used throughout a wide variety of papers. Not wanting to stop there, we explored whether these widely used techniques are in fact obtaining performance counter data correctly. As part of that assessment, we (iv) extended the seminal work of Weaver and McKee from almost 10 years ago on non-determinism in HPCs, and applied our findings to 56 papers across various application domains. In that follow-up study, we found the acceptance of HPCs in security applications is in stark contrast to other application areas - especially in the last five years. Given that, we studied an additional representative set of 41 works from the security literature that rely on HPCs, to better elucidate how the intricacies we discovered can impact the soundness and correctness of their approaches and conclusions. Toward that goal, we (i) empirically evaluated how failure to accommodate for various subtleties in the use of HPCs can undermine the effectiveness of security applications, specifically in the case of exploit prevention and malware detection. Lastly, we showed how (ii) an adversary can manipulate HPCs to bypass certain security defenses.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835366","Hardware-Performance-Counters;Malware-Detection;Exploit-Defense;Non-determinism","Program processors;Monitoring;Security;Pollution measurement;Hardware;Kernel;Tools","","76","","121","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"HVLearn: Automated Black-Box Analysis of Hostname Verification in SSL/TLS Implementations","S. Sivakorn; G. Argyros; K. Pei; A. D. Keromytis; S. Jana","Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Columbia University, New York, USA","2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","521","538","SSL/TLS is the most commonly deployed family of protocols for securing network communications. The security guarantees of SSL/TLS are critically dependent on the correct validation of the X.509 server certificates presented during the handshake stage of the SSL/TLS protocol. Hostname verification is a critical component of the certificate validation process that verifies the remote server's identity by checking if the hostname of the server matches any of the names present in the X.509 certificate. Hostname verification is a highly complex process due to the presence of numerous features and corner cases such as wildcards, IP addresses, international domain names, and so forth. Therefore, testing hostname verification implementations present a challenging task. In this paper, we present HVLearn, a novel black-box testing framework for analyzing SSL/TLS hostname verification implementations, which is based on automata learning algorithms. HVLearn utilizes a number of certificate templates, i.e., certificates with a common name (CN) set to a specific pattern, in order to test different rules from the corresponding specification. For each certificate template, HVLearn uses automata learning algorithms to infer a Deterministic Finite Automaton (DFA) that describes the set of all hostnames that match the CN of a given certificate. Once a model is inferred for a certificate template, HVLearn checks the model for bugs by finding discrepancies with the inferred models from other implementations or by checking against regular-expression-based rules derived from the specification. The key insight behind our approach is that the acceptable hostnames for a given certificate template form a regular language. Therefore, we can leverage automata learning techniques to efficiently infer DFA models that accept the corresponding regular language. We use HVLearn to analyze the hostname verification implementations in a number of popular SSL/TLS libraries and applications written in a diverse set of languages like C, Python, and Java. We demonstrate that HVLearn can achieve on average 11.21% higher code coverage than existing black/gray-box fuzzing techniques. By comparing the DFA models inferred by HVLearn, we found 8 unique violations of the RFC specifications in the tested hostname verification implementations. Several of these violations are critical and can render the affected implementations vulnerable to active man-in-the-middle attacks.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958596","","Testing;Servers;IP networks;Learning automata;Security;Protocols;Electronic mail","","41","","61","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"SoK: Quantifying Cyber Risk","D. W. Woods; R. Böhme","University of Innsbruck, Innsbruck, Austria; University of Innsbruck, Innsbruck, Austria","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","211","228","This paper introduces a causal model inspired by structural equation modeling that explains cyber risk outcomes in terms of latent factors measured using reflexive indicators. First, we use the model to classify empirical cyber harm studies. We discover cyber harms are not exceptional in terms of typical or extreme losses. The increasing frequency of data breaches is contested and stock market reactions to cyber incidents are becoming less damaging over time. Focusing on harms alone breeds fatalism; the causal model is most useful in evaluating the effectiveness of security interventions. We show how simple statistical relationships lead to spurious results in which more security spending or applying updates are associated with greater rates of compromise. When accounting for threat and exposure, indicators of security are shown to be important factors in explaining the variance in rates of compromise, especially when the studies use multiple indicators of the security level.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00053","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519490","cyber risk;security metrics;cyber harm;control effectiveness;science of security;causal model;structural equation modeling","Time-frequency analysis;Privacy;Focusing;Software;Security;Mathematical model;Risk management","","18","","133","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"mmSpy: Spying Phone Calls using mmWave Radars","S. Basak; M. Gowda","The Pennsylvania State University, University Park, PA; The Pennsylvania State University, University Park, PA","2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","1211","1228","This paper presents a system mmSpy that shows the feasibility of eavesdropping phone calls remotely. Towards this end, mmSpy performs sensing of earpiece vibrations using an off-the-shelf radar device that operates in the mmWave spectrum (77GHz, and 60GHz). Given that mmWave radars are becoming popular in a number of autonomous driving, remote sensing, and other IoT applications, we believe this is a critical privacy concern. In contrast to prior works that show the feasibility of detecting loudspeaker vibrations with larger amplitudes, mmSpy exploits smaller wavelengths of mmWave radar signals to detect subtle vibrations in the earpiece devices used in phonecalls. Towards designing this attack, mmSpy solves a number of challenges related to non-availability of large scale radar datasets, systematic correction of various sources of noises, as well as domain adaptation problems in harvesting training data. Extensive measurement-based validation achieves an endto-end accuracy of 83-44% in classifying digits and keywords over a range of 1-6ft, thereby compromising the privacy in applications such as exchange of credit card information. In addition, mmSpy shows the feasibility of reconstruction of the audio signals from the radar data, using which more sensitive information can be potentially leaked.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833568","side channel attacks;mmWave radars;speech privacy","Vibrations;Training;Radar remote sensing;Adaptation models;Radar detection;Training data;Radar","","8","","103","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures","E. Bagdasaryan; V. Shmatikov",Cornell Tech; Cornell Tech,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","769","786","We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to “spin” their outputs so as to support an adversary-chosen sentiment or point of view—but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization.Model spinning introduces a “meta-backdoor” into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary.Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims.To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call “pseudo-words,” and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary’s meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models.Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.1We use “spinned” rather than “spun” to match how the word is used in public relations.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833572","","Measurement;Training;Analytical models;Toxicology;Training data;Media;Data models","","7","","96","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Publicly Accountable Robust Multi-Party Computation","M. Rivinius; P. Reisert; D. Rausch; R. Küsters","Institute of Information Security University of Stuttgart, Stuttgart, Germany; Institute of Information Security University of Stuttgart, Stuttgart, Germany; Institute of Information Security University of Stuttgart, Stuttgart, Germany; Institute of Information Security University of Stuttgart, Stuttgart, Germany","2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","2430","2449","In recent years, lattice-based secure multi-party computation (MPC) has seen a rise in popularity and is used more and more in large scale applications like privacy-preserving cloud computing, electronic voting, or auctions. Many of these applications come with the following high security requirements: a computation result should be publicly verifiable, with everyone being able to identify a malicious party and hold it accountable, and a malicious party should not be able to corrupt the computation, force a protocol restart, or block honest parties or an honest third-party (client) that provided private inputs from receiving a correct result. The protocol should guarantee verifiability and accountability even if all protocol parties are malicious. While some protocols address one or two of these often essential security features, we present the first publicly verifiable and accountable, and (up to a threshold) robust SPDZ-like MPC protocol without restart. We propose protocols for accountable and robust online, offline, and setup computations. We adapt and partly extend the lattice-based commitment scheme by Baum et al. (SCN 2018) as well as other primitives like ZKPs. For the underlying commitment scheme and the underlying BGV encryption scheme we determine ideal parameters. We give a performance evaluation of our protocols and compare them to state-of-the-art protocols both with and without our target security features: public accountability, public verifiability and robustness.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833608","MPC;SPDZ;public-accountability;publicly-identifiable-abort;public-verifiability;robustness","Performance evaluation;Privacy;Cloud computing;Protocols;Force;Multi-party computation;Robustness","","5","","78","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"Analyzing Leakage of Personally Identifiable Information in Language Models","N. Lukas; A. Salem; R. Sim; S. Tople; L. Wutschitz; S. Zanella-Béguelin",University of Waterloo; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","346","363","Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10× more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179300","Language-Models;Data-Extraction;Data-Reconstruction;Personally-Identifiable-Information;Differential-Privacy;Scrubbing","Training;Data privacy;Differential privacy;Privacy;Analytical models;Pipelines;Training data","","3","","69","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"mmEcho: A mmWave-based Acoustic Eavesdropping Method","P. Hu; W. Li; R. Spolaor; X. Cheng","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1840","1856","Acoustic eavesdropping targeting private or confidential spaces is one of the most severe privacy threats. Soundproof rooms may reduce such risks, but they cannot prevent sophisticated eavesdropping, which has been an emerging research trend in recent years. Researchers have investigated such acoustic eavesdropping attacks via sensor-enabled side-channels. However, such attacks either make unrealistic assumptions or have considerable constraints. This paper introduces mmEcho, an acoustic eavesdropping system that uses a millimeter-wave radio signal to accurately measure the micrometer-level vibration of an object induced by sound waves. Compared with previous works, our eavesdropping method is highly accurate and requires no prior knowledge about the victim. We evaluate the performance of mmEcho under extensive real-world settings and scenarios. Our results show that mmEcho can accurately reconstruct audio from moving sources at various distances, orientations, reverberating objects, sound insulators, spoken languages, and sound levels.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179484","mmWave radar;Audio eavesdropping;Vibrations;Signal processing","Vibrations;Millimeter wave measurements;Millimeter wave technology;Vibration measurement;Signal processing;Insulators;Acoustics","","3","","56","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation","T. Gehr; M. Mirman; D. Drachsler-Cohen; P. Tsankov; S. Chaudhuri; M. Vechev","Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland","2018 IEEE Symposium on Security and Privacy (SP)","26 Jul 2018","2018","","","3","18","We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.","2375-1207","978-1-5386-4353-2","10.1109/SP.2018.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418593","Reliable Machine Learning;Robustness;Neural Networks;Abstract Interpretation","Robustness;Biological neural networks;Cats;Neurons;Safety;Perturbation methods","","354","5","44","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model","X. Ling; S. Ji; J. Zou; J. Wang; C. Wu; B. Li; T. Wang",Zhejiang University; Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies; Zhejiang University; Zhejiang University; Zhejiang University; UIUC; Lehigh University,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","673","690","Deep learning (DL) models are inherently vulnerable to adversarial examples – maliciously crafted inputs to trigger target DL models to misbehave – which significantly hinders the application of DL in security-sensitive domains. Intensive research on adversarial learning has led to an arms race between adversaries and defenders. Such plethora of emerging attacks and defenses raise many questions: Which attacks are more evasive, preprocessing-proof, or transferable? Which defenses are more effective, utility-preserving, or general? Are ensembles of multiple defenses more robust than individuals? Yet, due to the lack of platforms for comprehensive evaluation on adversarial attacks and defenses, these critical questions remain largely unsolved. In this paper, we present the design, implementation, and evaluation of DEEPSEC, a uniform platform that aims to bridge this gap. In its current implementation, DEEPSEC incorporates 16 state-of-the-art attacks with 10 attack utility metrics, and 13 state-of-the-art defenses with 5 defensive utility metrics. To our best knowledge, DEEPSEC is the first platform that enables researchers and practitioners to (i) measure the vulnerability of DL models, (ii) evaluate the effectiveness of various attacks/defenses, and (iii) conduct comparative studies on attacks/defenses in a comprehensive and informative manner. Leveraging DEEPSEC, we systematically evaluate the existing adversarial attack and defense methods, and draw a set of key findings, which demonstrate DEEPSEC’s rich functionality, such as (1) the trade-off between misclassification and imperceptibility is empirically confirmed; (2) most defenses that claim to be universally applicable can only defend against limited types of attacks under restricted settings; (3) it is not necessary that adversarial examples with higher perturbation magnitude are easier to be detected; (4) the ensemble of multiple defenses cannot improve the overall defense capability, but can improve the lower bound of the defense effectiveness of individuals. Extensive analysis on DEEPSEC demonstrates its capabilities and advantages as a benchmark platform which can benefit future adversarial learning research.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835375","Deep-Learning;Adversarial-machine-learning;benchmark-platform","Measurement;Perturbation methods;Security;Robustness;Terminology;Jacobian matrices;Training","","66","","67","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Private Approximate Nearest Neighbor Search with Sublinear Communication","S. Servan-Schreiber; S. Langowski; S. Devadas",MIT CSAIL; MIT CSAIL; MIT CSAIL,"2022 IEEE Symposium on Security and Privacy (SP)","27 Jul 2022","2022","","","911","929","Nearest neighbor search is a fundamental building-block for a wide range of applications. A privacy-preserving protocol for nearest neighbor search involves a set of clients who send queries to a remote database. Each client retrieves the nearest neighbor(s) to its query in the database without revealing any information about the query. To ensure database privacy, clients must learn as little as possible beyond the query answer, even if behaving maliciously by deviating from protocol. Existing protocols for private nearest neighbor search require heavy cryptographic tools, resulting in high computational and bandwidth overheads. In this paper, we present the first lightweight protocol for private nearest neighbor search. Our protocol is instantiated using two non-colluding servers, each holding a replica of the database. Our design supports an arbitrary number of clients simultaneously querying the database through the two servers. Each query consists of a single round of communication between the client and the two servers. No communication is required between the servers to answer queries. If at least one of the servers is non-colluding, we ensure that (1) no information is revealed on the client’s query, (2) the total communication between the client and the servers is sublinear in the database size, and (3) each query answer only leaks a small and bounded amount of information about the database to the client, even if the client is malicious. We implement our protocol and report its performance on real-world data. Our construction requires between 10 and 20 seconds of query latency over large databases of 10M feature vectors. Client overhead remained under 10ms of processing time per query and less than 10MB of communication.","2375-1207","978-1-6654-1316-9","10.1109/SP46214.2022.9833702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833702","approximate-nearest-neighbor;private;classifier;knn;ann;recommendation;privacy-preserving;distributed-point-functions;malicious;security;machine;learning;similarity;search;lightweight","Data privacy;Protocols;Databases;Bandwidth;Nearest neighbor methods;Servers;Cryptography","","7","","87","IEEE","27 Jul 2022","","","IEEE","IEEE Conferences"
"DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers","B. Bichsel; S. Steffen; I. Bogunovic; M. Vechev","ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","391","409","We present DP-Sniper, a practical black-box method that automatically finds violations of differential privacy.DP-Sniper is based on two key ideas: (i) training a classifier to predict if an observed output was likely generated from one of two possible inputs, and (ii) transforming this classifier into an approximately optimal attack on differential privacy.Our experimental evaluation demonstrates that DP-Sniper obtains up to 12.4 times stronger guarantees than state-of-the-art, while being 15.5 times faster. Further, we show that DP-Sniper is effective in exploiting floating-point vulnerabilities of naively implemented algorithms: it detects that a supposedly 0.1-differentially private implementation of the Laplace mechanism actually does not satisfy even 0.25-differential privacy.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519405","differential privacy;differential distinguishability;inference attacks;machine learning;classifiers","Training;Privacy;Differential privacy;Approximation algorithms;Classification algorithms;Security","","7","","50","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Runtime Recovery of Web Applications under Zero-Day ReDoS Attacks","Z. Bai; K. Wang; H. Zhu; Y. Cao; X. Jin",Johns Hopkins University; Peking University; Johns Hopkins University; Johns Hopkins University; Peking University,"2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1575","1588","Regular expression denial of service (ReDoS)— which exploits the super-linear running time of matching regular expressions against carefully crafted inputs—is an emerging class of DoS attacks to web services. One challenging question for a victim web service under ReDoS attacks is how to quickly recover its normal operation after ReDoS attacks, especially these zero-day ones exploiting previously unknown vulnerabilities.In this paper, we present RegexNet, the first payload-based, automated, reactive ReDoS recovery system for web services. RegexNet adopts a learning model, which is updated constantly in a feedback loop during runtime, to classify payloads of upcoming requests including the request contents and database query responses. If detected as a cause leading to ReDoS, RegexNet migrates those requests to a sandbox and isolates their execution for a fast, first-measure recovery.We have implemented a RegexNet prototype and integrated it with HAProxy and Node.js. Evaluation results show that RegexNet is effective in recovering the performance of web services against zero-day ReDoS attacks, responsive on reacting to attacks in sub-minute, and resilient to different ReDoS attack types including adaptive ones that are designed to evade RegexNet on purpose.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519496","Regular expression Denial of Service (ReDoS);Deep Neural Networks;Adversarial Machine Learning;Online Feedback Loop","Feedback loop;Privacy;Runtime;Databases;Prototypes;Web servers;Data models","","6","","53","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems","W. Zong; Y. -W. Chow; W. Susilo; K. Do; S. Venkatesh","Institute of Cybersecurity and Cryptology (iC²), University of Wollongong, Australia; Institute of Cybersecurity and Cryptology (iC²), University of Wollongong, Australia; Institute of Cybersecurity and Cryptology (iC²), University of Wollongong, Australia; Applied Artificial Intelligence Institute (A²I²), Deakin University, Australia; Applied Artificial Intelligence Institute (A²I²), Deakin University, Australia","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1667","1683","While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model’s performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179331","adversarial machine learning;automatic speech recognition;backdoor;deep learning;Trojan","Deep learning;Privacy;Acoustics;Trojan horses;Security;Automatic speech recognition;Microphones","","1","","42","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"SoK: Anti-Facial Recognition Technology","E. Wenger; S. Shan; H. Zheng; B. Y. Zhao",University of Chicago; University of Chicago; University of Chicago; University of Chicago,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","864","881","The rapid adoption of facial recognition (FR) technology by both government and commercial entities in recent years has raised concerns about civil liberties and privacy. In response, a broad suite of so-called ""anti-facial recognition"" (AFR) tools has been developed to help users avoid unwanted facial recognition. The set of AFR tools proposed in the last few years is wide-ranging and rapidly evolving, necessitating a step back to consider the broader design space of AFR systems and long-term challenges. This paper aims to fill that gap and provides the first comprehensive analysis of the AFR research landscape. Using the operational stages of FR systems as a starting point, we create a systematic framework for analyzing the benefits and tradeoffs of different AFR approaches. We then consider both technical and social challenges facing AFR tools and propose directions for future research in this field.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179445","machine-learning;privacy;privacy-enhancing-technology","Privacy;Systematics;Face recognition;Government;Security","","1","","170","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"User Authentication Based on Mouse Dynamics Using Deep Neural Networks: A Comprehensive Study","P. Chong; Y. Elovici; A. Binder","Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; ST Engineering Electronics-SUTD Cyber Security Laboratory, Singapore University of Technology and Design, Singapore; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Information Forensics and Security","5 Dec 2019","2020","15","","1086","1101","Recently conducted research demonstrated the potential use of mouse dynamics as a behavioral biometric for user authentication systems. However, the state-of-the-art methods in this field rely on classical machine learning methods that necessitate the design of hand crafted mouse features for feature extraction. To simplify the feature extraction process, we leverage various deep learning architectures for mouse movement sequences classification, including convolutional networks, recurrent networks, and a hybrid model which combines convolutional and recurrent layers. It is known that the training of these networks with random initialization of weights on small datasets will produce models that perform poorly. Therefore, we consider a two-dimensional convolutional neural network that allows transfer learning, which is a domain adaptation technique effective for learning on small datasets. Although employing such architecture may seem counterintuitive, since the temporal information is discarded from the input data, the architecture has outperformed all the other deep architectures investigated, as well as a classical machine learning method. In order to understand the features learned, we adopt the layer-wise relevance propagation (LRP) algorithm to compute relevance scores for each part of the mouse curves. In addition, the models are measured for their usability and effectiveness in realistic scenarios.","1556-6021","","10.1109/TIFS.2019.2930429","ST Engineering Electronics-SUTD Cyber Security Laboratory(grant numbers:STEE-Infosec–SUTD Corporate Lab); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768407","Behavioral biometrics;CNN;LRP;LSTM;mouse dynamics;weighted learning","Mice;Authentication;Feature extraction;Computer architecture;Training;Support vector machines;Biological system modeling","","39","","29","IEEE","22 Jul 2019","","","IEEE","IEEE Journals"
"APAS: Application-Specific Accelerators for RLWE-Based Homomorphic Linear Transformations","S. Bian; D. E. S. Kundi; K. Hirozawa; W. Liu; T. Sato","Department of Communications and Computer Engineering, Kyoto University, Kyoto, Japan; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Communications and Computer Engineering, Kyoto University, Kyoto, Japan; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Communications and Computer Engineering, Kyoto University, Kyoto, Japan","IEEE Transactions on Information Forensics and Security","4 Oct 2021","2021","16","","4663","4678","Recently, the application of multi-party secure computing schemes based on homomorphic encryption in the field of machine learning attracts attentions across the research fields. Previous studies have demonstrated that secure protocols adopting packed additive homomorphic encryption (PAHE) schemes based on the ring learning with errors (RLWE) problem exhibit significant practical merits, and are particularly promising in enabling efficient secure inference in machine-learning-as-a-service applications. In this work, we introduce a new technique for performing homomorphic linear transformation (HLT) over PAHE ciphertexts. Using the proposed HLT technique, homomorphic convolutions and inner products can be executed without the use of number theoretic transform and the rotate-and-add algorithms that were proposed in existing works. To maximize the efficiency of the HLT technique, we propose APAS, a hardware-software co-design framework consisting of approximate arithmetic units for the hardware acceleration of HLT. In the experiments, we use actual neural network architectures as benchmarks to show that APAS can improve the computational and communicational efficiency of homomorphic convolution by  $8\times $  and  $3\times $ , respectively, with an energy reduction of up to  $26\times $  as compared to the ASIC implementations of existing methods.","1556-6021","","10.1109/TIFS.2021.3114032","Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:20K19799,20H04156,20K21793); Japan Science and Technology Agency (JST) CREST(grant numbers:JPMJCR19K5); JST PRESTO(grant numbers:JPMJPR20M7); National Natural Science Foundation of China(grant numbers:62022041,61871216); VLSI Design and Education Center (VDEC), The University of Tokyo, in collaboration with Synopsys, Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541198","Homomorphic encryption;ring learning with errors;secure inference;neural networks;homomorphic linear transformation;number theoretic transform","Cryptography;Protocols;Computer architecture;Transforms;Standards;Libraries;Lattices","","4","","49","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"NTD: Non-Transferability Enabled Deep Learning Backdoor Detection","Y. Li; H. Ma; Z. Zhang; Y. Gao; A. Abuadbba; M. Xue; A. Fu; Y. Zheng; S. F. Al-Sarawi; D. Abbott","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electrical and Electronics Engineering, The University of Adelaide, Adelaide, SA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; Data61, CSIRO, Canberra, ACT, Australia; Data61, CSIRO, Canberra, ACT, Australia; Data61, CSIRO, Canberra, ACT, Australia; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Harbin Institute of Technology, Shenzhen, China; School of Electrical and Electronics Engineering, The University of Adelaide, Adelaide, SA, Australia; School of Electrical and Electronics Engineering, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Information Forensics and Security","20 Nov 2023","2024","19","","104","119","To mitigate recent insidious backdoor attacks on deep learning models, advances have been made by the research community. Nonetheless, state-of-the-art defenses are either limited to specific backdoor attacks (i.e., source-agnostic attacks) or non-user-friendly in that machine learning expertise and/or expensive computing resources are required. This work observes that all existing backdoor attacks have an inadvertent and inevitable intrinsic weakness, termed as non-transferability —that is, a trigger input hijacks a backdoored model but is not effective in another model that has not been implanted with the same backdoor. With this key observation, we propose non-transferability enabled backdoor detection to identify trigger inputs for a model-under-test during run-time. Specifically, our detection allows a potentially backdoored model-under-test to predict a label for an input. Moreover, our detection leverages a feature extractor to extract feature vectors for the input and a group of samples randomly picked from its predicted class label, and then compares the similarity between the input and the samples in the feature extractor’s latent space to determine whether the input is a trigger input or a benign one. The feature extractor can be provided by a reputable party or is a free pre-trained model privately reserved from any open platform (e.g., ModelZoo, GitHub, Kaggle) by a user and thus our detection does not require the user to have any machine learning expertise or perform costly computations. Extensive experimental evaluations on four common tasks affirm that our detection scheme has high effectiveness (low false acceptance rate) and usability (low false rejection rate) with low detection latency against different types of backdoor attacks.","1556-6021","","10.1109/TIFS.2023.3312973","National Natural Science Foundation of China(grant numbers:62002167,62072239,62372236); Natural Science Foundation of Jiangsu(grant numbers:BK20200461); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10243095","Backdoor countermeasure;NTD;non-transferability;deep learning","Computational modeling;Feature extraction;Iron;Task analysis;Training;Face recognition;Speech recognition","","2","","65","IEEE","7 Sep 2023","","","IEEE","IEEE Journals"
"Depth-Wise Separable Convolutions and Multi-Level Pooling for an Efficient Spatial CNN-Based Steganalysis","R. Zhang; F. Zhu; J. Liu; G. Liu","School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Information Forensics and Security","11 Dec 2019","2020","15","","1138","1150","For steganalysis, many studies showed that convolutional neural network (CNN) has better performances than the two-part structure of traditional machine learning methods. Existing CNN architectures use various tricks to improve the performance of steganalysis, such as fixed convolutional kernels, the absolute value layer, data augmentation and the domain knowledge. However, some designing of the network structure were not extensively studied so far, such as different convolutions (inception, xception, etc.) and variety ways of pooling(spatial pyramid pooling, etc.). In this paper, we focus on designing a new CNN network structure to improve detection accuracy of spatial-domain steganography. First, we use  $3\times 3$  kernels instead of the traditional  $5\times 5$  kernels and optimize convolution kernels in the preprocessing layer. The smaller convolution kernels are used to reduce the number of parameters and model the features in a small local region. Next, we use separable convolutions to utilize channel correlation of the residuals, compress the image content and increase the signal-to-noise ratio (between the stego signal and the image signal). Then, we use spatial pyramid pooling (SPP) to aggregate the local features and enhance the representation ability of features by multi-level pooling. Finally, data augmentation is adopted to further improve network performance. The experimental results show that the proposed CNN structure is significantly better than other five methods such as SRM, Ye-Net, Xu-Net, Yedroudj-Net and SRNet, when it is used to detect three spatial algorithms such as WOW, S-UNIWARD and HILL with a wide variety of datasets and payloads.","1556-6021","","10.1109/TIFS.2019.2936913","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800404); National Natural Science Foundation of China(grant numbers:U1636112,U1636212); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809687","Image steganalysis;convolutional neural networks;separable convolution;spatial pyramid pooling","Convolution;Feature extraction;Kernel;Correlation;Convergence;Payloads;Transform coding","","164","","59","IEEE","22 Aug 2019","","","IEEE","IEEE Journals"
"Privacy-Enhanced Federated Learning Against Poisoning Adversaries","X. Liu; H. Li; G. Xu; Z. Chen; X. Huang; R. Lu","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; CETC Cyberspace Security Research Institute Company Ltd., Chengdu, China; Faculty of Computer Science (FCS), University of New Brunswick (UNB), Fredericton, Canada","IEEE Transactions on Information Forensics and Security","15 Sep 2021","2021","16","","4574","4588","Federated learning (FL), as a distributed machine learning setting, has received considerable attention in recent years. To alleviate privacy concerns, FL essentially promises that multiple parties jointly train the model by exchanging gradients rather than raw data. However, intrinsic privacy issue still exists in FL, e.g., user’s training samples could be revealed by solely inferring gradients. Moreover, the emerging poisoning attack also poses a crucial security threat to FL. In particular, due to the distributed nature of FL, malicious users may submit crafted gradients during the training process to undermine the integrity and availability of the model. Furthermore, there exists a contradiction in simultaneously addressing two issues, that is, privacy-preserving FL solutions are dedicated to ensuring gradients indistinguishability, whereas the defenses against poisoning attacks tend to remove outliers based on their similarity. To solve such a dilemma, in this paper, we aim to build a bridge between the two issues. Specifically, we present a privacy-enhanced FL (PEFL) framework that adopts homomorphic encryption as the underlying technology and provides the server with a channel to punish poisoners via the effective gradient data extraction of the logarithmic function. To the best of our knowledge, the PEFL is the first effort to efficiently detect the poisoning behaviors in FL under ciphertext. Detailed theoretical analyses illustrate the security and convergence properties of the scheme. Moreover, the experiments conducted on real-world datasets show that the PEFL can effectively defend against label-flipping and backdoor attacks, two representative poisoning attacks in FL.","1556-6021","","10.1109/TIFS.2021.3108434","National Natural Science Foundation of China(grant numbers:62020106013,61972454,61802051,61772121,61728102); Sichuan Science and Technology Program(grant numbers:2020JDTD0007,2020YFG0298); Chongqing Science and Technology Commission(grant numbers:cstc2018jcyjAX0703); Fundamental Research Funds for Chinese Central Universities(grant numbers:ZYGX2020ZB027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524709","Federated learning;poisoning attack;privacy protection;cloud computing","Training;Data privacy;Privacy;Servers;Security;Computational modeling;Data models","","96","","42","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"Unsupervised Specific Emitter Identification Method Using Radio-Frequency Fingerprint Embedded InfoGAN","J. Gong; X. Xu; Y. Lei","Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; School of Electronic Countermeasure, National University of Defense Technology, Hefei, China","IEEE Transactions on Information Forensics and Security","31 Mar 2020","2020","15","","2898","2913","Machine learning approaches are becoming increasingly popular to improve the efficiency of specific emitter identification (SEI). However, in most non-cooperative SEI scenarios, supervised and semi-supervised learning approaches are often incompatible due to the lack of labeled datasets. To solve this challenge, an unsupervised SEI framework is proposed based on information maximized generative adversarial networks (InfoGANs) and radio frequency fingerprint embedding (RFFE). To enhance individual discriminability, a gray histogram is first constructed according to the bispectrum extracted from the received signal before being embedded into the proposed framework. In addition to the latent class input and the RFFE, the proposed InfoGAN incorporates a priori statistical characteristics of the wireless propagation channels in the form of a structured multimodal latent vector to further improve the GAN quality. The probabilistic distribution of the bispectrum is derived in closed-form and the convergence of the InfoGAN is analyzed to demonstrate the influence of the RFFE. Numerical results indicate that the proposed framework consistently outperforms state-of-the-art algorithms for unsupervised SEI applications, both in terms of evaluation score and classification accuracy.","1556-6021","","10.1109/TIFS.2020.2978620","National Natural Science Foundation of China(grant numbers:61271272); National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025169","Specific emitter identification;generative adversarial network;unsupervised deep learning;radio frequency fingerprint;Nakagami-m","Gallium nitride;Feature extraction;Training;Wireless communication;Convergence;Mutual information;Generators","","81","","50","IEEE","5 Mar 2020","","","IEEE","IEEE Journals"
"Private and Secure Distributed Matrix Multiplication With Flexible Communication Load","M. Aliasgari; O. Simeone; J. Kliewer","Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; Department of Engineering, King’s Communications, Learning, and Information Processing (KCLIP) Lab, Centre for Telecommunication Research (CTR), King’s College London, London, U.K.; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA","IEEE Transactions on Information Forensics and Security","10 Mar 2020","2020","15","","2722","2734","Large matrix multiplications are central to large-scale machine learning applications. These operations are often carried out on a distributed computing platform with a master server and multiple workers in the cloud operating in parallel. For such distributed platforms, it has been recently shown that coding over the input data matrices can reduce the computational delay, yielding a trade-off between recovery threshold, i.e., the number of workers required to recover the matrix product, and communication load, i.e., the total amount of data to be downloaded from the workers. In this paper, in addition to exact recovery requirements, we impose security and privacy constraints on the data matrices, and study the recovery threshold as a function of the communication load. We first assume that both matrices contain private information and that workers can collude to eavesdrop on the content of these data matrices. For this problem, we introduce a novel class of secure codes, referred to as secure generalized PolyDot (SGPD) codes, that generalize state-of-the-art non-secure codes for matrix multiplication. SGPD codes allow a flexible trade-off between recovery threshold and communication load for a fixed maximum number of colluding workers while providing perfect secrecy for the two data matrices. We then study a connection between secure matrix multiplication and private information retrieval. We specifically assume that one of the data matrices is taken from a public set known to all the workers. In this setup, the identity of the matrix of interest should be kept private from the workers. For this model, we present a variant of generalized PolyDot codes that can guarantee both secrecy of one matrix and privacy for the identity of the other matrix for the case of no colluding servers.","1556-6021","","10.1109/TIFS.2020.2972166","H2020 European Research Council(grant numbers:725731); National Science Foundation(grant numbers:CNS-1526547,CCF-1525629); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985291","Coded distributed computation;distributed learning;secret sharing;information theoretic security;private information retrieval","Servers;Encoding;Security;Distributed databases;Delays;Privacy;Indexes","","56","","44","IEEE","6 Feb 2020","","","IEEE","IEEE Journals"
"Wireless Anomaly Detection Based on IEEE 802.11 Behavior Analysis","H. Alipour; Y. B. Al-Nashif; P. Satam; S. Hariri","Cloud Identity Services and Security Division, Microsoft, Redmond, WA, USA; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Department of Electrical and Computer Engineering, The University of Arizona, Tucson, AZ, USA; Department of Electrical and Computer Engineering, The University of Arizona, Tucson, AZ, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2015","10","10","2158","2170","Wireless communication networks are pervading every aspect of our lives due to their fast, easy, and inexpensive deployment. They are becoming ubiquitous and have been widely used to transfer critical information, such as banking accounts, credit cards, e-mails, and social network credentials. The more pervasive the wireless technology is going to be, the more important its security issue will be. Whereas the current security protocols for wireless networks have addressed the privacy and confidentiality issues, there are unaddressed vulnerabilities threatening their availability and integrity (e.g., denial of service, session hijacking, and MAC address spoofing attacks). In this paper, we describe an anomaly based intrusion detection system for the IEEE 802.11 wireless networks based on behavioral analysis to detect deviations from normal behaviors that are triggered by wireless network attacks. Our anomaly behavior analysis of the 802.11 protocols is based on monitoring the n-consecutive transitions of the protocol state machine. We apply sequential machine learning techniques to model the n-transition patterns in the protocol and characterize the probabilities of these transitions being normal. We have implemented several experiments to evaluate our system performance. By cross validating the system over two different wireless channels, we have achieved a low false alarm rate (<;0.1%). We have also evaluated our approach against an attack library of known wireless attacks and has achieved more than 99% detection rate.","1556-6021","","10.1109/TIFS.2015.2433898","AFOSR DDDAS(grant numbers:FA95550-12-1-0241); National Science Foundation research projects(grant numbers:NSF IIP-0758579,NCS-0855087,IIP-1127873); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109166","Anomaly detection;IEEE 802.11 security;Intrusion detection;Wireless Network security;Protocol analysis;Wireless networks;Anomaly detection;IEEE 802.11 security;intrusion detection;wireless network security;protocol analysis;wireless networks","Protocols;IEEE 802.11 Standards;Intrusion detection;Detectors;Communication system security;Wireless networks","","56","","27","IEEE","15 May 2015","","","IEEE","IEEE Journals"
"Privacy-Preserving Byzantine-Robust Federated Learning via Blockchain Systems","Y. Miao; Z. Liu; H. Li; K. -K. R. Choo; R. H. Deng","School of Cyber Engineering, Xidian University, Xi’an, China; School of Cyber Engineering, Xidian University, Xi’an, China; Department of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; School of Information Systems, Singapore Management University, Bras Basah, Singapore","IEEE Transactions on Information Forensics and Security","15 Aug 2022","2022","17","","2848","2861","Federated learning enables clients to train a machine learning model jointly without sharing their local data. However, due to the centrality of federated learning framework and the untrustworthiness of clients, traditional federated learning solutions are vulnerable to poisoning attacks from malicious clients and servers. In this paper, we aim to mitigate the impact of the central server and malicious clients by designing a Privacy-preserving Byzantine-robust Federated Learning (PBFL) scheme based on blockchain. Specifically, we use cosine similarity to judge the malicious gradients uploaded by malicious clients. Then, we adopt fully homomorphic encryption to provide secure aggregation. Finally, we use blockchain system to facilitate transparent processes and implementation of regulations. Our formal analysis proves that our scheme achieves convergence and provides privacy protection. Our extensive experiments on different datasets demonstrate that our scheme is robust and efficient. Even if the root dataset is small, our scheme can achieve the same efficiency as FedSGD.","1556-6021","","10.1109/TIFS.2022.3196274","National Natural Science Foundation of China(grant numbers:62072361,62125205); Key Research and Development Program of Shaanxi(grant numbers:2022GY-019); Fundamental Research Funds for the Central Universities(grant numbers:JB211505); Henan Key Laboratory of Network Cryptography Technology and the State Key Laboratory of Mathematical Engineering and Advanced Computing(grant numbers:LNCT2020-A06); Cloud Technology Endowed Professorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849010","Federated learning;poisoning attacks;fully homomorphic encryption;blockchain","Servers;Blockchains;Collaborative work;Computational modeling;Training;Resists;Privacy","","55","","42","IEEE","3 Aug 2022","","","IEEE","IEEE Journals"
"PrivBioMTAuth: Privacy Preserving Biometrics-Based and User Centric Protocol for User Authentication From Mobile Phones","H. Gunasinghe; E. Bertino","Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Information Forensics and Security","3 Jan 2018","2018","13","4","1042","1057","We introduce a privacy preserving biometrics-based authentication solution by which users can authenticate to different service providers from mobile phones without involving identity providers in the transactions. Authentication is performed via zero-knowledge proof of knowledge, based on a cryptographic identity token that encodes the biometric identifier of the user and a secret provided by the user, making it three-factor authentication. Our approach for generating a unique, repeatable, and revocable biometric identifier from the user’s biometric image is based on a machine learning-based classification technique, which involves the features extracted from the user’s biometric image. We have implemented a prototype of the proposed authentication solution and evaluated our solution with respect to its performance, security, and privacy. The evaluation has been performed on a public data set of face images.","1556-6021","","10.1109/TIFS.2017.2777787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119873","Biometrics;authentication;privacy;security;identity management systems","Authentication;Face;Protocols;Privacy;Feature extraction;Mobile handsets","","50","","41","IEEE","24 Nov 2017","","","IEEE","IEEE Journals"
"Drone Pilot Identification by Classifying Radio-Control Signals","A. Shoufan; H. M. Al-Angari; M. F. A. Sheikh; E. Damiani","Information Security Center, Khalifa University, Abu Dhabi, UAE; Biomedical Engineering Department, Khalifa University, Abu Dhabi, UAE; Electrical and Computer Engineering Department, New York University, Abu Dhabi, UAE; Information Security Center, Khalifa University, Abu Dhabi, UAE","IEEE Transactions on Information Forensics and Security","8 May 2018","2018","13","10","2439","2447","Analysis of interactions with remotely controlled devices has been used to detect the onset of hijacking attacks, as well as for forensics analysis, e.g., to identify the human controller. Its effectiveness is known to depend on the remote device type as well as on the properties of the remote control signal. This paper shows that the radio control signal sent to an unmanned aerial vehicle (UAV) using a typical transmitter can be captured and analyzed to identify the controlling pilot using machine learning techniques. Twenty trained pilots have been asked to fly a high-end research drone through three different trajectories. Control data have been collected and used to train multiple classifiers. Best performance has been achieved by a random forest classifier that achieved accuracy around 90% using simple time-domain features. Extensive tests have shown that the classification accuracy depends on the flight trajectory and that the pitch, roll, yaw, and thrust control signals show different levels of significance for pilot identification. This result paves the way to a number of security and forensics applications, including continuous identification of UAV pilots to mitigate the risk of hijacking.","1556-6021","","10.1109/TIFS.2018.2819126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323415","Pilot identification;behavioral biometrics;unmanned aerial vehicles;random forest","Drones;Global Positioning System;Security;Radio transmitters;Trajectory","","50","","30","IEEE","23 Mar 2018","","","IEEE","IEEE Journals"
"APMSA: Adversarial Perturbation Against Model Stealing Attacks","J. Zhang; S. Peng; Y. Gao; Z. Zhang; Q. Hong","Changsha Semiconductor Technology and Application Innovation Research Institute, College of Semiconductors (College of Integrated Circuits), Hunan University, Changsha, China; College of Semiconductors (College of Integrated Circuits), Hunan University, Changsha, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","IEEE Transactions on Information Forensics and Security","3 Mar 2023","2023","18","","1667","1679","Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model’s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner’s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker’s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user’s hard-label inference request.","1556-6021","","10.1109/TIFS.2023.3246766","National Natural Science Foundation of China(grant numbers:62122023,U20A20202,62002167); Science and Technology Innovation Program of Hunan Province(grant numbers:2021RC4019); Natural Science Foundation of Fujian Province(grant numbers:2021J01544); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200461); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049136","Model stealing attacks;adversarial perturbation;defense","Perturbation methods;Predictive models;Data models;Training;Computational modeling;Semiconductor device modeling;Privacy","","47","","27","IEEE","20 Feb 2023","","","IEEE","IEEE Journals"
"Designing Cyber Insurance Policies: The Role of Pre-Screening and Security Interdependence","M. M. Khalili; P. Naghizadeh; M. Liu","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA","IEEE Transactions on Information Forensics and Security","26 Apr 2018","2018","13","9","2226","2239","Cyber insurance is a viable method for cyber risk transfer. However, it has been shown that depending on the features of the underlying environment, it may or may not improve the state of network security. In this paper, we consider a single profit-maximizing insurer (principal) with voluntarily participating insureds/clients (agents). We are particularly interested in two distinct features of cybersecurity and their impact on the contract design problem. The first is the interdependent nature of cybersecurity, whereby one entity's state of security depends not only on its own investment and effort, but also the efforts of others' in the same eco-system (i.e., externalities). The second is the fact that recent advances in Internet measurement combined with machine learning techniques now allow us to perform accurate quantitative assessments of security posture at a firm level. This can be used as a tool to perform an initial security audit, or pre-screening, of a prospective client to better enable premium discrimination and the design of customized policies. We show that security interdependency leads to a “profit opportunity” for the insurer, created by the inefficient effort levels exerted by interdependent agents who do not account for the risk externalities when insurance is not available; this is in addition to risk transfer that an insurer typically profits from. Security pre-screening then allows the insurer to take advantage of this additional profit opportunity by designing the appropriate contracts which incentivize agents to increase their effort levels, allowing the insurer to “sell commitment” to interdependent agents, in addition to insuring their risks. We identify conditions under which this type of contract leads to not only increased profit for the principal, but also an improved state of network security.","1556-6021","","10.1109/TIFS.2018.2812205","NSF(grant numbers:CNS-1422211,CNS-1616575,CNS-1739517); DHS(grant numbers:HSHQPM17X00233); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306901","Cybersecurity;cyber insurance;pre-screening;security interdependence","Security;Insurance;Contracts;Communication networks;Investment;Ethics;Hazards","","43","","27","IEEE","5 Mar 2018","","","IEEE","IEEE Journals"
"A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication","R. Xie; W. Xu; Y. Chen; J. Yu; A. Hu; D. W. K. Ng; A. L. Swindlehurst","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Informatics, The University of Edinburgh, Edinburgh, U.K; Purple Mountain Laboratories, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; Center for Pervasive Communications and Computing, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Information Forensics and Security","6 Sep 2021","2021","16","","4435","4450","Radio-frequency fingerprints (RFFs) are promising solutions for realizing low-cost physical layer authentication. Machine learning-based methods have been proposed for RFF extraction and discrimination. However, most existing methods are designed for the closed-set scenario where the set of devices is remains unchanged. These methods cannot be generalized to the RFF discrimination of unknown devices. To enable the discrimination of RFF from both known and unknown devices, we propose a new end-to-end deep learning framework for extracting RFFs from raw received signals. The proposed framework comprises a novel preprocessing module, called neural synchronization (NS), which incorporates the data-driven learning with signal processing priors as an inductive bias from communication-model based processing. Compared to traditional carrier synchronization techniques, which are static, this module estimates offsets by two learnable deep neural networks jointly trained by the RFF extractor. Additionally, a hypersphere representation is proposed to further improve the discrimination of RFF. Theoretical analysis shows that such a data-and-model framework can better optimize the mutual information between device identity and the RFF, which naturally leads to better performance. Experimental results verify that the proposed RFF significantly outperforms purely data-driven DNN-design and existing handcrafted RFF methods in terms of both discrimination and network generalizability.","1556-6021","","10.1109/TIFS.2021.3106166","National Key Research and Development(grant numbers:2020YFB1806600); NSFC(grant numbers:62022026,61871109,61941115); Natural Science Foundation of Jiangsu Province for Distinguished Young Scholars(grant numbers:BK20190012); University of New South Wales (UNSW) Digital Grid Futures Institute, UNSW, Sydney, through a cross-disciplinary fund scheme; Australian Research Council’s Discovery Project(grant numbers:DP210102169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517121","Physical layer authentication;radio frequency fingerprint (RFF);deep learning;open set;representation learning;hypersphere representation","Feature extraction;Authentication;Synchronization;Programmable logic arrays;Deep learning;Physical layer;Performance evaluation","","43","","52","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"DOReN: Toward Efficient Deep Convolutional Neural Networks with Fully Homomorphic Encryption","S. Meftah; B. H. M. Tan; C. F. Mun; K. M. M. Aung; B. Veeravalli; V. Chandrasekhar","Institute of Infocomm Research, Agency of Science, Technology and Research (A*STAR), Singapore; Institute of Infocomm Research, Agency of Science, Technology and Research (A*STAR), Singapore; Institute of Infocomm Research, Agency of Science, Technology and Research (A*STAR), Singapore; Institute of Infocomm Research, Agency of Science, Technology and Research (A*STAR), Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Infocomm Research, Agency of Science, Technology and Research (A*STAR), Singapore","IEEE Transactions on Information Forensics and Security","28 Jul 2021","2021","16","","3740","3752","Fully homomorphic encryption (FHE) is a powerful cryptographic primitive to secure outsourced computations against an untrusted third-party provider. With the growing demand for AI and the usefulness of machine learning as a service (MLaaS), the need for secure training and inference of artificial neural networks is rising. However, the computational complexity of existing FHE schemes has been a strong deterrent to this. Prior works suffered from accuracy degradation, lack of scalability, and ciphertext expansion issues. In this paper, we take the first step towards the problem of space-efficiency in evaluating deep neural networks through designing DOReN: a low depth, batched neuron that can simultaneously evaluate multiple quantized ReLU-activated neurons on encrypted data without approximations. Our circuit design reduced the complexity of the accumulator circuit depth from O(logm ·logn) to O(logm + logn) for n bit integers. The experimental results show that the amortized processing time of our homomorphic neuron is approximately 1.26 seconds for 300 inputs and less than 0.13 seconds for 10 inputs at 80 bit security, which is a 20 fold improvement upon Lou and Jiang, NeurIPS 2019.","1556-6021","","10.1109/TIFS.2021.3090959","Agency of Science, Technology and Research (A*STAR) through the RIE2020 Advanced Manufacturing and Engineering (AME) Programmatic Programme(grant numbers:A19E3b0099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460962","Fully homomorphic encryption;neural networks;depth optimization;ReLu","Biological neural networks;Neurons;Encryption;Adders;Cloud computing;Computational modeling;Table lookup","","36","","39","IEEE","21 Jun 2021","","","IEEE","IEEE Journals"
"Person Identification by Keystroke Dynamics Using Pairwise User Coupling","S. Mondal; P. Bours","University of Twente (UT), Enschede, AE, The Netherlands; Department of Information Security and Communication Technology, Norwegian University of Science and Technology, NO-2802, Gjøvik, Norway","IEEE Transactions on Information Forensics and Security","1 Mar 2017","2017","12","6","1319","1329","Due to the increasing vulnerabilities in cyberspace, security alone is not enough to prevent a breach, but cyber forensics or cyber intelligence is also required to prevent future attacks or to identify the potential attacker. The unobtrusive and covert nature of biometric data collection of keystroke dynamics has a high potential for use in cyber forensics or cyber intelligence. In this paper, we investigate the usefulness of keystroke dynamics to establish the person identity. We propose three schemes for identifying a person when typing on a keyboard. We use various machine learning algorithms in combination with the proposed pairwise user coupling technique and show the performance of each separate technique as well as the performance when combining two or more together. In particular, we show that pairwise user coupling in a bottom-up tree structure scheme gives the best performance, both concerning accuracy and time complexity. The proposed techniques are validated by using keystroke data. However, these techniques could equally well be applied to other pattern identification problems. We have also investigated the optimized feature set for person identification by using keystroke dynamics. Finally, we also examined the performance of the identification system when a user, unlike his normal behaviour, types with only one hand, and we show that performance then is not optimal, as was to be expected.","1556-6021","","10.1109/TIFS.2017.2658539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833085","Pairwise user coupling;keystroke dynamics;person identification;behavioural biometrics;cyber-forensics","Authentication;Couplings;Support vector machines;Keyboards;Computers;Biological neural networks;Training","","34","","33","IEEE","25 Jan 2017","","","IEEE","IEEE Journals"
"Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems","Y. Wang; E. Sarkar; W. Li; M. Maniatakos; S. E. Jabari","Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, New York City, NY, USA; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, New York City, NY, USA; Division of Engineering, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, Tandon School of Engineering, New York University, Brooklyn, New York City, NY, USA; Division of Engineering, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates","IEEE Transactions on Information Forensics and Security","4 Oct 2021","2021","16","","4772","4787","Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.","1556-6021","","10.1109/TIFS.2021.3114024","New York University Abu Dhabi (NYUAD) Center for Interacting Urban Networks (CITIES) by Tamkeen under NYUAD Research Institute(grant numbers:CG001); Swiss Re Institute under the Quantum Cities initiative; Center for CyberSecurity (CCS) by Tamkeen under NYUAD Research Institute(grant numbers:G1104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541185","Autonomous vehicles;artificial neural networks;information security","Games;Training;Deep learning;Computational modeling;Reinforcement learning;Perturbation methods;Integrated circuit modeling","","27","","58","IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Stealing Neural Network Structure Through Remote FPGA Side-Channel Analysis","Y. Zhang; R. Yasaei; H. Chen; Z. Li; M. A. A. Faruque","Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Information Forensics and Security","2 Sep 2021","2021","16","","4377","4388","Deep Neural Network (DNN) models have been extensively developed by companies for a wide range of applications. The development of a customized DNN model with great performance requires costly investments, and its structure (layers and hyper-parameters) is considered intellectual property and holds immense value. However, in this paper, we found the model secret is vulnerable when a cloud-based FPGA accelerator executes it. We demonstrate an end-to-end attack based on remote power side-channel analysis and machine-learning-based secret inference against different DNN models. The evaluation result shows that an attacker can reconstruct the layer and hyper-parameter sequence at over 90% accuracy using our method, which can significantly reduce her model development workload. We believe the threat presented by our attack is tangible, and new defense mechanisms should be developed against this threat.","1556-6021","","10.1109/TIFS.2021.3106169","NSF(grant numbers:ECCS-2028269,DGE-2039634); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517289","Deep neural network;cloud FPGA;side-channel analysis;hardware trojan","Field programmable gate arrays;Cloud computing;Computational modeling;Analytical models;Integrated circuit modeling;Hardware;Inverters","","25","","81","IEEE","19 Aug 2021","","","IEEE","IEEE Journals"
"A Transfer Learning Approach for Securing Resource-Constrained IoT Devices","S. Yılmaz; E. Aydogan; S. Sen","Department of Computer Engineering, Muğla Sıtkı Koçman University, Muğla, Turkey; Department of Computer Engineering, WISE Laboratory, Hacettepe University, Ankara, Turkey; Department of Computer Engineering, Akdeniz University, Antalya, Turkey","IEEE Transactions on Information Forensics and Security","6 Sep 2021","2021","16","","4405","4418","In recent years, Internet of Things (IoT) security has attracted significant interest by researchers due to new characteristics of IoT such as heterogeneity of devices, resource constraints, and new types of attacks targeting IoT. Intrusion detection, which is an indispensable part of a security system, is also included in these studies. In order to explore the complex characteristics of IoT, machine learning methods, which rely on long training time to generate intrusion detection models, are proposed in the literature. Furthermore, these systems need to learn a new/fresh model from scratch when the environment changes. This study explores the use of transfer learning in order to generate intrusion detection algorithms for such dynamically changing IoT. Transfer learning is an approach that stores knowledge learned from a problem domain/task and applies that knowledge to another problem domain/task. Here, it is employed in the following two settings: transferring knowledge for generating suitable intrusion algorithms for new devices, transferring knowledge for detecting new types of attacks. In this study, Routing Protocol for Low-Power and Lossy Network (RPL), a routing protocol for resource-constrained wireless networks, is used as an exemplar protocol and specific attacks against RPL are targeted. The experimental results show that the transfer learning approach gives better performance than the traditional approach. Moreover, the proposed approach significantly reduces learning time, which is an important factor for putting devices/networks in operation in a timely manner. Even though transfer learning has been considered a potential candidate for improving IoT security, to the best of our knowledge, this is the first application of transfer learning under these two settings in RPL-based IoT networks.","1556-6021","","10.1109/TIFS.2021.3096029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478846","IoT;security;transfer learning;intrusion detection;genetic programming;RPL","Transfer learning;Intrusion detection;Statistics;Sociology;Internet of Things;Task analysis;Genetic programming","","22","","43","IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"ForgeryNIR: Deep Face Forgery and Detection in Near-Infrared Scenario","Y. Wang; C. Peng; D. Liu; N. Wang; X. Gao","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, Shaanxi, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, Shaanxi, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, Shaanxi, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, Shaanxi, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Information Forensics and Security","9 Feb 2022","2022","17","","500","515","Deep face forgery and detection is an emerging topic due to the development of GANs. Face forgery detection relies greatly on existing databases for evaluation and adequate training examples for data-hungry machine learning algorithms. However, considering the wide application of face recognition in near-infrared scenarios, there is no publicly available face forgery database that includes near-infrared modality currently. In this paper, we present an attempt at constructing a large-scale dataset for face forgery detection in the near-infrared modality and propose a new forgery detection method based on knowledge distillation named cross-modality knowledge distillation aiming to use a teacher model which is pre-trained on the visible light-based (VIS) big data to guide the student model with a small amount of near-infrared (NIR) data. The proposed near-infrared face forgery dataset, named ForgeryNIR, contains a total of over 50,000 real and fake identities. A number of perturbations are applied to help simulate real-world scenarios. All source images in ForgeryNIR are collected from CASIA NIR-VIS 2.0, and fake images are generated via multiple GAN techniques. The proposed dataset fills the gap of face forgery detection research in the near-infrared modality. A comprehensive study on six representative detection baselines is conducted to evaluate the performance of face forgery detection algorithms in the NIR domain. We further construct a hard testing set, named ForgeryNIR+, which contains forged images that have bypassed existing face forgery detection methods. The proposed datasets will be publicly available and aim to help boost further research on face forgery detection, as well as NIR face detection and recognition.","1556-6021","","10.1109/TIFS.2022.3146766","Guangxi Natural Science Foundation Program(grant numbers:2021GXNSFDA075011); Key Research and Development Program of Shaanxi(grant numbers:2020ZDLGY08-08); National Key Research and Development Program of China(grant numbers:2018AAA0103202); National Natural Science Foundation of China(grant numbers:61922066,61876142,62036007,62072356); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693897","Near-infrared face;face forgery detection;deepfake","Face recognition;Forgery;Videos;Faces;Databases;Perturbation methods;Lighting","","20","","50","IEEE","26 Jan 2022","","","IEEE","IEEE Journals"
"GapFinder: Finding Inconsistency of Security Information From Unstructured Text","H. Jo; J. Kim; P. Porras; V. Yegneswaran; S. Shin","Graduate School of Information Security, School of Computing, KAIST, Daejeon, South Korea; School of Electrical Engineering, KAIST, Daejeon, South Korea; SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; School of Electrical Engineering, KAIST, Daejeon, South Korea","IEEE Transactions on Information Forensics and Security","28 Jul 2020","2021","16","","86","99","Textual data mining of open source intelligence on the Web has become an increasingly important topic across a wide range of domains such as business, law enforcement, military, and cybersecurity. Text mining efforts utilize natural language processing to transform unstructured web content into structured forms that can drive various machine learning applications and data indexing services. For example, applications for text mining in cybersecurity have produced a range of threat intelligence services that serve the IT industry. However, a less studied problem is that of automating the identification of semantic inconsistencies among various text input sources. In this paper, we introduce GapFinder, a new inconsistency checking system for identifying semantic inconsistencies within the cybersecurity domain. Specifically, we examine the problem of identifying technical inconsistencies that arise in the functional descriptions of open source malware threat reporting information. Our evaluation, using tens of thousands of relations derived from web-based malware threat reports, demonstrates the ability of GapFinder to identify the presence of inconsistencies.","1556-6021","","10.1109/TIFS.2020.3003570","Engineering Research Center Program through the National Research Foundation of Korea (NRF); Korean Government MSIT(grant numbers:NRF-2018R1A5A1059921); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121316","Cyber threat intelligence;CTI;inconsistency","Malware;Computer security;Semantics;Text mining;Blogs","","17","","40","IEEE","19 Jun 2020","","","IEEE","IEEE Journals"
"EEFED: Personalized Federated Learning of Execution&Evaluation Dual Network for CPS Intrusion Detection","X. Huang; J. Liu; Y. Lai; B. Mao; H. Lyu","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","IEEE Transactions on Information Forensics and Security","6 Dec 2022","2023","18","","41","56","In the modern interconnected world, intelligent networks and computing technologies are increasingly being incorporated in industrial systems. However, this adoption of advanced technology has resulted in increased cyber threats to cyber-physical systems. Existing intrusion detection systems are continually challenged by constantly evolving cyber threats. Machine learning algorithms have been applied for intrusion detection. In these techniques, a classification model is trained by learning cyber behavior patterns. However, these models typically require considerable high-quality datasets. Limited attack samples are available because of the unpredictability and constant evolution of cyber threats. To address these problems, we propose a novel federated Execution & Evaluation dual network framework (EEFED), which allows multiple federal participants to personalize their local detection models undermining the original purpose of Federated Learning. Thus, a general global detection model was developed for collaboratively improving the performance of a single local model against cyberattacks. The proposed personalized update algorithm and the optimizing backtracking parameters replacement policy effectively reduced the negative influence of federated learning in imbalanced and non-i.i.d distribution of data. The proposed method improved model stability. Furthermore, extensive experiments conducted on a network dataset in various cyber scenarios revealed that the proposed method outperformed single model and state-of-the-art methods.","1556-6021","","10.1109/TIFS.2022.3214723","National Key R&D Program of China (Key Technologies and Applications of Security and Trusted Industrial Control System(grant numbers:2020YFB2009500); Beijing Natural Science Foundation(grant numbers:L192020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919869","Federated learning;cyber-physical system (CPS);intrusion detection;cyber security;personalized model","Data models;Computational modeling;Training;Security;Computer crime;Intrusion detection;Federated learning","","13","","40","CCBYNCND","14 Oct 2022","","","IEEE","IEEE Journals"
"Earprint: Transient Evoked Otoacoustic Emission for Biometrics","Y. Liu; D. Hatzinakos","Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Information Forensics and Security","19 May 2017","2014","9","12","2291","2301","Biometrics is attracting increasing attention in privacy and security concerned issues, such as access control and remote financial transaction. However, advanced forgery and spoofing techniques are threatening the reliability of conventional biometric modalities. This has been motivating our investigation of a novel yet promising modality transient evoked otoacoustic emission (TEOAE), which is an acoustic response generated from cochlea after a click stimulus. Unlike conventional modalities that are easily accessible or captured, TEOAE is naturally immune to replay and falsification attacks as a physiological outcome from human auditory system. In this paper, we resort to wavelet analysis to derive the time-frequency representation of such nonstationary signal, which reveals individual uniqueness and long-term reproducibility. A machine learning technique linear discriminant analysis is subsequently utilized to reduce intrasubject variability and further capture intersubject differentiation features. Considering practical application, we also introduce a complete framework of the biometric system in both verification and identification modes. Comparative experiments on a TEOAE data set of biometric setting show the merits of the proposed method. Performance is further improved with fusion of information from both ears.","1556-6021","","10.1109/TIFS.2014.2361205","Natural Sciences and Engineering Research Council of Canada; Ontario Brain Institute, Toronto, ON, Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914592","Robust Biometric Modality;Transient Evoked Otoacoustic Emission;Time-frequency Analysis;Robust biometric modality;transient evoked otoacoustic emission;time-frequency analysis;linear discriminant analysis;biometric fusion","Biometrics (access control);Time-frequency analysis;Linear discriminant analysis;Auditory system;Feature extraction","","12","19","27","IEEE","1 Oct 2014","","","IEEE","IEEE Journals"
"Anti-Forensics of Environmental-Signature-Based Audio Splicing Detection and Its Countermeasure via Rich-Features Classification","H. Zhao; Y. Chen; R. Wang; H. Malik","Department of Electrical and Electronic Engineering, South University of Science and Technology of China, Shenzhen, China; Department of Electrical and Electronic Engineering, South University of Science and Technology of China, Shenzhen, China; Department of Electrical and Electronic Engineering, South University of Science and Technology of China, Shenzhen, China; Department of Electrical and Computer Engineering, University of Michigan–Dearborn, Dearborn, MI, USA","IEEE Transactions on Information Forensics and Security","19 May 2017","2016","11","7","1603","1617","Numerous methods for detecting audio splicing have been proposed. Environmental-signature-based methods are considered to be the most effective forgery detection methods. The performance of existing audio forensic analysis methods is generally measured in the absence of any anti-forensic attack. Effectiveness of these methods in the presence of anti-forensic attacks is therefore unknown. In this paper, we propose an effective anti-forensic attack for environmental-signature-based splicing detection method and countermeasures to detect the presence of the anti-forensic attack. For anti-forensic attack, dereverberation-based processing is proposed. Three dereverberation methods are considered to tamper with the acoustic environment signature. Experimental results indicate that the proposed dereverberation-based anti-forensic attack significantly degrades the performance of the selected splicing detection method. The proposed countermeasures exploit artifacts introduced by the anti-forensic processing. To detect the presence of potential anti-forensic processing, a machine learning-based framework is proposed. In particular, the proposed anti-forensic detection method uses a rich-feature model consisting of Fourier coefficients, spectral properties, high-order statistics of musical noise residuals, and modulation spectral coefficients to capture traces of dereverberation attacks. The performance of the proposed framework is evaluated on both synthetic data and real-world speech recordings. The experimental results show that the proposed rich-feature model can detect the presence of anti-forensic processing with an average accuracy of 95%.","1556-6021","","10.1109/TIFS.2016.2543205","2013 Guangdong Natural Science Funds for Distinguished Young Scholars(grant numbers:S2013050014223); National Science Foundation(grant numbers:CNS-1440929); National Plan for Science, Technology and Innovation, King Abdulaziz City for Science and Technology, Saudi Arabia(grant numbers:12-INF2634-02); National Natural Science Foundation of China(grant numbers:61402219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435302","Audio splicing detection;anti-forensics;spectral features;musical noise;modulation spectrum;Audio splicing detection;anti-forensics;spectral features;musical noise;modulation spectrum","Splicing;Forensics;Speech;Acoustics;Authentication;Robustness;Feature extraction","","9","","52","IEEE","17 Mar 2016","","","IEEE","IEEE Journals"
"Cancellable Template Design for Privacy-Preserving EEG Biometric Authentication Systems","M. Wang; S. Wang; J. Hu","School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Computing, Engineering and Mathematical Sciences, La Trobe University, Melbourne, VIC, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Information Forensics and Security","28 Sep 2022","2022","17","","3350","3364","As a promising candidate to complement traditional biometric modalities, brain biometrics using electroencephalography (EEG) data has received a widespread attention in recent years. However, compared with existing biometrics such as fingerprints and face recognition, research on EEG biometrics is still in its infant stage. Most of the studies focus on either designing signal elicitation protocols from the perspective of neuroscience or developing feature extraction and classification algorithms from the viewpoint of machine learning. These studies have laid the ground for the feasibility of using EEG as a biometric verification modality, but they have also raised security and privacy concerns as EEG data contains sensitive information. Existing research has used hash functions and cryptographic schemes to protect EEG data, but they do not provide functions for revoking compromised templates as in cancellable template design. This paper proposes the first cancellable EEG template design for privacy-preserving EEG-based verification systems, which can protect raw EEG signals containing sensitive privacy information (e.g., identity, health and cognitive status). A novel cancellable EEG template is developed based on EEG features extracted by a deep learning model and a non-invertible transform. The proposed transformation provides cancellable templates, while taking advantage of EEG elicitation protocol fusion to enhance biometric performance. The proposed verification system offers superior performance than the state-of-the-art, while protecting raw EEG data. Furthermore, we analyze the system’s capacity for resisting multiple attacks, and discuss some overlooked but critical issues and possible pitfalls involving hill-climbing attacks, second attacks, and classification-based verification systems.","1556-6021","","10.1109/TIFS.2022.3204222","Australian Research Council Discovery(grant numbers:DP200103207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875345","EEG biometrics;brain biometrics;verification system;privacy-preserving;cancellable biometrics;non-invertible transformation;template protection","Electroencephalography;Biometrics (access control);Feature extraction;Protocols;Brain modeling;Training;Biological system modeling","","9","","50","CCBY","5 Sep 2022","","","IEEE","IEEE Journals"
"A First Public Research Collection of High-Resolution Latent Fingerprint Time Series for Short- and Long-Term Print Age Estimation","R. Merkel; J. Dittmann; C. Vielhauer","Working Group Multimedia and Security, Otto-von-Guericke-University of Magdeburg, Magdeburg, Germany; Working Group Multimedia and Security, Otto-von-Guericke-University of Magdeburg, Magdeburg, Germany; Department of Informatics and Media, Brandenburg University of Applied Sciences, Brandenburg an der Havel, Germany","IEEE Transactions on Information Forensics and Security","26 Jun 2017","2017","12","10","2276","2291","The creation of publicly available image databases for the signal processing community is a very time-consuming, yet immensely valuable task, enabling scientific progress by providing the opportunity of an objective comparison and reproduction of results. This paper presents for the first time a public research collection of high-resolution latent fingerprint time series for age estimation, captured from a pool of 116 different test subjects. It comprises ten different sets with a total of 2,618 time series (117,384 scans), varying between capturing devices (CWL and CLSM), data types (intensity versus topography), aging periods (short-term aging: 24 h, long-term aging: 0.5 - 3 years) and resolutions (1,270 - 180,142 ppi). Most series are annotated with donor information (age and gender) and capturing conditions (scan parameters, ambient temperature, and humidity). The data are anonymized (using partial prints only) and an organizational revocation mechanism is included to assure non-identifiability of donors in the future. Baseline results for age estimation on all ten sets are provided in the form of correlation coefficients and machine-learning based age estimation (kappa), using 19 features from prior feature spaces as well as new ones (Tamura contrast, Benford's law, and improved dust feature). Classification results exhibit kappa values between 0.51 and 0.85, highlighting the progress made in this very challenging area in recent years and also emphasizing the need of future studies on the issue.","1556-6021","","10.1109/TIFS.2017.2705622","German Federal Ministry of Education and Science (BMBF) through the Research Programme (Digi-Dak, fingerprint related parts), (INSPECT, providing an important application scenario for latent prints on well-reflecting surfaces, such as credit-cards, keypads, or smartphone displays)(grant numbers:FKZ: 13N10816,FKZ: 13N10818,FKZ: 13N13473); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931576","Latent fingerprints;public research collection;age estimation;digitized forensics;computer forensics;fingerprint processing pipeline;baseline performance","Estimation;Time series analysis;Aging;Fingerprint recognition;Forensics;Sensors;Data privacy","","9","","41","CCBY","18 May 2017","","","IEEE","IEEE Journals"
"Privacy-Preserving Federated Learning via Functional Encryption, Revisited","Y. Chang; K. Zhang; J. Gong; H. Qian","Software Engineering Institute, East China Normal University, Shanghai, China; College of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China","IEEE Transactions on Information Forensics and Security","24 Mar 2023","2023","18","","1855","1869","Federated Learning (FL), emerging as a distributed machine learning, is a popular paradigm that allows multiple users to collaboratively train an intermediate model by exchanging local models without the training data leaving each user’s domain. However, FL still suffer from privacy risk such as leaking private information from users’ uploaded local models. To address the privacy concern, several approaches have been proposed to achieve privacy-preserving FL (PPFL) based on differential privacy (DP), multi-party computation (MPC), homomorphic encryption (HE) and functional encryption (FE). Compared with DP, MPC and HE, approaches based on FE are more advantageous and thus become the focus of this work. Moreover, all existing PPFL schemes via FE employ a multi-user extension of FE for a specific function, i.e., multi-input FE (MIFE). In this paper, we point out that existing FE-based PPFL schemes have faced with several security issues due to the misuse of MIFE. After reconsidering the security requirements of PPFL, we propose new goals of designing PPFL using FE. To achieve our goals, we propose a new FE called dual-mode decentralized multi-client FE (2DMCFE) and give a concreate construction for 2DMCFE. With 2DMCFE, we propose a new framework of PPFL where we establish a fresh 2DMCFE instance for each subset of users. Security proof shows the strong security of our framework under the semi-honest security setting. Furthermore, experiments conducted on real dataset demonstrate that our framework achieves comparable model accuracy and training efficiency to the basic FE-based scheme while providing stronger security guarantee.","1556-6021","","10.1109/TIFS.2023.3255171","National Natural Science Foundation of China(grant numbers:62002120,61802248); Shanghai Rising-Star Program(grant numbers:22QA1403800); Innovation Program of Shanghai Municipal Education Commission(grant numbers:2021-01-07-00-08-E00101); “Digital Silk Road” Shanghai International Joint Laboratory of Trustworthy Intelligent Software(grant numbers:22510750100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064312","Privacy-preserving;federated learning;functional encryption","Iron;Computational modeling;Data models;Training;Federated learning;Privacy;Task analysis","","8","","43","IEEE","9 Mar 2023","","","IEEE","IEEE Journals"
"Transformation-Aware Embeddings for Image Provenance","A. Bharati; D. Moreira; P. J. Flynn; A. de Rezende Rocha; K. W. Bowyer; W. J. Scheirer","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Institute of Computing, University of Campinas, Campinas SP, Brazil; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Information Forensics and Security","19 Feb 2021","2021","16","","2493","2507","A dramatic rise in the flow of manipulated image content on the Internet has led to a prompt response from the media forensics research community. New mitigation efforts leverage cutting-edge data-driven strategies and increasingly incorporate usage of techniques from computer vision and machine learning to detect and profile the space of image manipulations. This paper addresses Image Provenance Analysis, which aims at discovering relationships among different manipulated image versions that share content. One important task in provenance analysis, like most visual understanding problems, is establishing a visual description and dissimilarity computation method that connects images that share full or partial content. But the existing handcrafted or learned descriptors - generally appropriate for tasks such as object recognition - may not sufficiently encode the subtle differences between near-duplicate image variants, which significantly characterize the provenance of any image. This paper introduces a novel data-driven learning-based approach that provides the context for ordering images that have been generated from a single image source through various transformations. Our approach learns transformation-aware embeddings using weak supervision via composited transformations and a rank-based Edit Sequence Loss. To establish the effectiveness of the proposed approach, comparisons are made with state-of-the-art handcrafted and deep-learning-based descriptors, as well as image matching approaches. Further experimentation validates the proposed approach in the context of image provenance analysis and improves upon existing approaches.","1556-6021","","10.1109/TIFS.2021.3050061","DARPA and Air Force Research Laboratory (AFRL)(grant numbers:FA8750-16-2-0173); NVIDIA Corporation; Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) (DéjàVu Project)(grant numbers:2017/12646-3); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) (DeepEyes Grant); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq)(grant numbers:304497/2018-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316916","Image provenance analysis;image manipulation;deep learning;forensics;edit sequence loss","Media;Forensics;Task analysis;Visualization;Image matching;Image retrieval;Image coding","","8","","68","IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"MEG: Memory and Energy Efficient Garbled Circuit Evaluation on Smartphones","Q. Yang; G. Peng; P. Gasti; K. S. Balagani; Y. Li; G. Zhou","Department of Computer Science, College of William and Mary, Williamsburg, VA, USA; Google, Sunnyvale, CA, USA; School of Engineering and Computing Sciences, New York Institute of Technology, New York, NY, USA; School of Engineering and Computing Sciences, New York Institute of Technology, New York, NY, USA; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, College of William and Mary, Williamsburg, VA, USA","IEEE Transactions on Information Forensics and Security","31 Oct 2018","2019","14","4","913","922","Garbled circuits are general tools that allow two parties to compute any function without disclosing their respective inputs. Applications of this technique vary from distributed privacy-preserving machine learning tasks to secure outsourced authentication. Unfortunately, the energy cost of garbled circuit evaluation protocols is substantial. This limits the applicability of garbled circuits in scenarios that involve battery-operated devices, such as Internet-of-Things (IoT) devices and smartphones. In this paper, we propose MEG, a Memory- and Energy-efficient Garbled circuit evaluation mechanism. MEG utilizes batch data transmission and multi-threading to reduce memory and energy consumption. We implement MEG on an Android smartphone and compare its performance and energy consumption with state-of-the-art techniques using two garbled circuits of widely different sizes (AES-128 and 256-bit edit distance). Our results show that, compared with “plain” garbled circuit evaluation, MEG decreases memory consumption by more than 90%. When compared with current pipelined garbled circuit evaluation techniques, MEG's energy usage was 42% lower for AES-128 and 23% lower for EDT-256. Furthermore, our multi-thread implementation of MEG decreased circuit evaluation time by up to 56.7% for AES-128, and by up to 13.5% for EDT-256, compared with state-of-the-art pipelining techniques.","1556-6021","","10.1109/TIFS.2018.2868221","National Science Foundation(grant numbers:CNS-1619023,CNS-1618300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452954","Security;side-channel attacks","Logic gates;Protocols;Generators;Memory management;Smart phones;Pipeline processing;Wires","","7","","31","IEEE","31 Aug 2018","","","IEEE","IEEE Journals"
"Progressive Focusing Algorithm for Reliable Pose Estimation of Latent Fingerprints","C. Deerada; K. Phromsuthirak; A. Rungchokanun; V. Areekul","Department of Electrical Engineering, Kasetsart University, Bangkok, Thailand; Department of Electrical Engineering, Kasetsart University, Bangkok, Thailand; Department of Electrical Engineering, Kasetsart University, Bangkok, Thailand; Department of Electrical Engineering, Kasetsart University, Bangkok, Thailand","IEEE Transactions on Information Forensics and Security","11 Dec 2019","2020","15","","1232","1247","A pose of fingerprint composes of a reference point and its corresponding orientation. The reliable pose plays a crucial role in fingerprint alignment and registration. Pose estimation of latent fingerprints is a challenging problem. A few methods based on machine learning were proposed in the past decade. These methods try to predict a pose from corrupted information. In this paper, we propose a systematic feedback approach which can remedy the corrupted information while simultaneously estimates a reliable pose. Without manual segmentation, our fully automatic algorithm is able to progressively locate potential poses, and enhance weak friction ridges that form and support these poses through an iterative feedback. Using the NIST SD27 and MOLF DB4 latent fingerprint databases, our experimental results show that our proposed algorithm outperforms the state-of-the-art and existing commercial products for latent fingerprint pose estimation in terms of precision and identification accuracy.","1556-6021","","10.1109/TIFS.2019.2934865","Kasetsart University; Siew-Sngiem Karnchanachari Research Leadership and Young Professorship Awards; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794816","Pose estimation;focal point;reference point detection;latent fingerprint","Reliability;Pose estimation;Indexes;Noise measurement;Friction;Mathematical model","","7","","52","IEEE","12 Aug 2019","","","IEEE","IEEE Journals"
"Improvement of Min-Entropy Evaluation Based on Pruning and Quantized Deep Neural Network","H. Li; J. Zhang; Z. Li; J. Liu; Y. Wang","Key Laboratory of Advanced Transducers and Intelligent Control System, Ministry of Education of China, College of Physics and Optoelectronics, Taiyuan University of Technology, Taiyuan, China; Key Laboratory of Advanced Transducers and Intelligent Control System, Ministry of Education of China, College of Physics and Optoelectronics, Taiyuan University of Technology, Taiyuan, China; China Electric Power Research Institute, Beijing, China; Nations Technology Company Ltd, Shenzhen, China; Advanced Institute of Photonic Technology, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Information Forensics and Security","7 Feb 2023","2023","18","","1410","1420","In the field of information security, the unpredictability of random numbers plays determinant role according to the security of cryptographic systems. However, limited by the capability of pattern recognition and data mining, statistical-based methods for random number security assessment can only detect whether there are obvious statistical flaws in random sequences. In recent years, some machine learning-based techniques such as deep neural networks and prediction-based methods applied to random number security have exhibited superior performance. Concurrently, the proposed deep learning models bring out issues of large number of parameters, high storage space occupation and complex computation. In this paper, for the challenge of random number security analysis: building high-performance predictive models, we propose an effective analysis method based on pruning and quantized deep neural network. Firstly, we train a temporal pattern attention-based long short-term memory (TPA-LSTM) model with complex structure and good prediction performance. Secondly, through pruning and quantization operations, the complexity and storage space occupation of the TPA-LSTM model were reduced. Finally, we retrain the network to find the best model and evaluate the effectiveness of this method using various simulated data sets with known min-entropy values. By comparing with related work, the TPA-LSTM model provides more accurate estimates: the relative error is less than 0.43%. In addition, the model weight parameters are reduced by more than 98% and quantized to 2 bits (compression over 175x) without accuracy loss.","1556-6021","","10.1109/TIFS.2023.3240859","National Key Research and Development Program of China(grant numbers:2019YFB1803500); National Natural Science Foundation of China(grant numbers:62045009,61731014); Natural Science Foundation of Shanxi Province(grant numbers:202103021224038,20210302123185); International Cooperation of Key Research and Development Program of Shanxi Province(grant numbers:201903D421012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032186","Min-entropy;TPA-LSTM;random number;pruning;quantization","Security;Predictive models;Entropy;Deep learning;Data models;Quantization (signal);Neural networks","","6","","39","IEEE","30 Jan 2023","","","IEEE","IEEE Journals"
"Domain-Agnostic Document Authentication Against Practical Recapturing Attacks","C. Chen; S. Zhang; F. Lan; J. Huang","Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China; Guangdong Oppo Mobile Telecommunications Corporation Ltd., Shenzhen, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","17 Aug 2022","2022","17","","2890","2905","Recapturing attack can be employed as a simple but effective anti-forensic tool for digital document images. Inspired by the document inspection process that compares a questioned document against some known samples, we proposed a document recapture detection scheme by employing a Siamese network to compare and extract distinct features in a recaptured document image. The proposed algorithm takes advantage of both metric learning and image forensic techniques, and forms triplets by considering some important factors in document authentication, e.g., document types, resolutions, and content in each image patch. After training with our triplet selection strategy, the resulting feature embedding clusters the genuine samples near the reference while pushing the recaptured samples apart. In the experiment, we consider practical settings under domain differences, such as the variations in printing/imaging devices, substrates, recapturing channels, and document types. To evaluate the robustness of different approaches, we benchmark some popular off-the-shelf machine learning-based approaches, a state-of-the-art document image detection scheme, and the proposed schemes with different network backbones under various experimental protocols. Experimental results show that the proposed scheme consistently outperforms the state-of-the-art approaches under different experimental settings. Specifically, under the most challenging scenario in our experiment, i.e., evaluation across different types of documents (produced by different manufacturers, devices, and substrates), we have achieved 6.92% APCER (Attack Presentation Classification Error Rate) and 8.51% BPCER (Bona Fide Presentation Classification Error Rate) by the proposed network with ResNeXt101 backbone at 5.00% BPCER decision threshold.","1556-6021","","10.1109/TIFS.2022.3197054","Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010139003); NSFC(grant numbers:62072313,U19B2022); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515010563); Shenzhen Research and Development Program(grant numbers:20200813110043002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851649","Document image;recapture detection;deep learning","Feature extraction;Authentication;Substrates;Printing;Liquid crystal displays;Forgery;Databases","","5","","46","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Efficient Verifiable Protocol for Privacy-Preserving Aggregation in Federated Learning","T. Eltaras; F. Sabry; W. Labda; K. Alzoubi; Q. AHMEDELTARAS","Computer Science and Engineering Department, Qatar University, Doha, Qatar; Computer Science and Engineering Department, Qatar University, Doha, Qatar; Computer Science and Engineering Department, Qatar University, Doha, Qatar; Engineering Technology Department, Community College, Qatar University, Doha, Qatar; Computer Science and Engineering Department, Qatar University, Doha, Qatar","IEEE Transactions on Information Forensics and Security","15 May 2023","2023","18","","2977","2990","Federated learning has gained extensive interest in recent years owing to its ability to update model parameters without obtaining raw data from users, which makes it a viable privacy-preserving machine learning model for collaborative distributed learning among various devices. However, due to the fact that adversaries can track and deduce private information about users from shared gradients, federated learning is vulnerable to numerous security and privacy threats. In this work, a communication-efficient protocol for secure aggregation of model parameters in a federated learning setting is proposed where training is done on user devices while the aggregated trained model could be constructed on the server side without revealing the raw data of users. The proposed protocol is robust against users’ dropouts, and it enables each user to independently validate the aggregated result supplied by the server. The suggested protocol is secure in an honest-but-curious environment, and privacy is maintained even if the majority of parties are in collusion. A practical scenario for the proposed setting is discussed. Additionally, a simulation of the protocol is evaluated, and results demonstrate that it outperforms one of the state-of-art protocols, especially when the number of dropouts increases.","1556-6021","","10.1109/TIFS.2023.3273914","Qatar National Research Fund (QNRF); Qatar National Library; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10121168","Federated learning;privacy-preserving;secure aggregation;verifiable aggregation;cloud computing","Protocols;Computational modeling;Federated learning;Privacy;Differential privacy;Organizations;Data models","","5","","50","CCBY","8 May 2023","","","IEEE","IEEE Journals"
"Profiled Side-Channel Attack on Cryptosystems Based on the Binary Syndrome Decoding Problem","B. Colombier; V. -F. Drăgoi; P. -L. Cayrel; V. Grosso","TIMA, CNRS, Grenoble INP, Université Grenoble Alpes, Grenoble, France; Faculty of Exact Sciences, Aurel Vlaicu University, Arad, Romania; aboratoire Hubert Curien UMR 5516, CNRS, Université Jean Monnet, Saint-Etienne, France; aboratoire Hubert Curien UMR 5516, CNRS, Université Jean Monnet, Saint-Etienne, France","IEEE Transactions on Information Forensics and Security","4 Oct 2022","2022","17","","3407","3420","The NIST standardization process for post-quantum cryptography has been drawing the attention of researchers to the submitted candidates. One direction of research consists in implementing those candidates on embedded systems and that exposes them to physical attacks in return. The Classic McEliece cryptosystem, which is among the four finalists of round 3 in the Key Encapsulation Mechanism category, builds its security on the hardness of the syndrome decoding problem, which is a classic hard problem in code-based cryptography. This cryptosystem was recently targeted by a laser fault injection attack leading to message recovery. Regrettably, the attack setting is very restrictive and it does not tolerate any error in the faulty syndrome. Moreover, it depends on the very strong attacker model of laser fault injection, and does not apply to optimised implementations of the algorithm that make optimal usage of the machine words capacity. In this article, we propose a to change the angle and perform a message-recovery attack that relies on side-channel information only. We improve on the previously published work in several key aspects. First, we show that side-channel information, obtained with power consumption analysis, is sufficient to obtain an integer syndrome, as required by the attack framework. This is done by leveraging classic machine learning techniques that recover the Hamming weight information very accurately. Second, we put forward a computationally-efficient method, based on a simple dot product and information-set decoding algorithms, to recover the message from the, possibly inaccurate, recovered integer syndrome. Finally, we present a masking countermeasure against the proposed attack.","1556-6021","","10.1109/TIFS.2022.3198277","French National Research Agency in the Framework of the “Investissements d’avenir” Program(grant numbers:ANR-15-IDEX-02); LabEx PERSYVAL(grant numbers:ANR-11-LABX-0025-01); Ministry of Research, Innovation and Digitization, CNCS/CCCDI—UEFISCDI within PNCDI III(grant numbers:PN-III-P1-1.1-PD-2019-0285); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854893","Post-quantum cryptography;syndrome decoding problem;side-channel attack","Decoding;NIST;Side-channel attacks;Semiconductor lasers;Encryption;Encapsulation;Public key","","4","","52","IEEE","11 Aug 2022","","","IEEE","IEEE Journals"
"SGBoost: An Efficient and Privacy-Preserving Vertical Federated Tree Boosting Framework","J. Zhao; H. Zhu; W. Xu; F. Wang; R. Lu; H. Li","School of Cyber Engineering, Xidian University, Shaanxi, Xi’an, China; School of Cyber Engineering, Xidian University, Shaanxi, Xi’an, China; School of Cyber Engineering, Xidian University, Shaanxi, Xi’an, China; School of Cyber Engineering, Xidian University, Shaanxi, Xi’an, China; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; School of Cyber Engineering, Xidian University, Shaanxi, Xi’an, China","IEEE Transactions on Information Forensics and Security","5 Jan 2023","2023","18","","1022","1036","Aiming at balancing data privacy and availability, Google introduces the concept of federated learning, which can construct global machine learning models over multiple participants while keeping their raw data localized. However, the exchanged parameters in traditional federated learning may still reveal the data information. Meanwhile, the training data are usually partitioned vertically in real-world scenes, which causes difficulties in model construction. To tackle these problems, in this paper, we propose an efficient and privacy-preserving vertical federated tree boosting framework, namely SGBoost, where multiple participants can collaboratively perform model training and query without staying online all the time. Specifically, we first design secure bucket sharing and best split finding algorithms, with which the global tree model can be constructed over vertically partitioned data; meanwhile, the privacy of training data can be well guaranteed. Then, we design an oblivious query algorithm to utilize the trained model without leaking any query data or results. Moreover, SGBoost does not require multi-round interactions between participants, significantly improving the system efficiency. Detailed security analysis shows that SGBoost can well guarantee the privacy of raw data, weights, buckets, and split information. Extensive experiments demonstrate that SGBoost can achieve high accuracy comparable to centralized training and efficient performance.","1556-6021","","10.1109/TIFS.2022.3232955","National Natural Science Foundation of China(grant numbers:U22B2030,61972304); Science Foundation of the Ministry of Education(grant numbers:MCM20200101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002374","Vertical federated learning;tree boosting;privacy-preserving;efficiency","Training;Data models;Boosting;Computational modeling;Data privacy;Federated learning;Privacy","","4","","43","IEEE","28 Dec 2022","","","IEEE","IEEE Journals"
"Privacy-Enhancing Face Obfuscation Guided by Semantic-Aware Attribution Maps","J. Li; H. Zhang; S. Liang; P. Dai; X. Cao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen, China; School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","26 Jun 2023","2023","18","","3632","3646","Face recognition technology is increasingly being integrated into our daily life, e.g. Face ID. With the advancement of machine learning algorithms, the personal information such as age, gender, and race can be easily deduced from the recorded face images in these applications. This poses a serious privacy threat to individuals who do not want to be profiled, as face images are collected for biometric purposes. Existing methods mostly focus on adding the invisible adversarial perturbations into the images to make automatic inference infeasible. However, the application scenarios of these methods are limited due to the perturbations depending on the specific model. In this paper, we introduce a novel face privacy-enhancing framework by obfuscating the stored faces, which could maintain the data utility (face identity) while protecting the privacy of users (facial attributes). Specifically, we first develop a feature attribution module to discover the identity-related facial parts. Within this module, we introduce a pixel importance estimation model based on Shapley value to obtain a pixel-level attribution map, and then each pixel on the attribution map is aggregated into semantic facial parts, which are used to quantify the importance of different facial parts. Next, we design a privacy-enhancing module to generate the high-quality obfuscated images, which can modify the privacy semantic content and preserve the identity-related information. Using the proposed method, users can choose the single or multiple attributes to be obfuscated without affecting identity matching. Extensive experiments conducted on CelebA-HQ and VGGFace2-HQ benchmarks demonstrate the effectiveness and generalization ability of our method.","1556-6021","","10.1109/TIFS.2023.3282384","National Key Research and Development Program of China(grant numbers:2021YFB3100800); National Natural Science Foundation of China(grant numbers:62025604,62132006,62072454); Beijing Natural Science Foundation(grant numbers:L212004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143276","Privacy preserving;face obfuscation;feature attribution","Face recognition;Privacy;Semantics;Perturbation methods;Facial features;Biometrics (access control);Visualization","","2","","65","IEEE","2 Jun 2023","","","IEEE","IEEE Journals"
"VCD-FL: Verifiable, Collusion-Resistant, and Dynamic Federated Learning","S. Gao; J. Luo; J. Zhu; X. Dong; W. Shi","School of Information, Central University of Finance and Economics, Beijing, China; School of Information, Central University of Finance and Economics, Beijing, China; School of Information, Central University of Finance and Economics, Beijing, China; School of Computer Science and Technology, Xidian University, Xi’an, China; Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA","IEEE Transactions on Information Forensics and Security","29 Jun 2023","2023","18","","3760","3773","Federated learning (FL) is essentially a distributed machine learning paradigm that enables the joint training of a global model by aggregating gradients from participating clients without exchanging raw data. However, a malicious aggregation server may deliberately return designed results without any operation to save computation overhead, or even launch privacy inference attacks using crafted gradients. There are only a few schemes focusing on verifiable FL, and yet they cannot achieve collusion-resistant verification. In this paper, we propose a novel Verifiable, Collusion-resistant, and Dynamic FL (VCD-FL) to tackle this issue. Specifically, we first optimize Lagrange interpolation by gradient grouping and compression for achieving efficient verifiability of FL. To protect clients’ data privacy against collusion attacks, we propose a lightweight commitment scheme using irreversible gradient transformation. By integrating the proposed efficient verification mechanism with the novel commitment scheme, our VCD-FL can detect whether or not the aggregation server is involved in collusion attacks. Moreover, considering that clients might go offline due to some reason such as network anomaly and client crash, we adopt the secret sharing technique to eliminate the effect of federation dynamics on FL. In a nutshell, our VCD-FL can achieve collusion-resistant verification and collusion attack detection with supporting the correctness, privacy, and dynamics. Finally, we theoretically prove the effectiveness of our VCD-FL, make comprehensive comparisons, and conduct a series of experiments on MNIST dataset with MLP and CNN models. The theoretical proof and experimental analysis demonstrate that our VCD-FL is computationally efficient, robust against collusion attacks, and able to support the dynamics of FL.","1556-6021","","10.1109/TIFS.2023.3271268","Beijing Natural Science Foundation(grant numbers:M21036); National Natural Science Foundation of China(grant numbers:62072487,61972310); Zhejiang Provincial Natural Science Foundation(grant numbers:LD22F020002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10109820","Federated learning;privacy preservation;verifiability;collusion-resistant;dynamics","Interpolation;Privacy;Training;Data privacy;Servers;Computational modeling;Behavioral sciences","","2","","30","IEEE","27 Apr 2023","","","IEEE","IEEE Journals"
"Efficient Privacy-Preserving Inference Outsourcing for Convolutional Neural Networks","X. Yang; J. Chen; K. He; H. Bai; C. Wu; R. Du","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","9 Aug 2023","2023","18","","4815","4829","Inference outsourcing enables model owners to deploy their machine learning models on cloud servers to serve users. In this paradigm, the privacy of model owners and users should be considered. Existing solutions focus on Convolutional Neural Networks (CNNs) but their efficiency is much lower than GALA, which is a solution that only protects user privacy. Furthermore, these solutions adopt approximations that reduce the model accuracy and thus require model owners to retrain the models. In this paper, we present an efficient CNN inference outsourcing solution that protects the privacy of both model owners and users. Specifically, we design secure two-party computation protocols based on two non-colluding cloud servers, which calculate with additive secret shares of the model and the user’s input. Our protocols avoid the expensive permutation operations in linear calculations and approximations in non-linear calculations. We implement our solution on realistic CNNs and experimental results show that our solution is even 2–4 times faster than GALA.","1556-6021","","10.1109/TIFS.2023.3287072","National Key Research and Development Program of China(grant numbers:2022YFB3102100); Fundamental Research Funds for the Central Universities(grant numbers:2042022kf1195,2042022kf0046); National Natural Science Foundation of China(grant numbers:62076187,62172303); Key Research and Development Program of Hubei Province(grant numbers:2022BAA039); Key Research and Development Program of Shandong Province(grant numbers:2022CXPT055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154059","Privacy-preserving inference;convolutional neural networks","Computational modeling;Servers;Cryptography;Convolutional neural networks;Integrated circuit modeling;Protocols;Privacy","","2","","45","IEEE","16 Jun 2023","","","IEEE","IEEE Journals"
"SecKNN: FSS-Based Secure Multi-Party KNN Classification Under General Distance Functions","Z. Li; H. Wang; S. Zhang; W. Zhang; R. Lu","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada","IEEE Transactions on Information Forensics and Security","11 Dec 2023","2024","19","","1326","1341","As a practical machine learning method, the K-nearest neighbors (KNN) classification has received widespread attention. The achievement of the KNN classification relies heavily on a large amount of labeled data. However, in the real world, data is often held by different data owners. How to realize efficient joint computing among multiple data owners under the premise of protecting data security and privacy is an urgent problem to be solved. In this paper, we construct a secure multi-party KNN classification scheme (SecKNN) based on function secret sharing (FSS) technology, which is a novel cryptographic primitive and can achieve cheap communication and computation costs for secure computation. Compared with the existing works, our scheme dramatically reduces computational overhead and runs roughly 50.8 times faster than the state-of-the-art approach. Furthermore, our scheme supports the secure KNN classification under general distance functions such as Euclidean distance, Manhattan distance, and Hamming distance. To implement our SecKNN scheme, we design two efficient FSS schemes for Hamming distance function, which implements secure two-party and multi-party Hamming distance computation in a single round. They can be considered as independent research results. Finally, we give formal security proofs for the proposed protocols and validate the effectiveness and efficiency of our protocols through experiments.","1556-6021","","10.1109/TIFS.2023.3337940","National Natural Science Foundation of China(grant numbers:62071280,62272282,62302280,62332004); Natural Science Foundation of Shandong Province(grant numbers:ZR2023QF133,ZR2020KF011); National Key Research and Development Program of China(grant numbers:2023YFB2703700); Science and Technology Small and Medium Enterprises (SMEs) Innovation Ability Enhancement Project of Shandong Province(grant numbers:2022TSGC1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339363","Secure multi-party computation;function secret sharing;privacy-preserving k-nearest neighbors;Euclidean distance;Hamming distance;Manhattan distance","Protocols;Hamming distances;Cryptography;Computational efficiency;Servers;Euclidean distance;Costs","","1","","41","IEEE","1 Dec 2023","","","IEEE","IEEE Journals"
"APFed: Anti-Poisoning Attacks in Privacy-Preserving Heterogeneous Federated Learning","X. Chen; H. Yu; X. Jia; X. Yu","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; Department of Computer Science, City University of Hong Kong, Hong Kong, SAR, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Information Forensics and Security","22 Sep 2023","2023","18","","5749","5761","Federated learning (FL) is an emerging paradigm of privacy-preserving distributed machine learning that effectively deals with the privacy leakage problem by utilizing cryptographic primitives. However, how to prevent poisoning attacks in distributed situations has recently become a major FL concern. Indeed, an adversary can manipulate multiple edge nodes and submit malicious gradients to disturb the global model’s availability. Currently, most existing works rely on an Independently Identical Distribution (IID) situation and identify malicious gradients using plaintext. However, we demonstrates that current works cannot handle the data heterogeneity scenario challenges and that publishing unencrypted gradients imposes significant privacy leakage problems. Therefore, we develop APFed, a layered privacy-preserving defense mechanism that significantly mitigates the effects of poisoning attacks in data heterogeneity scenarios. Specifically, we exploit HE as the underlying technique and employ the median coordinate as the benchmark. Subsequently, we propose a secure cosine similarity scheme to identify poisonous gradients, and we innovatively use clustering as part of the defense mechanism and develop a hierarchical aggregation that enhances our scheme’s robustness in IID and non-IID scenarios. Extensive evaluations on two benchmark datasets demonstrate that APFed outperforms existing defense strategies while reducing the communication overhead by replacing the expensive remote communication method with inexpensive intra-cluster communication.","1556-6021","","10.1109/TIFS.2023.3315125","National Natural Science Foundation of China(grant numbers:62172123,62302122); Natural Science Foundation of Heilongjiang Province of China(grant numbers:YQ2021F007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250861","Federated learning;poisoning attack;defense strategy;privacy-preserving","Servers;Data privacy;Training;Data models;Computational modeling;Behavioral sciences;Threat modeling","","1","","39","IEEE","13 Sep 2023","","","IEEE","IEEE Journals"
"Algorithms and Analysis for Optimizing the Tracking Performance of Cyber Attacked Sensor-Equipped Connected Vehicle Networks","Z. Wang; R. S. Blum","Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA, USA; Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA, USA","IEEE Transactions on Information Forensics and Security","18 Nov 2021","2021","16","","5061","5076","Sensor-equipped connected vehicle networks (SECVNs) have the potential to enable substantially safer driving by improved object tracking, which is an important basic building block in SECVNs. Unfortunately, cyber-attacks on SECVNs pose a very serious threat which could lead to unacceptable outcomes, including fatalities. Recently there has been increasing focus on malicious attack detection and mitigation in SECVNs, and some of this work has considered attacks on sensor data to impact object tracking. Unfortunately, low complexity mitigation approaches which do not compromise performance are lacking. This paper describes an efficient machine-learning enhanced approach for tracking under cyber-attacks. By proper selection of some variances related to the sensor and prior probability density functions, under some assumptions the performance can be made as close as desired to a bound on the best possible performance. However, the complexity of this new approach is dramatically lower than the best existing published low complexity approach, which provides performance which is substantially inferior to that provided by the new approach. The new approach also provides much better scaling with the size of the SECVN. In particular, the complexity increases linearly in the number of sensors, while the best low complexity published approach has a complexity which grows quadratically in the number of sensors. The new approach is also applicable to other tracking applications.","1556-6021","","10.1109/TIFS.2021.3122070","U.S. Army Research Laboratory; U.S. Army Research Office(grant numbers:W911NF-17-1-0331); National Science Foundation(grant numbers:ECCS-1744129); Grant from the Commonwealth of Pennsylvania, Department of Community and Economic Development through the Pennsylvania Infrastructure Technology Alliance (PITA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585123","Cyber attacks;sensor-equipped connected vehicle networks;expectation-maximization algorithm","Trajectory;Global Positioning System;Complexity theory;Observers;Target tracking;Mathematical models;Wireless sensor networks","","1","","45","IEEE","25 Oct 2021","","","IEEE","IEEE Journals"
"CBSeq: A Channel-Level Behavior Sequence for Encrypted Malware Traffic Detection","S. Cui; C. Dong; M. Shen; Y. Liu; B. Jiang; Z. Lu","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Zhongguancun Laboratory, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","21 Aug 2023","2023","18","","5011","5025","Machine learning and neural networks have become increasingly popular solutions for encrypted malware traffic detection. They mine and learn complex traffic patterns, enabling detection by fitting boundaries between malware traffic and benign traffic. Compared with signature-based methods, they have higher scalability and flexibility. However, affected by the frequent variants and updates of malware, current methods suffer from a high false positive rate and do not work well for unknown malware traffic detection. It remains a critical task to achieve effective malware traffic detection. In this paper, we introduce CBSeq to address the above problems. CBSeq is a method that constructs a stable traffic representation, behavior sequence, to characterize attacking intent and achieve malware traffic detection. We novelly propose the channels with similar behavior as the detection object and extract side-channel content to construct behavior sequence. Unlike benign activities, the behavior sequences of malware and its variant’s traffic exhibit solid internal correlations. Moreover, we design the MSFormer, a powerful Transformer-based multi-sequence fusion classifier. It captures the internal similarity of behavior sequence, thereby distinguishing malware traffic from benign traffic. Our evaluations demonstrate that CBSeq performs effectively in various known malware traffic detection and exhibits superior performance in unknown malware traffic detection, outperforming state-of-the-art methods.","1556-6021","","10.1109/TIFS.2023.3300521","Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDC02040100); National Key Research and Development Program of China(grant numbers:2021YFB3101400); NSFC(grant numbers:61902376,61972039); National Engineering Research Center of Classified Protection and Safeguard Technology for Cybersecurity(grant numbers:C21640-3); China National Funds for Excellent Young Scientists(grant numbers:62222201); Beijing Nova Program(grant numbers:Z201100006820006,20220484174); Beijing Natural Science Foundation(grant numbers:M23020,L222098,7232041); Program of Key Laboratory of Network Assessment Technology, the Chinese Academy of Sciences; Program of Beijing Key Laboratory of Network Security and Protection Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198353","Malware traffic;encrypted traffic;behavior sequence;unknown detection;transformer","Malware;Behavioral sciences;Feature extraction;Fingerprint recognition;Denial-of-service attack;Telecommunication traffic;Solids","","","","54","IEEE","1 Aug 2023","","","IEEE","IEEE Journals"
"Fishing for Fraudsters: Uncovering Ethereum Phishing Gangs With Blockchain Data","J. Liu; J. Chen; J. Wu; Z. Wu; J. Fang; Z. Zheng","School of Software Engineering, Sun Yat-sen University, Zhuhai, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China","IEEE Transactions on Information Forensics and Security","1 Feb 2024","2024","19","","3038","3050","As one of the most typical cybercrime types, phishing scams have extended the devil’s hand to the emerging blockchain ecosystem in recent years. Especially huge economic losses have been caused by phishing scams in Ethereum, the second-largest blockchain system. Existing approaches for Ethereum phishing detection, however, typically use machine learning or transaction graph embedding methods to identify phishers in isolation and do not effectively uncover the group of transaction accounts linked to scams (which we term a “gang”). Since accounts are pseudonymous in Ethereum, these undisclosed conspirator accounts have potential risks to the system. In this paper, we conduct the first study that characterizes and detects Ethereum phishing gangs. We first investigate the transaction behaviors in phishing gangs from the perspectives of individuals, pairs, and higher-order patterns. Our analysis reveals that although the Ethereum transaction graph is sparse with a highly skewed degree distribution, phishing accounts in the same gang have closer relationships and share specific transaction patterns. Based on our findings, we formalize the phishing gang detection problem and introduce a novel detection model named PGDetector. Given a risky phishing account as a seed, PGDetector can find out the potential risky accounts sharing close relationships within the seed’s community based on genetic algorithm optimization. Experimental results on large-scale Ethereum transaction data demonstrate the effectiveness of PGDetector.","1556-6021","","10.1109/TIFS.2024.3359000","National Natural Science Foundation of China(grant numbers:62372485,62032025,62332004); Natural Science Foundation of Guangdong Province(grant numbers:2023A1515011314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415200","Ethereum;blockchain;phishing scam;gang detection;transaction data analysis;cryptocurrency crime forensics","Phishing;Blockchains;Behavioral sciences;Monitoring;Feature extraction;Task analysis;Real-time systems","","","","51","IEEE","26 Jan 2024","","","IEEE","IEEE Journals"
"TMG-GAN: Generative Adversarial Networks-Based Imbalanced Learning for Network Intrusion Detection","H. Ding; Y. Sun; N. Huang; Z. Shen; X. Cui","School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computing, National University of Singapore, Cluny Road, Singapore; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","4 Dec 2023","2024","19","","1156","1167","Internet of Things (IoT) devices are large in number, widely distributed, weak in protection ability, and vulnerable to various malicious attacks. Intrusion detection technology can provide good protection for network equipment. However, the normal traffic and abnormal traffic in the network are usually imbalanced. Imbalanced samples will seriously affect the performance of machine learning detection algorithm. Therefore, this paper proposes an intrusion detection method based on data augmentation, namely TMG-IDS. We name the proposed data augmentation model TMG-GAN, which is a data augmentation method based on generative adversarial networks (GAN). First, TMG-GAN has a multi-generator structure, which can be used to generate different types of attack data simultaneously. Second, we increase the classifier structure, which can optimize the generator and discriminator more efficiently based on the classification loss. Third, we calculate the cosine similarity between the generated samples and the original samples and other types of generated samples as a generator loss, which can further improve the quality of generated samples and reduce the class overlap area between the distributions of various generated samples. We conduct extensive experiments on two intrusion detection datasets, CICIDS2017 and UNSW-NB15. The experimental results show that compared with the advanced oversampling algorithm and the latest intrusion detection algorithm, the proposed TMG-IDS method has a good detection effect under the three indicators of Precision, Recall and F1-score.","1556-6021","","10.1109/TIFS.2023.3331240","Operation Wise Eyes(grant numbers:G20220126); Key Research and Development Projects in Hubei Province(grant numbers:2022BAA041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312801","Internet of Things (IoT);intrusion detection;generative adversarial networks (GAN);TMG-GAN","Intrusion detection;Generative adversarial networks;Training;Data models;Generators;Ensemble learning;Deep learning","","","","33","IEEE","8 Nov 2023","","","IEEE","IEEE Journals"
"HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning","P. Qiu; X. Zhang; S. Ji; C. Fu; X. Yang; T. Wang","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Software Technology, Zhejiang University, Ningbo, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Hefei Interdisciplinary Center, National University of Defense Technology, Hefei, China; College of Information Sciences and Technology, The Pennsylvania State University, State College, PA, USA","IEEE Transactions on Information Forensics and Security","13 Feb 2024","2024","19","","3435","3450","Vertical Federated Learning (VFL) is a trending collaborative machine learning model training solution. Existing industrial frameworks employ secure multi-party computation techniques such as homomorphic encryption to ensure data security and privacy. Despite these efforts, studies have revealed that data leakage remains a risk in VFL due to the correlations between intermediate representations and raw data. Neural networks can accurately capture these correlations, allowing an adversary to reconstruct the data. This emphasizes the need for continued research into securing VFL systems. Our work shows that hashing is a promising solution to counter data reconstruction attacks. The one-way nature of hashing makes it difficult for an adversary to recover data from hash codes. However, implementing hashing in VFL presents new challenges, including vanishing gradients and information loss. To address these issues, we propose HashVFL, which integrates hashing and simultaneously achieves learnability, bit balance, and consistency. Experimental results indicate that HashVFL effectively maintains task performance while defending against data reconstruction attacks. It also brings additional benefits in reducing the degree of label leakage, mitigating adversarial attacks, and detecting abnormal inputs. We hope our work will inspire further research into the potential applications of HashVFL.","1556-6021","","10.1109/TIFS.2024.3356164","National Key Research and Development Program of China(grant numbers:2022YFB3102100); NSFC(grant numbers:62102360); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409241","Vertical federated learning;deep hashing","Image reconstruction;Training;Privacy;Task analysis;Federated learning;Hash functions;Data models","","","","83","IEEE","19 Jan 2024","","","IEEE","IEEE Journals"
"Low Communication Secure Computation From Semi-Trusted Hardware","Y. Lu; B. Zhang; K. Ren","School of Cyber Science and Technology, Zhejiang University, Hangzhou, China; School of Cyber Science and Technology, Zhejiang University, Hangzhou, China; School of Cyber Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","6 Jul 2023","2023","18","","3962","3976","In privacy-preserving machine learning and many other applications, the involved parties want to obtain the computation result without revealing their private inputs. Secure computation aims to solve this problem, but current secure computation protocols often fail to provide efficient solutions due to large communication, especially in a real-life Internet network where the bandwidth and the delay can be unsatisfying. Assuming the existence of a trusted hardware component that is resilient to side-channel attacks and will faithfully compute a pre-agreed program, secure computation can be realized by each party sending its input to the hardware and receiving the execution result back. However, a recent work of Lu et al. (ESORICS’21) points out that the hardware components can’t be fully trusted. In this work, we improve the semi-trusted hardware model of Lu et al., and we propose secure computation protocols with low communication in the new model. We observe that the ESORICS’21 two-party computation protocol have some security flaws; in this work, we fix them and improve its online efficiency. Moreover, we propose an efficient constant-round secure multi-party computation protocol which has a communication cost of  $(n-1) {\lambda }+2(n-1)\ell $  bits, where  $n$  is the number of the parties,  ${\lambda }$  is the security parameter and  $\ell $  is the input/output size. The computation cost of our multi-party protocol is also much smaller than current best-known constant-round protocols.","1556-6021","","10.1109/TIFS.2023.3282134","National Key Research and Development Program of China(grant numbers:2021YFB3101601); National Natural Science Foundation of China(grant numbers:62072401,62232002); Open Project Program of Key Laboratory of Blockchain and Cyberspace Governance of Zhejiang Province; Input Output (iohk.io); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142026","Semi-trusted hardware model;multi-party computation;garbled circuit","Protocols;Hardware;Computational modeling;Integrated circuit modeling;Security;Costs;Servers","","","","35","IEEE","1 Jun 2023","","","IEEE","IEEE Journals"
"Defending Against Label-Only Attacks via Meta-Reinforcement Learning","D. Ye; T. Zhu; K. Gao; W. Zhou","Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; Faculty of Data Science, City University of Macau, Macau, China; Centre for Cyber Security and Privacy and the School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia; Faculty of Data Science, City University of Macau, Macau, China","IEEE Transactions on Information Forensics and Security","12 Feb 2024","2024","19","","3295","3308","Machine learning models are susceptible to a range of adversarial activities. These attacks are designed to either infer private information from the target model or deceive it. For instance, an attacker may attempt to discern if a given data example is from the model’s training set (membership inference attacks) or create adversarial examples to mislead the model to make incorrect predictions (adversarial example attacks). Numerous defense methods have been proposed to counter these attacks. However, these methods typically share two common limitations. Firstly, most are not designed to address label-only attacks, which is a newly emerged kind of attacks that rely solely on the hard labels predicted by the target model. Secondly, they are often developed to mitigate specific attacks rather than universally various attacks. To address these limitations, this paper proposes a novel defense method that focuses on the most challenging attacks, i.e., label-only attacks, and can handle various types of label-only attacks. The key idea is to strategically modify the target model’s predicted labels using a meta-reinforcement learning technique. This ensures that attackers receive incorrect labels while benign users continue to receive correct labels. Notably, the defender, i.e., the owner of the target model, can make effective decisions without knowledge of the attacker’s behavior. The experimental results demonstrate that our proposed method is an effective defense against a range of attacks, including label-only model stealing, label-only membership inference, label-only model inversion, and label-only adversarial example attacks.","1556-6021","","10.1109/TIFS.2024.3357292","Australian Research Council (ARC) Discovery Project of ARC(grant numbers:DP230100246); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411933","Deep learning model attacks;label-only attacks;reinforcement learning","Predictive models;Training;Computational modeling;Cloning;Data models;Adaptation models;Generative adversarial networks","","","","49","IEEE","22 Jan 2024","","","IEEE","IEEE Journals"
"AST-SafeSec: Adaptive Stress Testing for Safety and Security Co-Analysis of Cyber-Physical Systems","N. Kaloudi; J. Li","Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway","IEEE Transactions on Information Forensics and Security","11 Sep 2023","2023","18","","5567","5579","Cyber-physical systems are becoming more intelligent with the adoption of heterogeneous sensor networks and machine learning capabilities that deal with an increasing amount of input data. While this complexity aims to solve problems in various domains, it adds new challenges for the system assurance. One issue is the rise in the number of abnormal behaviors that affect system performance due to possible sensor faults and attacks. The combination of safety risks, which are usually caused by random sensor faults and security risks that can happen during any random system state, makes the full coverage testing of the cyber-physical system challenging. Existing techniques are inadequate to deal with complex safety and security co-risks against cyber-physical systems. In this paper, we propose AST-SafeSec, an analysis methodology for both safety and security aspects that utilizes reinforcement learning to identify the most likely adversarial paths at various normal or failure states of a cyber-physical system that can influence system behavior through its sensor data. The methodology is evaluated using an autonomous vehicle scenario by incorporating a security attack into the stochastic sensor elements of a vehicle. Evaluation results show that the methodology analyzes the interaction of malicious attacks with random faults and identifies the incident caused by the interactions and the most likely path that leads to the incident.","1556-6021","","10.1109/TIFS.2023.3309160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10231138","Co-analysis;cyber-physical systems (CPSs);cybersecurity;safety;stress testing;validation;verification","Safety;Security;Testing;Complexity theory;Behavioral sciences;Cyber-physical systems;Cyberattack","","","","57","IEEE","28 Aug 2023","","","IEEE","IEEE Journals"
"Resampling Estimation Based RPC Metadata Verification in Satellite Imagery","C. Gudavalli; M. Goebel; T. Nanjundaswamy; L. Nataraj; S. Chandrasekaran; B. S. Manjunath","Mayachitra, Inc., Santa Barbara, CA, USA; Electrical and Computer Engineering Department, University of California, Santa Barbara, Santa Barbara, CA, USA; Mayachitra, Inc., Santa Barbara, CA, USA; Mayachitra, Inc., Santa Barbara, CA, USA; Electrical and Computer Engineering Department, University of California, Santa Barbara, Santa Barbara, CA, USA; Electrical and Computer Engineering Department, University of California, Santa Barbara, Santa Barbara, CA, USA","IEEE Transactions on Information Forensics and Security","5 Jun 2023","2023","18","","3212","3221","Recent advances in machine learning and computer vision have made it simple to manipulate a variety of media, including satellite images. Most of the commercially available satellite images go through the process of orthorectification to remove potential distortions due to terrain variations. This orthorectification process typically involves the use of rational polynomial coefficients (RPC) that geometrically remap the pixels in the original image to the rectified image. This paper proposes the first method to verify the authenticity of RPC metadata in an orthorectified satellite image. The steps include calculating the Residual Discrete Fourier Transform (DFT) pattern from the image using a linear predictor based residual spectral analysis and comparing with Expected Residual pattern that is obtained using the RPC metadata associated with the image. If the metadata associated with orthorectified image is correct, then the Residual-DFT pattern (which represents image data) and the Expected-Residual-DFT pattern (which represents metadata) should be similar. We use SSIM (Structural Similarity Index Metric) to quantify the similarity and thereby verify if the data has been tampered or not. Detailed experimental results demonstrate that our method achieves over 97% accuracy in the majority of binary tampering detection tests.","1556-6021","","10.1109/TIFS.2023.3276640","National Geospatial-Intelligence Agency (NGA)(grant numbers:HM047619C0056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124789","Resampling estimation;digital image forensics;metadata tampering detection;signal processing","Metadata;Discrete Fourier transforms;Satellites;Estimation;Cameras;Indexes;Spectral analysis","","","","14","IEEE","15 May 2023","","","IEEE","IEEE Journals"
"Classification Utility, Fairness, and Compactness via Tunable Information Bottleneck and Rényi Measures","A. Gronowski; W. Paul; F. Alajaji; B. Gharesifard; P. Burlina","Department of Mathematics and Statistics, Queen’s University, Kingston, Canada; Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA; Department of Mathematics and Statistics, Queen’s University, Kingston, Canada; Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA; Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA","IEEE Transactions on Information Forensics and Security","19 Dec 2023","2024","19","","1630","1645","Designing machine learning algorithms that are accurate yet fair, not discriminating based on any sensitive attribute, is of paramount importance for society to accept AI for critical applications. In this article, we propose a novel fair representation learning method termed the Rényi Fair Information Bottleneck Method (RFIB) which incorporates constraints for utility, fairness, and compactness (compression) of representation, and apply it to image and tabular data classification. A key attribute of our approach is that we consider - in contrast to most prior work - both demographic parity and equalized odds as fairness constraints, allowing for a more nuanced satisfaction of both criteria. Leveraging a variational approach, we show that our objectives yield a loss function involving classical Information Bottleneck (IB) measures and establish an upper bound in terms of two Rényi measures of order  $ \boldsymbol {\alpha }$  on the mutual information IB term measuring compactness between the input and its encoded embedding. We study the influence of the  $ \boldsymbol {\alpha }$  parameter as well as two other tunable IB parameters on achieving utility/fairness trade-off goals, and show that the  $ \boldsymbol {\alpha }$  parameter gives an additional degree of freedom that can be used to control the compactness of the representation. Experimenting on three different image datasets (EyePACS, CelebA, and FairFace) and two tabular datasets (Adult and COMPAS), using both binary and categorical sensitive attributes, we show that on various utility, fairness, and compound utility/fairness metrics RFIB outperforms current state-of-the-art approaches.","1556-6021","","10.1109/TIFS.2023.3340094","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10345626","Deep learning;fair representation learning;equalized odds;demographic parity;classification;information bottleneck (IB);Rényi divergence;Rényi cross-entropy","Data models;Training data;Representation learning;Random variables;Mutual information;Entropy;Upper bound","","","","98","IEEE","5 Dec 2023","","","IEEE","IEEE Journals"
"Polarized Image Translation From Nonpolarized Cameras for Multimodal Face Anti-Spoofing","Y. Tian; Y. Huang; K. Zhang; Y. Liu; Z. Sun","School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Center for Research on Intelligent Perception and Computing, School of Artificial Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Center for Research on Intelligent Perception and Computing, School of Artificial Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","19 Sep 2023","2023","18","","5651","5664","In face antispoofing, it is desirable to have multimodal images to demonstrate liveness cues from various perspectives. However, in most face recognition scenarios, only a single modality, namely visible lighting (VIS) facial images is available. This paper first investigates the possibility of generating polarized (Polar) images from VIS cameras without changing the existing recognition devices to improve the accuracy and robustness of Presentation Attack Detection (PAD) in face biometrics. A novel multimodal face antispoofing framework is proposed based on the machine-learning relationship between VIS and Polar images of genuine faces. Specifically, a dual-modal central differential convolutional network (CDCN) is developed to capture the inherent spoofing features between the VIS and the generated Polar modalities. Quantitative and qualitative experimental results show that our proposed framework not only generates realistic Polar face images but also improves the state-of-the-art face anti-spoofing results on the VIS modal database (i.e. CASIA-SURF). Moreover, a polar face database, CASIA-Polar, has been constructed and will be shared with the public at https://biometrics.idealtest.org to inspire future applications within the biometric anti-spoofing field.","1556-6021","","10.1109/TIFS.2023.3310348","National Natural Science Foundation of China(grant numbers:62071468,62276263,62006225); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA27040202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234446","Face antispoofing;image translation;polarization;multimodal","Face recognition;Faces;Feature extraction;Imaging;Three-dimensional displays;Robustness;Costs","","","","59","CCBYNCND","30 Aug 2023","","","IEEE","IEEE Journals"
"Authentication of Copy Detection Patterns: A Pattern Reliability Based Approach","J. Tutt; O. Taran; R. Chaban; B. Pulfer; Y. Belousov; T. Holotyak; S. Voloshynovskiy","Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland; Department of Computer Science, University of Geneva, Geneva, Switzerland","IEEE Transactions on Information Forensics and Security","7 Feb 2024","2024","19","","3124","3134","Copy Detection Pattern (CDP) technology is a promising anti-counterfeiting solution for the protection of physical goods. In recent years, it has been shown that this technology is threatened by powerful deep learning attacks that are able to bypass original authentication schemes. In this paper, we tackle this problem by proposing a new CDP authentication scheme based on statistical knowledge discovered about the printing and imaging process. The novelty of our approach lies in providing means to measure the reliability of each local pattern appearing in the CDP. This allows to define new authentication measures to better differentiate original CDP from fakes. Our results show that this new system is capable of performing reliable CDP authentication with smartphones without the need for heavyweight machine learning tools requiring massive data entries.","1556-6021","","10.1109/TIFS.2024.3359510","Swiss National Science Foundation (SNF)(grant numbers:200021_182063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415406","Copy detection patterns;smartphone authentication;binary pattern-based channel;deep learning fakes","Authentication;Reliability;Symbols;Probes;Printing;Imaging;Smart phones","","","","21","IEEE","29 Jan 2024","","","IEEE","IEEE Journals"
"Efficient Sparse Least Absolute Deviation Regression With Differential Privacy","W. Liu; X. Mao; X. Zhang; X. Zhang","School of Mathematical Sciences, MoE Key Laboratory of Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, China; School of Mathematical Sciences, Ministry of Education Key Laboratory of Scientific and Engineering Computing, Shanghai Jiao Tong University, Shanghai, China; School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan, China; Department of Statistics, Iowa State University, Ames, IA, USA","IEEE Transactions on Information Forensics and Security","12 Jan 2024","2024","19","","2328","2339","In recent years, privacy-preserving machine learning algorithms have attracted increasing attention because of their important applications in many scientific fields. However, in the literature, most privacy-preserving algorithms demand learning objectives to be strongly convex and Lipschitz smooth, which thus cannot cover a wide class of robust loss functions (e.g., quantile/least absolute loss). In this work, we aim to develop a fast privacy-preserving learning solution for a sparse robust regression problem. Our learning loss consists of a robust least absolute loss and an  $\ell _{1}$  sparse penalty term. To fast solve the non-smooth loss under a given privacy budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE) algorithm for least absolute deviation regression. Our algorithm achieves a fast estimation by reformulating the sparse LAD problem as a penalized least square estimation problem and adopts a three-stage noise injection to guarantee the  $(\epsilon,\delta)$ -differential privacy. We show that our algorithm can achieve better privacy and statistical accuracy trade-off compared with the state-of-the-art privacy-preserving regression algorithms. In the end, we conduct experiments to verify the efficiency of our proposed FRAPPE algorithm.","1556-6021","","10.1109/TIFS.2023.3349054","NSFC(grant numbers:11825104); NSFC(grant numbers:12371273); Shanghai Rising-Star Program(grant numbers:23QA1404600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10379016","Robust regression;sparse learning;differential privacy;least absolute deviation","Privacy;Estimation;Differential privacy;Convergence;Perturbation methods;Approximation algorithms;Robustness","","","","52","IEEE","1 Jan 2024","","","IEEE","IEEE Journals"
"Towards Evaluating the Robustness of Neural Networks","N. Carlini; D. Wagner","University of California, Berkeley; University of California, Berkeley","2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","39","57","Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958570","","Neural networks;Robustness;Measurement;Speech recognition;Security;Malware;Resists","","3436","7","48","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Stealthy Porn: Understanding Real-World Adversarial Images for Illicit Online Promotion","K. Yuan; D. Tang; X. Liao; X. Wang; X. Feng; Y. Chen; M. Sun; H. Lu; K. Zhang",Indiana University Bloomington; Chinese University of Hong Kong; Indiana University Bloomington; Indiana University Bloomington; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese University of Hong Kong; Indiana University Bloomington; Chinese University of Hong Kong,"2019 IEEE Symposium on Security and Privacy (SP)","16 Sep 2019","2019","","","952","966","Recent years have witnessed the rapid progress in deep learning (DP), which also brings their potential weaknesses to the spotlights of security and machine learning studies. With important discoveries made by adversarial learning research, surprisingly little attention, however, has been paid to the real-world adversarial techniques deployed by the cybercriminal to evade image-based detection. Unlike the adversarial examples that induce misclassification using nearly imperceivable perturbation, real-world adversarial images tend to be less optimal yet equally effective. As a first step to understand the threat, we report in the paper a study on adversarial promotional porn images (APPIs) that are extensively used in underground advertising. We show that the adversary today's strategically constructs the APPIs to evade explicit content detection while still preserving their sexual appeal, even though the distortions and noise introduced are clearly observable to humans. To understand such real-world adversarial images and the underground business behind them, we develop a novel DP-based methodology called Male`na, which focuses on the regions of an image where sexual content is least obfuscated and therefore visible to the target audience of a promotion. Using this technique, we have discovered over 4,000 APPIs from 4,042,690 images crawled from popular social media, and further brought to light the unique techniques they use to evade popular explicit content detectors (e.g., Google Cloud Vision API, Yahoo Open NSFW model), and the reason that these techniques work. Also studied are the ecosystem of such illicit promotions, including the obfuscated contacts advertised through those images, compromised accounts used to disseminate them, and large APPI campaigns involving thousands of images. Another interesting finding is the apparent attempt made by cybercriminals to steal others' images for their advertising. The study highlights the importance of the research on real-world adversarial learning and makes the first step towards mitigating the threats it poses.","2375-1207","978-1-5386-6660-9","10.1109/SP.2019.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835391","adversarial-images;cybercrime;deep-learning","Google;Feature extraction;Detectors;Advertising;Ecosystems;Image recognition;Deep learning","","12","","61","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Bookworm Game: Automatic Discovery of LTE Vulnerabilities Through Documentation Analysis","Y. Chen; Y. Yao; X. Wang; D. Xu; C. Yue; X. Liu; K. Chen; H. Tang; B. Liu","Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS","2021 IEEE Symposium on Security and Privacy (SP)","26 Aug 2021","2021","","","1197","1214","In the past decade, the security of cellular networks has been increasingly under scrutiny, leading to the discovery of numerous vulnerabilities that expose the network and its users to a wide range of security risks, from denial of service to information leak. However, most of these findings have been made through ad-hoc manual analysis, which is inadequate for fundamentally enhancing the security assurance of a system as complex as the cellular network. An important observation is that the massive amount of technical documentation of cellular network can provide key insights into the protection it puts in place and help identify potential security flaws. Particularly, we found that such documentation often contains hazard indicators (HIs) – the statement that describes a risky operation (e.g., abort an ongoing procedure) when a certain event happens at a state, which can guide a test on the system to find out whether the operation can indeed be triggered by an unauthorized party to cause harm to the cellular core or legitimate users’ equipment. Based upon this observation, we present in this paper a new framework that makes the first step toward intelligent and systematic security analysis of cellular networks. Our approach, called Atomic, utilizes natural-language processing and machine learning techniques to scan a large amount of LTE documentation for HIs. The HIs discovered are further parsed and analyzed to recover state and event information for generating test cases. These test cases are further utilized to automatically construct tests in an LTE simulation environment, which runs the tests to detect the vulnerabilities in the LTE that allow the risky operations to happen without proper protection. In our research, we implemented Atomic and ran it on the LTE NAS specification, including 549 pages with 13,598 sentences and 283,850 words. In less than 5 hours, our prototype reported 42 vulnerabilities from 192 HIs discovered, including 10 never reported before, under two threat models. All these vulnerabilities have been confirmed through end-to-end attacks, which lead to unauthorized disruption of the LTE service a legitimate user’s equipment receives. We reported our findings to authorized parties and received their confirmation that these vulnerabilities indeed exist in major commercial carriers and $2,000 USD reward from Google.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519388","Cellular Network;4G;LTE;Vulnerability;Attack;Documentation Analysis;NLP","Cellular networks;Protocols;Systematics;Prototypes;Documentation;Manuals;Hazards","","5","","60","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning","H. Li; Q. Ye; H. Hu; J. Li; L. Wang; C. Fang; J. Shi","The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; Guangzhou University; Renmin University of China; Huawei International, Singapore; Huawei International, Singapore","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","1893","1907","Federated Learning (FL), the de-facto distributed machine learning paradigm that locally trains datasets at individual devices, is vulnerable to backdoor model poisoning attacks. By compromising or impersonating those devices, an attacker can upload crafted malicious model updates to manipulate the global model with backdoor behavior upon attacker-specified triggers. However, existing backdoor attacks require more information on the victim FL system beyond a practical black-box setting. Furthermore, they are often specialized to optimize for a single objective, which becomes ineffective as modern FL systems tend to adopt in-depth defense that detects backdoor models from different perspectives. Motivated by these concerns, in this paper, we propose 3DFed, an adaptive, extensible, and multi-layered framework to launch covert FL backdoor attacks in a black-box setting. 3DFed sports three evasion modules that camouflage backdoor models: backdoor training with constrained loss, noise mask, and decoy model. By implanting indicators into a backdoor model, 3DFed can obtain the attack feedback in the previous epoch from the global model and dynamically adjust the hyper-parameters of these backdoor evasion modules. Through extensive experimental results, we show that when all its components work together, 3DFed can evade the detection of all state-of-the-art FL backdoor defenses, including Deepsight, Foolsgold, FLAME, FL-Detector, and RFLBAT. New evasion modules can also be incorporated in 3DFed in the future as it is an extensible framework.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179401","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179401","","Training;Adaptation models;Privacy;Federated learning;Heuristic algorithms;Closed box;Robustness","","2","","57","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"BayBFed: Bayesian Backdoor Defense for Federated Learning","K. Kumari; P. Rieger; H. Fereidooni; M. Jadliwala; A. -R. Sadeghi",Technical University of Darmstadt; Technical University of Darmstadt; Technical University of Darmstadt; The University of Texas at San Antonio; Technical University of Darmstadt,"2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","737","754","Federated learning (FL) is an emerging technology that allows participants to jointly train a machine learning model without sharing their private data with others. However, FL is vulnerable to poisoning attacks such as backdoor attacks. Consequently, a variety of defenses have recently been proposed, which have primarily utilized intermediary states of the global model (i.e., logits) or distance of the local models (i.e., L2−norm) with respect to the global model to detect malicious backdoors in FL. However, as these approaches directly operate on client updates (or weights), their effectiveness depends on factors such as clients’ data distribution or the adversary’s attack strategies. In this paper, we introduce a novel and more generic backdoor defense framework, called BayBFed, which proposes to utilize probability distributions over client updates to detect malicious updates in FL: BayBFed computes a probabilistic measure over the clients’ updates to keep track of any adjustments made in the updates, and uses a novel detection algorithm that can leverage this probabilistic measure to efficiently detect and filter out malicious updates. Thus, it overcomes the shortcomings of previous approaches that arise due to the direct usage of client updates; nevertheless, our probabilistic measure will include all aspects of the local client training strategies. BayBFed utilizes two Bayesian NonParametric (BNP) extensions: (i) a Hierarchical Beta-Bernoulli process to draw a probabilistic measure given the clients’ updates, and (ii) an adaptation of the Chinese Restaurant Process (CRP), referred by us as CRP-Jensen, which leverages this probabilistic measure to detect and filter out malicious updates. We extensively evaluate our defense approach on five benchmark datasets: CIFAR10, Reddit, IoT intrusion detection, MNIST, and FMNIST, and show that it can effectively detect and eliminate malicious updates in FL without deteriorating the benign performance of the global model.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179362","","Weight measurement;Training;Federated learning;Social networking (online);Filtering algorithms;Benchmark testing;Probabilistic logic","","2","","49","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Private Eye: On the Limits of Textual Screen Peeking via Eyeglass Reflections in Video Conferencing","Y. Long; C. Yan; S. Xiao; S. Prasad; W. Xu; K. Fu","Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; College of Electrical Engineering, Zhejiang University, Hangzhou, China; Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","2023 IEEE Symposium on Security and Privacy (SP)","21 Jul 2023","2023","","","3432","3449","Personal video conferencing has become a new norm after COVID-19 caused a seismic shift from in-person meetings and phone calls to video conferencing for daily communications and sensitive business. Video leaks participants’ on-screen information because eyeglasses and other reflective objects unwittingly expose partial screen contents. Using mathematical modeling and human subjects experiments, this research explores the extent to which emerging webcams might leak recognizable textual and graphical information gleaming from eyeglass reflections captured by webcams. The primary goal of our work is to measure, compute, and predict the factors, limits, and thresholds of recognizability as webcam technology evolves in the future. Our work explores and characterizes the viable threat models based on optical attacks using multi-frame super resolution techniques on sequences of video frames. Our models and experimental results in a controlled lab setting show it is possible to reconstruct and recognize with over 75% accuracy on-screen texts that have heights as small as 10 mm with a 720p webcam. We further apply this threat model to web textual contents with varying attacker capabilities to find thresholds at which text becomes recognizable. Our user study with 20 participants suggests present-day 720p webcams are sufficient for adversaries to reconstruct textual content on big-font websites. Our models further show that the evolution towards 4K cameras will tip the threshold of text leakage to reconstruction of most header texts on popular websites. Besides textual targets, a case study on recognizing a closed-world dataset of Alexa top 100 websites with 720p webcams shows a maximum recognition accuracy of 94% with 10 participants even without using machine-learning models. Our research proposes near-term mitigations including a software prototype that users can use to blur the eyeglass areas of their video streams. For possible long-term defenses, we advocate an individual reflection testing procedure to assess threats under various settings, and justify the importance of following the principle of least privilege for privacy-sensitive scenarios.","2375-1207","978-1-6654-9336-9","10.1109/SP46215.2023.10179423","Analog Devices; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179423","","Threat modeling;Webcams;Text recognition;Target recognition;Computational modeling;Virtual assistants;Reflection","","1","","59","IEEE","21 Jul 2023","","","IEEE","IEEE Conferences"
"Guest Editorial Special Issue on Facial Biometrics in the Wild","A. A. Farag; B. Bhanu; E. Hancock; G. Medioni; J. Yang","University of Louisville, Louisville, KY, USA; University of California at Riverside, Riverside, CA, USA; University of York, Heslington, United Kingdom; University of Southern California, Los Angeles, CA, USA; Shanghai Jiaotong University, Xuhui, Shanghai, China","IEEE Transactions on Information Forensics and Security","11 Nov 2014","2014","9","12","2019","2023","Facial biometrics is a multidisciplinary field based on the methods and technologies of image analysis and machine learning, as applied to identifying people from facial information, and has progressed for over three decades in terms of theory, algorithms, and applications. While surveillance systems are in common practice and close-range facial recognition at entry points exist, the field has progressed beyond still image recognition in controlled imaging environments. In general, face recognition in the wild connotes recognition from still images and videos, unabated by age, pose, illumination, and expression (A-PIE) of individuals. Yet, we consider this to be only an aspect of facial biometrics in the wild, despite its importance.","1556-6021","","10.1109/TIFS.2014.2363512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6953278","","Special issues and sections;Face recognition;Biometrics (access control);Emotion recognition;Optical sensors;Face detection;Feature extraction","","2","","","IEEE","11 Nov 2014","","","IEEE","IEEE Journals"
"Special Issue on Signal Processing and Machine Learning for Education and Human Learning at Scale","",,"IEEE Transactions on Information Forensics and Security","16 Sep 2016","2016","11","9","2145","2145","Describes the above-named upcoming special issue or section. May include topics to be covered or calls for papers.","1556-6021","","10.1109/TIFS.2016.2606962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570338","","","","","","","IEEE","16 Sep 2016","","","IEEE","IEEE Journals"
"Special issue on signal processing and machine learning for education and human learning at scale","",,"IEEE Transactions on Information Forensics and Security","20 May 2016","2016","11","5","1093","1093","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","1556-6021","","10.1109/TIFS.2016.2559658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475996","","","","","","","IEEE","20 May 2016","","","IEEE","IEEE Journals"
"Special Issue on Signal Processing and Machine Learning for Education and Human Learning at Scale","",,"IEEE Transactions on Information Forensics and Security","16 Sep 2016","2016","11","10","2395","2395","Describes the above-named upcoming special issue or section. May include topics to be covered or calls for papers.","1556-6021","","10.1109/TIFS.2016.2607045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570358","","","","","","","IEEE","16 Sep 2016","","","IEEE","IEEE Journals"
"Special issue on signal processing and machine learning for education and human learning at scale","",,"IEEE Transactions on Information Forensics and Security","20 May 2016","2016","11","6","1380","1380","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","1556-6021","","10.1109/TIFS.2016.2559659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475928","","","","","","","IEEE","20 May 2016","","","IEEE","IEEE Journals"
"Table of contents","",,"2017 IEEE Symposium on Security and Privacy (SP)","26 Jun 2017","2017","","","v","x","The following topics are dealt with: security, privacy and machine learning.","2375-1207","978-1-5090-5533-3","10.1109/SP.2017.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958562","","","","","","","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
