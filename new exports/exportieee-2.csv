"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"The Ethics of Going Deep: Challenges in Machine Learning for Sensitive Security Domains","A. Eusebi; M. Vasek; E. Cockbain; E. Mariconti","UCL, London, UK; UCL, London, UK; UCL, London, UK; UCL, London, UK","2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","27 Jun 2022","2022","","","533","537","Sometimes, machine learning models can determine the trajectory of human life, and a series of cascading ethical failures could be irreversible. Ethical concerns are nevertheless set to increase, in particular when the injection of algorithmic forms of decision-making occurs in highly sensitive security contexts. In cybercrime, there have been cases of algorithms that have not identified racist and hateful speeches, as well as missing the identification of Image Based Sexual Abuse cases. Hence, this paper intends to add a voice of caution on the vulnerabilities pervading the different stages of a machine learning development pipeline and the ethical challenges that these potentially nurture and perpetuate. To highlight both the issues and potential fixes in an adversarial environment, we use Child Sexual Exploitation and its implications on the Internet as a case study, being 2021 its worst year according to the Internet Watch Foundation.","2768-0657","978-1-6654-9560-8","10.1109/EuroSPW55150.2022.00063","EPSRC(grant numbers:180330); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799317","machine learning;ethics;security;online child sexual abuse","Ethics;Machine learning algorithms;Pipelines;Power system protection;Focusing;Machine learning;Internet","","","","30","IEEE","27 Jun 2022","","","IEEE","IEEE Conferences"
"Poster: FLATEE: Federated Learning Across Trusted Execution Environments","A. Mondal; Y. More; R. H. Rooparaghunath; D. Gupta","Department of Computer Science, Ashoka University; Department of Computer Science, Ashoka University; Department of Computer Science, Ashoka University; Department of Computer Science, Ashoka University","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","707","709","Federated learning allows us to distributively train a machine learning model where multiple parties share local model parameters without sharing private data. However, parameter exchange may still leak information. Several approaches have been proposed to overcome this, based on multi-party computation, fully homomorphic encryption, etc.; many of these protocols are slow and impractical for real-world use as they involve a large number of cryptographic operations. In this paper, we propose the use of Trusted Execution Environments (TEE), which provide a platform for isolated execution of code and handling of data, for this purpose. We describe Flatee, an efficient privacy-preserving federated learning framework across TEEs, which considerably reduces training and communication time. Our framework can handle malicious parties (we do not natively solve adversarial data poisoning, though we describe a preliminary approach to handle this).","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581236","Federated Learning;Trusted Execution Environment;Secure Multi-Party Computation;Homomorphic Encryption;Differential Privacy","Training;Privacy;Protocols;Codes;Computational modeling;Machine learning;Collaborative work","","10","","12","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"CRC-PUF: A Machine Learning Attack Resistant Lightweight PUF Construction","E. Dubrova; O. Näslund; B. Degen; A. Gawell; Y. Yu","Royal Institute of Technology (KTH), Stockholm, Sweden; Royal Institute of Technology (KTH), Stockholm, Sweden; Royal Institute of Technology (KTH), Stockholm, Sweden; Royal Institute of Technology (KTH), Stockholm, Sweden; Royal Institute of Technology (KTH), Stockholm, Sweden","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","264","271","Adversarial machine learning is an emerging threat to security of Machine Learning (ML)-based systems. However, we can potentially use it as a weapon against ML-based attacks. In this paper, we focus on protecting Physical Unclonable Functions (PUFs) against ML-based modeling attacks. PUFs are an important cryptographic primitive for secret key generation and challenge-response authentication. However, none of the existing PUF constructions are both ML attack resistant and sufficiently lightweight to fit low-end embedded devices. We present a lightweight PUF construction, CRC-PUF, in which input challenges are de-synchronized from output responses to make a PUF model difficult to learn. The de-synchronization is done by an input transformation based on a Cyclic Redundancy Check (CRC). By changing the CRC generator polynomial for each new response, we assure that success probability of recovering the transformed challenge is at most 2-86 for 128-bit challenges and responses.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802444","Physical-Unclonable-Function-(PUF),-modeling-attack,-arbiter-PUF,-Cyclic-Redundancy-Check-(CRC),-machine-learning,-adversarial-machine-learning","Physical unclonable function;Hash functions;Delays;Switches;Machine learning;Cryptography;Reliability","","22","","45","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"SoK: Security and Privacy in Machine Learning","N. Papernot; P. McDaniel; A. Sinha; M. P. Wellman",Pennsylvania State University; Pennsylvania State University; University of Michigan; University of Michigan,"2018 IEEE European Symposium on Security and Privacy (EuroS&P)","9 Jul 2018","2018","","","399","414","Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive-new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.","","978-1-5386-4228-3","10.1109/EuroSP.2018.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406613","security;privacy;machine learning","Security;Machine learning;Data models;Training;Privacy;Computational modeling;Analytical models","","225","1","115","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Enhancing Privacy via Hierarchical Federated Learning","A. Wainakh; A. S. Guinea; T. Grube; M. Mühlhäuser","Telecooperation Lab, Technical University of Darmstadt; Telecooperation Lab, Technical University of Darmstadt; Telecooperation Lab, Technical University of Darmstadt; Telecooperation Lab, Technical University of Darmstadt","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","344","347","Federated learning suffers from several privacy-related issues that expose the participants to various threats. A number of these issues are aggravated by the centralized architecture of federated learning. In this paper, we discuss applying federated learning on a hierarchical architecture as a potential solution. We introduce the opportunities for more flexible decentralized control over the training process and its impact on the participants' privacy. Furthermore, we investigate possibilities to enhance the efficiency and effectiveness of defense and verification methods.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229863","Federated learning;hierarchical architecture;privacy enhancement","Servers;Privacy;Computer architecture;Training;Computational modeling;Collaborative work;Data models","","22","","33","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Forgotten Siblings: Unifying Attacks on Machine Learning and Digital Watermarking","E. Quiring; D. Arp; K. Rieck","Technische Universität Braunschweig Brunswick, Germany; Technische Universität Braunschweig Brunswick, Germany; Technische Universität Braunschweig Brunswick, Germany","2018 IEEE European Symposium on Security and Privacy (EuroS&P)","9 Jul 2018","2018","","","488","502","Machine learning is increasingly used in securitycritical applications, such as autonomous driving, face recognition, and malware detection. Most learning methods, however, have not been designed with security in mind and thus are vulnerable to different types of attacks. This problem has motivated the research field of adversarial machine learning that is concerned with attacking and defending learning methods. Concurrently, a separate line of research has tackled a very similar problem: In digital watermarking, a pattern is embedded in a signal in the presence of an adversary. As a consequence, this research field has also extensively studied techniques for attacking and defending watermarking methods. The two research communities have worked in parallel so far, unnoticeably developing similar attack and defense strategies. This paper is a first effort to bring these communities together. To this end, we present a unified notation of blackbox attacks against machine learning and watermarking. To demonstrate its efficacy, we apply concepts from watermarking to machine learning and vice versa. We show that countermeasures from watermarking can mitigate recent model-extraction attacks and, similarly, that techniques for hardening machine learning can fend off oracle attacks against watermarks. We further demonstrate a novel threat for watermarking schemes based on recent deep learning attacks from adversarial learning. Our work provides a conceptual link between two research fields and thereby opens novel directions for improving the security of both, machine learning and digital watermarking.","","978-1-5386-4228-3","10.1109/EuroSP.2018.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406619","Adversarial Learning;Digital Watermarking;Adversarial Signal Processing;Machine Learning;adversarial machine learning","Watermarking;Machine learning;Learning systems;Feature extraction;Media;Detectors","","31","1","70","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Reliability of IP Geolocation Services for Assessing the Compliance of International Data Transfers","M. Cozar; D. Rodriguez; J. M. Del Alamo; D. Guaman","ETSI Telecomunicacion Universidad Politécnica de Madrid, Madrid, Spain; ETSI Telecomunicacion Universidad Politécnica de Madrid, Madrid, Spain; ETSI Telecomunicacion Universidad Politécnica de Madrid, Madrid, Spain; Escuela Politécnica Nacional, Quito, Ecuador","2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","27 Jun 2022","2022","","","181","185","The General Data Protection Regulation sets strict requirements to allow personal data transfers outside the European Economic Area. Thus, knowing the geographical destination of data transfers is becoming increasingly important for different stakeholders such as data controllers that may become data exporters or data protection authorities who need to assess data processing compliance. To this end, several online databases and services provide geolocation data for IP addresses with different accuracy and reliability levels. This paper analyzes ten different IP geolocation services to understand their reliability against known ground truth and applies them to further assess whether 767 Android apps indeed carry out international personal data transfers. Our results show great discrepancy depending on the service used, thus demonstrating the uncertainty data controllers and supervisory authorities face to assess these data flows.","2768-0657","978-1-6654-9560-8","10.1109/EuroSPW55150.2022.00024","Comunidad de Madrid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799336","IP geolocation;Reliability;Compliance;GDPR;Privacy;Data Protection","Uncertainty;Geology;Europe;Machine learning;Data transfer;Reliability engineering;IP networks","","7","","18","IEEE","27 Jun 2022","","","IEEE","IEEE Conferences"
"Applying Machine Learning to use security oracles: a case study in virus and malware detection","D. Preuveneers; E. Lavens; W. Joosen","imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium","2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","27 Jun 2022","2022","","","240","251","Machine Learning (ML) has a significant potential to enhance the security posture of an organization by improving threat detection and discovery. The growing quality and quantity of data through measurements creates opportunities in this context. However, when an organization does not have sufficient labeled data to make predictions, it can rely on third parties for expert advise. In this work, we present a real-world case study of a company offering security services to other businesses on top of its portfolio of internal security assets. The security provider adopts ML to learn when to cost-effectively invoke a third party service/oracle (i.e. VirusTotal), first to boost its own detection rate in order to protect its customers, and second to obtain new ground truth to further optimize its future oracle use, all under fixed query budget constraints. While the decision making of the ML system was very successful in terms of avoiding false positives and false negatives, we nonetheless identified several security challenges when adopting ML for a cost-effective use of security oracles. We evaluate our ML solution on real data, elicit various lessons learned about the increased attack surface when using ML in security applications, and identify countermeasures to secure this ML pipeline.","2768-0657","978-1-6654-9560-8","10.1109/EuroSPW55150.2022.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799306","machine learning;security oracle;attacks;defenses;measurements","Costs;Computer viruses;Pipelines;Decision making;Machine learning;Companies;Security","","","","28","IEEE","27 Jun 2022","","","IEEE","IEEE Conferences"
"Detecting False Data Injection Attacks in Peer to Peer Energy Trading Using Machine Learning","S. Mohammadi; F. Eliassen; Y. Zhang; H. -A. Jacobsen","Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Faculty of Applied Science & Engineering, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3417","3431","In peer-to-peer (P2P) energy trading, the incorporation of distributed energy resources with unprotected data, originating from sources such as home energy management systems that are connected through the Internet, provokes vulnerabilities that can manifest security breaches. In this article, two threat scenarios based on a novel false data injection attack (FDIA) model in a local P2P energy trading system are explored. In these scenarios, an attacker gains free energy by manipulating prosumers’ consumption and demand. Precise and fast attack detection is needed to guarantee suitable countermeasures to prevent potential risks. We propose a novel instance-based machine learning (ML) classifier for detecting FDIAs. In contrast to black-box ML models, our algorithm provides a transparent decision-making procedure with significant predictive performance. We apply our detection model to a real-world dataset from Austin, Texas. Our experimental results show superior performance as compared to several popular interpretable and non-interpretable ML methods. On average, we achieve a 96.10 percent detection rate, a 96.18 percent accuracy rate, and a false negative rate of 1.97 percent with our approach.","1941-0018","","10.1109/TDSC.2021.3096213","Norges Forskningsråd(grant numbers:267967,287412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483661","False data injection;local P2P energy trading;prosumer;smart grid;false data injection attack detection;interpretable machine learning","Games;Machine learning;Security;Data models;Economics;Convergence;Support vector machines","","9","","42","IEEE","13 Jul 2021","","","IEEE","IEEE Journals"
"Mitch: A Machine Learning Approach to the Black-Box Detection of CSRF Vulnerabilities","S. Calzavara; M. Conti; R. Focardi; A. Rabitti; G. Tolomei",Università Ca’ Foscari; Università di Padova; Università Ca’ Foscari; Università Ca’ Foscari; Università di Padova,"2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","528","543","Cross-Site Request Forgery (CSRF) is one of the oldest and simplest attacks on the Web, yet it is still effective on many websites and it can lead to severe consequences, such as economic losses and account takeovers. Unfortunately, tools and techniques proposed so far to identify CSRF vulnerabilities either need manual reviewing by human experts or assume the availability of the source code of the web application. In this paper we present Mitch, the first machine learning solution for the black-box detection of CSRF vulnerabilities. At the core of Mitch there is an automated detector of sensitive HTTP requests, i.e., requests which require protection against CSRF for security reasons. We trained the detector using supervised learning techniques on a dataset of 5,828 HTTP requests collected on popular websites, which we make available to other security researchers. Our solution outperforms existing detection heuristics proposed in the literature, allowing us to identify 35 new CSRF vulnerabilities on 20 major websites and 3 previously undetected CSRF vulnerabilities on production software already analyzed using a state-of-the-art tool.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806728","Cross site request forgery;Machine learning;Web security","Security;Tools;Browsers;Forgery;Machine learning;Manuals;Task analysis","","14","","38","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models","B. Z. H. Zhao; A. Agrawal; C. Coburn; H. J. Asghar; R. Bhaskar; M. A. Kaafar; D. Webb; P. Dickinson","University of New South Wales; BITS Pilani; Cyber & Electronic Warfare Division, Defence Science and Technology Group, Australia; Macquarie University; Data61-CSIRO; Macquarie University; Cyber & Electronic Warfare Division, Defence Science and Technology Group, Australia; Cyber & Electronic Warfare Division, Defence Science and Technology Group, Australia","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","232","251","With an increase in low-cost machine learning APIs, advanced machine learning models may be trained on private datasets and monetized by providing them as a service. However, privacy researchers have demonstrated that these models may leak information about records in the training dataset via membership inference attacks. In this paper, we take a closer look at another inference attack reported in literature, called attribute inference, whereby an attacker tries to infer missing attributes of a partially known record used in the training dataset by accessing the machine learning model as an API. We show that even if a classification model succumbs to membership inference attacks, it is unlikely to be susceptible to attribute inference attacks. We demonstrate that this is because membership inference attacks fail to distinguish a member from a nearby non-member. We call the ability of an attacker to distinguish the two (similar) vectors as strong membership inference. We show that membership inference attacks cannot infer membership in this strong setting, and hence inferring attributes is infeasible. However, under a relaxed notion of attribute inference, called approximate attribute inference, we show that it is possible to infer attributes close to the true attributes. We verify our results on three publicly available datasets, five membership, and three attribute inference attacks reported in literature.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581166","Privacy;Attack;Inference;Membership;Attribute;Machine;Learning;Federated","Training;Privacy;Data privacy;Machine learning;Data models","","11","","37","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Effective Machine Learning-based Access Control Administration through Unlearning","J. M. Llamas; D. Preuveneers; W. Joosen","imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","50","57","With the rapid and increasing complexity of computer systems and software, there is a need for more effective, scalable, and secure access control methods. Machine learning (ML) has gained popularity in complementing manually crafted authorisation policies in such environments. However, given the dynamic and constantly evolving nature of software and access control systems, the administration of the latter presents a significant security challenge. This paper examines the administration problem of Machine Learning-based Access Control (MLBAC) systems through Machine Unlearning as a lightweight and secure method. More specifically, we explore this problem through exact and approximate unlearning and evaluate its impact using real-world data. We demonstrate the effectiveness of Machine Unlearning in both reverting policies and addressing potential vulnerabilities that may emerge during the model’s lifecycle. Compared to alternative options such as retraining from scratch, our approach reduces deployment and verification costs, making it a promising solution for MLBAC administration.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190682","machine learning;machine unlearning;administration;access control;security","Authorization;Costs;Machine learning;Software;Data models;Complexity theory;Behavioral sciences","","","","33","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"xFuzz: Machine Learning Guided Cross-Contract Fuzzing","Y. Xue; J. Ye; W. Zhang; J. Sun; L. Ma; H. Wang; J. Zhao","University of Science and Technology of China, Hefei, Anhui, China; Kyushu University, Fukuoka, Japan; University of Science and Technology of China, Hefei, Anhui, China; Singapore Management University, Singapore; University of Alberta, Alberta Machine Intelligence Institute, Edmonton, AB, Canada; Nanyang Technological University, Singapore; Kyushu University, Fukuoka, Japan","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","515","529","Smart contract transactions are increasingly interleaved by cross-contract calls. While many tools have been developed to identify a common set of vulnerabilities, the cross-contract vulnerability is overlooked by existing tools. Cross-contract vulnerabilities are exploitable bugs that manifest in the presence of more than two interacting contracts. Existing methods are however limited to analyze a maximum of two contracts at the same time. Detecting cross-contract vulnerabilities is highly non-trivial. With multiple interacting contracts, the search space is much larger than that of a single contract. To address this problem, we present xFuzz, a machine learning guided smart contract fuzzing framework. The machine learning models are trained with novel features (e.g., word vectors and instructions) and are used to filter likely benign program paths. Comparing with existing static tools, machine learning model is proven to be more robust, avoiding directly adopting manually-defined rules in specific tools. We compare xFuzz with three state-of-the-art tools on 7,391 contracts. xFuzz detects 18 exploitable cross-contract vulnerabilities, of which 15 vulnerabilities are exposed for the first time. Furthermore, our approach is shown to be efficient in detecting non-cross-contract vulnerabilities as well—using less than 20% time as that of other fuzzing tools, xFuzz detects twice as many vulnerabilities.","1941-0018","","10.1109/TDSC.2022.3182373","National Natural Science Foundation of China(grant numbers:61972373); Basic Research Program of Jiangsu Province(grant numbers:BK20201192); National Research Foundation Singapore; NSoE Programme(grant numbers:NSOE-TSS2019-03); CAS Pioneer Hundred Talents Program of China; Canada CIFAR AI Chairs Program; Amii RAP Program; Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2021-02549,RGPAS-2021-00034,DGECR-2021-00019); JSPS KAKENHI(grant numbers:JP20H04168,JP21H04877); JST-Mirai Program(grant numbers:JPMJMI20B8); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795233","Cross-contract vulnerability;fuzzing;machine learning;smart contract","Smart contracts;Fuzzing;Machine learning;Security;Codes;Engines;Training","","14","","56","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"On the Privacy Risks of Algorithmic Fairness","H. Chang; R. Shokri","Department of Computer Science, National University of Singapore (NUS); Department of Computer Science, National University of Singapore (NUS)","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","292","303","Algorithmic fairness and privacy are essential pillars of trustworthy machine learning. Fair machine learning aims at minimizing discrimination against protected groups by, for example, imposing a constraint on models to equalize their behavior across different groups. This can subsequently change the influence of training data points on the fair model, in a disproportionate way. We study how this can change the information leakage of the model about its training data. We analyze the privacy risks of group fairness (e.g., equalized odds) through the lens of membership inference attacks: inferring whether a data point is used for training a model. We show that fairness comes at the cost of privacy, and this cost is not distributed equally: the information leakage of fair models increases significantly on the unprivileged subgroups, which are the ones for whom we need fair learning. We show that the more biased the training data is, the higher the privacy cost of achieving fairness for the unprivileged subgroups will be. We provide comprehensive empirical analysis for general machine learning algorithms.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00028","Singapore Ministry of Education Academic Research Fund(grant numbers:R-252-000-660-133); NUS(grant numbers:NUS ECRAFY19 P16); Intel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581219","Trustworthy Machine Learning;Group Fairness;Data Privacy;Membership Inference Attacks","Training;Privacy;Data privacy;Machine learning algorithms;Costs;Training data;Machine learning","","25","","45","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"A Pragmatic Approach to Membership Inferences on Machine Learning Models","Y. Long; L. Wang; D. Bu; V. Bindschaedler; X. Wang; H. Tang; C. A. Gunter; K. Chen","University of Illinois, Urbana-Champaign; Indiana University Bloomington; Indiana University Bloomington; University of Florida; Indiana University Bloomington; Indiana University Bloomington; University of Illinois, Urbana-Champaign; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences","2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","521","534","Membership Inference Attacks (MIAs) aim to determine the presence of a record in a machine learning model's training data by querying the model. Recent work has demonstrated the effectiveness of MIA on various machine learning models and corresponding defenses have been proposed. However, both attacks and defenses have focused on an adversary that indiscriminately attacks all the records without regard to the cost of false positives or negatives. In this work, we revisit membership inference attacks from the perspective of a pragmatic adversary who carefully selects targets and make predictions conservatively. We design a new evaluation methodology that allows us to evaluate the membership privacy risk at the level of individuals and not only in aggregate. We experimentally demonstrate that highly vulnerable records exist even when the aggregate attack precision is close to 50% (baseline). Specifically, on the MNIST dataset, our pragmatic adversary achieves a precision of 95.05% whereas the prior attack only achieves a precision of 51.7%.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230385","","Training;Privacy;Analytical models;Costs;Aggregates;Training data;Machine learning","","26","","38","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"A Hybrid Solution for Constrained Devices to Detect Microarchitectural Attacks","N. -F. Polychronou; P. -H. Thevenon; M. Puys; V. Beroulle","Univ. Grenoble Alpes, CEA, LETI, DSYS, LSES, Grenoble, France; Univ. Grenoble Alpes, CEA, LETI, DSYS, LSES, Grenoble, France; Univ. Grenoble Alpes, CEA, LETI, DSYS, LSES, Grenoble, France; Univ. Grenoble Alpes, Grenoble-INP, LCIS, Grenoble, France","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","259","269","We are seeing an increase in cybersecurity attacks on resource-constrained systems such as the Internet of Things (IoT) and Industrial IoT (I-IoT) devices. Recently, a new category of attacks has emerged called microarchitectural attacks. It targets hardware units of the system such as the processor or memory and is often complicated if not impossible to remediate since it imposes modifying the hardware. In default of remediation, some solutions propose to detect these attacks. Yet, most of them are not suitable for embedded systems since they are based on complex machine learning algorithms.In this paper, we propose an edge-computing security solution for attack detection that uses a local-remote machine learning implementation to find an equilibrium between accuracy and decision-making latency while addressing the memory, performance, and communication bandwidth constraints of resource-constrained systems. We demonstrate effectiveness in the detection of multiple microarchitectural attacks such as Row hammer or cache attacks on an embedded device with an accuracy of 98.75% and a FPR near 0%. To limit the overhead on the communication bus, the proposed solution allows to locally classify as trusted 99% of the samples during normal operation and thus filtering them out.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190668","IoT;Security;HPCs;Edge-Computing;Local Remote Detection;Microarchitectural Attacks","Performance evaluation;Microarchitecture;Machine learning algorithms;Image edge detection;Bandwidth;Machine learning;Minimization","","","","37","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"LUNA: Quantifying and Leveraging Uncertainty in Android Malware Analysis through Bayesian Machine Learning","M. Backes; M. Nauman","CISPA, Saarland University / Max Planck Institute for Software Systems, Germany; CISPA, Saarland University / Max Planck Institute for Software Systems, Germany","2017 IEEE European Symposium on Security and Privacy (EuroS&P)","3 Jul 2017","2017","","","204","217","Android's growing popularity seems to be hindered only by the amount of malware surfacing for this open platform. Machine learning algorithms have been successfully used for detecting the rapidly growing number of malware families appearing on a daily basis. Existing solutions along these lines, however, have a common limitation: they are all based on classical statistical inference and thus ignore the concept of uncertainty invariably involved in any prediction task. In this paper, we show that ignoring this uncertainty leads to incorrect classification of both benign and malicious apps. To reduce these errors, we utilize Bayesian machine learning - an alternative paradigm based on Bayesian statistical inference - which preserves the concept of uncertainty in all steps of calculation. We move from a black-box to a white-box approach to identify the effects different features (such as sensitive resource usage, declared activities, services and intent filters etc.) have on the classification status of an app. We show that incorporating uncertainty in the learning pipeline helps to reduce incorrect decisions, and significantly improves the accuracy of classification. We achieve a false positive rate of 0.2% compared to the previous best of 1%. We present sufficient details to allow the reader to reproduce our results through openly available probabilistic programming tools and to extend our techniques well beyond the boundaries of this paper.","","978-1-5090-5762-7","10.1109/EuroSP.2017.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961981","Malware analysis;Uncertainty;Bayesian Machine Learning","Uncertainty;Bayes methods;Androids;Humanoid robots;Malware;Security;Pipelines","","14","","29","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Generating Adversarial Examples Against Machine Learning-Based Intrusion Detector in Industrial Control Systems","J. Chen; X. Gao; R. Deng; Y. He; C. Fang; P. Cheng","State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and the College of Control Science and Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1810","1825","Deploying machine learning (ML)-based intrusion detection systems (IDS) is an effective way to improve the security of industrial control systems (ICS). However, ML models themselves are vulnerable to adversarial examples, generated by deliberately adding subtle perturbation to the input sample that some people are not aware of, causing the model to give a false output with high confidence. In this article, our goal is to investigate the possibility of stealthy cyber attacks towards IDS, including injection attack, function code attack and reconnaissance attack, and enhance its robustness to adversarial attack. However, adversarial algorithms are subject to communication protocol and legal range of data in ICS, unlike only limited by the distance between original samples and newly generated samples in image domain. We propose two strategies - optimal solution attack and GAN attack - oriented to flexibility and volume of data, formulating an optimization problem to find stealthy attacks, where the former is appropriate for not too large and more flexible samples while the latter provides a more efficient solution for larger and not too flexible samples. Finally, we conduct experiments on a semi-physical ICS testbed with a high detection performance ensemble ML-based detector to show the effectiveness of our attacks. The results indicate that new samples of reconnaissance and function code attack produced by both optimal solution and GAN algorithm possess 80 percent higher probability to evade the detector, still maintaining the same attack effect. In the meantime, we adopt adversarial training as a method to defend against adversarial attack. After training on the mixture of orginal dataset and newly generated samples, the detector becomes more robust to adversarial examples.","1941-0018","","10.1109/TDSC.2020.3037500","National Key Research and Development Program of China(grant numbers:2018YFB0803501); National Natural Science Foundation of China(grant numbers:62073285); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257170","Machine learning security;intrusion detection systems;industrial control systems;adversarial examples","Integrated circuits;Detectors;Reconnaissance;Generative adversarial networks;Protocols;Integrated circuit modeling;Machine learning","","20","","54","IEEE","12 Nov 2020","","","IEEE","IEEE Journals"
"Protecting Decision Boundary of Machine Learning Model With Differentially Private Perturbation","H. Zheng; Q. Ye; H. Hu; C. Fang; J. Shi","Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Kowloon, Hong Kong; Huawei International, Singapore; Huawei International, Singapore","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","2007","2022","Machine learning service API allows model owners to monetize proprietary models by offering prediction services to third-party users. However, existing literature shows that model parameters are vulnerable to extraction attacks which accumulate prediction queries and their responses to train a replica model. As countermeasures, researchers have proposed to reduce the rich API output, such as hiding the precise confidence. Nonetheless, even with response being only one bit, an adversary can still exploit fine-tuned queries with differential property to infer the decision boundary of the underlying model. In this article, we propose boundary differential privacy (BDP) against such attacks by obfuscating the prediction responses with noises. BDP guarantees an adversary cannot learn the decision boundary of any two classes by a predefined precision no matter how many queries are issued to the prediction API. We first design a perturbation algorithm called boundary randomized response for a binary model. Then we prove it satisfies $\epsilon$ε-BDP, followed by a generalization of this algorithm to a multiclass model. Finally, we generalize a hard boundary to soft boundary and design an adaptive perturbation algorithm that can still work in the latter case. The effectiveness and high utility of our solution are verified by extensive experiments on both linear and non-linear models.","1941-0018","","10.1109/TDSC.2020.3043382","National Natural Science Foundation of China(grant numbers:U1636205,61572413); Research Grants Council, Hong Kong SAR, China(grant numbers:15238116,15222118,15218919,C1008-16G); Research Project from Huawei; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286504","Model defense;boundary differential privacy;model extraction;adversarial machine learning","Predictive models;Machine learning;Differential privacy;Perturbation methods;Computational modeling;Adaptation models;Prediction algorithms","","14","","34","CCBY","8 Dec 2020","","","IEEE","IEEE Journals"
"Comments on “VERSA: Verifiable Secure Aggregation for Cross-Device Federated Learning”","F. Luo; H. Wang; X. Yan","College of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; Department of New Networks, Peng Cheng Laboratory, Shenzhen, China; School of Computer Science, South China Normal University, Guangzhou, China","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","499","500","Federated learning (FL) allows a large number of users to collaboratively train machine learning (ML) models by sending only their local gradients to a central server for aggregation in each training iteration, without sending their raw training data. The main security issues of FL, that is, the privacy of the gradient vector and the correctness verification of the aggregated gradient, are gaining increasing attention from industry and academia. To protect the privacy of the gradient, a secure aggregation was proposed; to verify the correctness of the aggregated gradient, a verifiable secure aggregation that requires the server to provide a verifiable aggregated gradient was proposed. In 2021, Hahn et al. proposed VERSA, a verifiable secure aggregation. However, in this article, we will point out a flaw in VERSA, which indicates that VERSA does not work. To address the flaw, we present several approaches with different advantages and disadvantages. We hope that by identifying the flaw, similar errors can be avoided in future designs of verifiable secure aggregation.","1941-0018","","10.1109/TDSC.2023.3253082","PCL(grant numbers:PCL2022A03); Guangxi Natural Science Foundation(grant numbers:2022GXNSFBA035650); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061268","Federated learning;machine learning;privacy-preserving;verifiable secure aggregation","Servers;Protocols;Security;Privacy;Federated learning;Public key;Data models","","","","9","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"DL-FHMC: Deep Learning-Based Fine-Grained Hierarchical Learning Approach for Robust Malware Classification","A. Abusnaina; M. Abuhamad; H. Alasmary; A. Anwar; R. Jang; S. Salem; D. Nyang; D. Mohaisen","Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, Loyola University, Chicago, IL, USA; Department of Computer Science, King Khalid University, Abha, Saudi Arabia; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, North Dakota State University, Fargo, ND, USA; Department of Cyber Security, Ewha Women University, Seoul, South Korea; Department of Computer Science, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3432","3447","The acceptance of the Internet of Things (IoT) for both household and industrial applications is accompanied by the rapid growth of IoT malware. With the increase of their attack surface, analyzing, understanding, and detecting IoT malicious behavior are crucial. Traditionally, machine and deep learning-based approaches are used for malware detection and behavioral understanding. However, recent research has shown the susceptibility of those approaches to adversarial attacks by introducing noise to the feature space. In this work, we introduce DL-FHMC, a fine-grained hierarchical learning approach for robust IoT malware detection. DL-FHMC utilizes Control Flow Graph (CFG)-based behavioral patterns for adversarial IoT malicious software detection. In particular, we extract a comprehensive list of behavioral patterns from a large dataset of malicious IoT binaries, represented by the shared execution flows, and use them as a modality for malicious behavior detection. Leveraging machine learning and subgraph isomorphism matching algorithms, DL-FHMC provides state-of-the-art performance in detecting malware samples and adversarial examples (AEs). We first highlight the caveats of CFG-based IoT malware detection systems, showing the adversarial capabilities in generating practical functionality-preserving AEs with reduced overhead using Graph Embedding and Augmentation (GEA) techniques. We then introduce Suspicious Behavior Detector, a component that extracts comprehensive behavioral patterns from three popular IoT malicious families, Gafgyt, Mirai, and Tsunami, for AEs detection with high accuracy. The proposed detector operates as a model-independent standalone module, with no prior assumptions of the adversarial attacks nor their configurations.","1941-0018","","10.1109/TDSC.2021.3097296","Global Research Lab; National Research Foundation; Ministry of Science, Information, and Communication Technologies(grant numbers:NRF-2016K1A1A2912757); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484718","Adversarial machine learning;deep learning;Internet of Things;malware detection;adversarial attacks","Malware;Deep learning;Static analysis;Robustness;Machine learning;Internet of Things;Machine learning algorithms","","18","","59","IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"A Wolf in Sheep’s Clothing: Query-Free Evasion Attacks Against Machine Learning-Based Malware Detectors with Generative Adversarial Networks","D. Gibert; J. Planes; Q. Le; G. Zizzo","CeADAR/University College Dublin, Dublin, Ireland; University of Lleida, Lleida, Spain; CeADAR/University College Dublin, Dublin, Ireland; IBM Research Europe, Dublin, Ireland","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","415","426","Malware detectors based on machine learning (ML) have been shown to be susceptible to adversarial malware examples. However, current methods to generate adversarial malware examples still have their limits. They either rely on detailed model information (gradient-based attacks), or on detailed outputs of the model - such as class probabilities (score-based attacks), neither of which are available in real-world scenarios. Alternatively, adversarial examples might be crafted using only the label assigned by the detector (label-based attack) to train a substitute network or an agent using reinforcement learning. Nonetheless, label-based attacks might require querying a black-box system from a small number to thousands of times, depending on the approach, which might not be feasible against malware detectors.This work presents a novel query-free approach to craft adversarial malware examples to evade ML-based malware detectors. To this end, we have devised a GAN-based framework to generate adversarial malware examples that look similar to benign executables in the feature space. To demonstrate the suitability of our approach we have applied the GAN-based attack to three common types of features usually employed by static ML-based malware detectors: (1) Byte histogram features, (2) API-based features, and (3) String-based features. Results show that our model-agnostic approach performs on par with MalGAN, while generating more realistic adversarial malware examples without requiring any query to the malware detectors. Furthermore, we have tested the generated adversarial examples against state-of-the-art multimodal and deep learning malware detectors, showing a decrease in detection performance, as well as a decrease in the average number of detections by the antimalware engines in VirusTotal.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00052","Enterprise Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190713","Adversarial Malware Examples;Generative Adversarial Networks;Machine Learning;Malware Detection;Evasion Attack","Knowledge engineering;Deep learning;Histograms;Computer viruses;Detectors;Reinforcement learning;Feature extraction","","1","","24","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"EzPC: Programmable and Efficient Secure Two-Party Computation for Machine Learning","N. Chandran; D. Gupta; A. Rastogi; R. Sharma; S. Tripathi","Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Indian Institute of Technology, New Delhi, India","2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","496","511","We present EzPC, a secure two-party computation (2PC) framework that generates efficient 2PC protocols from high-level, easy-to-write programs. EzPC provides formal correctness and security guarantees while maintaining performance and scalability. Previous language frameworks, such as CBMC-GC, ObliVM, SMCL, and Wysteria, generate protocols that use either arithmetic or boolean circuits exclusively. Our compiler is the first to generate protocols that combine both arithmetic and boolean circuits for better performance. We empirically demonstrate that the performance of the protocols generated by EzPC is comparable to or better than (in some cases upto 19x) their state-of-the-art, hand-crafted implementations, while EzPC protocols also outperform their boolean circuits only counterparts by as much as 25x.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806756","SMC;MPC;Compilers;Machine Learning","Protocols;Cryptography;Program processors;Semantics;Scalability;Tools","","42","","64","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Machine Learning Based Resilience Testing of an Address Randomization Cyber Defense","G. Mani; M. Haliem; B. Bhargava; I. Manickam; K. Kochpatcharin; M. Kim; E. Vugrin; W. Wang; C. Jenkins; P. Angin; M. Yu","Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Sandia National Laboratories, Albuquerque, NM, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Sandia National Laboratories, Albuquerque, NM, USA; Software and Information Systems Department, University of North Carolina at Charlotte, Charlotte, NC, USA; Sandia National Laboratories, Albuquerque, NM, USA; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey; Department of Computer Science, Roosevelt University, Chicago, IL, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4853","4867","Moving target defenses (MTDs) are widely used as an active defense strategy for thwarting cyberattacks on cyber-physical systems by increasing diversity of software and network paths. Recently, machine Learning (ML) and deep Learning (DL) models have been demonstrated to defeat some of the cyber defenses by learning attack detection patterns and defense strategies. It raises concerns about the susceptibility of MTD to ML and DL methods. In this article, we analyze the effectiveness of ML and DL models when it comes to deciphering MTD methods and ultimately evade MTD-based protections in real-time systems. Specifically, we consider a MTD algorithm that periodically randomizes address assignments within the MIL-STD-1553 protocol—a military standard serial data bus. Two ML and DL-based tasks are performed on MIL-STD-1553 protocol to measure the effectiveness of the learning models in deciphering the MTD algorithm: 1) determining whether there is an address assignments change i.e., whether the given system employs a MTD protocol and if it does 2) predicting the future address assignments. The supervised learning models (random forest and k-nearest neighbors) effectively detected the address assignment changes and classified whether the given system is equipped with a specified MTD protocol. On the other hand, the unsupervised learning model (K-means) was significantly less effective. The DL model (long short-term memory) was able to predict the future addresses with varied effectiveness based on MTD algorithm's settings.","1941-0018","","10.1109/TDSC.2023.3234561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015577","Cyber security;cyber-attacks;cyber resilience;long-short-term-memory;moving target defense;random forest;supervised learning;unsupervised learning","Military standards;Protocols;Real-time systems;Task analysis;Prediction algorithms;Reliability;Machine learning algorithms","","1","","75","IEEE","11 Jan 2023","","","IEEE","IEEE Journals"
"NPMML: A Framework for Non-Interactive Privacy-Preserving Multi-Party Machine Learning","T. Li; J. Li; X. Chen; Z. Liu; W. Lou; Y. T. Hou","School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi'an, China; College of Cyber Science and the College of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2021","2021","18","6","2969","2982","In the recent decade, deep learning techniques have been widely adopted for founding artificial Intelligent applications, which led to successes in many data analysis tasks, such as risk assessment, medical predictions, and face recognition. Since the effectiveness of deep learning is directly proportional to the amount of data available, a large-scale collection of massive data is essential. Considering privacy and security concerns often prevent data owners from contributing sensitive data for training, researchers proposed several techniques to provide privacy guarantees of data in machine learning systems that contains multiple parties. However, all these works incurred frequent interactions between data owners during training, such that they came at a high communicational cost for data owners. To this end, in this article, we propose a new server-aid framework called non-interactive privacy-preserving multi-party machine learning (NPMML), which supports secure machine learning tasks without the participation of data owners. The NPMML framework significantly reduces data owners’ communicational overheads in multi-party machine learning. Moreover, we design a concrete construction for multi-layer neural networks based on NPMML. Finally, we evaluate the performance of NPMML by prototype implementation. The experimental result demonstrates that NPMML is communicational-efficient for data owners.","1941-0018","","10.1109/TDSC.2020.2971598","National Natural Science Foundation of China(grant numbers:61802078,61702125); China Postdoctoral Science Foundation(grant numbers:204728); National Natural Science Foundation of China(grant numbers:61722203); National Natural Science Foundation of China(grant numbers:U1936218); National Natural Science Foundation of China(grant numbers:61960206014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981947","Privacy preserving;neural network learning;encryption;non-interactive learning","Training data;Machine learning;Neural networks;Data models;Data privacy;Cryptography","","27","","33","IEEE","4 Feb 2020","","","IEEE","IEEE Journals"
"Fall of Giants: How popular text-based MLaaS fall against a simple evasion attack","L. Pajola; M. Conti","Department of Mathematics, University of Padua, Padua, Italy; Department of Mathematics, University of Padua, Padua, Italy","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","198","211","The increased demand for machine learning applications made companies offer Machine-Learning-as-a-Service (MLaaS). In MLaaS (a market estimated 8000M USD by 2025), users pay for well-performing ML models without dealing with the complicated training procedure. Among MLaaS, text-based applications are the most popular ones (e.g., language translators). Given this popularity, MLaaS must provide resiliency to adversarial manipulations. For example, a wrong translation might lead to a misunderstanding between two parties. In the text domain, state-of-the-art attacks mainly focus on strategies that leverage ML models' weaknesses. Unfortunately, not much attention has been given to the other pipeline' stages, such as the indexing stage (i.e., when a sentence is converted from a textual to a numerical representation) that, if manipulated, can significantly affect the final performance of the application. In this paper, we propose a novel text evasion technique called “Zero-Width attack” (ZeW) that leverages the injection of human non-readable characters, affecting indexing stage mechanisms. We demonstrate that our simple yet effective attack deceives MLaaS of “giants” such as Amazon, Google, IBM, and Microsoft. Our case study, based on the manipulation of hateful tweets, shows that out of 12 analyzed services, only one is resistant to our injection strategy. We finally introduce and test a simple input validation defense that can prevent our proposed attack.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581208","NLP;evasion attack;input validation","Training;Computational modeling;Terrorism;Pipelines;Sociology;Machine learning;Companies","","6","","57","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Feature Selection Strategies for HTTP Botnet Traffic Detection","I. Letteri; G. Della Penna; P. Caianiello","Dept. of Information Engineering, Computer Science and Mathematics University of L’Aquila, L’Aquila, Italy; Dept. of Information Engineering, Computer Science and Mathematics University of L’Aquila, L’Aquila, Italy; Dept. of Information Engineering, Computer Science and Mathematics University of L’Aquila, L’Aquila, Italy","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","202","210","We report about a HTTP botnet detection strategy based on a behavioral analysis of raw traffic data with the aim at minimizing resources necessary for the detection. It involves the selective choice of traffic characteristical features and their extraction with engineered probes, in a context of evolving malicious traffic. We develop the extraction software for eight selected features and experiment with a Multilayer Perceptron Classifier (MLP) over a benchmark traffic dataset for botnet detection, achieving a good 98.03% accuracy. In the effort to optimize the classifier overall performance by reducing data redundancy, we compute a statistics on Decision Tree Classifiers (DT) in order to rank features and observe that, by selecting out few of the lowest ranked ones (3), we can maintain MLP accuracy at 97.54% yet reducing probing resources and costs. We obtain a small further improvement in MLP performance, avoid the lengthy process of running the statistics of the DTs on actual data, and boost the ranking/selecting-out process by means of mutual partion entropy computation.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802421","Malware-Detection,-HTTP-Botnets,-Network-Security,-Machine-Learning,-Feature-Selection,-Information-Theory,-Mutual-Information","Feature extraction;Botnet;Malware;Entropy;Machine learning;Mutual information;Computer science","","7","","37","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Membership Inference Attacks Against Machine Learning Models via Prediction Sensitivity","L. Liu; Y. Wang; G. Liu; K. Peng; C. Wang","National Engineering Research Center for Educational Big Data, Central China Normal University, Wuhan, Hubei, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2341","2347","Machine learning (ML) has achieved huge success in recent years, but is also vulnerable to various attacks. In this article, we concentrate on membership inference attacks and propose Aster, which merely requires the target model's black-box API and a data sample to determine whether this sample was used to train the given ML model or not. The key idea of Aster is that the training data of a fully trained ML model usually has lower prediction sensitivities compared with that of the non-training data (i.e., testing data). Less sensitivity means that when perturbing a training sample's feature value in the corresponding feature space, the prediction of the perturbed sample obtained from the target model tends to be consistent with the original prediction. In this article, we quantify the prediction sensitivity with the Jacobian matrix which could reflect the relationship between each feature's perturbation and the corresponding prediction's change. Then we regard the samples with a lower as training data. Aster can breach the membership privacy of the target model's training data with no prior knowledge about the target model or its training data. The experiment results on four datasets show that our method outperforms three state-of-the-art inference attacks.","1941-0018","","10.1109/TDSC.2022.3180828","National Natural Science Foundation of China(grant numbers:61872416,62171189,62002104); Key Research and Development Program of Hubei Province(grant numbers:2020BAB120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793586","Machine learning;membership inference attack;prediction sensitivity;Jacobian matrix","Predictive models;Data models;Jacobian matrices;Training;Computational modeling;Sensitivity;Training data","","6","","22","IEEE","10 Jun 2022","","","IEEE","IEEE Journals"
"Man-in-the-Middle Attacks Against Machine Learning Classifiers Via Malicious Generative Models","D. Wang; C. Li; S. Wen; S. Nepal; Y. Xiang","School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; CSIRO's Data 61, North Ryde, NSW, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; CSIRO's Data 61, North Ryde, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","31 Aug 2021","2021","18","5","2074","2087","Deep Neural Networks (DNNs) are vulnerable to deliberately crafted adversarial examples. In the past few years, many efforts have been spent on exploring query-optimisation attacks to find adversarial examples of either black-box or white-box DNN models, as well as the defending countermeasures against those attacks. In this article, we explore vulnerabilities of DNN models under the umbrella of Man-in-the-Middle (MitM) attacks, which have not been investigated before. From the perspective of an MitM adversary, the aforementioned adversarial example attacks are not viable anymore. First, such attacks must acquire the outputs from the models multiple times before actually launching attacks, which is difficult for the MitM adversary in practice. Second, such attacks are one-off and cannot be directly generalised onto new data examples, which decreases the rate of return for the attacker. In contrast, using generative models to craft adversarial examples on the fly can mitigate the drawbacks. However, the adversarial capability of the generative models, such as Variational Auto-Encoder (VAE), has not been extensively studied. Therefore, given a classifier, we investigate using a VAE decoder to either transform benign inputs to their adversarial counterparts or decode outputs from benign VAE encoders to be adversarial examples. The proposed method can endue more capability to MitM attackers. Based on our evaluation, the proposed attack can achieve above 95 percent success rates on both MNIST and CIFAR10 datasets, which is better or comparable with state-of-the-art query-optimisation attacks. In the meantime, the attack is 104 times faster than the query-optimisation attacks.","1941-0018","","10.1109/TDSC.2020.3021008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9183938","Deep neural network;adversarial example;security","Decoding;Optimization;Computational modeling;Gallium nitride;Machine learning;Transforms;Task analysis","","10","","47","IEEE","1 Sep 2020","","","IEEE","IEEE Journals"
"Temporal Analysis of Distribution Shifts in Malware Classification for Digital Forensics","F. Zola; J. L. Bruse; M. Galar","Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia/San Sebastian, Spain; Vicomtech, Basque Research and Technology Alliance (BRTA), Donostia/San Sebastian, Spain; Institute of Smart Cities, Public University of Navarre, Pamplona, Spain","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","439","450","In recent years, malware diversity and complexity have increased substantially, so the detection and classification of malware families have become one of the key objectives of information security. Machine learning (ML)-based approaches have been proposed to tackle this problem. However, most of these approaches focus on achieving high classification performance scores in static scenarios, without taking into account a key feature of malware: it is constantly evolving. This leads to ML models being outdated and performing poorly after only a few months, leaving stakeholders exposed to potential security risks. With this work, our aim is to highlight the issues that may arise when applying ML-based classification to malware data. We propose a three-step approach to carry out a forensics exploration of model failures. In particular, in the first step, we evaluate and compare the concept drift generated by models trained using a rolling windows approach for selecting the training dataset. In the second step, we evaluate model drift based on the amount of temporal information used in the training dataset. Finally, we perform an in-depth misclassification and feature analysis to emphasize the interpretation of the results and to highlight drift causes. We conclude that caution is warranted when training ML models for malware analysis, as concept drift and clear performance drops were observed even for models trained on larger datasets. Based on our results, it may be more beneficial to train models on fewer but recent data and re-train them after a few months in order to maintain performance.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190715","Concept drift;temporal analysis;malware classification;forensic exploration;explainability","Training;Analytical models;Digital forensics;Information security;Machine learning;Malware;Data models","","","","48","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"CBA-Detector: A Self-Feedback Detector Against Cache-Based Attacks","B. Zheng; J. Gu; J. Wang; C. Weng","East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China; East China Normal University, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3231","3243","Cloud computing is convenient to provide adequate resources for tenants. However, since multiple tenants share the underlying hardware resources, malicious tenants can use the shared processor to launch cache-based attacks. Such attacks can help malicious tenants steal private data of other tenants bypassing isolation mechanisms provided by the system, resulting in information leakage. Moreover, Spectre and Meltdown vulnerabilities can even extract memory contents arbitrarily with the help of cache attacks. Therefore, cache-based attacks pose a serious threat to the security of cloud platforms. To defeat such attacks, many detection methods have been proposed. However, most methods induce high false positives because they completely rely on the hardware performance counters (HPCs) and detect attacks with static criteria. To solve this problem, this article proposes a self-feedback detector named CBA-Detector to detect cache-based attacks in real time. Specifically, CBA-Detector first uses machine learning technologies to create models for identifying suspicious programs with abnormal hardware behaviors, then analyzes suspicious programs from the instruction level to identify real attacks and provide feedback. Based on the feedback, the models can be updated to further improve their detection accuracy. As our experiments show, CBA-Detector can accurately identify cache-based attacks in real time and introduces a little overhead. Besides, the misjudgment rate decreases with the running time.","1941-0018","","10.1109/TDSC.2021.3089882","National Natural Science Foundation of China(grant numbers:61772204,61732014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9457125","Cache-based side-channel attacks;self-feedback;false positives;machine learning","Hardware;Real-time systems;Machine learning;Detectors;Side-channel attacks;Time measurement;Decision trees","","5","","36","IEEE","16 Jun 2021","","","IEEE","IEEE Journals"
"Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection","A. Demontis; M. Melis; B. Biggio; D. Maiorca; D. Arp; K. Rieck; I. Corona; G. Giacinto; F. Roli","Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Institute of System Security, Technische Universität Braunschweig, Rebenring 56, Braunschweig, Germany; Institute of System Security, Technische Universität Braunschweig, Rebenring 56, Braunschweig, Germany; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy","IEEE Transactions on Dependable and Secure Computing","9 Jul 2019","2019","16","4","711","724","To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.","1941-0018","","10.1109/TDSC.2017.2700270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917369","Android malware detection;static analysis;secure machine learning;computer security","Androids;Humanoid robots;Malware;Security;Feature extraction;Tools;Algorithm design and analysis","","139","","50","IEEE","2 May 2017","","","IEEE","IEEE Journals"
"Privacy of DNS-over-HTTPS: Requiem for a Dream?","L. Csikor; H. Singh; M. S. Kang; D. M. Divakaran","National University of Singapore; IIIT, Naya Raipur; KAIST; Trustwave","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","252","271","The recently proposed DNS-over-HTTPS (DoH) protocol is becoming increasingly popular in addressing the privacy concerns of exchanging plain-text DNS messages over potentially malicious transit networks (e.g., mass surveillance at ISPs). By employing HTTPS to encrypt DNS communications, DoH traffic inherently becomes indistinguishable from regular encrypted Web traffic, rendering active disruption (e.g., downgrading to the plain-text DNS) by transit networks extremely hard. In this work, we investigate whether DoH traffic is indeed indistinguishable from encrypted Web traffic. To this end, we collect several DoH traffic traces corresponding to 25 resolvers (including major ones, e.g., Google and Cloudftare) by visiting thousands of domains in Alexa's list of top-ranked websites at different geographical locations and environments. Based on the collected traffic, we train a machine learning model to classify HTTPS traffic as either Web or DoH. With our DoH identification model in place, we show that an authoritarian ISP can identify ∼97.4% (∼90%) of the DoH packets correctly in a closed-world (open-world) setting while only misclassifying 1 in 10,000 Web packets. To counter this DoH identification model, we propose an effective mitigation technique, making the identification model impractical for ISPs to filter and consequently downgrade DoH to plain-text DNS communications.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00026","National Research Foundation; National University of Singapore; National Research Foundation of Korea (NRF); Korea government (MSIT)(grant numbers:2021R1C1C1008462); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581227","DoH;DNS;privacy;machine learning","Privacy;Protocols;Virtual assistants;Surveillance;Machine learning;Rendering (computer graphics);Information filters","","11","","73","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Enhancing Vulnerability Prioritization: Data-Driven Exploit Predictions with Community-Driven Insights","J. Jacobs; S. Romanosky; O. Suciu; B. Edwards; A. Sarabi",Cyentia Institute; RAND Corporation; University of Maryland; Cyentia Institute; University of Michigan,"2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","194","206","The number of disclosed vulnerabilities has been steadily increasing over the years. At the same time, organizations face significant challenges patching their systems, leading to a need to prioritize vulnerability remediation in order to reduce the risk of attacks. Unfortunately, existing vulnerability scoring systems are either vendor-specific, proprietary, or are only commercially available. Moreover, these and other prioritization strategies based on vulnerability severity are poor predictors of actual vulnerability exploitation because they do not incorporate new information that might impact the likelihood of exploitation. In this paper we present the efforts behind building a Special Interest Group (SIG) that seeks to develop a completely data-driven exploit scoring system that produces scores for all known vulnerabilities, that is freely available, and which adapts to new information. The Exploit Prediction Scoring System (EPSS) SIG consists of more than 170 experts from around the world and across all industries, providing crowd-sourced expertise and feedback. Based on these collective insights, we describe the design decisions and trade-offs that lead to the development of the next version of EPSS. This new machine learning model provides an 82% performance improvement over past models in distinguishing vulnerabilities that are exploited in the wild and thus may be prioritized for remediation.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190703","vulnerability prioritization;exploit prediction;vulnerabilities;exploits;machine learning","Industries;Soft sensors;Current measurement;Machine learning;Organizations;Predictive models;Size measurement","","3","","43","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"Backdoor Attack on Machine Learning Based Android Malware Detectors","C. Li; X. Chen; D. Wang; S. Wen; M. E. Ahmed; S. Camtepe; Y. Xiang","School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of IT, Monash University, Clayton, VIC, Australia; CSIRO Data61, Marsfield, NSW, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; CSIRO Data61, Marsfield, NSW, Australia; CSIRO Data61, Marsfield, NSW, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3357","3370","Machine learning (ML) has been widely used for malware detection on different operating systems, including Android. To keep up with malware's evolution, the detection models usually need to be retrained periodically (e.g., every month) based on the data collected in the wild. However, this leads to poisoning attacks, specifically backdoor attacks, which subvert the learning process and create evasion ‘tunnels’ for manipulated malware samples. To date, we have not found any prior research that explored this critical problem in Android malware detectors. Although there are already some similar works in the image classification field, most of those similar ideas cannot be borrowed to solve this problem, because the assumption that the attacker has full control of the training data collection or labelling process is not realistic in real-world malware detection scenarios. In this article, we are motivated to study the backdoor attack against Android malware detectors. The backdoor is created and injected into the model stealthily without access to the training data and activated when an app with the trigger is presented. We demonstrate the proposed attack on four typical malware detectors that have been widely discussed in academia. Our evaluation shows that the proposed backdoor attack achieves up to 99 percent evasion rate over 750 malware samples. Moreover, the above successful attack is realised by a small size of triggers (only four features) and a very low data poisoning rate (0.3 percent).","1941-0018","","10.1109/TDSC.2021.3094824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477038","Malware detection;backdoor attack;machine learning;computer security;data poisoning","Malware;Detectors;Training;Feature extraction;Labeling;Computational modeling;Training data","","17","","46","IEEE","7 Jul 2021","","","IEEE","IEEE Journals"
"Defending Against Membership Inference Attacks With High Utility by GAN","L. Hu; J. Li; G. Lin; S. Peng; Z. Zhang; Y. Zhang; C. Dong","Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangdong, China; School of Computing, Newcastle University, Newcastle upon Tyne, U.K.","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2144","2157","The success of machine learning (ML) depends on the availability of large-scale datasets. However, recent studies have shown that models trained on such datasets are vulnerable to privacy attacks, among which membership inference attack (MIA) brings serious privacy risk. MIA allows an adversary to infer whether a sample belongs to the training dataset of the target model or not. Though a variety of defenses against MIA have been proposed such as differential privacy and adversarial regularization, they also result in lower model accuracy and thus make the models less unusable. In this article, aiming at maintaining the accuracy while protecting the privacy against MIA, we propose a new defense against membership inference attacks by generative adversarial network (GAN). Specifically, sensitive data is used to train a GAN, then the GAN generate the data for training the actual model. To ensure that the model trained with GAN on small datasets can has high utility, two different GAN structures with special training techniques are utilized to deal with the image data and table data, respectively. Experiment results show that the defense is more effective on different data sets against the existing attack schemes, and is more efficient compared with most advanced MIA defenses.","1941-0018","","10.1109/TDSC.2022.3174569","National Natural Science Foundation of China(grant numbers:U1936218,61802383,62102107,62072132,62002074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773984","Membership inference attack;generative adversarial network;machine learning;privacy","Data models;Training;Generative adversarial networks;Privacy;Machine learning;Computational modeling;Training data","","10","","45","IEEE","12 May 2022","","","IEEE","IEEE Journals"
"Toward Verifiable and Privacy Preserving Machine Learning Prediction","C. Niu; F. Wu; S. Tang; S. Ma; G. Chen","Department of Computer Science and Engineering, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Naveen Jindal School of Management, University of Texas at Dallas, Richardson, TX, USA; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China; Department of Computer Science and Engineering, Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1703","1721","The ubiquitous needs for extracting insights from data are driving the emergence of service providers to offer predictions given the inputs from customers. During this process, it is important and highly nontrivial for the service providers to generate proofs of honest predictions without leaking the key parameters of their trained models. In addition, the customers are usually unwilling to reveal their sensitive inputs. In this article, we proposed MVP, which enables Machine learning prediction in a Verifiable and Privacy preserving fashion. MVP features the properties of polynomial decomposition and prime-order bilinear groups to simultaneously facilitate oblivious evaluation and batch outcome verification while maintaining function privacy and input privacy. We further instantiated MVP with Support Vector Machines (SVMs) and extensively evaluated its performance for the spam detection task on three practical Short Message Service (SMS) datasets. Our analysis and evaluation results reveal that MVP achieves the desired properties while incurring low computation and communication overhead.","1941-0018","","10.1109/TDSC.2020.3035591","Science and Technology Innovation 2030(grant numbers:2018AAA0100905); China NSF(grant numbers:62025204,61972252,61972254,61672348,61672353); Joint Scientific Research Foundation of the State Education Ministry(grant numbers:6141A02033702); Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing(grant numbers:2018A09); Alibaba Group through Alibaba Innovation Research Program; Tencent Rhino Bird Key Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247447","Machine learning prediction;verifiability;function privacy;input privacy","Privacy;Support vector machines;Training;Kernel;Computational modeling;Encryption","","19","","75","IEEE","3 Nov 2020","","","IEEE","IEEE Journals"
"PRADA: Protecting Against DNN Model Stealing Attacks","M. Juuti; S. Szyller; S. Marchal; N. Asokan",Aalto University; Aalto University; Aalto University; Aalto University,"2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","512","527","Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806737","Adversarial machine learning;model extraction;model stealing;deep neural network","Predictive models;Computational modeling;Training;Mathematical model;Data mining;Business;Neural networks","","180","1","56","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Sponge Examples: Energy-Latency Attacks on Neural Networks","I. Shumailov; Y. Zhao; D. Bates; N. Papernot; R. Mullins; R. Anderson","University of Cambridge, UK; University of Cambridge, UK; University of Cambridge, UK; University of Toronto and Vector Institute, Canada; University of Cambridge, UK; University of Cambridge, UK","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","212","231","The high energy costs of neural network training and inference led to the use of acceleration hardware such as GPUs and TPUs. While such devices enable us to train large-scale neural networks in datacenters and deploy them on edge devices, their designers' focus so far is on average-case performance. In this work, we introduce a novel threat vector against neural networks whose energy consumption or decision latency are critical. We show how adversaries can exploit carefully-crafted sponge examples, which are inputs designed to maximise energy consumption and latency, to drive machine learning (ML) systems towards their worst-case performance. Sponge examples are, to our knowledge, the first denial-of-service attack against the ML components of such systems. We mount two variants of our sponge attack on a wide range of state-of-the-art neural network models, and find that language models are surprisingly vulnerable. Sponge examples frequently increase both latency and energy consumption of these models by a factor of 30×. Extensive experiments show that our new attack is effective across different hardware platforms (CPU, GPU and an ASIC simulator) on a wide range of different language tasks. On vision tasks, we show that sponge examples can be produced and a latency degradation observed, but the effect is less pronounced. To demonstrate the effectiveness of sponge examples in the real world, we mount an attack against Microsoft Azure's translator and show an increase of response time from 1ms to 6s (6000×). We conclude by proposing a defense strategy: shifting the analysis of energy consumption in hardware from an average-case to a worst-case perspective.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00024","CIFAR; EPSRC; NSERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581273","availability attacks;adversarial machine learning;adversarial examples;sponge examples;latency attacks;denial of service","Deep learning;Training;Performance evaluation;Knowledge engineering;Energy consumption;Neural networks;Graphics processing units","","22","","85","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"A Semi-supervised Learning Methodology for Malware Categorization using Weighted Word Embeddings","H. L. Duarte-Garcia; C. D. Morales-Medina; A. Hernandez-Suarez; G. Sanchez-Perez; K. Toscano-Medina; H. Perez-Meana; V. Sanchez","Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Instituto Politecnico Nacional, ESIME Culhuacan, Mexico City, Mexico; Department of Computer Science, University of Warwick, Coventry CV4 7AL, UK","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","238","246","Due to the vertiginous growth of malicious actors, malware has been crafted, distributed and propagated around the world with new and sophisticated techniques. Classical malware detection procedures, mostly based on signatures and heuristic searches, are now being replaced with machine learning-based (ML) solutions. However, some challenges are still present. Firstly, supervised approaches use anti-virus tags to create hand-crafted datasets, resulting in a lack of taxonomy and uncertainty if a given observation is classified with a proper label. Secondly, off-line and feed-forward approaches may result in complex and time consuming feature extraction tasks. In this work, we propose a novel method that reinforces malware characterization by capturing rich relevance and contextual patterns into an n-dimensional weighted word embedding vector (WEV) space. Results prove that by clustering similar WEVs via unsupervised learning, malware can be categorized into four major families, improving detection with less resources.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802412","malware;windows-api;machine-learning;word2vec;clustering","","","8","","34","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"WhatsThat? On the Usage of Hierarchical Clustering for Unsupervised Detection & Interpretation of Network Attacks","P. Mulinka; K. Fukuda; P. Casas; L. Kencl",CTU Czech Technical University; NII National Institute of Informatics; AIT Austrian Institute of Technology; CTU Czech Technical University,"2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","574","583","The automatic detection and interpretation of network attacks through machine learning is a well-known problem, for which no general solution is available. Super-vised learning and anomaly detection approaches require prior knowledge on the system under analysis, either in the form of normal operation profiles, or on the specific attacks to detect. As a consequence, both approaches have clear limitations when it comes to detecting, and in particular interpreting, previously unseen attacks and anomalies. In this paper we present WhatsThat, a novel approach to unsupervised network anomaly detection, which can both detect and interpret anomalous behaviors in a completely black-box manner, without relying on any ground-truth on the system under analysis. WhatsThat relies on hierarchical-clustering techniques to discover and characterize anomalous patterns present in nested or hierarchically structured multi-dimensional data, which is common in network traffic - e.g., due to multi-layer protocols. The solution uses unsupervised cluster validity metrics to automatically explore the data structure, and builds on automatic identification of relevant features to provide meaningful descriptions for the detected patterns. We showcase WhatsThat in the detection and interpretation of network attacks hidden in real, large-scale network traffic collected at a transit Internet backbone network. While WhatsThat is mainly tailored for unsupervised anomaly detection and interpretation, it can also be applied to the unsupervised analysis of any kind of nested or hierarchically structured multi-dimensional data, showing the potential of hierarchical clustering for general unsupervised data analysis.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229656","Unsupervised machine learning;anomaly detection and diagnosis;hierarchical-clustering;density-based clustering;network measurements","Clustering algorithms;Feature extraction;Data collection;Data analysis;Anomaly detection;Security;Monitoring","","2","","38","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Membership Inference Against DNA Methylation Databases","I. Hagestedt; M. Humbert; P. Berrang; I. Lehmann; R. Eils; M. Backes; Y. Zhang","CISPA Helmholtz Center for Information Security; Cyber-Defence Campus, Armasuisse S+t; CISPA Helmholtz Center for Information Security; Helmholtz Centre for Environmental Research Leipzig, UFZ; Berlin Institue of Health; CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security","2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","509","520","Biomedical data sharing is one of the key elements fostering the advancement of biomedical research but poses severe risks towards the privacy of individuals contributing their data, as already demonstrated for genomic data. In this paper, we study whether and to which extent DNA methylation data, one of the most important epigenetic elements regulating human health, is prone to membership inference attacks, a critical type of attack that reveals an individual's participation in a given database. We design and evaluate three different attacks exploiting published summary statistics, among which one is based on machine learning and another is exploiting the dependencies between genome and methylation data. Our extensive evaluation on six datasets containing a diverse set of tissues and diseases collected from more than 1,300 individuals in total shows that such membership inference attacks are effective, even when the target's methylation profile is not accessible. It further shows that the machine-learning approach outperforms the statistical attacks, and that learned models are transferable across different datasets.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00039","German Federal Ministry of Education and Research (BMBF)(grant numbers:FKZ: 16KIS0656); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230417","epigenetics;membership inference","Data privacy;Databases;Genomics;DNA;Machine learning;Epigenetics;Bioinformatics","","3","","60","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Robust Transparency Against Model Inversion Attacks","Y. Alufaisan; M. Kantarcioglu; Y. Zhou","EXPEC Computer Operations Department, Saudi Aramco, Dhahran, Saudi Arabia; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2061","2073","Transparency has become a critical need in machine learning (ML) applications. Designing transparent ML models helps increase trust, ensure accountability, and scrutinize fairness. Some organizations may opt-out of transparency to protect individuals’ privacy. Therefore, there is a great demand for transparency models that consider both privacy and security risks. Such transparency models can motivate organizations to improve their credibility by making the ML-based decision-making process comprehensible to end-users. Differential privacy (DP) provides an important technique to disclose information while protecting individual privacy. However, it has been shown that DP alone cannot prevent certain types of privacy attacks against disclosed ML models. DP with low $\epsilon$ε values can provide high privacy guarantees, but may result in significantly weaker ML models in terms of accuracy. On the other hand, setting $\epsilon$ε value too high may lead to successful privacy attacks. This raises the question whether we can disclose accurate transparent ML models while preserving privacy. In this article we introduce a novel technique that complements DP to ensure model transparency and accuracy while being robust against model inversion attacks. We show that combining the proposed technique with DP provide highly transparent and accurate ML models while preserving privacy against model inversion attacks.","1941-0018","","10.1109/TDSC.2020.3019508","National Institutes of Health(grant numbers:1R01HG006844); National Science Foundation(grant numbers:CICI- 1547324,IIS-1633331,CNS-1837627,OAC-1828467,IIS-1939728); ARO(grant numbers:W911NF-17-1-0356); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178452","Transparency;privacy-preserving;model-inversion attack","Privacy;Machine learning;Predictive models;Robustness;Computational modeling;Organizations","","7","","28","IEEE","26 Aug 2020","","","IEEE","IEEE Journals"
"Fault Injection Analytics: A Novel Approach to Discover Failure Modes in Cloud-Computing Systems","D. Cotroneo; L. De Simone; P. Liguori; R. Natella","Computer and System Engineering, University of Naples federico II, Naples, Italy; Information Technology and Electrical Engineering, University of Naples Federico II, Naples, Italy; University of Naples Federico II, Napoli, Italy; University of Naples Federico II, Napoli, Italy","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1476","1491","Cloud computing systems fail in complex and unexpected ways due to unexpected combinations of events and interactions between hardware and software components. Fault injection is an effective means to bring out these failures in a controlled environment. However, fault injection experiments produce massive amounts of data, and manually analyzing these data is inefficient and error-prone, as the analyst can miss severe failure modes that are yet unknown. This article introduces a new paradigm (fault injection analytics) that applies unsupervised machine learning on execution traces of the injected system, to ease the discovery and interpretation of failure modes. We evaluated the proposed approach in the context of fault injection experiments on the OpenStack cloud computing platform, where we show that the approach can accurately identify failure modes with a low computational cost.","1941-0018","","10.1109/TDSC.2020.3025289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201357","Fault injection;failure mode analysis;cloud computing;openstack;unsupervised machine learning","Anomaly detection;Cloud computing;Machine learning;Instruments;Training;Hardware;Software","","11","","86","IEEE","21 Sep 2020","","","IEEE","IEEE Journals"
"Byzantine-Resilient Multi-Agent System","R. Guerraoui; A. Maurer","Computer Science, Swiss Federal Institute of Technology Lausanne, Lausanne, Switzerland; SCCS, UM6P, Ben Guerir, Morocco","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4032","4038","We consider the problem of making a multi-agent system (MAS) resilient to Byzantine failures through replication. We consider a very general model of MAS, where randomness can be involved in the behavior of each agent. We propose the first universal scheme to make such a MAS Byzantine-resilient.","1941-0018","","10.1109/TDSC.2021.3116488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552535","Distributed systems;fault tolerance;reliability and robustness","Multi-agent systems;Machine learning;Clustering algorithms;Clocks;Probabilistic logic;Machine learning algorithms;Cryptography","","3","","34","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"Vulnerability Detection on Mobile Applications Using State Machine Inference","W. van der Lee; S. Verwer","Department of Intelligent Systems, Delft University of Technology, The Netherlands; Department of Intelligent Systems, Delft University of Technology, The Netherlands","2018 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","9 Jul 2018","2018","","","1","10","Although the importance of mobile applications grows every day, recent vulnerability reports argue the application's deficiency to meet modern security standards. Testing strategies alleviate the problem by identifying security violations in software implementations. This paper proposes a novel testing methodology that applies state machine learning of mobile Android applications in combination with algorithms that discover attack paths in the learned state machine. The presence of an attack path evidences the existence of a vulnerability in the mobile application. We apply our methods to real-life apps and show that the novel methodology is capable of identifying vulnerabilities.","","978-1-5386-5445-3","10.1109/EuroSPW.2018.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406555","state machine learning;mobile application security;vulnerability detection;model inference","Testing;Mobile applications;Androids;Humanoid robots;Security;Inference algorithms;Machine learning","","","","21","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Exploiting the Auto-Encoder Residual Error for Intrusion Detection","G. Andresini; A. Appice; N. Di Mauro; C. Loglisci; D. Malerba","Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy; Department of Computer Science, University of Bari Aldo Moro, Bari, Italy","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","281","290","Intrusion Detection Systems aim to address the problem of correctly identifying unforeseen network attacks. The attack detection problem has been already tackled through supervised and unsupervised machine learning approaches. While the former methods lead to models very accurate on already seen samples, the latter provide models robust on unforeseen samples by trading-off a high accuracy on seen ones. In this paper, we combine deep unsupervised neural networks with supervised neural networks aiming at improving the classification accuracy on unforeseen attacks. Auto-encoders neural networks are used both for feature engineering and as anomaly detectors. Experimental results on a challenging dataset prove the validity of the proposed approach when compared to other state-of-the-art methods.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802420","Intrusion-detection,-Autoencoding,-Supervised-and-unsupervised-deep-learning,-Classification,-Anomaly-detection","","","20","","34","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"I Spy with My Little Eye: Analysis and Detection of Spying Browser Extensions","A. Aggarwal; B. Viswanath; L. Zhang; S. Kumar; A. Shah; P. Kumaraguru","IIIT - Delhi, India; UC Santa Barbara; Northeastern University; CEG, Guindy, India; IIIT - Delhi, India; IIIT - Delhi, India","2018 IEEE European Symposium on Security and Privacy (EuroS&P)","9 Jul 2018","2018","","","47","61","In this work, we take a step towards understanding and defending against spying browser extensions. These are extensions repurposed to capture online activities of a user and communicate the collected sensitive information to a third-party domain. We conduct an empirical study of such extensions on the Chrome Web Store. First, we present an in-depth analysis of the spying behavior of these extensions. We observe that these extensions steal a variety of sensitive user information, such as the complete browsing history (e.g., the sequence of web traversals), online social network (OSN) access tokens, IP address, and geolocation. Second, we investigate the potential for automatically detecting spying extensions by applying machine learning schemes. We show that using a Recurrent Neural Network (RNN), the sequence of browser API calls made by an extension can be a robust feature, outperforming hand-crafted features (used in prior work on malicious extensions) to detect spying extensions. Our RNN based detection scheme achieves a high precision (90.02%) and recall (93.31%) in detecting spying extensions.","","978-1-5386-4228-3","10.1109/EuroSP.2018.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406590","spying browser extensions;information tracking","Browsers;History;Feature extraction;Manuals;Privacy;Electronic mail;Recurrent neural networks","","11","","41","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Generation of Static YARA-Signatures Using Genetic Algorithm","A. Zhdanov","INRIA, Rennes, France","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","220","228","This article is dedicated to the subject of malware detection using static YARA-signatures and Genetic Algorithm (GA). It proposes a solution and does a comparative analysis of two algorithms. The first uses n-gram distributions and an algorithm of machine learning known as Maximization-Maximization algorithm based on Multinomial Naive Bayes analysis. The second algorithm offers a solution to the problem via a method of directional generation of YARA-rules based on the GA which relates to the Artificial Intelligence (AI) methods. On the grounds of literature review, it can be seen that such application of the GA is novel in the static signatures generation domain and, in particular, for the YARA-rules generation and is considered to be the main contribution of the article. Also, advantages of the method applying the GA are shown on the basis of experiments conducted for cleanware and malware datasets.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802463","malware-detection;static-signatures;n-gram-analysis;YARA-rules;Genetic-Algorithm","","","5","","34","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Revealing Similarities in Android Malware by Dissecting their Methods","M. Pasetto; N. Marastoni; M. D. Preda","University of Verona, Verona, Italy; University of Verona, Verona, Italy; University of Verona, Verona, Italy","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","625","634","One of the most challenging problems in the fight against Android malware is finding a way to classify them according to their behavior, in order to be able to utilize previously gathered knowledge in analysis and prevention.In this paper we introduce a novel technique that discovers similarities between Android malware samples by comparing fragments of executed traces (strands) generated from their most suspect methods. This way we can accurately pinpoint which (possibly) malicious behaviors are shared between these different samples, allowing for easier analysis and classification.We implement this approach in a tool, StrAndroid, that we evaluate on a few dataset of malware and ransomware samples, comparing its results to an existing similarity tool.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229730","Program Analysis;Similarity;Android Malware","Malware;Tools;Static analysis;Ransomware;Reverse engineering;Operating systems;Machine learning","","1","","35","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Lightweight and Identifier-Oblivious Engine for Cryptocurrency Networking Anomaly Detection","W. Fan; H. -J. Hong; J. Kim; S. Wuthier; M. Nakashima; X. Zhou; C. -H. Chow; S. -Y. Chang","Department of Communications and Networking, Xi'an Jiaotong-Liverpool University, Suzhou, Jiangsu, China; Computer Science, University of Colorado Colorado Springs, Colorado Springs, Colorado, CS, USA; Computer Science, Texas A&M University-Commerce, Commerce, TX, USA; Computer Science, University of Colorado Colorado Springs, Colorado Springs, Colorado, CS, USA; Quantitative Health Sciences, Cleveland Clinic Foundation, Cleveland, OH, USA; Computer Science, University of Colorado Colorado Springs, Colorado Springs, Colorado, CS, USA; Computer Science, University of Colorado Colorado Springs, Colorado Springs, Colorado, CS, USA; Computer Science, University of Colorado Colorado Springs, Colorado Springs, Colorado, CS, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1302","1318","The distributed cryptocurrency networking is critical because the information delivered through it drives the mining consensus protocol and the rest of the operations. However, the cryptocurrency peer-to-peer (P2P) network remains vulnerable, and the existing security approaches are either ineffective or inefficient because of the permissionless requirement and the broadcasting overhead. We design and build a Lightweight and Identifier-Oblivious eNgine (LION) for the anomaly detection of the cryptocurrency networking. LION is not only effective in permissionless networking but is also lightweight and practical for the computation-intensive miners. We build LION for anomaly detection and use traffic analyses so that it minimally affects the mining rate and is substantially superior in its computational efficiency than the previous approaches based on machine learning. We implement a LION prototype on an active Bitcoin node to show that LION yields less than 1% of mining rate reduction subject to our prototype, in contrast to the state-of-the-art machine-learning approaches costing 12% or more depending on the algorithms subject to our prototype as well, while having detection accuracy of greater than 97% F1-score against the attack prototypes and real-world anomalies. LION therefore can be deployed on the existing miners without the need to introduce new entities in the cryptocurrency ecosystem.","1941-0018","","10.1109/TDSC.2022.3152937","Colorado State Bill 18-086; National Science Foundation(grant numbers:1922410); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720221","Blockchain;cryptocurrency;bitcoin;P2P network;anomaly detection;statistical analysis;traffic analysis","Bitcoin;Prototypes;Peer-to-peer computing;Anomaly detection;Consensus protocol;Engines;Machine learning","","6","","45","IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Subsequent Embedding in Targeted Image Steganalysis: Theoretical Framework and Practical Applications","D. Megías; D. Lerch-Hostalot","Internet Interdisciplinary Institute (IN3), CYBERCAT-Center for Cybersecurity Research of Catalonia, Universitat Oberta de Catalunya (UOC), Barcelona, Catalonia, Spain; Internet Interdisciplinary Institute (IN3), CYBERCAT-Center for Cybersecurity Research of Catalonia, Universitat Oberta de Catalunya (UOC), Barcelona, Catalonia, Spain","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1403","1421","Steganalysis is a collection of techniques used to detect whether secret information is embedded in a carrier using steganography. Most of the existing steganalytic methods are based on machine learning, which requires training a classifier with “laboratory” data. However, applying machine-learning classification to a new data source is challenging, since there is typically a mismatch between the training and the testing sets. In addition, other sources of uncertainty affect the steganlytic process, including the mismatch between the targeted and the actual steganographic algorithms, unknown parameters –such as the message length– and having a mixture of several algorithms and parameters, which would constitute a realistic scenario. This article presents subsequent embedding as a valuable strategy that can be incorporated into modern steganalysis. Although this solution has been applied in previous works, a theoretical basis for this strategy was missing. Here, we cover this research gap by introducing the “directionality” property of features concerning data embedding. Once a consistent theoretical framework sustains this strategy, new practical applications are also described and tested against standard steganography, moving steganalysis closer to real-world conditions.","1941-0018","","10.1109/TDSC.2022.3154967","NVIDIA Corporation(grant numbers:RTI2018-095094-B-C22); EIG CONCERT-Japan(grant numbers:PCI2020-120689-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722958","Steganography;steganalysis;machine learning;cover source mismatch;stego source mismatch;uncertainty","Training;Testing;Uncertainty;Steganography;Machine learning;Databases;Bit rate","","3","","43","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"Improving the Accuracy of IR-Level Fault Injection","L. Palazzi; G. Li; B. Fang; K. Pattabiraman","Electrical and Computer Engineering, The University of British Columbia, Vancouver, British Columbia, Canada; Electrical and Computer Engineering, The University of British Columbia, Vancouver, British Columbia, Canada; Electrical and Computer Engineering, The University of British Columbia, Vancouver, British Columbia, Canada; Electrical and Computer Engineering, The University of British Columbia, Vancouver, British Columbia, Canada","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","243","258","Fault injection (FI) is a commonly used experimental technique to evaluate the resilience of software techniques for tolerating hardware faults. Software-implemented FI can be performed at different levels of abstraction in the system stack; FI performed at the compiler’s intermediate representation (IR) level has the advantage that it is closer to the program being evaluated and is hence easier to derive insights from for the design of software fault-tolerance mechanisms. Unfortunately, it is not clear how accurate IR-level FI is vis-a-vis FI performed at the assembly code level, and prior work has presented contradictory findings. In this article, we perform a comprehensive evaluation of the accuracy of IR-level FI across a range of benchmark programs and compiler optimization levels. Our results show that IR-level FI is as accurate as assembly-level FI for silent data corruption (SDC) probability estimation across different benchmarks and optimization levels. Further, we present a machine-learning-based technique for improving the accuracy of <italic>crash</italic> probability measurements made by IR-level FI, which takes advantage of an observed correlation between program crash probabilities and instructions that operate on memory address values. We find that the machine learning technique provides comparable accuracy for IR-level FI as assembly code level FI for program crashes.","1941-0018","","10.1109/TDSC.2020.2980273","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035448","Resilience;fault injection;IR-level fault injection;intermediate representation;machine learning;LLVM;PIN","Computer crashes;Hardware;Optimization;Program processors;Registers;Tools","","3","","59","IEEE","13 Mar 2020","","","IEEE","IEEE Journals"
"A Support Vector Machine-Based Framework for Detection of Covert Timing Channels","P. L. Shrestha; M. Hempel; F. Rezaei; H. Sharif","Department of Computer and Electronics Engineering, University of Nebraska-Lincoln 1110 S. 67th St., Omaha, NE; Department of Computer and Electronics Engineering, University of Nebraska-Lincoln 1110 S. 67th St., Omaha, NE; Department of Computer and Electronics Engineering, University of Nebraska-Lincoln 1110 S. 67th St., Omaha, NE; Department of Computer and Electronics Engineering, University of Nebraska-Lincoln 1110 S. 67th St., Omaha, NE","IEEE Transactions on Dependable and Secure Computing","10 Mar 2016","2016","13","2","274","283","Covert channels exploit side channels within existing network resources to transmit secret messages. They are integrated into the elements of network resources that were not even designed for the purpose of communication. This means that traditional security features like firewalls cannot detect them. Their ability to evade detection makes covert channels a grave security concern. Hence, it is imperative to detect and disrupt them. However, a generic mechanism that can be used to detect a large variety of covert channels is missing. In this paper, we propose a support vector machine (SVM)-based framework for reliable detection of covert communications. The machine learning framework utilizes the fingerprints derived from the traffic under investigation to classify the traffic as covert or overt. We trained our classifier using the fingerprints from four popular and diverse covert timing channel algorithms and tested each of them independently. We have shown that the machine learning framework has great potential to blindly detect covert channels, even when the covert message size is reduced.","1941-0018","","10.1109/TDSC.2015.2423680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087364","Covert Channels;Detection;Machine Learning;Traffic Fingerprints;Covert channels;detection;machine learning;traffic fingerprints","Fingerprint recognition;Entropy;Support vector machines;Delays;Receivers;Classification algorithms","","47","","23","IEEE","16 Apr 2015","","","IEEE","IEEE Journals"
"Learning from the Ones that Got Away: Detecting New Forms of Phishing Attacks","C. N. Gutierrez; T. Kim; R. D. Corte; J. Avery; D. Goldwasser; M. Cinque; S. Bagchi","Purdue University, West Lafayette, IN, USA; Purdue University, West Lafayette, IN, USA; Universitá degli Studi di Napoli Federico II, Napoli, NA, Italy; Northrop Grumman Corporation, Falls Church, VA, USA; Purdue University, West Lafayette, IN, USA; Universitá degli Studi di Napoli Federico II, Napoli, NA, Italy; Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","9 Nov 2018","2018","15","6","988","1001","Phishing attacks continue to pose a major threat for computer system defenders, often forming the first step in a multi-stage attack. There have been great strides made in phishing detection; however, some phishing emails appear to pass through filters by making simple structural and semantic changes to the messages. We tackle this problem through the use of a machine learning classifier operating on a large corpus of phishing and legitimate emails. We design SAFe-PC (Semi-Automated Feature generation for Phish Classification), a system to extract features, elevating some to higher level features, that are meant to defeat common phishing email detection strategies. To evaluate SAFe-PC , we collect a large corpus of phishing emails from the central IT organization at a tier-1 university. The execution of SAFe-PC on the dataset exposes hitherto unknown insights on phishing campaigns directed at university users. SAFe-PC detects more than 70 percent of the emails that had eluded our production deployment of Sophos, a state-of-the-art email filtering tool. It also outperforms SpamAssassin, a commonly used email filtering tool. We also developed an online version of SAFe-PC, that can be incrementally retrained with new samples. Its detection performance improves with time as new samples are collected, while the time to retrain the classifier stays constant.","1941-0018","","10.1109/TDSC.2018.2864993","US National Science Foundation(grant numbers:CNS-1548114); Northrop Grumman through their Cybersecurity Research Consortium; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440723","Phishing attacks;machine learning security;online learning;university-based phishing campaigns","Phishing;Electronic mail;Feature extraction;Training;Online services;Blacklisting;Machine learning","","51","","28","IEEE","19 Aug 2018","","","IEEE","IEEE Journals"
"Key-Recovery Attacks on KIDS, a Keyed Anomaly Detection System","J. E. Tapiador; A. Orfila; A. Ribagorda; B. Ramos","Department of Computer Science, Universidad Carlos III de Madrid, Leganes, Madrid, Spain; Department of Computer Science, Universidad Carlos III de Madrid, Leganes, Madrid, Spain; Department of Computer Science, Universidad Carlos III de Madrid, Leganes, Madrid, Spain; Department of Computer Science, Universidad Carlos III de Madrid, Leganes, Madrid, Spain","IEEE Transactions on Dependable and Secure Computing","12 May 2015","2015","12","3","312","325","Most anomaly detection systems rely on machine learning algorithms to derive a model of normality that is later used to detect suspicious events. Some works conducted over the last years have pointed out that such algorithms are generally susceptible to deception, notably in the form of attacks carefully constructed to evade detection. Various learning schemes have been proposed to overcome this weakness. One such system is Keyed IDS (KIDS), introduced at DIMVA “10. KIDS” core idea is akin to the functioning of some cryptographic primitives, namely to introduce a secret element (the key) into the scheme so that some operations are infeasible without knowing it. In KIDS the learned model and the computation of the anomaly score are both key-dependent, a fact which presumably prevents an attacker from creating evasion attacks. In this work we show that recovering the key is extremely simple provided that the attacker can interact with KIDS and get feedback about probing requests. We present realistic attacks for two different adversarial settings and show that recovering the key requires only a small amount of queries, which indicates that KIDS does not meet the claimed security properties. We finally revisit KIDS' central idea and provide heuristic arguments about its suitability and limitations.","1941-0018","","10.1109/TDSC.2013.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598669","Adversarial classification;anomaly detection;intrusion detection systems;secure machine learning","Payloads;Training;Computational modeling;Intrusion detection;Machine learning algorithms;Feature extraction","","22","","23","IEEE","13 Sep 2013","","","IEEE","IEEE Journals"
"Fault Injection for TensorFlow Applications","N. Narayanan; Z. Chen; B. Fang; G. Li; K. Pattabiraman; N. DeBardeleben","University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; Pacific Northwest National Laboratory, Richland, WA, USA; University of Iowa, Iowa, IA, USA; University of British Columbia, Vancouver, BC, Canada; Los Alamos National Laboratory, New Mexico, USA","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","2677","2695","As machine learning (ML) has seen increasing adoption in safety-critical domains (e.g., autonomous vehicles), the reliability of ML systems has also grown in importance. While prior studies have proposed techniques to enable efficient error-resilience (e.g., selective instruction duplication), a fundamental requirement for realizing these techniques is a detailed understanding of the application's resilience. In this work, we present TensorFI 1 and TensorFI 2, high-level fault injection (FI) frameworks for TensorFlow-based applications. TensorFI 1 and 2 are able to inject both hardware and software faults in any general TensorFlow 1 and 2 program respectively. Both are configurable FI tools that are flexible, easy to use, and portable. They can be integrated into existing TensorFlow programs to assess their resilience for different fault types (e.g., bit-flips in particular operations or layers). We use TensorFI 1 and TensorFI 2 to evaluate the resilience of 11 and 10 ML programs respectively, all written in TensorFlow, including DNNs used in the autonomous vehicle domain. The results give us insights into why some of the models are more resilient. We also measure the performance overheads of the two injectors, and present 4 case studies, two for each tool, to demonstrate their utility.","1941-0018","","10.1109/TDSC.2022.3175930","Natural Sciences and Engineering Research Council of Canada; Huawei Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831186","Deep neural networks;error resilience;fault injection;machine learning;TensorFlow","Resilience;Software;Reliability;Hardware;Data models;Computational modeling;Safety","","","","45","IEEE","18 Jul 2022","","","IEEE","IEEE Journals"
"Learn to Forget: Machine Unlearning via Neuron Masking","Z. Ma; Y. Liu; X. Liu; J. Liu; J. Ma; K. Ren","School of Cyber Engineering, Xidian University, Xian, China; School of Cyber Engineering, Xidian University, Xian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Institute of Cyberspace Research, Zhejiang University, Zhejiang, China; School of Cyber Engineering, Xidian University, Xian, China; Institute of Cyberspace Research, Zhejiang University, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","3194","3207","Nowadays, machine learning models, especially neural networks, have became prevalent in many real-world applications. These models are trained based on a one-way trip from user data: as long as users contribute their data, there is no way to withdraw. To this end, machine unlearning becomes a popular research topic, which allows the model trainer to unlearn unexpected data from a trained machine learning model. In this article, we propose the first uniform metric called forgetting rate to measure the effectiveness of a machine unlearning method. It is based on the concept of membership inference and describes the transformation rate of the eliminated data from “memorized” to “unknown” after conducting unlearning. We also propose a novel unlearning method called Forsaken. It is superior to previous work in either utility or efficiency (when achieving the same forgetting rate). We benchmark Forsaken with eight standard datasets to evaluate its performance. The experimental results show that it can achieve more than 90% forgetting rate on average and only causeless than 5% accuracy loss.","1941-0018","","10.1109/TDSC.2022.3194884","National Natural Science Foundation of China(grant numbers:U21A20464,61872283,62072109,61902291); Natural Science Basic Research Program of Shaanxi(grant numbers:2021JC-22,2022JZ-33); Key Research Development Program of Shaanxi(grant numbers:2022GY-029 (CNKLSTISS)); Higher Education Discipline Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844865","Machine unlearning;neuron masking;neural network","Training;Data models;Machine learning;Computational modeling;Adaptation models;Task analysis;Measurement","","4","","48","IEEE","29 Jul 2022","","","IEEE","IEEE Journals"
"Automatic Evasion of Machine Learning-Based Network Intrusion Detection Systems","H. Yan; X. Li; W. Zhang; R. Wang; H. Li; X. Zhao; F. Li; X. Lin","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; School of Computer Science, University of Guelph, Guelph, ON, Canada; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academic of Sciences, Beijing, China; School of Computer Science, University of Guelph, Guelph, ON, Canada","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","153","167","Network intrusion detection systems (IDS) are often considered effective to thwart cyber attacks. Currently, state-of-the-art (SOTA) IDSs are mainly based on machine learning (ML) including deep learning (DL) models, which suffer from their own security issues, especially evasion attacks by using adversarial examples. However, previous studies mostly focus on extracted features rather than the traffic sample itself, and/or assume that the adversary knows the information of the target model more or less, which severely restricts attack feasibility in practice. In this paper, we re-investigate this problem in a more realistic label-only black-box scenario and propose a practical evasion attack strategy to solve the above limitations. In this newly considered case that the adversary morphs the traffic sample and only obtains the results accepted or rejected without other knowledge, we successfully leverage the model extraction and transfer attack to evade the detection. The entire attack strategy is automated and a comprehensive evaluation is performed. Final results show that the proposed strategy effectively evades seven typical ML-based IDSs and one SOTA DL-based IDS with an average success rate of over $75\%$75%. We also discuss the corresponding countermeasures against our attack, which finally highlight the need for effective defenses against our attack.","1941-0018","","10.1109/TDSC.2023.3247585","National Key Research and Development Program of China(grant numbers:2022YFB3103400); Shaanxi innovation team(grant numbers:2018TD-007); Higher Education Discipline Innovation Project(grant numbers:B16037); University of Guelph; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049650","Adversarial traffic example;black-box evasion attack;model extraction attack;network intrusion detection system;transfer attack","Closed box;Feature extraction;Glass box;Detectors;Protocols;Training;Optimization","","3","","50","IEEE","22 Feb 2023","","","IEEE","IEEE Journals"
"Compiler-Agnostic Function Detection in Binaries","D. Andriesse; A. Slowinska; H. Bos","Computer Science Institute, Vrije Universiteit Amsterdam; Amsterdam Department of Informatics; Computer Science Institute, Vrije Universiteit Amsterdam","2017 IEEE European Symposium on Security and Privacy (EuroS&P)","3 Jul 2017","2017","","","177","189","We propose Nucleus, a novel function detection algorithm for binaries. In contrast to prior work, Nucleus is compiler-agnostic, and does not require any learning phase or signature information. Instead of scanning for signatures, Nucleus detects functions at the Control Flow Graph-level, making it inherently suitable for difficult cases such as non-contiguous or multi-entry functions. We evaluate Nucleus on a diverse set of 476 C and C ++ binaries, compiled with gcc, clang and Visual Studio for x86 and x64, at optimization levels O0-O3. We achieve consistently good performance, with a mean F-score of 0.95.","","978-1-5090-5762-7","10.1109/EuroSP.2017.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961979","Disassembly;static analysis;function detection;reverse engineering","Optimization;Flow graphs;Security;Databases;Visualization;Detectors;Maintenance engineering","","49","2","32","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"ATVSA: Vehicle Driver Profiling for Situational Awareness","R. Khan; N. Saxena; O. Rana; P. Gope","School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; School of Computer Science and Informatics, Cardiff University, Cardiff, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield, United Kingdom","2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","27 Jun 2022","2022","","","348","357","Increasing connectivity and automation in vehicles leads to a greater potential attack surface. Such vulnerabilities within vehicles can also be used for auto-theft, increasing the potential for attackers to disable anti-theft mechanisms implemented by vehicle manufacturers. We utilize patterns derived from Controller Area Network (CAN) bus traffic to verify driver “behavior”, as a basis to prevent vehicle theft. Our proposed model uses semi-supervised learning that continuously profiles a driver, using features extracted from CAN bus traffic. We have selected 15 key features and obtained an accuracy of 99% using a dataset comprising a total of 51 features across 10 different drivers. We use a number of data analysis algorithms, such as J48, Random Forest, JRip and clustering, using 94K records. Our results show that J48 is the best performing algorithm in terms of training and testing (1.95 seconds and 0.44 seconds recorded, respectively). We also analyze the effect of using a sliding window on algorithm performance, altering the size of the window to identify the impact on prediction accuracy.","2768-0657","978-1-6654-9560-8","10.1109/EuroSPW55150.2022.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799292","Anti-theft;driver profiling;situational awareness;security;vehicle","Training;Clustering algorithms;Semisupervised learning;Feature extraction;Prediction algorithms;Classification algorithms;Unsupervised learning","","1","","39","IEEE","27 Jun 2022","","","IEEE","IEEE Conferences"
"AutoAttacker: A reinforcement learning approach for black-box adversarial attacks","I. Tsingenopoulos; D. Preuveneers; W. Joosen","imec-DistriNet, KU Leuven, Celestijnenlaan 200A, Heverlee, Belgium; imec-DistriNet, KU Leuven, Celestijnenlaan 200A, Heverlee, Belgium; imec-DistriNet, KU Leuven, Celestijnenlaan 200A, Heverlee, Belgium","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","229","237","Recent research has shown that machine learning models are susceptible to adversarial examples, allowing attackers to trick a machine learning model into making a mistake and producing an incorrect output. Adversarial examples are commonly constructed or discovered by using gradient-based methods that require white-box access to the model. In most real-world AI system deployments, having complete access to the machine learning model is an unrealistic threat model. However, it is possible for an attacker to construct adversarial examples even in the black-box case - where we assume solely a query capability to the model - with a variety of approaches each with its advantages and shortcomings. We introduce AutoAttacker, a novel reinforcement learning framework where agents learn how to operate around the black-box model by querying it, to effectively extract the underlying decision behaviour, and to undermine it successfully. AutoAttacker is a first of kind framework that uses reinforcement learning and assumes nothing about the differentiability or structure of the underlying function and is thus robust towards common defenses like gradient obfuscation or adversarial training. Finally, without differentiable output, as in binary classification, most methods cease to operate and require either an approximation of the gradient, or another approach altogether. Our approach, however, maintains the capability to function when the output descriptiveness diminishes.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802476","reinforcement-learning;black-box-attack;adversarial-machine-learning","","","6","","37","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"OPUPO: Defending Against Membership Inference Attacks With Order-Preserving and Utility-Preserving Obfuscation","Y. Liu; H. Li; G. Huang; W. Hua","School of Information Engineering, Hangzhou Medical College, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China; Zhejiang Lab, Hangzhou, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4690","4701","In this work, we present OPUPO to protect machine learning classifiers against black-box membership inference attacks by alleviating the prediction difference between training and non-training samples. Specifically, we apply order-preserving and utility-preserving obfuscation to prediction vectors. The order-preserving constraint strictly maintains the order of confidence scores in the prediction vectors, guaranteeing that the model's classification accuracy is not affected. The utility-preserving constraint, on the other hand, enables adaptive distortions to the prediction vectors in order to protect their utility. Moreover, OPUPO is proved to be adversary resistant that even well-informed defense-aware adversaries cannot restore the original prediction vectors to bypass the defense. We evaluate OPUPO on machine learning and deep learning classifiers trained with four popular datasets. Experiments verify that OPUPO can effectively defend against state-of-the-art attack techniques with negligible computation overhead. In specific, the inference accuracy could be reduced from as high as 87.66% to around 50%, i.e., random guess, and the prediction time will increase by only 0.44% on average. The experiments also show that OPUPO could achieve better privacy-utility trade-off than existing defenses.","1941-0018","","10.1109/TDSC.2022.3232111","National Natural Science Foundation of China(grant numbers:52007173,U19B2042); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LQ20E070002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999284","Membership inference attack;machine learning;privacy;defense;obfuscation","Privacy;Predictive models;Closed box;Training;Analytical models;Distortion;Micromechanical devices","","","","61","IEEE","26 Dec 2022","","","IEEE","IEEE Journals"
"More Efficient Secure Matrix Multiplication for Unbalanced Recommender Systems","Z. Huang; C. Hong; C. Weng; W. -j. Lu; H. Qu","Security, Alibaba Group, Hangzhou, China; Security, Alibaba Group, Hangzhou, China; Computer Science, Northwestern University, Evanston, IL, USA; Security, Alibaba Group, Hangzhou, China; Security, Alibaba Group, Hangzhou, China","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","551","562","With recent advances in homomorphic encryption (HE), it becomes feasible to run non-interactive machine learning (ML) algorithms on encrypted data without decryption. In this work, we propose novel encoding methods to pack matrix in a compact way and more efficient methods to perform matrix multiplication on homomorphically encrypted data, leading to a speed boost of $1.5\times - 20\times$1.5×-20× for slim rectangular matrix multiplication compared with state-of-the-art. Moreover, we integrate our optimized secure matrix arithmetic with the MPI distributed computing framework, achieving scalable parallel secure matrix computation. Equipped with the optimized matrix multiplication, we propose uSCORE, a privacy-preserving cross-domain recommendation system for the unbalanced scenario, where a big data owner provides recommendation as a service to a client who has less data and computation power. Our design delegates most of the computation to the service provider, and has a low communication cost, which previous works failed to achieve. For a client who has 16 million user-item pairs to update, it only needs about 3 minutes (in the LAN setting) to prepare the encrypted data. The server can finish the update process on the encrypted data in less than half an hour, effectively reducing the client's test error from 0.72 to 0.62.","1941-0018","","10.1109/TDSC.2021.3139318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665288","Homomorphic encryption;secure matrix multiplication;collaborative filtering;sparse SVD optimization","Cryptography;Matrix decomposition;Machine learning;Sparse matrices;Encoding;Computational modeling;Training","","3","","47","CCBY","29 Dec 2021","","","IEEE","IEEE Journals"
"The Limitations of Deep Learning in Adversarial Settings","N. Papernot; P. McDaniel; S. Jha; M. Fredrikson; Z. B. Celik; A. Swami","Department of Computer Science and Engineering, Penn State University; Department of Computer Science and Engineering, Penn State University; Department of Computer Science and Engineering, Penn State University; Computer Sciences Department, University of Wisconsin-Madison; Department of Computer Science and Engineering, Penn State University; United States Army Research Laboratory, Adelphi, Maryland","2016 IEEE European Symposium on Security and Privacy (EuroS&P)","12 May 2016","2016","","","372","387","Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.","","978-1-5090-1752-2","10.1109/EuroSP.2016.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467366","","Neurons;Training;Machine learning;Biological neural networks;Distortion;Force","","1820","7","37","IEEE","12 May 2016","","","IEEE","IEEE Conferences"
"Jekyll: Attacking Medical Image Diagnostics using Deep Generative Models","N. Mangaokar; J. Pu; P. Bhattacharya; C. K. Reddy; B. Viswanath",Virginia Tech; Virginia Tech; University of Virginia; Virginia Tech; Virginia Tech,"2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","139","157","Advances in deep neural networks (DNNs) have shown tremendous promise in the medical domain. However, the deep learning tools that are helping the domain, can also be used against it. Given the prevalence of fraud in the healthcare domain, it is important to consider the adversarial use of DNNs in manipulating sensitive data that is crucial to patient healthcare. In this work, we present the design and implementation of a DNN-based image translation attack on biomedical imagery. More specifically, we propose Jekyll, a neural style transfer framework that takes as input a biomedical image of a patient and translates it to a new image that indicates an attacker-chosen disease condition. The potential for fraudulent claims based on such generated ‘fake’ medical images is significant, and we demonstrate successful attacks on both X-rays and retinal fundus image modalities. We show that these attacks manage to mislead both medical professionals and algorithmic detection schemes. Lastly, we also investigate defensive measures based on machine learning to detect images generated by Jekyll.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230370","deep learning, generative models, medical image diagnostics, attacks, defenses","Deep learning;Machine learning algorithms;Biological system modeling;X-rays;Biomedical measurement;Retina;Medical diagnostic imaging","","13","","84","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning","L. K. Shar; L. C. Briand; H. B. K. Tan","Interdisciplinary Centre for ICT Security, Reliability and Trust, 4 rue Alphonse Weicker, Luxembourg; Interdisciplinary Centre for ICT Security, Reliability and Trust, 4 rue Alphonse Weicker, Luxembourg; Department of Information Engineering, Nanyang Avenue, Singapore","IEEE Transactions on Dependable and Secure Computing","9 Nov 2015","2015","12","6","688","707","Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing.","1941-0018","","10.1109/TDSC.2014.2373377","National Research Fund, Luxembourg(grant numbers:FNR/P10/03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963442","Vulnerability prediction;security measures;input validation and sanitization;program analysis;empirical study;Vulnerability prediction;security measures;input validation and sanitization;program analysis;empirical study","Computer security;Data models;Semisupervised learning;HTML;Servers;Software protection;Predictive models","","88","1","51","IEEE","20 Nov 2014","","","IEEE","IEEE Journals"
"Phishing Email Detection Using Persuasion Cues","R. Valecha; P. Mandaokar; H. R. Rao","Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","747","756","Phishing is an attempt to acquire sensitive information from an unsuspecting victim by malicious means. Recent studies have shown that phishers often use persuasion techniques to get positive responses from the recipients. Still missing from this literature are studies assessing effectiveness of persuasion cues in phishing email detection. Specifically focusing on gain and loss persuasion cues, we address the following research questions: In detecting phishing emails, (1) how effective are the gain persuasion cues, (2) how effective are the loss persuasion cues, and (3) how effective is an integrated model of gain and loss persuasion? In order to address the research questions, we create three machine learning models, with relevant gain persuasion cues, loss persuasion cues, and combined gain and loss persuasion cues respectively, and compare the estimates with a baseline model that does not account for the persuasion cues. The results show that the three phishing detection models with relevant persuasion cues significantly outperform the baseline model by approximately 5% to 20% percent in terms of F-score, thus representing reliable methods for phishing email detection. The objective of this study is to develop anti-phishing methods that utilize persuasion cues for detecting phishing emails. Such research is useful because a deeper understanding of persuasion cues can inform the design of effective countermeasures for detecting and blocking phishing emails.","1941-0018","","10.1109/TDSC.2021.3118931","National Science Foundation(grant numbers:2126504,1651475); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565347","Security and protection;Social science methods or tools;Social technologies","Phishing;Electronic mail;Security;Feature extraction;Natural language processing;Focusing;Social sciences","","9","","62","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"“Get a higher return on your savings!”: Comparing adverts for cryptocurrency investment scams across platforms","G. A. Siu; A. Hutchings","Computer Laboratory, University of Cambridge, Cambridge, United Kingdom; Computer Laboratory, University of Cambridge, Cambridge, United Kingdom","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","158","169","This work compares machine learning methods using supervised, semi-supervised and unsupervised learning, to classify advertisements for cryptocurrency related investment scams found in the online forum Bitcointalk, and the social media platform Reddit. We extract more than 24.2 million posts from Bitcointalk and use Reddit’s API to collect 2,108 submissions. We train and compare several multiclass text classification approaches and use the models with highest accuracy and F-measure to identify cryptocurrency investment scam advertisements found on both platforms. We discover around five percent of all posts collected on both sites are potential scams. We then use another text classifier to identify the scam actors involved in these investment scam advertisements. We also discover the lures used within these fraudulent adverts and find the main differences in luring techniques used between Bitcointalk and Reddit. We identify that the most prevalent lure type uses the financial principle, followed by the distraction principle in Bitcointalk, and by the authority principle in Reddit. Finally, we use subreddits as communities’ proxies and compare scam advertisements within them to identify whether pensioners are being specifically targeted by cryptocurrency scam adverts. Our results show that retirement subreddits do not contain a higher number of cryptocurrency investment scam adverts compared to other investment focused subreddits.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00023","European Research Council; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190672","cryptocurrency;cybercrime;investment scams;machine learning","Social networking (online);Biological system modeling;Soft sensors;Text categorization;Machine learning;Predictive models;Cryptocurrency","","","","60","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack","H. Yan; X. Li; H. Li; J. Li; W. Sun; F. Li","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; Department of Computer and Information Technology, Purdue University, West Lafayette, IN, USA; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academic of Sciences, Beijing, China","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2680","2694","Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.","1941-0018","","10.1109/TDSC.2021.3069258","Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University(grant numbers:19151110473); Purdue University; China Scholarship Council(grant numbers:201906960075); National Natural Science Foundation of China(grant numbers:61932015); National Key R&D Project(grant numbers:2017YFB0802203); Shaanxi Innovation Team Project(grant numbers:2018TD-007); Higher Education Discipline Innovation Project(grant numbers:B16037); National Natural Science Foundation of China(grant numbers:U1836203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389670","Machine learning;model extraction attack;extraction status assessment;differential privacy;privacy budget allocation","Adaptation models;Privacy;Monitoring;Data models;Mathematical model;Training;Differential privacy","","15","","45","IEEE","29 Mar 2021","","","IEEE","IEEE Journals"
"Model Protection: Real-Time Privacy-Preserving Inference Service for Model Privacy at the Edge","J. Hou; H. Liu; Y. Liu; Y. Wang; P. -J. Wan; X. -Y. Li","School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Institute for AI Industry Research (AIR), Tsinghua University, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4270","4284","Major cloud service providers with well-equipped infrastructure, experienced machine learning (ML) expertise, and enriched training datasets are building ML-as-a-Service (MLaaS) systems, in which clients can query ML-based prediction services with their data. Instead of moving private data to the cloud, in this work, we design, implement, and evaluate a novel secure ML system to enable MLaaS on edge devices. To protect the proprietary ML models on edge devices from revealing to the clients while maintaining a real-time inference is challenging. Existing privacy-preserving ML techniques can hardly satisfy real-time requirements. In our solution, we employ a secure enclave (e.g., SGX) to offer security and provide better efficiency than cryptographic techniques. However, the enclave alone cannot achieve real-time capability due to its limited capacity. We observe that the ML model imposes a severe accuracy degradation when adding noise to a few model weights. Based on this, we design a suite of novel solutions to optimize the performance of secure enclave-based inference service at the edge by enclosing only $1\%$1% computation within secure enclaves. Our work can achieve up to a $7.8\times$7.8× increase in efficiency and a $27\times$27× reduction in memory usage compared to the state-of-the-art.","1941-0018","","10.1109/TDSC.2021.3126315","National Key R&D Program of China(grant numbers:2018YFB0803400); Tsinghua University - AsiaInfo Technologies Inc.; Joint Rsearch Center(grant numbers:20203910074); China National Funds for Distinguished Young Scientists(grant numbers:61625205); National Natural Science Foundation of China(grant numbers:62132018,61751211,61572347,61520106007); Key Research Program of Frontier Sciences, CAS(grant numbers:QYZDY-SSW-JSC002); National Science Foundation(grant numbers:CNS-1526638); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609559","Model privacy;machine learning as a service;data privacy;edge computing","Computational modeling;Data models;Load modeling;Privacy;Codes;Real-time systems;Predictive models","","7","","54","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"FairTest: Discovering Unwarranted Associations in Data-Driven Applications","F. Tramèr; V. Atlidakis; R. Geambasu; D. Hsu; J. -P. Hubaux; M. Humbert; A. Juels; H. Lin","Stanford; Columbia University; Columbia University; Columbia University; EPFL; Saarland University; Cornell Tech, Jacobs Institute; EPFL","2017 IEEE European Symposium on Security and Privacy (EuroS&P)","3 Jul 2017","2017","","","401","416","In a world where traditional notions of privacy are increasingly challenged by the myriad companies that collect and analyze our data, it is important that decision-making entities are held accountable for unfair treatments arising from irresponsible data usage. Unfortunately, a lack of appropriate methodologies and tools means that even identifying unfair or discriminatory effects can be a challenge in practice. We introduce the unwarranted associations (UA) framework, a principled methodology for the discovery of unfair, discriminatory, or offensive user treatment in data-driven applications. The UA framework unifies and rationalizes a number of prior attempts at formalizing algorithmic fairness. It uniquely combines multiple investigative primitives and fairness metrics with broad applicability, granular exploration of unfair treatment in user subgroups, and incorporation of natural notions of utility that may account for observed disparities. We instantiate the UA framework in FairTest, the first comprehensive tool that helps developers check data-driven applications for unfair user treatment. It enables scalable and statistically rigorous investigation of associations between application outcomes (such as prices or premiums) and sensitive user attributes (such as race or gender). Furthermore, FairTest provides debugging capabilities that let programmers rule out potential confounders for observed unfair effects. We report on use of FairTest to investigate and in some cases address disparate impact, offensive labeling, and uneven rates of algorithmic error in four data-driven applications. As examples, our results reveal subtle biases against older populations in the distribution of error in a predictive health application and offensive racial labeling in an image tagger.","","978-1-5090-5762-7","10.1109/EuroSP.2017.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961993","Algorithmic Fairness;Systems;Statistics","Computer bugs;Measurement;Tools;Testing;Medical services;Google;Machine learning algorithms","","67","1","59","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Ekiden: A Platform for Confidentiality-Preserving, Trustworthy, and Performant Smart Contracts","R. Cheng; F. Zhang; J. Kos; W. He; N. Hynes; N. Johnson; A. Juels; A. Miller; D. Song",UC Berkeley; Cornell Tech; Oasis Labs; UC Berkeley; UC Berkeley; UC Berkeley; Cornell Tech; UIUC; UC Berkeley,"2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","185","200","Smart contracts are applications that execute on blockchains. Today they manage billions of dollars in value and motivate visionary plans for pervasive blockchain deployment. While smart contracts inherit the availability and other security assurances of blockchains, however, they are impeded by blockchains' lack of confidentiality and poor performance. We present Ekiden, a system that addresses these critical gaps by combining blockchains with Trusted Execution Environments (TEEs). Ekiden leverages a novel architecture that separates consensus from execution, enabling efficient TEE-backed confidentiality-preserving smart-contracts and high scalability. Our prototype (with Tendermint as the consensus layer) achieves example performance of 600× more throughput and 400× less latency at 1000× less cost than the Ethereum mainnet. Another contribution of this paper is that we systematically identify and treat the pitfalls arising from harmonizing TEEs and blockchains. Treated separately, both TEEs and blockchains provide powerful guarantees, but hybridized, though, they engender new attacks. For example, in naïve designs, privacy in TEE-backed contracts can be jeopardized by forgery of blocks, a seemingly unrelated attack vector. We believe the insights learned from Ekiden will prove to be of broad importance in hybridized TEE-blockchain systems.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806762","smart contracts;blockchain;trusted hardware;confidentiality preserving smart contracts","Blockchain;Smart contracts;Hardware;Cryptography","","194","14","81","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Edge and Fog Computing in Critical Infrastructures: Analysis, Security Threats, and Research Challenges","P. Tedeschi; S. Sciancalepore","Division of Information and Computing Technology (ICT), College of Science and Engineering (CSE), Hamad Bin Khalifa University (HBKU), Doha, Qatar; Division of Information and Computing Technology (ICT), College of Science and Engineering (CSE), Hamad Bin Khalifa University (HBKU), Doha, Qatar","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","1","10","The increasing integration of information and communication technologies has undoubtedly boosted the efficiency of Critical Infrastructures (CI). However, the first wave of IoT devices, together with the management of enormous amount of data generated by modern CIs, has created serious architectural issues. While the emerging Fog and Multi-Access Edge Computing (FMEC) paradigms can provide a viable solution, they also bring inherent security issues, that can cause dire consequences in the context of CIs. In this paper, we analyze the applications of FMEC solutions in the context of CIs, with a specific focus on related security issues and threats for the specific while broad scenarios: a smart airport, a smart port, and a smart offshore oil and gas extraction field. Leveraging these scenarios, a set of general security requirements for FMEC is derived, together with crucial research challenges whose further investigation is cornerstone for a successful adoption of FMEC in CIs.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802409","Edge Computing;Fog Computing;Critical Infrastructures;Security","Airports;Oils;Cloud computing;Security;Companies;Containers;Marine vehicles","","20","","31","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Learning Analytics Perspective: Evidencing Learning from Digital Datasets in Cybersecurity Exercises","K. Maennel","Department of Software Sciences, School of Information Technologies, Tallinn University of Technology, Tallinn, Estonia","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","27","36","Cybersecurity exercises are gaining in popularity in university curricula and professional training paths and are seen as an effective teaching method. Such exercises provide digital datasets to facilitate a learning analytics approach such as by using the traces that learners leave behind to improve the learning process and environment. While there are various learning measurement efforts from digital datasets in the existing literature, a holistic learning analytics approach incorporated into cybersecurity exercises is still lacking. We propose a practical reference model for incorporating a learning analytics approach into the cybersecurity exercise life-cycle. To facilitate this application, we have performed an extensive review of existing academic research on applying learning analytics in the context of cybersecurity exercises. We specifically focus on the learning indicators used to measure empirical impact and training effectiveness that could indicate achievement of defined learning outcomes. This reference model and overview of existing learning analytics use cases and learning metrics in various types of exercises can help educators, organisers and cyber range developers. This results in more adaptive exercise design and measurement using evidence-based data and connects digital learning traces to skills and competencies.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229751","cybersecurity;learning analytics;learning metrics;training;exercises","Computer security;Training;Analytical models;Measurement;Tools;Data models;Conferences","","13","","88","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Mitigating Adversarial Gray-Box Attacks Against Phishing Detectors","G. Apruzzese; V. S. Subrahmanian","Hilti Chair of Data and Application Security, University of Liechtenstein, Vaduz, Liechtenstein; Department of Computer Science & Buffett Institute of Global Affairs, Northwestern University, Evanston, IL, USA","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","3753","3769","Although machine learning based algorithms have been extensively used for detecting phishing websites, there has been relatively little work on how adversaries may attack such “phishing detectors” (PDs for short). In this paper, we propose a set of Gray-Box attacks on PDs that an adversary may use which vary depending on the knowledge that he has about the PD. We show that these attacks severely degrade the effectiveness of several existing PDs. We then propose the concept of operation chains that iteratively map an original set of features to a new set of features and develop the “Protective Operation Chain” (${{\sf POC}}$POC for short) algorithm. ${{\sf POC}}$POC leverages the combination of random feature selection and feature mappings in order to increase the attacker's uncertainty about the target PD. Using 3 existing publicly available datasets plus a fourth that we have created and will release upon the publication of this article1, we show that ${{\sf POC}}$POC is more robust to these attacks than past competing work, while preserving predictive performance when no adversarial attacks are present. Moreover, ${{\sf POC}}$POC is robust to attacks on 13 different classifiers, not just one. These results are shown to be statistically significant at the $p < 0.001$p<0.001 level.","1941-0018","","10.1109/TDSC.2022.3210029","ONR(grant numbers:N00014-20-1-2407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904297","Phishing detection;cybersecurity;adversarial attacks;dataset","Phishing;Detectors;Feature extraction;Machine learning;Robustness;Standards;Machine learning algorithms","","4","","83","IEEE","27 Sep 2022","","","IEEE","IEEE Journals"
"CARONTE: Crawling Adversarial Resources Over Non-Trusted, High-Profile Environments","M. Campobasso; P. Burda; L. Allodi","University of Bologna, IT; Eindhoven University of Technology, NL; Eindhoven University of Technology, NL","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","433","442","The monitoring of underground criminal activities is often automated to maximize the data collection and to train ML models to automatically adapt data collection tools to different communities. On the other hand, sophisticated adversaries may adopt crawling-detection capabilities that may significantly jeopardize researchers' opportunities to perform the data collection, for example by putting their accounts under the spotlight and being expelled from the community. This is particularly undesirable in prominent and high-profile criminal communities where entry costs are significant (either monetarily or for example for background checking or other trust-building mechanisms). This paper presents CARONTE, a tool to semi-automatically learn virtually any forum structure for parsing and data-extraction, while maintaining a low profile for the data collection and avoiding the requirement of collecting massive datasets to maintain tool scalability. We showcase the tool against four underground forums, and compare the network traffic it generates (as seen from the adversary's position, i.e. the underground community's server) against state-of-the-art tools for web-crawling as well as human users.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802484","underground;stealth monitoring;data collection;high-profile communities","","","4","","43","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"The Case for Virtual PLC-enabled Honeypot Design","S. Y. Chowdhury; B. Dudley; R. Sun","Florida International University; Dragos, Inc; Florida International University","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","351","357","Programmable logic controllers (PLCs) are essential components of Industrial Control System (ICS) in acting as a practical link between the cyber and physical worlds. In recent years, we have seen an increase in attacks targeting PLCs. Honeypot for PLCs, as an effective technique to gather attacker information and attack tactics, is limited in vendor-specific implementation, configuration, extensibility, and scalability. With the emergence of virtual PLCs, this paper introduces a honeypot, named PLCHoney, to overcome the existing challenges in a cost-effective approach. We designed and implemented PLCHoney with a proxy profiler, dockerized virtual PLCs, a physical process simulator, and a security analysis engine. PLCHoney was able to correctly simulate responses to various internet requests and tested effectively on a network of virtualized traffic light applications. We enabled further security analysis with a dataset containing PLC I/O status, collected with and without attacks. We envision that PLCHoney paves the avenue for the future development of PLC-based honeypots.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190640","Honeypot;IDS;PLC;ICS","Integrated circuits;Scalability;Industrial control;Programmable logic devices;Pipelines;Extensibility;Software","","","","37","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"Unsafe Behavior Detection with Adaptive Contrastive Learning in Industrial Control Systems","X. Zheng; T. Wang; S. Y. Chowdhury; R. Sun; D. Luo","Florida International University, Miami, United States; State College, Pennsylvania State University, United States; Florida International University, Miami, United States; Florida International University, Miami, United States; Florida International University, Miami, United States","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","363","369","Unsafe behavior detection is crucial for maintaining safe and reliable operations in various industrial control systems. However, the scarcity of labeled samples for model training poses significant challenges for existing methods. Self-supervised learning, particularly contrastive learning, offers a promising solution due to its ability to learn from unlabelled data. In this paper, we present AdaTCL, a contrastive learning framework with adaptive augmentations, to detect unsafe behavior in industrial control systems. By dividing instances into task-irrelevant and informative parts and applying lossless transform functions, AdaTCL prevents ad-hoc decisions and laborious trial-and-error tuning for augmentation selection, which improves the generalization capability of contrastive learning. Our experiments demonstrate that AdaTCL significantly outperforms classic and recent baselines highlighting the potential of state-of-the-art self-supervised learning techniques for industrial control systems.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190657","","Training;Adaptation models;Adaptive systems;Industrial control;Self-supervised learning;Transforms;Behavioral sciences","","","","65","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"A ‘Human-in-the-Loop’ approach for Information Extraction from Privacy Policies under Data Scarcity","M. Gebauer; F. Maschhur; N. Leschke; E. Grünewald; F. Pallas","Information Systems Engineering - TU Berlin, Berlin; Information Systems Engineering - TU Berlin, Berlin; Information Systems Engineering - TU Berlin, Berlin; Information Systems Engineering - TU Berlin, Berlin; Information Systems Engineering - TU Berlin, Berlin","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","76","83","Machine-readable representations of privacy policies are door openers for a broad variety of novel privacy-enhancing and, in particular, transparency-enhancing technologies (TETs). In order to generate such representations, transparency information needs to be extracted from written privacy policies. However, respective manual annotation and extraction processes are laborious and require expert knowledge. Approaches for fully automated annotation, in turn, have so far not succeeded due to overly high error rates in the specific domain of privacy policies. In the end, a lack of properly annotated privacy policies and respective machine-readable representations persists and enduringly hinders the development and establishment of novel technical approaches fostering policy perception and data subject informedness.In this work, we present a prototype system for a ‘ Human-in-the-Loop’ approach to privacy policy annotation that integrates ML-generated suggestions and ultimately human annotation decisions. We propose an ML-based suggestion system specifically tailored to the constraint of data scarcity prevalent in the domain of privacy policy annotation. On this basis, we provide meaningful predictions to users thereby streamlining the annotation process. Additionally, we also evaluate our approach through a prototypical implementation to show that our ML-based extraction approach provides superior performance over other recently used extraction models for legal documents.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190661","","Privacy;Data privacy;Annotations;Law;Error analysis;Prototypes;Manuals","","","","32","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"Efficient and Private Scoring of Decision Trees, Support Vector Machines and Logistic Regression Models Based on Pre-Computation","M. De Cock; R. Dowsley; C. Horst; R. Katti; A. C. A. Nascimento; W. -S. Poon; S. Truex","Institute of Technology, University of Washington Tacoma, Tacoma, WA; Department of Computer Science, Aarhus University, Aarhus, Denmark; Institute of Technology, University of Washington Tacoma, Tacoma, WA; Institute of Technology, University of Washington Tacoma, Tacoma, WA; Institute of Technology, University of Washington Tacoma, Tacoma, WA; Institute of Technology, University of Washington Tacoma, Tacoma, WA; Georgia Institute of Technology, Atlanta, GA","IEEE Transactions on Dependable and Secure Computing","12 Mar 2019","2019","16","2","217","230","Many data-driven personalized services require that private data of users is scored against a trained machine learning model. In this paper we propose a novel protocol for privacy-preserving classification of decision trees, a popular machine learning model in these scenarios. Our solutions is composed out of building blocks, namely a secure comparison protocol, a protocol for obliviously selecting inputs, and a protocol for multiplication. By combining some of the building blocks for our decision tree classification protocol, we also improve previously proposed solutions for classification of support vector machines and logistic regression models. Our protocols are information theoretically secure and, unlike previously proposed solutions, do not require modular exponentiations. We show that our protocols for privacy-preserving classification lead to more efficient results from the point of view of computational and communication complexities. We present accuracy and runtime results for seven classification benchmark datasets from the UCI repository.","1941-0018","","10.1109/TDSC.2017.2679189","European research Council(grant numbers:669255); European Union's Horizon 2020(grant numbers:731583 (SODA)); Danish National Research Foundation; The National Science Foundation of China(grant numbers:61361136003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873244","Private classification;decision trees;support vector machines;logistic regression;secure multiparty computation;secret sharing;privacy-preserving computation","Protocols;Cryptography;Computational modeling;Decision trees;Data models;Logistics","","80","","54","IEEE","7 Mar 2017","","","IEEE","IEEE Journals"
"On the Performance of Detecting Injection of Fabricated Messages into the CAN Bus","L. b. Othmane; L. Dhulipala; M. Abdelkhalek; N. Multari; M. Govindarasu","Secure Development of Smart Cyber-Physical Systems Laboratory, Iowa State University, Ames, IA, USA; Secure Development of Smart Cyber-Physical Systems Laboratory, Iowa State University, Ames, IA, USA; Iowa State University, Ames, IA, USA; Pacific Northwest National Laboratories, Richland, WA, USA; Iowa State University, Ames, IA, USA","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","468","481","There have been several public demonstrations of attacks on connected vehicles showing the ability of an attacker to take control of a targeted vehicle by injecting messages into their Controller Area Network (CAN) bus. In this article, using injected speed reading and Revolutions Per Minute (RPM) reading messages in in-motion vehicle, we examine the ability of the Pearson correlation and the unsupervised learning methods k-means clustering and Hidden Markov Model (HMM) to differentiate ’no-attack’ and ’under-attack’ states of the given vehicle. We found that the Pearson correlation distinguishes the two states, the k-means clustering method has an acceptable accuracy but high false positive rate and HMM detects attacks with acceptable detection rate but has a high false positive in detecting attacks from speed readings when there is no attack. The accuracy of these unsupervised learning methods are comparable to the ones of the supervised learning methods used by CAN bus Intrusion Detection System (IDS) suppliers. In addition, the article shows that studying CAN anomaly detection techniques using off-vehicle test facilities may not properly evaluate the performance of the detection techniques. The results suggest using other features besides the data content of the CAN messages and integrate knowledge about how the Electronic Control Units (ECUs) collaborate in building effective techniques for the detection of injection of fabricated message attacks.","1941-0018","","10.1109/TDSC.2020.2990192","Pacific Northwest National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076852","Anomaly detection;CAN bus;machine learning","Hidden Markov models;Protocols;Pins;Connected vehicles;Unsupervised learning;Automobiles","","14","","48","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"D-Fence: A Flexible, Efficient, and Comprehensive Phishing Email Detection System","J. Lee; F. Tang; P. Ye; F. Abbasi; P. Hay; D. M. Divakaran",Trustwave; Trustwave; Trustwave; Trustwave; Trustwave; Trustwave,"2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","578","597","Phishing continues to be a major security concern for organizations around the globe. Past works proposed classifiers to detect phishing emails; however many of them are based on rules, whereas others are typically standalone models focusing on one specific component of emails (say, URL strings). In this work, we take a different approach and propose a multi-modular and comprehensive phishing email detection system, called D-Fence. The different modules of D-Fence — structure module, text module, and URL module — detect phishing attempts in different components of an email. This allows D-Fence to cover larger attack surfaces while also offering flexible (model) configurations with reduced computational overhead. We carry out experiments on a large-scale real-world email dataset comprising mails from multiple enterprises. Our evaluations demonstrate the effectiveness of D-Fence in detecting phishing emails that do not have malicious intentions manifesting in all email components; D-Fence achieves a high recall of 0.99 at a low false-positive rate of 1 in 10K. Furthermore, we perform systematic evaluations to find and evaluate cost-efficient model configurations for D-Fence; the results reveal that D-Fence maintains high detection capability while bringing significant savings in computational time.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00045","National Research Foundation, Prime Minister's Office, Singapore; National University of Singapore; Singapore Telecommunications Ltd.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581199","phishing detection;phishing email;machine learning;deep learning","Uniform resource locators;Systematics;Phishing;Computational modeling;Focusing;Organizations;Electronic mail","","11","","0","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"Towards a New Thermal Monitoring Based Framework for Embedded CPS Device Security","N. Patel; P. Krishnamurthy; H. Amrouch; J. Henkel; M. Shamouilian; R. Karri; F. Khorrami","NYU Tandon School of Engineering, Brooklyn, NY, USA; NYU Tandon School of Engineering, Brooklyn, NY, USA; Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany; NYU Tandon School of Engineering, Brooklyn, NY, USA; NYU Tandon School of Engineering, Brooklyn, NY, USA; NYU Tandon School of Engineering, Brooklyn, NY, USA","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","524","536","This article introduces a thermal side channel as a proxy for the behavior of embedded processors to detect changes in the behavior in a cyber-physical system. Such changes may be due to software/hardware attacks and altered processors. Since control system processes are periodic computations, the thermal side channels exhibit a temporal pattern. This enables the detection of altered code and changed device characteristics. We present a machine learning approach to estimate the activity of the embedded device from the time sequence of thermal images and show that deviations from expected behavior can be detected. The approach is validated on a multi-core processor running a periodic computational code. The infrared imager collects thermal imagery from the processor, which is cooled from the backside. Instead of an external imager, one can deploy a finite number of on-chip temperature sensors. This article shows that integrating on-chip temperature sensors allows robust real-time monitoring of the processor behavior. Finally, we offer a machine learning approach to optimally place the on-chip sensors to aid detection.","1941-0018","","10.1109/TDSC.2020.2973959","Office of Naval Research(grant numbers:N00014-15-1-2182,N00014-17-1-2006,N00014-18-1-2672); Boeing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999554","Side-channel analysis and countermeasures;embedded systems security;cyber-physical systems;thermal side channel;real-time monitoring;temperature sensors;software and hardware attacks;machine learning","Temperature sensors;Monitoring;System-on-chip;Program processors;Temperature measurement;Field programmable gate arrays","","14","","33","IEEE","14 Feb 2020","","","IEEE","IEEE Journals"
"LoMar: A Local Defense Against Poisoning Attack on Federated Learning","X. Li; Z. Qu; S. Zhao; B. Tang; Z. Lu; Y. Liu","Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; School of Computer Science, University of Oklahoma, Tulsa, OK, USA; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","437","450","Federated learning (FL) provides a high efficient decentralized machine learning framework, where the training data remains distributed at remote clients in a network. Though FL enables a privacy-preserving mobile edge computing framework using IoT devices, recent studies have shown that this approach is susceptible to poisoning attacks from the side of remote clients. To address the poisoning attacks on FL, we provide a two-phase defense algorithm called ${\underline{Lo}cal\ \underline{Ma}licious\ Facto\underline{r}}$Lo̲calMa̲liciousFactor̲ (LoMar). In phase I, LoMar scores model updates from each remote client by measuring the relative distribution over their neighbors using a kernel density estimation method. In phase II, an optimal threshold is approximated to distinguish malicious and clean updates from a statistical perspective. Comprehensive experiments on four real-world datasets have been conducted, and the experimental results show that our defense strategy can effectively protect the FL system. Specifically, the defense performance on Amazon dataset under a label-flipping attack indicates that, compared with FG+Krum, LoMar increases the target label testing accuracy from $96.0\%$96.0% to $98.8\%$98.8%, and the overall averaged testing accuracy from $90.1\%$90.1% to $97.0\%$97.0%.","1941-0018","","10.1109/TDSC.2021.3135422","National Science Foundation(grant numbers:IIS-2047570,CNS-2044516); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650669","Distributed artificial intelligence;security and protection;defense;distribution functions;distributed architectures","Data models;Training;Training data;Analytical models;Optimization;Machine learning;Kernel","","30","","61","IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Trust Beyond Border: Lightweight, Verifiable User Isolation for Protecting In-Enclave Services","W. Wang; W. Liu; H. Chen; X. Wang; H. Tian; D. Lin","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Indiana University Bloomington, Bloomington, IN, USA; Indiana University Bloomington, Bloomington, IN, USA; Indiana University Bloomington, Bloomington, IN, USA; Ant Group, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","522","538","Due to the absence of in-enclave isolation, today's trusted execution environment (TEE), specifically Intel's Software Guard Extensions (SGX), does not have the capability to securely run different users’ tasks within a single enclave, which is required for supporting real-world services, such as an in-enclave machine learning model that classifies the data from various sources, or a microservice (e.g., data search) that performs a very small task (within sub-seconds) for a user and therefore cannot afford the resources and the delay for creating a separate enclave for each user. To address this challenge, we developed Liveries, a technique that enables lightweight, verifiable in-enclave user isolation for protecting time-sharing services. Our approach restricts an in-enclave thread's privilege when configuring an enclave, and further performs integrity check and sanitization on critical enclave data upon user switches. For this purpose, we developed a novel technique that ensures the protection of sensitive user data (e.g., session keys) even in the presence of the adversary who may have compromised the enclave. Our study shows that the new technique is lightweight (1% overhead) and verifiable (about 3200 lines of code), making a step towards assured protection of real-world in-enclave services.","1941-0018","","10.1109/TDSC.2021.3138427","National Key R&D Program of China(grant numbers:2020YFB1805402); National Natural Science Foundation of China(grant numbers:61802397); NIH(grant numbers:R01HG010798); National Science Foundation(grant numbers:CNS-1838083); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664230","Trusted execution environment;cloud computing;in-enclave isolation;security","Task analysis;Codes;Software;Runtime;Microservice architectures;Machine learning;Instruction sets","","1","","84","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"XAI-Based Microarchitectural Side-Channel Analysis for Website Fingerprinting Attacks and Defenses","B. Gulmezoglu","Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4039","4051","Website Fingerprinting attacks aim to track the visited websites in browsers and infer confidential information about users. Several studies showed that recent advancements in Machine Learning (ML) and Deep Learning (DL) algorithms made it possible to implement website fingerprinting attacks even though various defense techniques are present in the network. Nevertheless, trained models for website detection are not analyzed deeply to identify the leakage sources which are not always visible to both attackers and Cyber Threat Intelligence engineers. This study focuses on explaining ML and DL models in the context of microarchitecture-based website fingerprinting attacks. In the attack model, performance counters and cache occupancy side-channels are implemented on Google Chrome and Tor browsers. After ML and DL models are trained, LIME and saliency map XAI methods are applied to examine the leakage points in the side-channel data. In order to match the leakage samples in the measurements to the network traces, a novel dataset is collected by utilizing Google Chrome and Firefox browser developer tools. Next, the efficiency of explainable methods are analyzed with XAI metrics. Finally, an XAI-based obfuscation defense technique is proposed as a countermeasure against microarchitecture-based website fingerprinting attacks","1941-0018","","10.1109/TDSC.2021.3117145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9556572","Microarchitectural attacks;website fingerprinting;explainable artificial intelligence","Browsers;Analytical models;Monitoring;Computational modeling;Microarchitecture;Mathematical models;Hardware","","7","","76","IEEE","1 Oct 2021","","","IEEE","IEEE Journals"
"Resource-Aware Detection and Defense System against Multi-Type Attacks in the Cloud: Repeated Bayesian Stackelberg Game","O. A. Wahab; J. Bentahar; H. Otrok; A. Mourad","Department of Computer Science and Engineering, Université du Québec en Outaouais, Gatineau, QC, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montréal, QC, Canada; Department of ECE, Khalifa University of Science, Technology and Research, Abu Dhabi, UAE; Department of Mathematics and Computer Science, Lebanese American University, Beirut, Lebanon","IEEE Transactions on Dependable and Secure Computing","9 Mar 2021","2021","18","2","605","622","Cloud-based systems are subject to various attack types launched by Virtual Machines (VMs) manipulated by attackers having different goals and skills. The existing detection and defense mechanisms might be suitable for simple attack environments but become ineffective when the system faces advanced attack scenarios wherein simultaneous attacks of different types are involved. This is because these mechanisms overlook the attackers' strategies in the detection system's design, ignore the system's resource constraints, and lack sufficient knowledge about the attackers' types and abilities. To address these shortcomings, we propose a repeated Bayesian Stackelberg game consisting of the following phases: risk assessment framework that identifies the VMs' risk levels, live-migration-based defense mechanism that protects services from being successful targets for attackers, machine-learning-based technique that collects malicious data from VMs using honeypots and employs one-class Support Vector Machine to learn the attackers' types distributions, and resource-aware Bayesian Stackelberg game that provides the hypervisor with the detection load's optimal distribution over VMs that maximizes the detection of multi-type attacks. Experiments conducted using Amazon's datacenter and Amazon Web Services honeypot data reveal that our solution maximizes the detection, minimizes the number of attacked services, and runs efficiently compared to the state-of-the-art detection and defense strategies, namely Collabra, probabilistic migration, Stackelberg, maxmin, and fair allocation.","1941-0018","","10.1109/TDSC.2019.2907946","Fonds de Recherche du Québec— Nature et technologies; Natural Sciences and Engineering Research Council of Canada; Khalifa University of Science, Technology and Research; National Council for Scientific Research; Lebanese American University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675527","Adversarial artificial intelligence;intrusion detection;game theory;machine learning;data-driven optimization;Moving Target Defense (MTD);honeypots;security risk assessment","Cloud computing;Monitoring;Bayes methods;Games;Virtual machine monitors;Intrusion detection","","33","","34","IEEE","27 Mar 2019","","","IEEE","IEEE Journals"
"Securely and Efficiently Outsourcing Decision Tree Inference","Y. Zheng; H. Duan; C. Wang; R. Wang; S. Nepal","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; Data61, CSIRO, Marsfield, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1841","1855","Outsourcing machine learning inference services to the cloud is getting increasingly popular. However, this also entails privacy risks to the provider's proprietary model and the client's sensitive data. Focusing on inference with decision trees, this article proposes a framework for securely and efficiently outsourcing decision tree inference. Targeting both privacy and efficiency, we propose a customized protocol using only lightweight cryptography in the online execution of secure inference. We resort to additive secret sharing and tackle the problems in various components including secure input feature selection, decision node evaluation, and inference result generation. Our protocol requires no interaction from the provider and client during online secure inference, a distinct advantage over prior works for practical deployment as they all operate under the client-provider setting where synchronous and continuous interaction is required. Performance evaluation demonstrates our security design's efficiency, as well as substantial performance benefits for the client (up to four orders of magnitude in computation and 163 times in communication), as opposed to prior art in the non-outsourcing setting. To facilitate the practical usage for meeting more service demands, we also investigate the extensions for secure outsourced inference of random forests and categorical feature-based decision trees.","1941-0018","","10.1109/TDSC.2020.3040012","Research Grants Council of Hong Kong(grant numbers:CityU 11217819,CityU 11217620); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); National Natural Science Foundation of China(grant numbers:61572412); Australian Government's Cooperative Research Centres Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268141","Machine learning inference;cloud computing;secure outsourcing;privacy;decision trees","Decision trees;Cryptography;Outsourcing;Servers;Security;Protocols;Computational modeling","","19","","46","IEEE","24 Nov 2020","","","IEEE","IEEE Journals"
"Blindfolded Evaluation of Random Forests with Multi-Key Homomorphic Encryption","A. Aloufi; P. Hu; H. W. H. Wong; S. S. M. Chow","Golisano College of Computing and Information Sciences, Rochester Institute of Technology, Rochester, NY, United States; Golisano College of Computing and Information Sciences, Rochester Institute of Technology, Rochester, NY, United States; Department of Information Engineering, Chinese University of Hong Kong, Sha Tin, Hong Kong; Department of Information Engineering, Chinese University of Hong Kong, Sha Tin, Hong Kong","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1821","1835","Decision tree and its generalization of random forests are a simple yet powerful machine learning model for many classification and regression problems. Recent works propose how to privately evaluate a decision tree in a two-party setting where the feature vector of the client or the decision tree model (such as the threshold values of its nodes) is kept secret from another party. However, these works cannot be extended trivially to support the outsourcing setting where a third-party who should not have access to the model or the query. Furthermore, their use of an interactive comparison protocol does not support branching program, hence requires interactions with the client to determine the comparison result before resuming the evaluation task. In this paper, we propose the first secure protocol for collaborative evaluation of random forests contributed by multiple owners. They outsource evaluation tasks to a third-party evaluator. Upon receiving the client's encrypted inputs, the cloud evaluates obliviously on individually encrypted random forest models and calculates the aggregated result. The system is based on our new secure comparison protocol, secure counting protocol, and a multi-key somewhat homomorphic encryption on top of symmetric-key encryption. This allows us to reduce communication overheads while achieving round complexity lower than existing work.","1941-0018","","10.1109/TDSC.2019.2940020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827928","Applied cryptography;decision tree;homomorphic encryption;machine learning;random forest","Protocols;Encryption;Decision trees;Collaboration;Computational modeling","","12","","42","IEEE","10 Sep 2019","","","IEEE","IEEE Journals"
"Behavior-Based Anomaly Detection in Log Data of Physical Access Control Systems","F. Skopik; M. Wurzenberger; G. Höld; M. Landauer; W. Kuhn","Center for Digital Safety and Security, AIT Austrian Institute of Technology, Vienna, Austria; Center for Digital Safety and Security, AIT Austrian Institute of Technology, Vienna, Austria; Center for Digital Safety and Security, AIT Austrian Institute of Technology, Vienna, Austria; Center for Digital Safety and Security, AIT Austrian Institute of Technology, Vienna, Austria; PKE Holding AG, Technologie und Systeme, Vienna, Austria","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","3158","3175","Behavior-based anomaly detection (AD) approaches for enterprise-IT security are not easily applicable to other domains, such as embedded devices and IoT nodes in cyber-physical systems. AD approaches are usually highly optimized for specific purposes, tightly bound to domain-specific technologies and rely on a specific syntax of investigated data. Data from cyber-physical systems is however highly diverse, often poorly documented and not easily ingested for automated analysis. AECID provides an anomaly detection approach, that monitors unstructured textual event data (i.e., log data), and implements self-learning for autonomous operation. A parser generator establishes a model of normal system behavior on top of observed events, which then can be leveraged to detect anomalies as deviations from that baseline. The unsupervised anomaly detection approaches of AECID apply machine learning techniques to perform sequence analysis, correlation analysis and statistical tests of events represented in log data. This paper discusses AECID's applicability in a building security system use case. A proof of concept demonstrates the effective detection of anomalies in log data of a building access control system stemming from card misuse, including stolen access cards and cloned cards.","1941-0018","","10.1109/TDSC.2022.3197265","ICT(grant numbers:DECEPT 873980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852310","Access control;anomaly detection;behavior modeling;intrusion detection;log data;machine learning;security","Buildings;Behavioral sciences;Anomaly detection;Access control;Generators;Training;Permission","","3","","46","CCBY","8 Aug 2022","","","IEEE","IEEE Journals"
"Sais: Self-Adaptive Identification of Security Bug Reports","S. Mostafa; B. Findley; N. Meng; X. Wang","Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1779","1792","Among various bug reports (BRs), security bug reports (SBRs) are unique because they require immediate concealment and fixes. When SBRs are not identified in time, attackers can exploit the vulnerabilities. Prior work identifies SBRs via text mining, which requires a predefined keyword list and trains a classifier with known SBRs and non-security bug reports (NSBRs). The former approach is not reliable, because (1) as the contexts of security vulnerabilities and terminology of SBRs change over time, the predefined list will become out-dated; and (2) users may have insufficient SBRs for training. We introduce a semi-supervised learning-based approach, Sais, to adaptively and reliably identify SBRs. Given a project's BRs containing some labeled SBRs, many more NSBRs, and unlabeled BRs, Sais iteratively mines keywords, trains a classifier based on the keywords from the labeled data, classifies unlabeled BRs, and augments its training data with the newly labeled BRs. Our evaluation shows that Sais is useful for identifying SBRs.","1941-0018","","10.1109/TDSC.2019.2939132","National Science Foundation(grant numbers:CNS-1748109,CCF-1846467); HRD(grant numbers:C-SPECC 1736209,DHS-14-ST-062-001); ONR(grant numbers:N00014-17-1-2498); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823031","Security bug reports;self learning;bug triaging","Computer bugs;Security;Training;Data models;Databases;Semisupervised learning;Software","","4","","58","IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"Advanced Persistent Threat Detection Using Data Provenance and Metric Learning","K. A. Akbar; Y. Wang; G. Ayoade; Y. Gao; A. Singhal; L. Khan; B. Thuraisingham; K. Jee","University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; National Institute of Standards and Technology (NIST), Gaithersburg, MD, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","3957","3969","Advanced persistent threats (APT) have increased in recent times as a result of the rise in interest by nation-states and sophisticated corporations to obtain high-profile information. Typically, APT attacks are more challenging to detect since they leverage zero-day attacks and common benign tools. Furthermore, these attack campaigns are often prolonged to evade detection. We leverage an approach that uses a provenance graph to obtain execution traces of host nodes in order to detect anomalous behavior. By using the provenance graph, we extract features that are then used to train an online adaptive metric learning. Online metric learning is a deep learning method that learns a function to minimize the separation between similar classes and maximizes the separation between dis- similar instances. We compare our approach with baseline models and we show our method outperforms the baseline models by increasing detection accuracy on average by 11.3% and increases True positive rate (TPR) on average by 18.3%. We also show that our method outperforms several state-of-the-art models performances in comprehensive attack datasets in both binary and multi-class settings.","1941-0018","","10.1109/TDSC.2022.3221789","ONR(grant numbers:N00014-17-1-2995); NSA(grant numbers:H98230-15-1-0271); Air Force Office of Scientific Research(grant numbers:FA9550-14-1-0173); National Science Foundation(grant numbers:DGE-1931800,OAC- 1828467,DGE-1723602,DMS-1737978,MRI-1828467); IBM; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9947295","Advanced persistent threat;data provenance;metric learning","Measurement;Trojan horses;Real-time systems;Feature extraction;Databases;Calculators;Training data","","3","","42","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Spotting Anomalies at the Edge: Outlier Exposure-Based Cross-Silo Federated Learning for DDoS Detection","V. Pourahmadi; H. A. Alameddine; M. A. Salahuddin; R. Boutaba","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; Ericsson Security Research, Montreal, QC, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","4002","4015","Distributed Denial-of-Service (DDoS) attacks are expected to continue plaguing service availability in emerging networks which rely on distributed edge clouds to offer critical, latency-sensitive applications. However, edge servers increase the network attack surface, which is exacerbated with the massive number of connected Internet of Things (IoT) devices that can be weaponized to launch DDoS attacks. Therefore, it is crucial to detect DDoS attacks early, i.e., at the network edge. In this paper, we empower the network edge with intelligent DDoS detection by learning from similarities between different data and DDoS attacks available across the edge servers. To this end, we develop a novel Outlier Exposure (OE)-enabled cross-silo Federated Learning framework, namely FedOE. FedOE enables distributed training of OE-based ML models using a limited number of labeled outliers (i.e., attack flows) experienced at edge servers. We propose a novel OE-based Autoencoder (oAE) that can better discriminate anomalies in comparison to the widely adopted traditional Autoencoder, using a tailored, OE-based loss function. We evaluate oAE in FedOE and demonstrate its ability to generalize to zero-day attacks, with just 50 labeled attack flows per edge server. The results show that oAE achieves a high F1-score for most DDoS attacks, outclassing its non-OE counterpart.","1941-0018","","10.1109/TDSC.2022.3224896","Ericsson Canada; Natural Sciences and Engineering Research Council of Canada(grant numbers:536445-18); Innovation for Defence Excellence and Security; Ministère de la Défense Nationale; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964111","Edge intelligence;federated learning;outlier exposure;anomaly detection;DDoS detection","Image edge detection;Anomaly detection;Servers;Denial-of-service attack;Data models;Training;Computer crime","","3","","46","IEEE","25 Nov 2022","","","IEEE","IEEE Journals"
"Real-Time Error Detection in Nonlinear Control Systems Using Machine Learning Assisted State-Space Encoding","S. Banerjee; B. Samynathan; J. A. Abraham; A. Chatterjee","Intel Labs, Santa Clara, CA, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Dependable and Secure Computing","9 Mar 2021","2021","18","2","576","592","Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The method is demonstrated on two test case simulations - an inverted pendulum balancing problem and a sliding mode controller driven brake-by-wire (BBW) system. In addition, hardware results from error injection experiments in an ARM core representation on an FPGA and artificial sensor degradations on a self-balancing robot prove the practical feasibility of implementation.","1941-0018","","10.1109/TDSC.2019.2903049","National Science Foundation(grant numbers:CCF:1421353); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658148","Nonlinear Systems;Nonlinear Control;Real-time Systems;State-space Checksum;Fault Detection","Nonlinear systems;Hardware;Real-time systems;Nonlinear control systems;Actuators;Degradation","","5","","50","IEEE","4 Mar 2019","","","IEEE","IEEE Journals"
"Achieving Privacy-Preserving Online Diagnosis With Outsourced SVM in Internet of Medical Things Environment","B. Xie; T. Xiang; X. Liao; J. Wu","Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4113","4126","Online diagnosis is one of the data services, which can use the machine learning model placed on the cloud and collected physical data from internet of medical things (IoMT) for better medical services. However, the collected user data, diagnosis results and the deployed machine learning model contain sensitive information of users and the healthcare provider, which may lead to serious privacy leakage. To achieve a secure outsourced diagnosis, both high security and low burden for users should be considered. However, the existing works can not solve these problems simultaneously. In this article, based on two non-colluding servers, a privacy-preserving cloud-aided diagnosis scheme for IoMT is proposed. Concretely, a hybrid data encryption method based on homomorphic encryption and AES is used to generate user requests in an efficient way. Besides, we propose a class of secure two-party protocols using homomorphic encryption, such as secure kernel function computation, secure multiplication, and secure comparison, and a privacy-preserving diagnosis scheme based on multi-class SVM with these building blocks is constructed, which can keep users offline in the diagnosis process. Finally, the security analysis and evaluation further illustrate that our scheme is superior to the prior works in terms of security and user-friendliness.","1941-0018","","10.1109/TDSC.2021.3119897","National Key R&D Program of China(grant numbers:2018AAA0100101); National Natural Science Foundation of China(grant numbers:61932006,61772434,U20A20176); Chongqing technology innovation and application development(grant numbers:cstc2020jscx-msxmX0156); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573266","Homomorphic encryption;SVM;medical diagnosis;cloud computing;privacy-preserving","Support vector machines;Kernel;Cloud computing;Servers;Training;Privacy;Computational modeling","","17","","40","IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"An Automatic Attribute-Based Access Control Policy Extraction From Access Logs","L. Karimi; M. Aldairi; J. Joshi; M. Abdelhakim","School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2304","2317","With the rapid advances in computing and information technologies, traditional access control models have become inadequate in terms of capturing fine-grained, and expressive security requirements of newly emerging applications. An attribute-based access control (ABAC) model provides a more flexible approach to addressing the authorization needs of complex and dynamic systems. While organizations are interested in employing newer authorization models, migrating to such models pose as a significant challenge. Many large-scale businesses need to grant authorizations to their user populations that are potentially distributed across disparate and heterogeneous computing environments. Each of these computing environments may have its own access control model. The manual development of a single policy framework for an entire organization is tedious, costly, and error-prone. In this article, we present a methodology for automatically learning ABAC policy rules from access logs of a system to simplify the policy development process. The proposed approach employs an unsupervised learning-based algorithm for detecting patterns in access logs and extracting ABAC authorization rules from these patterns. In addition, we present two policy improvement algorithms, including rule pruning and policy refinement algorithms to generate a higher quality mined policy. Finally, we implement a prototype of the proposed approach to demonstrate its feasibility.","1941-0018","","10.1109/TDSC.2021.3054331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335250","Access control;attribute based access control;policy mining;policy engineering;machine learning;clustering","Authorization;Computational modeling;Unsupervised learning;Organizations;Data mining;Biological system modeling;Medical services","","26","","41","IEEE","25 Jan 2021","","","IEEE","IEEE Journals"
"Secure Aggregation is Insecure: Category Inference Attack on Federated Learning","J. Gao; B. Hou; X. Guo; Z. Liu; Y. Zhang; K. Chen; J. Li","Tianjin Key Laboratory of Network and Data Security Technology, College of Cyber Science, College of Computer Science, Nankai University, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, College of Cyber Science, College of Computer Science, Nankai University, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, College of Cyber Science, College of Computer Science, Nankai University, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, College of Cyber Science, College of Computer Science, Nankai University, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, College of Cyber Science, College of Computer Science, Nankai University, Tianjin, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Computer Science, Guangzhou University, Guangzhou, China","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","147","160","Federated learning allows a large number of resource-constrained clients to train a globally-shared model together without sharing local data. These clients usually have only a few classes (categories) of data for training, where the data distribution is non-iid (not independent identically distributed). In this article, we put forward the concept of category privacy for the first time to indicate which classes of data a client has, which is an important but ignored privacy goal in the federated learning with non-iid data. Although secure aggregation protocols are designed for federated learning to protect the input privacy of clients, we perform the first systematic study on category inference attack and demonstrate that these protocols cannot fully protect category privacy. We design a differential selection strategy and two de-noising approaches to achieve the attack goal successfully. In our evaluation, we apply the attack to non-iid federated learning settings with various datasets. On MNIST, CIFAR-10, AG_news, and DBPedia dataset, our attack achieves $>90\%$>90% accuracy measured in F1-score in most cases. We further consider a possible detection method and propose two strategies to make the attack more inconspicuous.","1941-0018","","10.1109/TDSC.2021.3128679","National Natural Science Foundation of China(grant numbers:62032012); National Key Research and Development Program of China(grant numbers:2020YFB1005700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618806","Federated learning;inference attack;secure aggregation;machine learning","Collaborative work;Training;Privacy;Servers;Data models;Data privacy;Protocols","","9","","39","IEEE","17 Nov 2021","","","IEEE","IEEE Journals"
"Towards Learning-Based, Content-Agnostic Detection of Social Bot Traffic","Y. Feng; J. Li; L. Jiao; X. Wu","Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2149","2163","With the fast-growing popularity of online social networks (OSNs), the security and privacy of OSN ecosystems becomes essential for the public. Among threats OSNs face, malicious social bots have become the most common and detrimental. They are often employed to violate users’ privacy, distribute spam, and disturb the financial market, posing a compelling need for effective social bot detection solutions. Unlike traditional social bot detection approaches that have strict requirements on data sources (e.g., private payload information, social relationships, or activity histories), this article proposes a method called BotFlowMon that relies only on content-agnostic flow-level data as input to identify OSN bot traffic. BotFlowMon introduces several new algorithms and techniques to classify social bot traffic from real OSN user traffic, including aggregating network flow records to obtain OSN transaction data, fusing transaction data to extract features and visualize flows, and an innovative density-valley-based clustering algorithm to subdivide each transaction into individual actions. The evaluation shows BotFlowMon can identify the traffic from social bots with a 96.1 percent accuracy, which, based on the worst case study on a testing machine, only takes no more than 0.71 seconds on average after it sees the traffic.","1941-0018","","10.1109/TDSC.2020.3047399","National Science Foundation(grant numbers:1564348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308962","Online social network;social bot;sybil account;NetFlow;machine learning;data mining","Social networking (online);Feature extraction;Real-time systems;IP networks;Data privacy;Crowdsourcing;Anomaly detection","","7","","73","IEEE","25 Dec 2020","","","IEEE","IEEE Journals"
"$\sf {DBank}$DBank: Predictive Behavioral Analysis of Recent Android Banking Trojans","C. Bai; Q. Han; G. Mezzour; F. Pierazzi; V. S. Subrahmanian","Department of Computer Science, Institute for Security, Technology, and Society, Dartmouth College, Hanover, NH, USA; Department of Computer Science, Institute for Security, Technology, and Society, Dartmouth College, Hanover, NH, USA; Department of Computer Science and Logistic and the TICLab, Universite Internationale de Rabat, Sala El Jadida, Morocco; King’s College London, London, United Kingdom; Department of Computer Science, Institute for Security, Technology, and Society, Dartmouth College, Hanover, NH, USA","IEEE Transactions on Dependable and Secure Computing","13 May 2021","2021","18","3","1378","1393","Using a novel dataset of Android banking trojans (ABTs), other Android malware, and goodware, we develop the $\sf {DBank}$DBank system to predict whether a given Android APK is a banking trojan or not. We introduce the novel concept of a Triadic Suspicion Graph (TSG for short) which contains three kinds of nodes: goodware, banking trojans, and API packages. We develop a novel feature space based on two classes of scores derived from TSGs: suspicion scores (SUS) and suspicion ranks (SR)—the latter yields a family of features that generalize PageRank. While TSG features (based on SUS/SR scores) provide very high predictive accuracy on their own in predicting recent (2016-2017) ABTs, we show that the combination of TSG features with previously studied lightweight static and dynamic features in the literature yields the highest accuracy in distinguishing ABTs from goodware, while preserving the same accuracy of prior feature combinations in distinguishing ABTs from other Android malware. In particular, $\sf {DBank}$DBank’s overall accuracy in predicting whether an APK is a banking trojan or not is up to 99.9% AUC with 0.3% false positive rate. Moreover, we have already reported two unlabeled APKs from VirusTotal (which $\sf {DBank}$DBank has detected as ABTs) to the Google Android Security Team—in one case, we discovered it before any of the 63 anti-virus products on VirusTotal did, and in the other case, we beat 62 of 63 anti-viruses on VirusTotal. This suggests that $\sf {DBank}$DBank is capable of making new discoveries in the wild before other established vendors. We also show that our novel TSG features have some interesting defensive properties as they are robust to knowledge of the training set by an adversary: even if the adversary uses 90% of our training set and uses the exact TSG features that we use, it is difficult for him to infer $\sf {DBank}$DBank’s predictions on APKs. We additionally identify the features that best separate and characterize ABTs from goodware as well as from other Android malware. Finally, we develop a detailed data-driven analysis of five major recent ABT families: FakeToken, Svpeng, Asacub, BankBot, and Marcher, and identify the features that best separate them from goodware and other-malware.","1941-0018","","10.1109/TDSC.2019.2909902","Army Research Office(grant numbers:W911NF1410358); Office of Naval Research(grant numbers:N00014-18-1-2670,N00014-16-12896,N000141612739); NATO Science for Peace and Security(grant numbers:SPS G5319); UK EPSRC(grant numbers:EP/L022710/2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684321","Android banking trojans;machine learning;graph models;malware","Banking;Trojan horses;Smart phones;Training;Security;Google","","6","","44","IEEE","9 Apr 2019","","","IEEE","IEEE Journals"
"On the Effectiveness of Using Graphics Interrupt as a Side Channel for User Behavior Snooping","H. Ma; J. Tian; D. Gao; C. Jia","School of Computing and Information Systems, Singapore Management University, Singapore, Singapore; National Key Laboratory of Science and Technology on Information System Security, Beijing, China; School of Computing and Information Systems, Singapore Management University, Singapore, Singapore; College of Cyber Science, Nankai University, Tianjin, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3257","3270","Graphics Processing Units (GPUs) are now a key component of many devices and systems, including those in the cloud and data centers, thus are also subject to side-channel attacks. Existing side-channel attacks on GPUs typically leak information from graphics libraries like OpenGL and CUDA, which require creating contentions within the GPU resource space and are being mitigated with software patches. This article evaluates potential side channels exposed at a lower-level interface between GPUs and CPUs, namely the graphics interrupts. These signals could indicate unique signatures of GPU workload, allowing a spy process to infer the behavior of other processes. We demonstrate the practicality and generality of such side-channel exploitation with a variety of assumed attack scenarios. Simulations on both Nvidia and Intel graphics adapters showed that our attack could achieve high accuracy, while in-depth studies were also presented to explore the low-level rationale behind such effectiveness. On top of that, we further propose a practical mitigation scheme which protects GPU workloads against the graphics-interrupt-based side-channel attack by piggybacking mask payloads on them to generate interfering graphics interrupt “noises”. Experiments show that our mitigation technique effectively prohibited spy processes from inferring user behaviors via analyzing runtime patterns of graphics interrupt with only trivial overhead.","1941-0018","","10.1109/TDSC.2021.3091159","National Research Foundation Singapore(grant numbers:AISG-100E-2018-004); National Key R&D Program of China(grant numbers:2018YFA0704703); National Natural Science Foundation of China(grant numbers:61972215,61972073); Natural Science Foundation of Tianjin City(grant numbers:20JCZDJC00640); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462555","Side-channel attacks;GPU;graphics interrupts;machine learning","Graphics processing units;Side-channel attacks;Payloads;Graphics;Engines;Streaming media;Linux","","5","","34","IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Online Failure Prediction for Complex Systems: Methodology and Case Studies","J. R. Campos; E. Costa; M. Vieira","University of Coimbra, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, Coimbra, Portugal; University of Coimbra, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, Coimbra, Portugal; University of Coimbra, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, Coimbra, Portugal","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","3520","3534","Online Failure Prediction (OFP) allows proactively taking countermeasures before a failure occurs, such as saving data or restarting a system. However, despite its potential contribution to improving dependability, OFP still presents key limitations. Besides the problem of choosing the optimal set of features, assessing predictive models is complex and common procedures for supporting comparison are not available. There is, in fact, little work on developing and assessing failure predictors for complex systems. In this aricle, we present two extensive case studies on distinct Operating Systems (OSs), Linux and Windows, showing that it is possible to create models that can predict different types of incoming failures, highlighting various important considerations such as the operational requirements of the target system. To drive the case studies, we define a well-structured framework for a fair and sound assessment and comparison of alternative predictive solutions. It includes scenarios for choosing the most adequate metrics for the assessment, comparing alternative models, and selecting the best predictor, while considering the need to tolerate perturbations in the data. In practice, we show that, by following a well-defined process, it is possible to develop accurate failure predictors and establish a ranking of the models under evaluation in different scenarios and OSs.","1941-0018","","10.1109/TDSC.2022.3192671","COMPETE 2020; FCT(grant numbers:POCI-01-0247-FEDER-045907); FCT; I.P./MCTES; National Funds(grant numbers:scope of CISUC R&D,UIDB/00326/2020,UIDP/00326/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9834129","Online failure prediction;reliability;availability;machine learning","Predictive models;Data models;Prediction algorithms;Benchmark testing;Robustness;Perturbation methods;Task analysis","","","","47","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"A Machine Learning-Based Intrusion Detection System for Securing Remote Desktop Connections to Electronic Flight Bag Servers","R. Bitton; A. Shabtai","Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel","IEEE Transactions on Dependable and Secure Computing","13 May 2021","2021","18","3","1164","1181","Remote desktop protocols (RDP) are commonly used for connecting and interacting with computers remotely. In this case, a server component runs on the remote computer and shares its desktop (i.e., screen) with the client component which runs on an end user device. In recent years, a number of vulnerabilities have been identified in two widely used remote desktop implementations, Microsoft Remote Desktop and RealVNC. These vulnerabilities may expose the remote server to a new attack vector. This concern is increased when it comes to a cyber-physical system (CPS) in which a client device with a low trust level connects to the critical system via the remote desktop server. In order to mitigate this risk, in this paper we propose a network based intrusion detection system (NIDS) specifically designed for securing the remote desktop connections. The propose method utilizes an innovative anomaly detection technique based on machine learning for detecting malicious TCP packets, which can carry exploits aimed at the RDP server. An empirical evaluation conducted on an avionic system setup consisting of a commercial tablet (Samsung Galaxy Tab) connected through a Virtual Network Computing (VNC) remote desktop implementation to a real electronic flight bag (EFB) server shows that the proposed method can detect malicious packets carrying real exploits (reported in recent years) with a true positive rate of 0.863 and a false positive rate of 0.0001.","1941-0018","","10.1109/TDSC.2019.2914035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703153","Anomaly detection;Network-based intrusion detection system (NIDS);machine learning;Remote desktop;Electronic flight bag","Servers;Protocols;Aerospace electronics;Anomaly detection;Authentication;Intrusion detection","","24","","37","IEEE","30 Apr 2019","","","IEEE","IEEE Journals"
"Efficient and Secure Outsourcing of Differentially Private Data Publishing With Multiple Evaluators","J. Li; H. Ye; T. Li; W. Wang; W. Lou; Y. T. Hou; J. Liu; R. Lu","Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; College of Cyber Science and the College of Computer Science, Nankai University, Tianjin, China; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China; Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","67","76","Since big data becomes a main impetus to the next generation of IT industry, data privacy has received considerable attention in recent years. To deal with the privacy challenges, differential privacy has been widely discussed and related private mechanisms are proposed as privacy-enhancing techniques. However, with today’s differential privacy techniques, it is difficult to generate a sanitized dataset that can suit every machine learning task. In order to adapt to various tasks and budgets, different kinds of privacy mechanisms have to be implemented, which inevitably incur enormous costs for computation and interaction. To this end, in this article, we propose two novel schemes for outsourcing differential privacy. The first scheme efficiently achieves outsourcing differential privacy by using our preprocessing method and secure building blocks. To support the queries from multiple evaluators, we give the second scheme that employs a trusted execution environment to aggregately implement privacy mechanisms on multiple queries. During data publishing, our proposed schemes allow providers to go off-line after uploading their datasets, so that they achieve a low communication cost which is one of the critical requirements for a practical system. Finally, we report an experimental evaluation on UCI datasets, which confirms the effectiveness of our schemes.","1941-0018","","10.1109/TDSC.2020.3015886","National Natural Science Foundation of China for Outstanding Youth Foundation(grant numbers:61722203); National Natural Science Foundation of China(grant numbers:U1936218); National Natural Science Foundation of China(grant numbers:61802078,61702125); Virginia Commonwealth Cyber Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164984","Differential privacy;cloud computing;outsourcing;encryption","Publishing;Outsourcing;Privacy;Cloud computing;Task analysis","","42","","44","IEEE","11 Aug 2020","","","IEEE","IEEE Journals"
"Anomaly Detection in Operating System Logs with Deep Learning-Based Sentiment Analysis","H. Studiawan; F. Sohel; C. Payne","Discipline of Information Technology, Media & Communications, Murdoch University, Perth, WA, Australia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2136","2148","The purpose of sentiment analysis is to detect an opinion or polarity in text data. We can apply such an analysis to detect negative sentiment, which represents the anomalous activities in operating system (OS) logs. Existing methods involve manual searching, predefined rules, or traditional machine learning techniques to detect such suspicious events. In this article, we propose a novel deep learning-based sentiment analysis technique to check whether there are anomalous activities in OS logs. Log messages are modeled as sentences and we identify the sentiments using the gated recurrent unit (GRU) networks. OS log datasets inherently have a class imbalance in the sense that the number of negative sentiment is much lower than that of the number of positive ones. In order to address the class imbalance, we build a GRU layer on top of a class imbalance solver using the Tomek link method. Experimental results demonstrate that the proposed method can detect anomalous events in OS logs with an overall F1 and accuracy of 99.84 and 99.93 percent, respectively.","1941-0018","","10.1109/TDSC.2020.3037903","Indonesia Lecturer Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9259061","Anomaly detection;sentiment analysis;deep learning;operating system logs;class imbalance","Sentiment analysis;Deep learning;Anomaly detection;Data models;Support vector machines;Operating systems;Social networking (online)","","23","","65","IEEE","13 Nov 2020","","","IEEE","IEEE Journals"
"Lifespan and Failures of SSDs and HDDs: Similarities, Differences, and Prediction Models","R. Pinciroli; L. Yang; J. Alter; E. Smirni","Computer Science Department, Gran Sasso Science Institute, L'Aquila, Italy; Computer Science Department, William and Mary, Williamsburg, VA, USA; Ipsos North America, Washington, DC, USA; Computer Science Department, William and Mary, Williamsburg, VA, USA","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","256","272","Data center downtime typically centers around IT equipment failure. Storage devices are the most frequently failing components in data centers. We present a comparative study of hard disk drives (HDDs) and solid state drives (SSDs) that constitute the typical storage in data centers. Using six-year field data of 100,000 HDDs of different models from the same manufacturer from the Backblaze dataset and six-year field data of 30,000 SSDs of three models from a Google data center, we characterize the workload conditions that lead to failures. We illustrate that their root failure causes differ from common expectations and that they remain difficult to discern. For the case of HDDs we observe that young and old drives do not present many differences in their failures. Instead, failures may be distinguished by discriminating drives based on the time spent for head positioning. For SSDs, we observe high levels of infant mortality and characterize the differences between infant and non-infant failures. We develop several machine learning failure prediction models that are shown to be surprisingly accurate, achieving high recall and low false positive rates. These models are used beyond simple prediction as they aid us to untangle the complex interaction of workload characteristics that lead to failures and identify failure root causes from monitored symptoms.","1941-0018","","10.1109/TDSC.2021.3131571","National Science Foundation(grant numbers:CCF-1717532,CCF-1649087,IIS-1838022); CoVA CCI(grant numbers:C-Q122-WM-02); MIUR PRIN Project SEDUCE 2017TWRCNB; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629263","Supervised learning;classification;data centers;storage devices;SSD;HDD","Data centers;Data models;Predictive models;Drives;Temperature measurement;Temperature distribution;Magnetic heads","","3","","55","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Evaluating Explanation Methods for Deep Learning in Security","A. Warnecke; D. Arp; C. Wressnegger; K. Rieck","Technische Universität Braunschweig, Germany; Technische Universität Braunschweig, Germany; Karlsruhe Institute of Technology, Germany; Technische Universität Braunschweig, Germany","2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","158","174","Deep learning is increasingly used as a building block of security systems. Unfortunately, neural networks are hard to interpret and typically opaque to the practitioner. The machine learning community has started to address this problem by developing methods for explaining the predictions of neural networks. While several of these approaches have been successfully applied in the area of computer vision, their application in security has received little attention so far. It is an open question which explanation methods are appropriate for computer security and what requirements they need to satisfy. In this paper, we introduce criteria for comparing and evaluating explanation methods in the context of computer security. These cover general properties, such as the accuracy of explanations, as well as security-focused aspects, such as the completeness, efficiency, and robustness. Based on our criteria, we investigate six popular explanation methods and assess their utility in security systems for malware detection and vulnerability discovery. We observe significant differences between the methods and build on these to derive general recommendations for selecting and applying explanation methods in computer security.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00018","German Federal Ministry of Education and Research (BMBF)(grant numbers:FKZ 16KIS0534); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230374","AI based security or privacy enhancing tools;Security of AI","Deep learning;Computer vision;Neural networks;Training data;Robustness;Malware;Computer security","","42","","57","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"A Siamese Adversarial Anonymizer for Data Minimization in Biometric Applications","G. Garofalo; T. Van hamme; D. Preuveneers; W. Joosen","imec-DistriNet, KU Leuven, Heverlee, Belgium; imec-DistriNet, KU Leuven, Heverlee, Belgium; imec-DistriNet, KU Leuven, Heverlee, Belgium; imec-DistriNet, KU Leuven, Heverlee, Belgium","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","334","343","A wealth of sensors measure our day to day activities. Sharing these biometric data with cloud-enabled applications provides users with useful insights about their health status. However, due to their high re-identification potential, granting medical and fitness applications access to sensors data raises serious privacy concerns.In this work, we aim to find a data representation that sup-presses identity while retaining utility, i.e. data minimization. To this end, we define a component that is plugged into a machine learning pipeline and trained following a min-max adversarial optimization strategy. By using our component during training, we limit the need to rely on third parties and, therefore, reduce the risks of leakages and unintended processing. We evaluate privacy vs. utility trade-off on a real-world sharing scenario involving motion data, i.e. accelerometer and gyroscope measurements. Privacy is estimated by analyzing if an honest-but-curious attacker can effectively link two traces to the same user. In our experiments, we observe a decrease in test verification accuracy, i.e. the identifiability of the users, from 85% to 57%. This leaves an attacker with a representation that he can match only slightly better compared to a random guess. On top of that, the privatized gait representation is accompanied by an increase in performance w.r.t. the main tasks.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229760","Anonymization;Adversarial Learning;Mobile Sensors;Gait;Soft biometrics","Feature extraction;Biometrics (access control);Privacy;Task analysis;Sensors;Training;Optimization","","1","","30","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Industrial Control System Network Intrusion Detection by Telemetry Analysis","S. Ponomarev; T. Atkison","College of Engineering and Science, Louisiana Tech University, Ruston, LA; Cyber Engineering and Computer Science Department, Louisiana Tech University, Ruston, LA","IEEE Transactions on Dependable and Secure Computing","10 Mar 2016","2016","13","2","252","260","Until recently, industrial control systems (ICSs) used “air-gap” security measures, where every node of the ICS network was isolated from other networks, including the Internet, by a physical disconnect. Attaching ICS networks to the Internet benefits companies and engineers who use them. However, as these systems were designed for use in the air-gapped security environment, protocols used by ICSs contain little to no security features and are vulnerable to various attacks. This paper proposes an approach to detect the intrusions into network attached ICSs by measuring and verifying data that is transmitted through the network but is not inherently the data used by the transmission protocol-network telemetry. Using simulated PLC units, the developed IDS was able to achieve 94.3 percent accuracy when differentiating between machines of an attacker and engineer on the same network, and 99.5 percent accuracy when differentiating between attacker and engineer on the Internet.","1941-0018","","10.1109/TDSC.2015.2443793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122336","Networked control systems;Nonlinear network analysis;Control systems;Intrusion detection;Telemetry;Networked control systems;nonlinear network analysis;control systems;intrusion detection;telemetry","Telemetry;Protocols;SCADA systems;Security;Delays;Internet","","113","","40","IEEE","11 Jun 2015","","","IEEE","IEEE Journals"
"EnsembleHMD: Accurate Hardware Malware Detectors with Specialized Ensemble Classifiers","K. N. Khasawneh; M. Ozsoy; C. Donovick; N. Abu-Ghazaleh; D. Ponomarev","Department of Computer Science and Engineering, and Departments of Electronic Engineering, University of California, Riverside; Security and Privacy Lab., Intel Corp., Hillsboro; Department of Computer Science, Stanford University, Stanford; Department of Computer Science and Engineering, and Departments of Electronic Engineering, University of California, Riverside; Department of Computer Science, Binghamton University, Binghamton","IEEE Transactions on Dependable and Secure Computing","29 Apr 2020","2020","17","3","620","633","Hardware-based malware detectors (HMDs) are a promising new approach to defend against malware. HMDs collect low-level architectural features and use them to classify malware from normal programs. With simple hardware support, HMDs can be always on, operating as a first line of defense that prioritizes the application of more expensive and more accurate software-detector. In this paper, our goal is to increase the accuracy of HMDs, to improve detection, and reduce overhead. We use specialized detectors targeted towards a specific type of malware to improve the detection of each type. Next, we use ensemble learning techniques to improve the overall accuracy by combining detectors. We explore detectors based on logistic regression (LR) and neural networks (NN). The proposed detectors reduce the false-positive rate by more than half compared to using a single detector, while increasing their sensitivity. We develop metrics to estimate detection overhead; the proposed detectors achieve more than 16.6× overhead reduction during online detection compared to an idealized software-only detector, with an 8× improvement in relative detection time. NN detectors outperform LR detectors in accuracy, overhead (by 40 percent), and time-to-detection of the hardware component (by 5×). Finally, we characterize the hardware complexity by extending an open-core and synthesizing it on an FPGA platform, showing that the overhead is minimal.","1941-0018","","10.1109/TDSC.2018.2801858","National Science Foundation(grant numbers:CNS-1422401,CNS-1619322,CNS-1617915); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8280556","Malware detection;specialized detectors;ensemble learning;architecture;security","Detectors;Malware;Hardware;Artificial neural networks;Training;Feature extraction","","27","","55","IEEE","5 Feb 2018","","","IEEE","IEEE Journals"
"Towards an Interpretable Autoencoder: A Decision-Tree-Based Autoencoder and its Application in Anomaly Detection","D. L. Aguilar; M. A. Medina-Pérez; O. Loyola-González; K. -K. R. Choo; E. Bucheli-Susarrey","Tecnologico de Monterrey, Atizapán, Estado de México, México; Altair Management Consultants, Madrid, Spain; Altair Management Consultants, Madrid, Spain; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA; Tecnologico de Monterrey, Atizapán, Estado de México, México","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1048","1059","The importance of understanding and explaining the associated classification results in the utilization of artificial intelligence (AI) in many different practical applications (e.g., cyber security and forensics) has contributed to the trend of moving away from black-box / opaque AI towards explainable AI (XAI). In this article, we propose the first interpretable autoencoder based on decision trees, which is designed to handle categorical data without the need to transform the data representation. Furthermore, our proposed interpretable autoencoder provides a natural explanation for experts in the application area. The experimental findings show that our proposed interpretable autoencoder is among the top-ranked anomaly detection algorithms, along with one-class Support Vector Machine (SVM) and Gaussian Mixture. More specifically, our proposal is on average 2% below the best Area Under the Curve (AUC) result and 3% over the other Average Precision scores, in comparison to One-class SVM, Isolation Forest, Local Outlier Factor, Elliptic Envelope, Gaussian Mixture Model, and eForest.","1941-0018","","10.1109/TDSC.2022.3148331","National Council of Science and Technology of Mexico; National Science Foundation(grant numbers:HRD-1736209); National Security Agency(grant numbers:H98230-21-1-0171,H98230-20-1-0392); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705133","Interpretable artificial intelligence;autoencoder;decision tree;anomaly detection;explainable artificial intelligence (XAI)","Decision trees;Encoding;Decoding;Anomaly detection;Neural networks;Computer architecture;Computational modeling","","15","","56","IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"Automatic Extraction of Access Control Policies from Natural Language Documents","M. Narouei; H. Takabi; R. Nielsen","Department of Computer Sciene and Engineering, University of North Texas, Denton; Department of Computer Sciene and Engineering, University of North Texas, Denton; Department of Computer Sciene and Engineering, University of North Texas, Denton","IEEE Transactions on Dependable and Secure Computing","29 Apr 2020","2020","17","3","506","517","A fundamental management responsibility is securing information systems. Almost all applications that deal with safety, privacy, or defense include some form of access control. There are a plethora of access control models in the information security realm such as role-based access control and attribute-based access control. However, the initial development of access control policies (ACPs) can be very challenging. Most organizations have high-level requirement specifications that include a set of ACPs, which describe allowable operations of the system. It is time consuming and error-prone to manually sift through these documents and extract ACPs. In this paper, we propose a new framework towards extracting ACPs from unrestricted natural language documents using semantic role labeling (SRL). We were able to correctly identify ACP elements with an average F1 score of 75 percent, which bested the previous work by 15 percent. Furthermore, as SRL tools are often trained on publicly available corpora such as Wall Street Journal, we investigated the idea of improving SRL performance using domain-related knowledge. We utilized domain adaptation and semi-supervised learning techniques and were able to improve the SRL performance by 2 percent using only a small amount of access control data.","1941-0018","","10.1109/TDSC.2018.2818708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323229","Access control policy;policy engineering;semantic role labeling;domain adaptation;semi-supervised learning;natural language processing;transfer learning","Access control;Natural languages;Privacy;Semantics;Organizations;Tools;Permission","","13","","40","IEEE","23 Mar 2018","","","IEEE","IEEE Journals"
"AppScanner: Automatic Fingerprinting of Smartphone Apps from Encrypted Network Traffic","V. F. Taylor; R. Spolaor; M. Conti; I. Martinovic","Department of Computer Science, University of Oxford, Oxford, United Kingdom; Department of Mathematics, University of Padua, Padua, Italy; Department of Mathematics, University of Padua, Padua, Italy; Department of Computer Science, University of Oxford, Oxford, United Kingdom","2016 IEEE European Symposium on Security and Privacy (EuroS&P)","12 May 2016","2016","","","439","454","Automatic fingerprinting and identification of smartphone apps is becoming a very attractive data gathering technique for adversaries, network administrators, investigators and marketing agencies. In fact, the list of apps installed on a device can be used to identify vulnerable apps for an attacker to exploit, uncover a victim's use of sensitive apps, assist network planning, and aid marketing. However, app fingerprinting is complicated by the vast number of apps available for download, the wide range of devices they may be installed on, and the use of payload encryption protocols such as HTTPS/TLS. In this paper, we present a novel methodology and a framework implementing it, called AppScanner, for the automatic fingerprinting and real-time identification of Android apps from their encrypted network traffic. To build app fingerprints, we run apps automatically on a physical device to collect their network traces. We apply various processing strategies to these network traces before extracting the features that are used to train our supervised learning algorithms. Our fingerprint generation methodology is highly scalable and does not rely on inspecting packet payloads, thus our framework works even when HTTPS/TLS is employed. We built and deployed this lightweight framework and ran a thorough set of experiments to assess its performance. We automatically profiled 110 of the most popular apps in the Google Play Store and were later able to re-identify them with more than 99% accuracy.","","978-1-5090-1752-2","10.1109/EuroSP.2016.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467370","","IP networks;Cryptography;Web pages;Payloads;Feature extraction;Mobile communication;Ports (Computers)","","186","1","25","IEEE","12 May 2016","","","IEEE","IEEE Conferences"
"DroidEvolver: Self-Evolving Android Malware Detection System","K. Xu; Y. Li; R. Deng; K. Chen; J. Xu","School of Information Systems, Singapore Management University; School of Information Systems, Singapore Management University; School of Information Systems, Singapore Management University; School of Cyber Security, University of Chinese Academy of Sciences; School of Information Systems, Singapore Management University","2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","47","62","Given the frequent changes in the Android framework and the continuous evolution of Android malware, it is challenging to detect malware over time in an effective and scalable manner. To address this challenge, we propose DroidEvolver, an Android malware detection system that can automatically and continually update itself during malware detection without any human involvement. While most existing malware detection systems can be updated by retraining on new applications with true labels, DroidEvolver requires neither retraining nor true labels to update itself, mainly due to the insight that DroidEvolver makes necessary and lightweight update using online learning techniques with evolving feature set and pseudo labels. The detection performance of DroidEvolver is evaluated on a dataset of 33,294 benign applications and 34,722 malicious applications developed over a period of six years. Using 6,286 applications dated in 2011 as the initial training set, DroidEvolver achieves high detection F-measure (95.27%), which only declines by 1.06% on average per year over the next five years for classifying 57,539 newly appeared applications. Note that such new applications could use new techniques and new APIs, which are not known to DroidEvolver when initialized with 2011 applications. Compared with the state-of-the-art overtime malware detection system MAMADROID, the F-measure of DroidEvolver is 2.19 times higher on average (10.21 times higher for the fifth year), and the efficiency of DroidEvolver is 28.58 times higher than MAMADROID during malware detection. DroidEvolver is also shown robust against typical code obfuscation techniques.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806731","Android Security;Malware Detection Overtime","Feature extraction;Malware;Aging;Training;Adaptation models;Manuals;Labeling","","75","","45","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Mining ABAC Rules from Sparse Logs","C. Cotrini; T. Weghorn; D. Basin","Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland","2018 IEEE European Symposium on Security and Privacy (EuroS&P)","9 Jul 2018","2018","","","31","46","Different methods have been proposed to mine attribute-based access control (ABAC) rules from logs. In practice, these logs are sparse in that they contain only a fraction of all possible requests. However, for sparse logs, existing methods mine and validate overly permissive rules, enabling privilege abuse. We define a novel measure, reliability, that quantifies how overly permissive a rule is and we show why other standard measures like confidence and entropy fail in quantifying overpermissiveness. We build upon state-of-the-art subgroup discovery algorithms and our new reliability measure to design Rhapsody, the first ABAC mining algorithm with correctness guarantees: Rhapsody mines a rule if and only if the rule covers a significant number of requests, its reliability is above a given threshold, and there is no equivalent shorter rule. We evaluate Rhapsody on different real-world scenarios using logs from Amazon and a computer lab at ETH Zurich. Our results show that Rhapsody generalizes better and produces substantially smaller rules than competing approaches.","","978-1-5386-4228-3","10.1109/EuroSP.2018.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406589","Access control;ABAC;Mining;Authorization","Reliability;Standards;Organizations;Data mining;Authorization","","33","","41","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Explainable Security","L. Viganò; D. Magazzeni","Department of Informatics, King’s College London, London, UK; Department of Informatics, King’s College London, London, UK","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","293","300","In 2017, the Defense Advanced Research Projects Agency (DARPA) launched the Explainable Artificial Intelligence (XAI) program that aims to create a suite of new AI techniques that enable end users to understand, appropriately trust, and effectively manage the emerging generation of AI systems. In this paper, inspired by DARPA's XAI program, we propose a new paradigm in security research: Explainable Security (XSec). We discuss the ""Six Ws"" of XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and complex characteristics: XSec involves several different stakeholders (i.e., the system's developers, analysts, users and attackers) and is multi-faceted by nature (as it requires reasoning about system model, threat model and properties of security, privacy and trust as well as concrete attacks, vulnerabilities and countermeasures). We define a roadmap for XSec that identifies several possible research directions.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229719","Security paradigm;Security model;Threat model;Security properties;Privacy;Trust;Usable security;Security economics;Gamification","Security;Authentication;Artificial intelligence;Stakeholders;Privacy;Password;Face recognition","","31","","49","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"Malware Detection Using 1-Dimensional Convolutional Neural Networks","A. Sharma; P. Malacaria; M. Khouzani","School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","247","256","This work introduces a highly accurate and efficient malware detection system based on 1-dimensional convolutional neural networks. The system takes as input a binary file and classifies it as malicious or benign. There is minimal pre-processing of the binaries, with features discovery left to the network during training. A crucial difference with other convolutional neural networks (CNN) based approaches is the use of 1-dimensional convolutions; this methodological choice is shown to have significant positive consequences for the detector. In order to compare the detector with state-of-the-art techniques a TF-IDF based benchmark malware detector is also implemented: experiments show an improved accuracy of the proposed CNN detector while maintaining similar training times. The system is also compared, on a publicly available dataset of 11130 binaries, with an existing embedding based CNN detector. The proposed system outperforms, both in accuracy and training time the embedding based CNN.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802378","malware-detection;convolutionary-neuronal-networks;deep-learning","","","18","","28","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Inferring OpenVPN State Machines Using Protocol State Fuzzing","L. -A. Daniel; E. Poll; J. de Ruiter","University of Rennes, Rennes; Radboud University; Radboud University","2018 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","9 Jul 2018","2018","","","11","19","The reliability of a security protocol is of the utmost importance but can easily be compromised by a vulnerability in the implementation. A crucial aspect of an implementation is the protocol's state machine. The state machine of an implementation can be inferred by black box testing using regular inference. These inferred state machines provide a good insight into implementations and can be used to detect any spurious behavior. We apply this technique to different implementations of OpenVPN: the standard OpenVPN and the OpenVPN-NL implementations. Although OpenVPN is a widely used TLS-based VPN solution, there is no official specification of the protocol, which makes it a particularly interesting target to analyze. We infer state machines of the server-side implementation and focus on particular phases of the protocol. Finally we analyze those state machines, show that they can reveal a lot of information about the implementation which is missing from the documentation, and discuss the possibility to include state machines in a formal specification.","","978-1-5386-5445-3","10.1109/EuroSPW.2018.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406556","OpenVPN;Protocol state fuzzing;LearnLib;Regular inference","Protocols;Servers;Security;Reliability;Tunneling;Fuzzing;Virtual private networks","","11","","30","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"REHAD: Using Low-Frequency Reconfigurable Hardware for Cache Side-Channel Attacks Detection","Y. Mao; V. Migliore; V. Nicomette","LAAS-CNRS, Université de Toulouse, CNRS, INSA, Toulouse, France; LAAS-CNRS, Université de Toulouse, CNRS, INSA, Toulouse, France; LAAS-CNRS, Université de Toulouse, CNRS, INSA, Toulouse, France","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","704","709","Cache side-channel attacks consist, for a malicious process, to infer the current state of the cache by measuring the time it takes to access the memory, and indirectly gain knowledge about other processes sharing this same physical cache. Because cache side-channel attacks leverage a hardware leakage without requiring any physical access to the devices, they represent very serious threats. Among the runtime detection techniques for cache side-channel attacks, hardware solutions are usually fine-grained and benefit from less performance overhead than software solutions. However, they are not flexible enough to suit the rapid evolution and appearance of software attacks. In this paper we describe REHAD, a novel attack detection architecture that uses reconfigurable hardware. More precisely, it includes a hardware detection module that can be reconfigured by means of a trusted software kernel, to adapt to the level of threats and to detect new attacks. This architecture also benefits from hardware parallelism to fill the frequency gap between reconfigurable hardware and core processor. REHAD has been integrated into the ORCA softcore RISCV on a FPGA and two common cache side-channel attacks have been successfully detected.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229737","Reconfigurable architectures;intrusion detection;microarchitectural timing attacks;RISC-V","Hardware;Software;Monitoring;Computer architecture;Clocks;Side-channel attacks;Field programmable gate arrays","","5","","16","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"The Impact of Adverse Events in Darknet Markets: an Anomaly Detection Approach","Z. Ursani; C. Peersman; M. Edwards; C. Chen; A. Rashid","Department of Computer Science, University of Bristol, Bristol, United Kingdom; Department of Computer Science, University of Bristol, Bristol, United Kingdom; Department of Computer Science, University of Bristol, Bristol, United Kingdom; Department of Computer Science, University of Bristol, Bristol, United Kingdom; Department of Computer Science, University of Bristol, Bristol, United Kingdom","2021 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","29 Oct 2021","2021","","","227","238","In this paper, the notion of anomaly detection is introduced for the ﬁrst time in the area of darknet markets (DNMs). Our hypothesis is that like popular social media platforms DNMs also exhibit anomalous behaviour. However, we propose that the meaning of anomalies in DNMs differs from social media anomalies. The social media anomalies are a cause of threat to the real world, while DNM anomalies are caused by threats from the real world. We present an unsupervised learning method developed to detect anomalies. The model is based on a weighted sum of a feature set trained through an evolutionary algorithm. Our approach successfully identifies anomalies in 35 DNMs – both at the community level and at the level of its user types. Our analysis shows that most of the anomalies found align with well-known adverse events–either as a direct consequence or as a cascading effect of the root event. Moreover, the model identified additional anomalies, which we were able to link to other events through post hoc analysis. Furthermore, we show that the adverse event of market shutdown generates a two-pronged impact on the ecosystem, i.e., it not only triggers startups of new markets but it does also inflict anomalies to current markets which may become fatal in some cases. We conclude that this two-pronged impact can be exploited by law enforcement agencies to produce maximum disruption in DNMs.","2768-0657","978-1-6654-1012-0","10.1109/EuroSPW54576.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583702","Darknet Markets;Anomaly Detection;Adverse Events;Unsupervised Learning;Evolutionary Algorithm","Analytical models;Social networking (online);Law enforcement;Ecosystems;Evolutionary computation;Anomaly detection;Unsupervised learning","","3","","51","IEEE","29 Oct 2021","","","IEEE","IEEE Conferences"
"Defensive Programming for Smart Home Cybersecurity","M. T. Rossi; R. Greca; L. Iovino; G. Giacinto; A. Bertoli","Gran Sasso Science Institute, L’Aquila, Italy; Gran Sasso Science Institute, L’Aquila, Italy; Gran Sasso Science Institute, L’Aquila, Italy; University of Cagliari, Cagliari, Italy; ISTI - CNR, Pisa, Italy","2020 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","22 Oct 2020","2020","","","600","605","Cybersecurity has become a real issue in the development of smart services in the smart home domain, which is formed by a System of Systems where several smart objects are connected to each other and to the Internet. However, these connections expose the devices to possible attackers inside or outside the network, who may exploit software or hardware vulnerabilities to achieve malicious goals. To alleviate this issue, the use of defensive programming assertions can allow the behaviour of smart objects to be monitored and checked for correctness. Furthermore, open source intelligence tools, such as the Shodan search engine, provide features that could be leveraged to detect potential vulnerabilities. In this paper, we propose an approach for the monitoring of Systems of Systems in the smart home domain exploiting the defensive programming paradigm in combination with Shodan APIs.","","978-1-7281-8597-2","10.1109/EuroSPW51379.2020.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229690","Cybersecurity;Smart home;SoS;Shodan;Defensive Programming","Smart homes;Smart meters;Programming;Monitoring;Production;IP networks;Hardware","","2","","26","IEEE","22 Oct 2020","","","IEEE","IEEE Conferences"
"NN-EMD: Efficiently Training Neural Networks Using Encrypted Multi-Sourced Datasets","R. Xu; J. Joshi; C. Li","IBM Research, San Jose, CA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2807","2820","Training complex neural network models using third-party cloud-based infrastructure among multiple data sources is a promising approach among existing machine learning solutions. However, privacy concerns of large-scale data collections and recent regulations have restricted the availability and use of privacy sensitive data in the third-party infrastructure. To address such privacy issues, a promising emerging approach is to train a neural network model over an encrypted dataset. Specifically, the model training process can be outsourced to a third party such as a cloud service that is backed by significant computing power, while the encrypted training data keeps the data confidential from the third party. Compared to training a traditional machine learning model over encrypted data, however, it is extremely challenging to train a deep neural network (DNN) model over encrypted data for two reasons: first, it requires large-scale computation over huge datasets; second, the existing solutions for computation over encrypted data, such as using homomorphic encryption, is inefficient. Further, for enhanced performance of a DNN model, we also need to use huge training datasets composed of data from multiple data sources that may not have pre-established trust relationships among each other. We propose a novel framework, NN-EMD, to train DNN over encrypted multiple datasets collected from multiple sources. Toward this, we propose a set of secure computation protocols using hybrid functional encryption schemes. We evaluate our framework for performance with regards to the training time and model accuracy on the MNIST datasets. We show that compared to other existing frameworks, our proposed NN-EMD framework can significantly reduce the training time, while providing comparable model accuracy and privacy guarantees as well as supporting multiple data sources. Furthermore, the depth and complexity of neural networks do not affect the training time despite introducing a privacy-preserving NN-EMD setting.","1941-0018","","10.1109/TDSC.2021.3074439","Fundamental Research Funds for the Central Universities(grant numbers:2019RC038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409653","Secure computation;neural networks;deep learning;privacy-preserving;functional encryption","Training;Artificial neural networks;Computational modeling;Data privacy;Encryption;Data models;Protocols","","14","","60","IEEE","20 Apr 2021","","","IEEE","IEEE Journals"
"RevFRF: Enabling Cross-Domain Random Forest Training With Revocable Federated Learning","Y. Liu; Z. Ma; Y. Yang; X. Liu; J. Ma; K. Ren","School of Cyber Engineering, Xidian University, Xi'an, China; School of Cyber Engineering, Xidian University, Xi'an, China; School of Cyber Engineering, Xidian University, Xi'an, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; School of Cyber Engineering, Xidian University, Xi'an, China; Institute of Cyberspace Research, Zhejiang University, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","3671","3685","Random forest is one of the most heated machine learning tools in a wide range of industrial scenarios. Recently, federated learning enables efficient distributed machine learning without direct revealing of private participant data. In this article, we present a novel framework of federated random forest (RevFRF), and further emphatically discuss the participant revocation problem of federated learning based on RevFRF. Specifically, RevFRF first introduces a suite of homomorphic encryption based secure protocols to implement federated random forest (RF). The protocols cover the whole lifecycle of an RF model, including construction, prediction and participant revocation. Then, referring to the practical application scenarios of RevFRF, the existing federated learning frameworks ignore a fact that even every participant in federated learning cannot maintain the cooperation with others forever. In company-level cooperation, allowing the remaining companies to use a trained model that contains the memories from an off-lying company potentially leads to a significant conflict of interest. Therefore, we propose the revocable federated learning concept and illustrate how RevFRF implements participant revocation in applications. Through theoretical analysis and experiments, we show that the protocols can efficiently implement federated RF and ensure the memories of a revoked participant in the trained RF to be securely removed.","1941-0018","","10.1109/TDSC.2021.3104842","National Natural Science Foundation of China(grant numbers:61872283,U1764263,62072109,U1804263); CNKLSTISS through the China 111(grant numbers:B16037); Shaanxi Science & Technology Coordination & Innovation Project(grant numbers:2016TZCG-6-3); Key R&D Program of Shaanxi Province(grant numbers:2019ZDLGY12-04,2020ZDLGY09-06); Natural Science Basic Research Program of Shaanxi(grant numbers:2021JC-22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9514457","Privacy-preserving;random forest;revocable federated learning","Radio frequency;Collaborative work;Companies;Data models;Servers;Privacy;Data privacy","","11","","43","IEEE","16 Aug 2021","","","IEEE","IEEE Journals"
"Verifiable Homomorphic Secret Sharing for Low Degree Polynomials","X. Chen; L. F. Zhang; J. Liu","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","2882","2895","An $(n,m,t)$(n,m,t)-homomorphic secret sharing (HSS) scheme for a function family $\mathcal F$F allows $n$n clients to share their data $x_{1}, \ldots,x_{n}$x1,...,xn among $m$m servers and then distribute the computation of any function $f\in {\mathcal F}$f∈F to the servers such that: (i) any $t$t colluding servers learn no information about the data; (ii) each server is able to compute a partial result and $f(x_{1}, \ldots,x_{n})$f(x1,...,xn) can be reconstructed from the servers’ partial results. HSS schemes cannot guarantee correct reconstruction, if some servers are malicious and provide wrong partial results. Recently, verifiable HSS (VHSS) has been introduced to achieve an additional property: (iii) any $t$t colluding servers cannot persuade the client(s) to accept their partial results and reconstruct a wrong value. The property (iii) is usually achieved by the client verifying the servers’ partial results. A VHSS scheme is compact if the verification is substantially faster than locally computing $f(x_{1},\ldots,x_{n})$f(x1,...,xn). Of the existing VHSS schemes for polynomials, some are not compact; the others are compact but impose very heavy workload on the servers, even for low degree polynomials (e.g., they are at least 4000× slower than the existing HSS schemes in order to evaluate polynomials of degree $\leq 5$≤5, which have many applications such as privacy-preserving machine learning). In this paper, we propose both a single-client VHSS (SVHSS) model and a multi-client VHSS (MVHSS) model. Our SVHSS allows a client to use a secret key to share its data among servers; our MVHSS allows multiple clients to share their data with a public key. For any integers $m,t>0$m,t>0, we constructed both an $(m,t)$(m,t)-SVHSS scheme and an $(m,t)$(m,t)-MVHSS scheme that satisfy the properties of (i)-(iii). Our constructions are based on level-$k$k homomorphic encryptions. The $(m,t)$(m,t)-SVHSS and $(m,t)$(m,t)-MVHSS are compact and allow the computations of degree-$d$d polynomials for $d\leq ((k+1)m-1)/t$d≤((k+1)m-1)/t and $d\leq ((k+1)(m-t)-1)/t$d≤((k+1)(m-t)-1)/t, respectively. Experiments show that our schemes are much more efficient than the existing compact VHSS for low degree polynomials. For example, to compute polynomials of degree $\leq 5$≤5, our MVHSS scheme is at least 420× faster. By applying SVHSS and MVHSS, we may add verifiability to privacy-preserving machine learning (PPML) algorithms. Experiments show that the resulting schemes are at least 52× and 20× faster than the existing verifiable PPML schemes.","1941-0018","","10.1109/TDSC.2022.3194321","Natural Science Foundation of Shanghai(grant numbers:21ZR1443000); Ministry of Education - Singapore(grant numbers:RG12/19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842386","Homomorphic secret sharing;verifiability;homomorphic encryption;privacy","Servers;Cryptography;Genomics;Bioinformatics;Computational modeling;Data models;Data privacy","","3","","53","IEEE","27 Jul 2022","","","IEEE","IEEE Journals"
"A New Facial Authentication Pitfall and Remedy in Web Services","D. Cole; S. Newman; D. Lin","EECS Department, University of Missouri, Columbia, MO, USA; EECS Department, University of Missouri, Columbia, MO, USA; EECS Department, University of Missouri, Columbia, MO, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2635","2647","Facial authentication has become more and more popular on personal devices. Due to the ease of use, it has great potential to be widely deployed for web-service authentication in the near future whereby people can easily log on to online accounts from different devices without memorizing lengthy passwords. However, the growing number of attacks on machine learning especially the Deep Neural Networks (DNN) which is commonly used for facial recognition, imposes big challenges on the successful roll-out of such web-service face authentication. Although there have been studies on defending some machine learning attacks, we are not aware of any specific effort devoted to the web-service facial authentication setting. In this article, we first demonstrate a new data poisoning attack that does not require to have any knowledge of the server-side and just needs a handful of malicious photo injections to enable an attacker to easily impersonate the victim in the existing facial authentication systems. We then propose a novel defensive approach called DEFEAT that leverages deep learning techniques to automatically detect such attacks. We have conducted extensive experiments on real datasets and our experimental results show that our defensive approach achieves more than 90 percent detection accuracy.","1941-0018","","10.1109/TDSC.2021.3067794","National Science Foundation(grant numbers:DGE-1946619,CNS-2027398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382920","Deep learning;facial recognition;security;data poisoning","Authentication;Face recognition;Web services;Training;Perturbation methods;Data models;Password","","1","","64","IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability","H. Aghakhani; D. Meng; Y. -X. Wang; C. Kruegel; G. Vigna","University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara","2021 IEEE European Symposium on Security and Privacy (EuroS&P)","4 Nov 2021","2021","","","159","178","A recent source of concern for the security of neural networks is the emergence of clean-label dataset poisoning attacks, wherein correctly labeled poison samples are injected into the training dataset. While these poison samples look legitimate to the human observer, they contain malicious characteristics that trigger a targeted misclassification during inference. We propose a scalable and transferable clean-label poisoning attack against transfer learning, which creates poison images with their center close to the target image in the feature space. Our attack, Bullseye Polytope, improves the attack success rate of the current state-of-the-art by 26.75% in end-to-end transfer learning, while increasing attack speed by a factor of 12. We further extend Bullseye Polytope to a more practical attack model by including multiple images of the same object (e.g., from different angles) when crafting the poison samples. We demonstrate that this extension improves attack transferability by over 16% to unseen images (of the same object) without using extra poison samples.","","978-1-6654-1491-3","10.1109/EuroSP51992.2021.00021","DARPA(grant numbers:FA8750-19-C-0003,HR0011-18-C-0060); Intel Corp; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9581207","Dataset Poisoning;Machine Learning Robustness","Training;Toxicology;Transfer learning;Neural networks;Observers;Robustness;Security","","20","","50","IEEE","4 Nov 2021","","","IEEE","IEEE Conferences"
"DLA: Dense-Layer-Analysis for Adversarial Example Detection","P. Sperl; C. -Y. Kao; P. Chen; X. Lei; K. Böttinger","Fraunhofer AISEC, Germany; Fraunhofer AISEC, Germany; Fraunhofer AISEC, Germany; Fraunhofer AISEC, Germany; Fraunhofer AISEC, Germany","2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","198","215","In recent years Deep Neural Networks (DNNs) have achieved remarkable results and even showed superhuman capabilities in a broad range of domains. This led people to trust in DNN classifications even in security-sensitive environments like autonomous driving. Despite their impressive achievements, DNNs are known to be vulnerable to adversarial examples. Such inputs contain small perturbations to intentionally fool the attacked model. In this paper, we present a novel end-to-end framework to detect such attacks without influencing the target model's performance. Inspired by research in neuron-coverage guided testing we show that dense layers of DNNs carry security-sensitive information. With a secondary DNN we analyze the activation patterns of the dense layers during classification run-time, which enables effective and real-time detection of adversarial examples. Our prototype implementation successfully detects adversarial examples in image, natural language, and audio processing. Thereby, we cover a variety of target DNN architectures. In addition to effectively defending against state-of-the-art attacks, our approach generalizes between different sets of adversarial examples. Our experiments indicate that we are able to detect future, yet unknown, attacks. Finally, during white-box adaptive attacks, we show our method cannot be easily bypassed.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230412","Deep Learning;Adversarial Machine Learning;Neural Network Security","Training;Adaptation models;Perturbation methods;Neurons;Natural languages;Prototypes;Network security","","16","","77","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating","G. Lovisotto; S. Eberz; I. Martinovic",University of Oxford; University of Oxford; University of Oxford,"2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","184","197","In this work, we investigate the concept of biometric backdoors: a template poisoning attack on biometric systems that allows adversaries to stealthily and effortlessly impersonate users in the long-term by exploiting the template update procedure. We show that such attacks can be carried out even by attackers with physical limitations (no digital access to the sensor) and zero knowledge of training data (they know neither decision boundaries nor user template). Based on the adversaries' own templates, they craft several intermediate samples that incrementally bridge the distance between their own template and the legitimate user's. As these adversarial samples are added to the template, the attacker is eventually accepted alongside the legitimate user. To avoid detection, we design the attack to minimize the number of rejected samples. We design our method to cope with weak assumptions for the attacker and we evaluate the effectiveness of this approach on state-of-the-art face recognition pipelines based on deep neural networks. We find that in white-box scenarios, adversaries can successfully carry out the attack in over 70 % of cases with less than ten injection attempts. Even in black-box scenarios, we find that exploiting the transferability of adversarial samples from surrogate models can lead to successful attacks in around 15 % of cases. Finally, we design a poisoning detection technique that leverages the consistent directionality of template updates in feature space to discriminate between legitimate and malicious updates. We evaluate such a countermeasure with a set of intra-user variability factors which may present the same directionality characteristics, obtaining equal error rates for the detection between 7-14% and leading to over 99% of attacks being detected after only two sample injections. We design our method to cope with weak assumptions for the attacker and we evaluate the effectiveness of this approach on state-of-the-art face recognition pipelines based on deep neural networks. We find that in white-box scenarios, adversaries can successfully carry out the attack in over 70 % of cases with less than ten injection attempts. Even in black-box scenarios, we find that exploiting the transferability of adversarial samples from surrogate models can lead to successful attacks in around 15 % of cases. Finally, we design a poisoning detection technique that leverages the consistent directionality of template updates in feature space to discriminate between legitimate and malicious updates. We evaluate such a countermeasure with a set of intra-user variability factors which may present the same directionality characteristics, obtaining equal error rates for the detection between 7-14% and leading to over 99% of attacks being detected after only two sample injections.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00020","Engineering and Physical Sciences Research Council(grant numbers:EP/N509711/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230411","authentication;biometrics;template update;adversarial machine learning;face recognition","Deep learning;Bridges;Error analysis;Face recognition;Design methodology;Biological system modeling;Pipelines","","15","","48","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Towards Understanding Limitations of Pixel Discretization Against Adversarial Attacks","J. Chen; X. Wu; V. Rastogi; Y. Liang; S. Jha",University of Wisconsin-Madison; Google; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison,"2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","480","495","Wide adoption of artificial neural networks in various domains has led to an increasing interest in defending adversarial attacks against them. Preprocessing defense methods such as pixel discretization are particularly attractive in practice due to their simplicity, low computational overhead, and applicability to various systems. It is observed that such methods work well on simple datasets like MNIST, but break on more complicated ones like ImageNet under recently proposed strong white-box attacks. To understand the conditions for success and potentials for improvement, we study the pixel discretization defense method, including more sophisticated variants that take into account the properties of the dataset being discretized. Our results again show poor resistance against the strong attacks. We analyze our results in a theoretical framework and offer strong evidence that pixel discretization is unlikely to work on all but the simplest of the datasets. Furthermore, our arguments present insights why some other preprocessing defenses may be insecure.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806764","machine learning;adversarial attacks;preprocessing defense;pixel discretization","Robustness;Perturbation methods;Training;Neural networks;Measurement;Deep learning;Data models","","10","","35","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Follow the money: The relationship between currency exchange and illicit behaviour in an underground forum","G. A. Siu; B. Collier; A. Hutchings","Computer Laboratory University of Cambridge, Cambridge, United Kingdom; Science, Technology, and Innovation Studies, University of Edinburgh, Edinburgh, United Kingdom; Computer Laboratory University of Cambridge, Cambridge, United Kingdom","2021 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","29 Oct 2021","2021","","","191","201","Underground forums are used to discuss and organise cybercrime (as well as more conventional social activities). These forums are also commonly used for exchanging various digital currencies, either gained through the profits of crime or through less controversial means. Understanding the link between discussions of illicit behaviour and currency exchange can provide insights to identify money laundering and other parts of the cybercrime supply chain. In this paper we use natural language processing to classify posts from HackForums by crime type over a period of more than 10 years. To the best of our knowledge, this is the first time that this type of classification has been used for this large forum dataset. Although the majority of conversations in the forum were identified as relating to non-criminal discussions, we concentrate on the types of crimes being discussed by those exchanging currencies. We find the most popular topics are related to trading credentials and bots and malware. PayPal was one of the most widely advertised digital currencies and we observe significant displacement from Liberty Reserve to Bitcoin after the former was taken down in 2013. Rather than an explicit ‘cashing out’ mechanism, in which cryptocurrencies gained through crime flow into state-backed fiat currencies, we instead see a circulation of capital between different forms, as cash is held and then cashed back and forward according to movements in the wider currency market. We continue our examination of discussions of cryptocurrencies and explore how the underground market has reacted to new opportunities, with a qualitative case study about Facebook’s putative ‘Diem’ coin. We find that while most discussions are related to the technical details and potential investment opportunities, some potential cybercrime use-cases are raised.","2768-0657","978-1-6654-1012-0","10.1109/EuroSPW54576.2021.00027","Engineering and Physical Sciences Research Council; Economic and Social Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583690","cybercrime;cryptocurrency;currency exchange;Facebook Diem;machine learning;natural language processing","Online banking;Social networking (online);Biological system modeling;Supply chains;Bitcoin;Tools;Cryptography","","9","","23","IEEE","29 Oct 2021","","","IEEE","IEEE Conferences"
"Adversarial Out-domain Examples for Generative Models","D. Pasquini; M. Mingione; M. Bernaschi","Department of Computer Science, Sapienza University, Rome, Italy; Department of Statistics, Sapienza University, Rome, Italy; Institute for Applied Computing (IAC) CNR, Rome, Italy","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","272","280","Deep generative models are rapidly becoming a common tool for researchers and developers. However, as exhaustively shown for the family of discriminative models, the test-time inference of deep neural networks cannot be fully controlled and erroneous behaviors can be induced by an attacker. In the present work, we show how a malicious user can force a pre-trained generator to reproduce arbitrary data instances by feeding it suitable adversarial inputs. Moreover, we show that these adversarial latent vectors can be shaped so as to be statistically indistinguishable from the set of genuine inputs. The proposed attack technique is evaluated with respect to various GAN images generators using different architectures, training processes and for both conditional and not-conditional setups.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802456","Generative-adversarial-models;Attacks-against-machine-learning;Adversarial-input","","","6","","37","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"ATLAS: Automatically Detecting Discrepancies Between Privacy Policies and Privacy Labels","A. Jain; D. Rodriguez; J. M. D. Alamo; N. Sadeh","School of Computer Science, Carnegie Mellon University; Universidad Politécnica de Madrid; Universidad Politécnica de Madrid; School of Computer Science, Carnegie Mellon University","2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","31 Jul 2023","2023","","","94","107","Privacy policies are long, complex documents that end-users seldom read. Privacy labels aim to ameliorate these issues by providing succinct summaries of salient data practices. In December 2020, Apple began requiring that app developers submit privacy labels describing their apps’ data practices. Yet, research suggests that app developers often struggle to do so. In this paper, we automatically identify possible discrepancies between mobile app privacy policies and their privacy labels. Such discrepancies could be indicators of potential privacy compliance issues. We introduce the Automated Privacy Label Analysis System (ATLAS). ATLAS includes three components: a pipeline to systematically retrieve iOS App Store listings and privacy policies; an ensemble-based classifier capable of predicting privacy labels from the text of privacy policies with 91.3% accuracy using state-of-the-art NLP techniques; and a discrepancy analysis mechanism that enables a large-scale privacy analysis of the iOS App Store. Our system has enabled us to analyze 354,725 iOS apps. We find several interesting trends. For example, only 40.3% of apps in the App Store provide easily accessible privacy policies, and only 29.6% of apps provide both accessible privacy policies and privacy labels. Among apps that provide both, 88.0% have at least one possible discrepancy between the text of their privacy policy and their privacy label, which could be indicative of a potential compliance issue. We find that, on average, apps have 5.32 such potential compliance issues. We hope that ATLAS will help app developers, researchers, regulators, and mobile app stores alike. For example, app developers could use our classifier to check for discrepancies between their privacy policies and privacy labels, and regulators could use our system to help review apps at scale for potential compliance issues.","2768-0657","979-8-3503-2720-5","10.1109/EuroSPW59978.2023.00016","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190673","Natural Language Processing;Machine Learning;Transformers;Privacy Policies;Privacy Labels;iOS","Privacy;Data privacy;Regulators;Pipelines;Natural languages;Market research;Mobile applications","","4","","37","IEEE","31 Jul 2023","","","IEEE","IEEE Conferences"
"A Credibility Analysis System for Assessing Information on Twitter","M. Alrubaian; M. Al-Qurishi; M. M. Hassan; A. Alamri","Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia; Research Chair of Pervasive and Mobile Computing, King Saud University, Riyadh, Saudi Arabia","IEEE Transactions on Dependable and Secure Computing","9 Jul 2018","2018","15","4","661","674","Information credibility on Twitter has been a topic of interest among researchers in the fields of both computer and social sciences, primarily because of the recent growth of this platform as a tool for information dissemination. Twitter has made it increasingly possible to offer near-real-time transfer of information in a very cost-effective manner. It is now being used as a source of news among a wide array of users around the globe. The beauty of this platform is that it delivers timely content in a tailored manner that makes it possible for users to obtain news regarding their topics of interest. Consequently, the development of techniques that can verify information obtained from Twitter has become a challenging and necessary task. In this paper, we propose a new credibility analysis system for assessing information credibility on Twitter to prevent the proliferation of fake or malicious information. The proposed system consists of four integrated components: a reputation-based component, a credibility classifier engine, a user experience component, and a feature-ranking algorithm. The components operate together in an algorithmic form to analyze and assess the credibility of Twitter tweets and users. We tested the performance of our system on two different datasets from 489,330 unique Twitter accounts. We applied 10-fold cross-validation over four machine learning algorithms. The results reveal that a significant balance between recall and precision was achieved for the tested dataset.","1941-0018","","10.1109/TDSC.2016.2602338","King Saud University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551232","Credibility;reputation;classification;user experience;feature-ranking;Twitter","Twitter;Computational modeling;Classification algorithms;Machine learning algorithms;Algorithm design and analysis;Earthquakes","","35","","31","IEEE","24 Aug 2016","","","IEEE","IEEE Journals"
"Image Transformation-Based Defense Against Adversarial Perturbation on Deep Learning Models","A. Agarwal; R. Singh; M. Vatsa; N. Ratha","Department of Computer Science and Engineering, IIIT-Delhi, New Delhi, Delhi, India; Department of Computer Science and Engineering, IIT Jodhpur, Karwar, Rajasthan, India; Department of Computer Science and Engineering, IIT Jodhpur, Karwar, Rajasthan, India; SUNY-Buffalo, Buffalo, NY, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2106","2121","Deep learning algorithms provide state-of-the-art results on a multitude of applications. However, it is also well established that they are highly vulnerable to adversarial perturbations. It is often believed that the solution to this vulnerability of deep learning systems must come from deep networks only. Contrary to this common understanding, in this article, we propose a non-deep learning approach that searches over a set of well-known image transforms such as Discrete Wavelet Transform and Discrete Sine Transform, and classifying the features with a support vector machine-based classifier. Existing deep networks-based defense have been proven ineffective against sophisticated adversaries, whereas image transformation-based solution makes a strong defense because of the non-differential nature, multiscale, and orientation filtering. The proposed approach, which combines the outputs of two transforms, efficiently generalizes across databases as well as different unseen attacks and combinations of both (i.e., cross-database and unseen noise generation CNN model). The proposed algorithm is evaluated on large scale databases, including object database (validation set of ImageNet) and face recognition (MBGC) database. The proposed detection algorithm yields at-least 84.2% and 80.1% detection accuracy under seen and unseen database test settings, respectively. Besides, we also show how the impact of the adversarial perturbation can be neutralized using a wavelet decomposition-based filtering method of denoising. The mitigation results with different perturbation methods on several image databases demonstrate the effectiveness of the proposed method.","1941-0018","","10.1109/TDSC.2020.3027183","Department of Science and Technology Ministry of Science and Technology(grant numbers:Swarnajayanti Fellowship); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207872","Adversarial attack;detection;mitigation;image transformations;deep learning","Perturbation methods;Databases;Machine learning;Transforms;Detection algorithms;Integrated circuits;Machine learning algorithms","","23","","107","IEEE","28 Sep 2020","","","IEEE","IEEE Journals"
"CuRTAIL: ChaRacterizing and Thwarting AdversarIal Deep Learning","M. Javaheripi; M. Samragh; B. D. Rouhani; T. Javidi; F. Koushanfar","University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA; University of California San Diego, La Jolla, CA, USA","IEEE Transactions on Dependable and Secure Computing","8 Mar 2021","2021","18","2","736","752","Recent advances in adversarial Deep Learning (DL) have opened up a new and largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. This article introduces CuRTAIL, a novel end-to-end computing framework to characterize and thwart potential adversarial attacks and significantly improve the reliability (safety) of a victim DL model. We formalize the goal of preventing adversarial attacks as an optimization problem to minimize the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. This, in turn, ensures the effectiveness of the defense in the face of generic attacks. We evaluate the robustness of our proposed methodology against the state-of-the-art adaptive attacks in a white-box setting considering that the adversary knows everything about the victim model and its defenders. Extensive evaluations for analyzing MNIST, CIFAR10, and ImageNet data corroborate the effectiveness of CuRTAIL framework against adversarial samples. The computations in each modular redundancy can be performed independently of the other redundancy modules. As such, CuRTAIL detection algorithm can be completely parallelized among multiple hardware settings to achieve maximum throughput. We further provide an open-source Application Programming Interface (API) to facilitate the adoption of the proposed framework for various applications.","1941-0018","","10.1109/TDSC.2020.3024191","ARO(grant numbers:W911NF1910317); SRC-Auto(grant numbers:2019-AU-2899); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197614","Deep learning;model reliability;adversarial samples;white-box attacks","Machine learning;Robustness;Computational modeling;Redundancy;Hardware;Machine learning algorithms","","6","","48","IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"MANDA: On Adversarial Example Detection for Network Intrusion Detection System","N. Wang; Y. Chen; Y. Xiao; Y. Hu; W. Lou; Y. T. Hou","Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1139","1153","With the rapid advancement in machine learning (ML), ML-based Intrusion Detection Systems (IDSs) are widely deployed to protect networks from various attacks. One of the biggest challenges is that ML-based IDSs suffer from adversarial example (AE) attacks. By applying small perturbations (e.g., slightly increasing packet inter-arrival time) to the intrusion traffic, an AE attack can flip the prediction of a well-trained IDS. We address this challenge by proposing MANDA, a MANifold and Decision boundary-based AE detection system. Through analyzing AE attacks, we notice that 1) an AE tends to be close to its original manifold (i.e., the cluster of samples in its original class) regardless of which class it is misclassified into; and 2) AEs tend to be close to the decision boundary to minimize the perturbation scale. Based on the two observations, we design MANDA for accurate AE detection by exploiting inconsistency between manifold evaluation and IDS model inference and evaluating model uncertainty on small perturbations. We evaluate MANDA on both binary IDS and multi-class IDS on two datasets (NSL-KDD and CICIDS) under three state-of-the-art AE attacks. Our experimental results show that MANDA achieves high true-positive rate (98.41%) with a 5% false-positive rate.","1941-0018","","10.1109/TDSC.2022.3148990","Office of Naval Research(grant numbers:N00014-19-1-2621); National Science Foundation(grant numbers:CNS-1837519,CNS-1916902); Virginia Commonwealth Cyber Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709532","Adversarial example (AE);AE detection;intrusion detection system","Manifolds;Perturbation methods;Generative adversarial networks;Detectors;Adaptation models;Task analysis;Social networking (online)","","17","","60","IEEE","10 Feb 2022","","","IEEE","IEEE Journals"
"Design and Evaluation of a Multi-Domain Trojan Detection Method on Deep Neural Networks","Y. Gao; Y. Kim; B. G. Doan; Z. Zhang; G. Zhang; S. Nepal; D. C. Ranasinghe; H. Kim","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; Data61, CSIRO, Marsfield, NSW, Australia; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Data61, CSIRO, Marsfield, NSW, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; Department of Computer Science and Engineering, College of Computing, Sungkyunkwan University, Suwon, Gyeonggi-do, South Korea","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2349","2364","Trojan attacks on deep neural networks (DNNs) exploit a backdoor embedded in a DNN model that can hijack any input with an attacker’s chosen signature trigger. Emerging defence mechanisms are mainly designed and validated on vision domain tasks (e.g., image classification) on 2D Convolutional Neural Network (CNN) model architectures; a defence mechanism that is general across vision, text, and audio domain tasks is demanded. This work designs and evaluates a run-time Trojan detection method exploiting STRong Intentional Perturbation of inputs that is a multi-domain input-agnostic Trojan detection defence across Vision, Text and Audio domains—thus termed as STRIP-ViTA. Specifically, STRIP-ViTA is demonstratively independent of not only task domain but also model architectures. Most importantly, unlike other detection mechanisms, it requires neither machine learning expertise nor expensive computational resource, which are the reason behind DNN model outsourcing scenario—one main attack surface of Trojan attack. We have extensively evaluated the performance of STRIP-ViTA over: i) CIFAR10 and GTSRB datasets using 2D CNNs for vision tasks; ii) IMDB and consumer complaint datasets using both LSTM and 1D CNNs for text tasks; and iii) speech command dataset using both 1D CNNs and 2D CNNs for audio tasks. Experimental results based on more than 30 tested Trojaned models (including publicly Trojaned model) corroborate that STRIP-ViTA performs well across all nine architectures and five datasets. Overall, STRIP-ViTA can effectively detect trigger inputs with small false acceptance rate (FAR) with an acceptable preset false rejection rate (FRR). In particular, for vision tasks, we can always achieve a 0 percent FRR and FAR given strong attack success rate always preferred by the attacker. By setting FRR to be 3 percent, average FAR of 1.1 and 3.55 percent are achieved for text and audio tasks, respectively. Moreover, we have evaluated STRIP-ViTA against a number of advanced backdoor attacks and compare its effectiveness with other recent state-of-the-arts.","1941-0018","","10.1109/TDSC.2021.3055844","National Natural Science Foundation of China(grant numbers:62002167); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200461); NRFK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343758","STRIP-ViTA;trojan detection;backdoor attack;deep learning;AI security","Trojan horses;Task analysis;Computational modeling;Perturbation methods;Training;Predictive models;Computer architecture","","15","","43","IEEE","1 Feb 2021","","","IEEE","IEEE Journals"
"Sustainable Ensemble Learning Driving Intrusion Detection Model","X. Li; M. Zhu; L. T. Yang; M. Xu; Z. Ma; C. Zhong; H. Li; Y. Xiang","State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, Shaanxi, China; School of Information Technology, Deakin University, Melbourne Burwood, VIC, Australia","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1591","1604","Nowadays, in machine learning based intrusion detection systems, ensemble learning is a commonly adopted method to improve the detection accuracy. Unfortunately, the existing works have not considered the accumulation and reuse of historical knowledge, as well as the sensitivity of the detection model to different types of attacks, which leads to a low detection accuracy. To address the issue, this article proposes a model based on sustainable ensemble learning. In the model training stage, by taking the individual classifiers probability output and classification confidence as the training data, we build multi-class regression models such that ensemble learning adapts to different attacks. Besides, in the updating stage, an iterative updating method is presented, where the parameters and decision results of the historical model are added to the training process of the new ensemble model to realize the incremental learning. Experiment results show that the proposed model significantly outperforms the existing solutions in terms of detection accuracy, false alarm, stability and robustness.","1941-0018","","10.1109/TDSC.2021.3066202","National Key Research and Development Program of China(grant numbers:2017YFB0801805); National Natural Science Foundation of China(grant numbers:U1708262,U1736203); Key Research and Development Plan of Xinjiang Production and Construction Corps(grant numbers:2019AB001); Fundamental Research Funds for the Central Universities(grant numbers:JB211505); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380306","Network security;intrusion detection;ensemble learning;sustainability","Intrusion detection;Adaptation models;Training;Data models;Feature extraction;Sensitivity;Genetic algorithms","","13","","43","IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Website Fingerprinting Through the Cache Occupancy Channel and its Real World Practicality","A. Shusterman; Z. Avraham; E. Croitoru; Y. Haskal; L. Kang; D. Levi; Y. Meltser; P. Mittal; Y. Oren; Y. Yarom","Ben-Gurion University of the Negev, Be'er Sheva, Israel; Ben-Gurion University of the Negev, Be'er Sheva, Israel; Ben-Gurion University of the Negev, Be'er Sheva, Israel; Ben-Gurion University of the Negev, Be'er Sheva, Israel; School of Computer Science, University of Adelaide, Adelaide, SA, Australia; Ben-Gurion University of the Negev, Be'er Sheva, Israel; Ben-Gurion University of the Negev, Be'er Sheva, Israel; Princeton University, Princeton, NJ, USA; Ben-Gurion University of the Negev, Be'er Sheva, Israel; University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2042","2060","Website fingerprinting attacks use statistical analysis on network traffic to compromise user privacy. The classical attack model used to evaluate website fingerprinting attacks assumes an on-path adversary, who observes traffic traveling between the user's computer and the network. In this article we investigate a different attack model, in which the adversary sends JavaScript code to the target user's computer. This code mounts a cache side-channel attack to identify other websites being browsed. Using machine learning techniques to classify traces of cache activity, we achieve high classification accuracy in both the open-world and the closed-world models. Our attack is more resistant than network-based fingerprinting to the effects of response caching, and resilient both to network-based defenses and to side-channel countermeasures. We carry out a real-world evaluation of several aspects of our attack, exploring the impact of the changes in websites and browsers over time, as well as of the attacker's ability to guess the software and hardware configuration of the target user's computer. To protect against cache-based website fingerprinting, new defense mechanisms must be introduced to privacy-sensitive browsers and websites. We investigate one such mechanism, and show that it reduces the effectiveness of the attack and completely eliminates it when used in the Tor Browser.","1941-0018","","10.1109/TDSC.2020.2988369","ARC Centre of Excellence for Mathematical & Statistical Frontiers; ARC Discovery Early Career Researcher(grant numbers:DE200101577); Intel Corporation; Israel Science Foundation(grant numbers:702/16,703/16); National Science Foundation(grant numbers:CNS-1409415,CNS-1704105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072556","Side-channel attacks;deep learning;microarchitecture;cache memory;fingerprint recognition;privacy","Browsers;Privacy;Computational modeling;Side-channel attacks;Tools;Relays","","12","","114","IEEE","20 Apr 2020","","","IEEE","IEEE Journals"
"F-PAD: Private Attribute Disclosure Risk Estimation in Online Social Networks","X. Han; H. Huang; L. Wang","School of Information Management & Engineering, Shanghai University of Finance and Economics, Shanghai, China; School of Information Management & Engineering, Shanghai University of Finance and Economics, Shanghai, China; Key Lab of High Confidence Software Technologies, Ministry of Education, Department of Computer Science and Technology, EECS, Peking University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","11 Nov 2019","2019","16","6","1054","1069","In online social networks, users always expect to share some information for benefits (e.g., personalized services) while hiding the others for privacy. Unfortunately, the hidden information is likely to be predicted by various powerful inference attacks with the rapid advances in machine learning. Then, what is the risk that a user's private information could be disclosed? What countermeasures can be taken to fight against the privacy violation for the user? To tackle these issues, this article proposes a general Framework for Private Attribute Disclosure estimation (F-PAD) including three steps: 1) private attribute prediction; 2) disclosure model training; 3) disclosure risk estimation. Not like most prior risk estimation studies focusing on one specific attack model and private attribute, F-PAD can estimate disclosure risk for individual users in terms of disclosure probability and risk level within a high confidence given a basket of potential inference attack models; furthermore, F-PAD can adapt to various attributes (e.g., gender, age) and offer countermeasures to help users lower the risk. Extensive experiment studies on two real social network datasets, Facebook and Book-Crossing, have verified the effectiveness of F-PAD in `current city', `gender' and `age' disclosure risk estimation.","1941-0018","","10.1109/TDSC.2019.2934096","National Natural Science Foundation of China(grant numbers:71601106); Shanghai Science & Technology Innovation Project(grant numbers:18511103700); State Language Commission of China Key Program(grant numbers:ZDI135-18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895669","Online social networks (OSNs);private information;inference attack;information disclosure risk;risk estimation","Social networking (online);Privacy;Risk management;Information exchange","","10","","53","IEEE","11 Nov 2019","","","IEEE","IEEE Journals"
"EBSNN: Extended Byte Segment Neural Network for Network Traffic Classification","X. Xiao; W. Xiao; R. Li; X. Luo; H. Zheng; S. Xia","Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3521","3538","Network traffic classification is important to intrusion detection and network management. Most of existing methods are based on machine learning techniques and rely on the features extracted manually from flows or packets. However, with the rapid growth of network applications, it is difficult for these approaches to handle new complex applications. In this article, we design a novel neural network, the Extended Byte Segment Neural Network (EBSNN), to classify netwrk traffic. EBSNN first divides a packet into header segments and payload segments, which are then fed into encoders composed of the recurrent neural networks with the attention mechanism. Based on the outputs, another encoder learns the high-level representation of the whole packet. In particular, side-channel features are learned from header segments to improve the performance. Finally, the label of the packet is obtained by the softmax function. Furthermore, EBSNN can classify network flows by examining the first few packets. Thorough experiments on the real-world datasets show that EBSNN achieves better performance than the state-of-the-art methods in both the application identification task and the website identification task.","1941-0018","","10.1109/TDSC.2021.3101311","National Key Research and Development Program of China(grant numbers:2018YFB1800204); National Natural Science Foundation of China(grant numbers:61972219,61773229,61771273); Hong Kong RGC(grant numbers:152279/16E,152239/18E); HK ITF(grant numbers:GHP/052/19SZ); Research and Development Program of Shenzhen(grant numbers:JCYJ20190813174403598,SGDX20190918101201696,JCYJ20190813165003837); National Key Research and Development Program of China(grant numbers:2018YFB1800601); Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School(grant numbers:HW2021013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503323","Recurrent neural network;traffic classification;application identification;website identification","Deep learning;Feature extraction;Payloads;Cryptography;Task analysis;Protocols;Recurrent neural networks","","10","","61","IEEE","2 Aug 2021","","","IEEE","IEEE Journals"
"Architectures for Detecting Interleaved Multi-Stage Network Attacks Using Hidden Markov Models","T. Shawly; A. Elghariani; J. Kobes; A. Ghafoor","Electrical Engineering Department (Rabigh), King Abdulaziz University, Jeddah, Saudi Arabia; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; Northrop Grumman Corporation, Washington, DC, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2316","2330","With the growing amount of cyber threats, the need for development of high-assurance cyber systems is becoming increasingly important. The objective of this article is to address the challenges of modeling and detecting sophisticated network attacks, such as multiple interleaved attacks. We present the interleaving concept and investigate how interleaving multiple attacks can deceive intrusion detection systems. Using one of the important statistical machine learning (ML) techniques, Hidden Markov Models (HMM), we develop two architectures that take into account the stealth nature of the interleaving attacks, and that can detect and track the progress of these attacks. These architectures deploy a database of HMM templates of known attacks and exhibit varying performance and complexity. For performance evaluation, in the presence of multiple multi-stage attack scenarios, various metrics are proposed which include (1) attack risk probability, (2) detection error rate, and (3) the number of correctly detected stages. Extensive simulation experiments are used to demonstrate the efficacy of the proposed architectures.","1941-0018","","10.1109/TDSC.2019.2948623","Northrop Grumman Corporation; National Science Foundation(grant numbers:IIS-0964639); King Abdulaziz University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880499","Cyber systems;network security;intrusion detection;Hidden Markov Model;interleaved attacks","Hidden Markov models;Intrusion detection;Computer crime;Computer architecture;Complexity theory;Servers","","10","","42","IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"Towards Private and Scalable Cross-Media Retrieval","S. Hu; L. Y. Zhang; Q. Wang; Z. Qin; C. Wang","School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; School of Information Technology, Deakin University, VIC, Australia; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Institute of Cyberspace Research, Zhejiang University, Hangzhou, Zhejiang, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Dependable and Secure Computing","13 May 2021","2021","18","3","1354","1368","Cross-media retrieval (CMR) is an attractive networked application where a server responds to queries with retrieval results of different modalities. Different from traditional information retrieval, CMR relies on a more enriched set of machine learning techniques to produce semantic models projecting multimodal data into a common space. A larger training dataset usually gives more accurate models, leading to a better retrieval result. Despite very promising with potential underpinnings in network analytics and multimedia applications, applying CMR in such contexts also faces severe privacy challenges, due to the fact that various data scattering among multiple parties may be sensitive and not allowed to be shared publicly. Studies jointly considering cross-media analytics, privacy protection, collaborative learning, and distributed networking contexts, are relatively sparse. In this work, we propose the first practical system for privacy-preserving cross-media retrieval by utilizing trusted processors. Our scheme enables secure aggregation of the data from distinct parties, and secure canonical correlation analysis (CCA) over collaborated data to obtain semantic models. Verification mechanisms are designed to defend against active attacks from a malicious adversary. Furthermore, to deal with large data sets, we provide a set of optimization methods to accomodate to limited trusted memory and improve the efficiency of training process in CMR. We consider issues such as data block splitting to manage memory overhead, ordering of operations as well as parameters reuse and release to simplify I/O, and parallel computation to speed up dual operations. Our experiments over both synthetic and real datasets show that our solution is very efficient in practice, outperforms the existing solutions, and performs comparably with the original CMR system.","1941-0018","","10.1109/TDSC.2019.2926968","NSFC(grant numbers:61822207,U1636219); Equipment Pre-Research Joint Fund of Ministry of Education of China(grant numbers:6141A02033327); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); NSFC(grant numbers:61702221); Research Grants Council of Hong Kong(grant numbers:CityU 11276816,CityU 11212717,CityU C1008-16G); National Natural Science Foundation of China(grant numbers:61572412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755511","Cross-media retrieval;privacy protection;trusted processor;SGX;collaborative learning","Servers;Encryption;Correlation;Semantics;Training;Media","","7","","48","IEEE","4 Jul 2019","","","IEEE","IEEE Journals"
"Robust Detection of Malicious URLs With Self-Paced Wide & Deep Learning","Y. Liang; Q. Wang; K. Xiong; X. Zheng; Z. Yu; D. Zeng","School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","717","730","As cybercrimes grow in scale with devastating economic costs, it is important to protect potential victims against diverse attacks. In spite of the diversity of cybercrimes, it is the uniform resource locators (URLs) that connect vulnerable users with potential attacks. Although numerous solutions (e.g., rule-based solutions and machine learning-based methods) are proposed for malicious URL detection, they cannot provide robust performance due to the diversity of cybercrimes and cannot cope with the explosive growth of malicious URLs with the evolution of obfuscation strategies. In this paper, we propose a deep learning-based system, dubbed as CyberLen, to detect malicious URLs robustly and effectively. Specifically, we use factorization machine (FM) to learn the latent interaction among lexical features. For the deep structural features, position embedding is introduced for token vectorization to reduce the ambiguity of URL tokens. Meanwhile, temporal convolution network (TCN) is utilized to learn the long-distance dependency among URL tokens. To fuse heterogeneous features, self-paced wide $\&$& deep learning strategy is proposed to train a robust model effectively. The proposed solution is evaluated on a large-scale URL dataset. Our experimental results show that position embedding is constructive to reducing the ambiguity of URL tokens, and the self-paced wide $\&$& deep learning strategy shows superior performance in terms of F1 score and convergence speed.","1941-0018","","10.1109/TDSC.2021.3121388","National Key Research and Development Program of China(grant numbers:2018AAA0100500); Natural Science Foundation of China(grant numbers:61902320); Fundamental Research Funds for the Central Universities(grant numbers:31020180QD140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582835","Self-paced learning;temporal convolutional network;factorization machine;wide  $\&$   &     deep;malicious URL detection","Uniform resource locators;Feature extraction;Computer crime;Deep learning;Costs;Task analysis;Training","","4","","57","IEEE","20 Oct 2021","","","IEEE","IEEE Journals"
"Guard: Attack-Resilient Adaptive Load Balancing in Distributed Streaming Systems","A. Daghistani; M. Khayat; M. Felemban; W. G. Aref; A. Ghafoor","Department of Computer Engineering, Umm Al-Qura University, Makkah, Saudi Arabia; Department of Computer Engineering, Umm Al-Qura University, Makkah, Saudi Arabia; Department of Computer Engineering, Interdisciplinary Research Center for Intelligent Secure Systems, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Department of Computer Science and Purdue's Center for Education and Research in Information Assurance and Security (CERIAS), Purdue University, West Lafayette, IN, USA; Elmore Family School of Electrical and Computer Engineering and Purdue's Center for Education and Research in Information Assurance and Security (CERIAS), Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4172","4186","The performance of distributed streaming systems relies on how even the workload is distributed among their machines. However, data and query workloads are skewed and change rapidly. Therefore, multiple adaptive load-balancing mechanisms have been proposed in the literature to rebalance distributed streaming systems according to the changes in their workloads. This paper introduces a novel attack model that targets adaptive load-balancing mechanisms of distributed streaming systems. The attack reduces the throughput and the availability of the system by making it stay in a continuous state of rebalancing. This paper proposes Guard, a component that detects and blocks attacks that target the adaptive load balancing of distributed streaming systems. Guard uses an unsupervised machine-learning technique to detect malicious users that are involved in the attack. Guard does not block any user unless it detects that the user is malicious. Guard does not depend on a specific application. Experimental evaluation for a high-intensity attack illustrates that Guard improves the throughput and the availability of the system by 85% and 86%, respectively. Moreover, Guard improves the minimum availability that the attacker achieves by 325%.","1941-0018","","10.1109/TDSC.2021.3123071","National Science Foundation(grant numbers:IIS-1910216,III-1815796); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591429","Attack-resilient;malicious activity;adaptive load balancing;distributed streaming systems","Adaptive systems;Throughput;Load modeling;Adaptation models;Social networking (online);Real-time systems;Load management","","","","45","IEEE","27 Oct 2021","","","IEEE","IEEE Journals"
"A Comprehensive Defense Framework Against Model Extraction Attacks","W. Jiang; H. Li; G. Xu; T. Zhang; R. Lu","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","685","700","As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.","1941-0018","","10.1109/TDSC.2023.3261327","National Natural Science Foundation of China(grant numbers:62020106013,61972454,61802051,61772121,61728102); Sichuan Science and Technology Program(grant numbers:2020JDTD0007,2020YFG0298); Fundamental Research Funds for Chinese Central Universities(grant numbers:ZYGX2020ZB027); Singapore Ministry of Education AcRF Tier 2(grant numbers:MOET2EP20121-0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10080996","Deep learning;machine-learning-as-a-service;model extraction attacks","Adaptation models;Computational modeling;Training;Predictive models;Watermarking;Perturbation methods;Data models","","8","","61","IEEE","24 Mar 2023","","","IEEE","IEEE Journals"
"Towards Optimal Triage and Mitigation of Context-Sensitive Cyber Vulnerabilities","S. Hore; F. Moomtaheen; A. Shah; X. Ou","University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1270","1285","Cyber vulnerabilities are security deficiencies in computer and network systems of organizations, which can be exploited by an adversary to cause significant damage. The technology and security personnel resources currently available in organizations to mitigate the vulnerabilities are highly inadequate. As a result, systems routinely remain unpatched, thus making them vulnerable to security breaches from the adversaries. The potential consequences of an exploited vulnerability depend upon the context as well as the severity of the vulnerability, which may differ among networks and organizations. Furthermore, security personnel tend to have varying levels of expertise and technical proficiencies associated with different computer and network devices. There exists a critical need to develop a resource-constrained approach for effectively identifying and mitigating important context-sensitive cyber vulnerabilities. In this article, we develop an advanced analytics and optimization framework to address this need and compare our approach with rule-based methods employed in real-world cybersecurity operations centers, as well as a vulnerability prioritization method from recent literature. First, we propose a machine learning-based vulnerability priority scoring system (VPSS) to calculate the priority scores for each of the vulnerabilities found in an organization’s network and quantify organizational context-based vulnerability exposure. Next, we propose a decision-support system, which consists of a two-step sequential optimization approach. The first model selects the high priority vulnerability instances from the dense report subject to resource constraints, and the second model then optimally allocates them to the security personnel with matching skill types for mitigation. Experiment results conducted using a real-world vulnerability data set show that our approach 1) outperforms both the rule-based methods and the vulnerability prioritization method from literature in prioritizing context-sensitive vulnerabilities, which are found across highly susceptible organizationally relevant host machines, and 2) maximizes the pairs of vulnerability instance type and the respective security analyst skill type for optimal mitigation.","1941-0018","","10.1109/TDSC.2022.3152164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718144","Context-sensitive vulnerability triage and mitigation;cyber vulnerability management;machine learning;mixed integer programming;sequential optimization;vulnerability priority scoring system","Security;Organizations;Measurement;Personnel;Optimization;Software;Resource management","","2","","27","IEEE","22 Feb 2022","","","IEEE","IEEE Journals"
"Multi-Modal Side Channel Data Driven Golden-Free Detection of Software and Firmware Trojans","P. Krishnamurthy; V. R. Surabhi; H. Pearce; R. Karri; F. Khorrami","Department of ECE, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of ECE, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of ECE, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of ECE, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of ECE, NYU Tandon School of Engineering, Brooklyn, NY, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4664","4677","This study explores data-driven detection of firmware/software Trojans in embedded systems without golden models. We consider embedded systems such as single board computers and industrial controllers. While prior literature considers side channel based anomaly detection, this study addresses the following central question: is anomaly detection feasible when using low-fidelity simulated data without using data from a known-good (golden) system? To study this question, we use data from a simulator-based proxy as a stand-in for unavailable golden data from a known-good system. Using data generated from the simulator, one-class classifier machine learning models are applied to detect discrepancies against expected side channel signal patterns and their inter-relationships. Side channels fused for Trojan detection include multi-modal side channel measurement data (such as Hardware Performance Counters, processor load, temperature, and power consumption). Additionally, fuzzing is introduced to increase detectability of Trojans. To experimentally evaluate the approach, we generate low-fidelity data using a simulator implemented with a component-based model and an information bottleneck based on Gaussian stochastic models. We consider example Trojans and show that fuzzing-aided golden-free Trojan detection is feasible using simulated data as a baseline.","1941-0018","","10.1109/TDSC.2022.3231632","DARPA MTO(grant numbers:FA8750-20-1-0502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998113","Anomaly detection;embedded system;golden-free;machine learning;trojan detection","Trojan horses;Data models;Temperature measurement;Embedded systems;Frequency measurement;Anomaly detection;Integrated circuit modeling","","","","46","IEEE","23 Dec 2022","","","IEEE","IEEE Journals"
"Enhancing Robustness of On-Line Learning Models on Highly Noisy Data","Z. Zhao; R. Birke; R. Han; B. Robu; S. Bouchenak; S. B. Mokhtar; L. Y. Chen","Université Grenoble Alpes, Saint-Martin-d'Hères, France; ABB Research, Baden, Switzerland; Beijing Institute of Technology, Beijing, China; Université Grenoble Alpes, Saint-Martin-d'Hères, France; INSA Lyon, Villeurbanne, France; INSA Lyon, Villeurbanne, France; TU Delft, Delft, CD, The Netherlands","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2177","2192","Classification algorithms have been widely adopted to detect anomalies for various systems, e.g., IoT, cloud and face recognition, under the common assumption that the data source is clean, i.e., features and labels are correctly set. However, data collected from the wild can be unreliable due to careless annotations or malicious data transformation for incorrect anomaly detection. In this article, we extend a two-layer on-line data selection framework: Robust Anomaly Detector (RAD) with a newly designed ensemble prediction where both layers contribute to the final anomaly detection decision. To adapt to the on-line nature of anomaly detection, we consider additional features of conflicting opinions of classifiers, repetitive cleaning, and oracle knowledge. We on-line learn from incoming data streams and continuously cleanse the data, so as to adapt to the increasing learning capacity from the larger accumulated data set. Moreover, we explore the concept of oracle learning that provides additional information of true labels for difficult data points. We specifically focus on three use cases, (i) detecting 10 classes of IoT attacks, (ii) predicting 4 classes of task failures of big data jobs, and (iii) recognising 100 celebrities faces. Our evaluation results show that RAD can robustly improve the accuracy of anomaly detection, to reach up to 98.95 percent for IoT device attacks (i.e., +7%), up to 85.03 percent for cloud task failures (i.e., +14%) under 40 percent label noise, and for its extension, it can reach up to 77.51 percent for face recognition (i.e., +39%) under 30 percent label noise. The proposed RAD and its extensions are general and can be applied to different anomaly detection algorithms.","1941-0018","","10.1109/TDSC.2021.3063947","French LabEx PERSYVAL-Lab(grant numbers:ANR-11-LABX-0025-01); Swiss National Science Foundation(grant numbers:407540_167266); National Natural Science Foundation of China(grant numbers:61872337); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369874","Unreliable data;anomaly detection;failures;attacks;machine learning","Noise measurement;Data models;Anomaly detection;Predictive models;Task analysis;Face recognition;Machine learning algorithms","","2","","57","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Detection of Repackaged Android Malware with Code-Heterogeneity Features","K. Tian; D. Yao; B. G. Ryder; G. Tan; G. Peng","Department of Computer Science, Virginia Tech, Blacksburg; Department of Computer Science, Virginia Tech, Blacksburg; Department of Computer Science, Virginia Tech, Blacksburg; Department of Computer Science and Engineering, Penn State University, University Park; Wuhan University, Wuhan, China","IEEE Transactions on Dependable and Secure Computing","15 Jan 2020","2020","17","1","64","77","During repackaging, malware writers statically inject malcode and modify the control flow to ensure its execution. Repackaged malware is difficult to detect by existing classification techniques, partly because of their behavioral similarities to benign apps. By exploring the app's internal different behaviors, we propose a new Android repackaged malware detection technique based on code heterogeneity analysis. Our solution strategically partitions the code structure of an app into multiple dependence-based regions (subsets of the code). Each region is independently classified on its behavioral features. We point out the security challenges and design choices for partitioning code structures at the class and method level graphs, and present a solution based on multiple dependence relations. We have performed experimental evaluation with over 7,542 Android apps. For repackaged malware, our partition-based detection reduces false negatives (i.e., missed detection) by 30-fold, when compared to the non-partition-based approach. Overall, our approach achieves a false negative rate of 0.35 percent and a false positive rate of 2.97 percent.","1941-0018","","10.1109/TDSC.2017.2745575","DARPA APAC(grant numbers:FA8750-15-2-0076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8018581","Android security;malware detection;repackaged malware","Malware;Feature extraction;Android (operating system);Security;Semantics","","52","","45","IEEE","29 Aug 2017","","","IEEE","IEEE Journals"
"Shielding Collaborative Learning: Mitigating Poisoning Attacks Through Client-Side Detection","L. Zhao; S. Hu; Q. Wang; J. Jiang; C. Shen; X. Luo; P. Hu","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; State Key Laboratory of Cryptography, Beijing, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; MOE Key Laboratory for Intelligent Networks and Network Security, Xi'an, Shaanxi, China; School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2029","2041","Collaborative learning allows multiple clients to train a joint model without sharing their data with each other. Each client performs training locally and then submits the model updates to a central server for aggregation. Since the server has no visibility into the process of generating the updates, collaborative learning is vulnerable to poisoning attacks where a malicious client can generate a poisoned update to introduce backdoor functionality to the joint model. The existing solutions for detecting poisoned updates, however, fail to defend against the recently proposed attacks, especially in the non-IID (independent and identically distributed) setting. In this article, we present a novel defense scheme to detect anomalous updates in both IID and non-IID settings. Our key idea is to realize client-side cross-validation, where each update is evaluated over other clients' local data. The server will adjust the weights of the updates based on the evaluation results when performing aggregation. To adapt to the unbalanced distribution of data in the non-IID setting, a dynamic client allocation mechanism is designed to assign detection tasks to the most suitable clients. During the detection process, we also protect the client-level privacy to prevent malicious clients from knowing the participations of other clients, by integrating differential privacy with our design without degrading the detection performance. Our experimental evaluations on three real-world datasets show that our scheme is significantly robust to two representative poisoning attacks.","1941-0018","","10.1109/TDSC.2020.2986205","NSFC(grant numbers:61822207,U1636219); Equipment Pre-Research Joint Fund of Ministry of Education of China(grant numbers:6141A02033327); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); NSFC(grant numbers:U1804263); Plan for Scientific Innovation Talent of Henan Province(grant numbers:184200510018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066920","Poisoning attack;collaborative learning;deep learning;privacy","Collaborative work;Data models;Servers;Computational modeling;Training;Task analysis;Training data","","45","","46","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"DatingSec: Detecting Malicious Accounts in Dating Apps Using a Content-Based Attention Network","X. He; Q. Gong; Y. Chen; Y. Zhang; X. Wang; X. Fu","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Saarbrucken, Germany; School of Computer Science, Fudan University, Shanghai, China; Institute of Computer Science, University of Goettingen, Gottingen, Germany","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2193","2208","Dating apps have gained tremendous popularity during the past decade. Compared with traditional offline dating means, dating apps ease the process of partner finding significantly. While bringing convenience to hundreds of millions of users, dating apps are vulnerable to become targets of adversaries. In this article, we focus on malicious user detection in dating apps. Existing methods overlooked the signals hidden in the textual information of user interactions, particularly the interplay of temporal-spatial behaviors and textual information, leading to limited detection performance. To tackle this, we propose DatingSec, a novel malicious user detection system for dating apps. Concretely, DatingSec leverages long short-term memory neural networks (LSTM) and an attentive module to capture the interplay of users' temporal-spatial behaviors and user-generated textual content. We evaluate DatingSec on a real-world dataset collected from Momo, a widely used dating app with more than 180 million users. Experimental results show that DatingSec outperforms state-of-the-art methods and achieves an F1-score of 0.857 and AUC of 0.940.","1941-0018","","10.1109/TDSC.2021.3068307","National Natural Science Foundation of China(grant numbers:62072115,71731004,61602122,61971145); European Union's Horizon 2020 research(grant numbers:824019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384217","Dating apps;malicious account detection;deep learning;attention mechanism;text analytics","Social networking (online);Neural networks;Feature extraction;Blogs;User-generated content;Safety;Privacy","","18","","74","IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Agora: A Privacy-Aware Data Marketplace","V. Koutsos; D. Papadopoulos; D. Chatzopoulos; S. Tarkoma; P. Hui","Department of Computer Science and Engeneering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engeneering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engeneering, The Hong Kong University of Science and Technology, Hong Kong; University of Helsinki, Helsinki, Finland; Department of Computer Science and Engeneering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","3728","3740","We propose Agora, the first blockchain-based data marketplace that enables multiple privacy-concerned parties to get compensated for contributing and exchanging data, without relying on a trusted third party during the exchange. Agora achieves data privacy, output verifiability, and atomicity of payments by leveraging cryptographic techniques, and is designed as a decentralized application via smart contracts. Particularly, data generators provide encrypted data to data brokers who use a functional secret key to learn nothing but the output of a specific, agreed upon, function over the raw data. Data consumers can purchase decrypted outputs from the brokers, accompanied by corresponding proofs of correctness. We implement a working prototype of Agora on Ethereum and experimentally evaluate its performance and deployment costs. As a core building block of Agora, we propose a new functional encryption scheme with additional public parameters that operate as a trust anchor for verifying decrypted results.","1941-0018","","10.1109/TDSC.2021.3105099","Hong Kong RGC(grant numbers:ECS-26208318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516884","Data marketplace;functional encryption;blockchain","Generators;Iron;Smart contracts;Data privacy;Encryption;Cryptography;Blockchains","","9","","66","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Cleaning the NVD: Comprehensive Quality Assessment, Improvements, and Analyses","A. Anwar; A. Abusnaina; S. Chen; F. Li; D. Mohaisen","Northeastern University, Boston, MA, USA; Department of Computer Science, University of Central Florida (UCF), Orlando, FL, USA; Department of Computer Science, George Mason University, Fairfax, VA, USA; School of Electrical & Computer Engineering (ECE), Georgia Institute of Technology, Atlanta, GA, USA; Department of Computer Science, University of Central Florida (UCF), Orlando, FL, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4255","4269","Vulnerability databases are vital sources of information on emergent software security concerns. Security professionals, from system administrators to developers to researchers, heavily depend on these databases to track vulnerabilities and analyze security trends. However, are these databases reliable and accurate? In this article, we explore this question with the National Vulnerability Database (NVD), the U.S. government's repository of vulnerability information that arguably serves as the industry standard. Through a systematic investigation, we uncover inconsistent or incomplete data in the NVD that can impact its practical uses, affecting information such as the vulnerability publication dates, applications affected by the vulnerability, their severity scores, and their high level type categorization. We explore the extent of these discrepancies and identify methods for their automated corrections. Finally, we demonstrate the impact that these data issues can pose by comparing analyses using the original and our rectified versions of the NVD. Ultimately, our investigation of the NVD not only produces an improved source of vulnerability information, but also provides important insights and guidance for the security community on the curation and use of such data sources.","1941-0018","","10.1109/TDSC.2021.3125270","Global Research Lab.; National Research Foundation; Information and Communication Technologies and Future Planning(grant numbers:NRF-2016K1A1A2912757); National Science Foundation(grant numbers:CNS-2007153); Commonwealth Cyber Initiative; UCF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601266","Vulnerability analysis;CVSS;NVD","Security;Databases;Standards;Reliability;Market research;Tools;Systematics","","8","","52","IEEE","4 Nov 2021","","","IEEE","IEEE Journals"
"Defeating Misclassification Attacks Against Transfer Learning","B. Wu; S. Wang; X. Yuan; C. Wang; C. Rudolph; X. Yang","Department of Information Technology, Monash University, Clayton, VIC, Australia; CSIRO Data61, Clayton, VIC, Australia; Department of Information Technology, Monash University, Clayton, VIC, Australia; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Information Technology, Monash University, Clayton, VIC, Australia; Department of Information Technology, Monash University, Clayton, VIC, Australia","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","886","901","Transfer learning is prevalent as a technique to efficiently generate new models (Student models) based on the knowledge transferred from a pre-trained model (Teacher model). However, Teacher models are often publicly available for sharing and reuse, which inevitably introduces vulnerability to trigger severe attacks against transfer learning systems. In this article, we take a first step towards mitigating one of the most advanced misclassification attacks in transfer learning. We design a distilled differentiator via activation-based network pruning to enervate the attack transferability while retaining accuracy. We adopt an ensemble structure from variant differentiators to improve the defence robustness. To avoid the bloated ensemble size during inference, we propose a two-phase defence, in which inference from the Student model is first performed to narrow down the candidate differentiators to be assembled, and later only a small, fixed number of them can be chosen to validate clean or reject adversarial inputs effectively. Our comprehensive evaluations on both large and small image recognition tasks confirm that the Student models with our defence of only 5 differentiators are immune to over 90% of the adversarial inputs with an accuracy loss of less than 10%. Our comparison also demonstrates that our design outperforms prior problematic defences.","1941-0018","","10.1109/TDSC.2022.3144988","Monash-Data61 collaborative research project(grant numbers:Data61 CRP43); Research Grants Council of Hong Kong(grant numbers:CityU 11217819,N_CityU139/21,R6021-20F); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693248","Deep neural network;defence against adversarial examples;transfer learning;pre-trained model","Transfer learning;Task analysis;Mathematical models;Training;Computational modeling;Data models;Perturbation methods","","4","","46","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Hybrid Knowledge and Data Driven Synthesis of Runtime Monitors for Cyber-Physical Systems","X. Zhou; B. Ahmed; J. H. Aylor; P. Asare; H. Alemzadeh","Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA; University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA; University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","12","30","Recent advances in sensing and computing technology have led to the proliferation of Cyber-Physical Systems (CPS) in safety-critical domains. However, the increasing device complexity, shrinking technology sizes, and shorter time to market have resulted in significant challenges in ensuring the reliability, safety, and security of CPS. This article presents a hybrid knowledge and data-driven approach for designing run-time context-aware safety monitors that can detect early signs of hazards and mitigate them in CPS. We propose a framework for formal specification of unsafe system context using Signal Temporal Logic (STL) combined with two optimization approaches for scenario-specific refinement and integration of STL specifications using data collected from closed-loop CPS simulations. We demonstrate the effectiveness of our approach in simulation using an autonomous driving system (ADS) and two closed-loop artificial pancreas systems (APS) as well as a publicly-available clinical trial dataset. The results show that a safety monitor developed with the proposed approaches demonstrates up to 4.7 times increase in average prediction accuracy (F1 score) over several well-designed baseline monitors while reducing both false-positive and false-negative rates in most scenarios.","1941-0018","","10.1109/TDSC.2023.3242653","Commonwealth of Virginia(grant numbers:CoVA CCI: C-Q122-WM-02); National Science Foundation(grant numbers:1748737,2146295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038568","Anomaly detection;cyber-physical systems;hazard analysis;resilience;run-time verification;safety","Monitoring;Hazards;Context modeling;Sensors;Actuators;Data models;Software","","2","","110","IEEE","6 Feb 2023","","","IEEE","IEEE Journals"
"Orchestration or Automation: Authentication Flaw Detection in Android Apps","S. Ma; J. Li; S. Nepal; D. Ostry; D. Lo; S. K. Jha; R. H. Deng; E. Bertino","Data61, CSIRO, University of Queensland, Eveleigh, NSW, Australia; Shanghai Jiaotong University, Shanghai, China; Data61, CSIRO, University of Queensland, Eveleigh, NSW, Australia; Data61, CSIRO, University of Queensland, Eveleigh, NSW, Australia; Singapore Management University, Singapore; University of New South Wales Sydney, Sydney, NSW, Australia; Singapore Management University, Singapore; Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2165","2178","Passwords are pervasively used to authenticate users’ identities in mobile apps. To secure passwords against attacks, protection is applied to the password authentication protocol (PAP). The implementation of the protection scheme becomes an important factor in protecting PAP against attacks. We focus on two basic protection in Android, i.e., SSL/TLS-based PAP and timestamp-based PAP. Previously, we proposed an automated tool, GLACIATE, to detect authentication flaws. We were curious whether orchestration (i.e., involving manual-effort) works better than automation. To answer this question, we propose an orchestrated approach, AuthExploit and compare its effectiveness GLACIATE. We study requirements for correct implementation of PAP and then apply GLACIATE to identify protection enhancements automatically. Through dependency analysis, GLACIATE matches the implementations against the abstracted flaws to recognise defective apps. To evaluate AuthExploit, we collected 1,200 Android apps from Google Play. We compared AuthExploit with the automation tool, GLACIATE, and two other orchestration tools, ${\sf MalloDroid}$MalloDroid and ${\sf SMV-Hunter}$SMV-Hunter. The results demonstrated that orchestration tools detect flaws more precisely although the F1 score of GLACIATE is higher than AuthExploit. Further analysis of the results reveals that highly popular apps and e-commerce apps are not more secure than other apps.","1941-0018","","10.1109/TDSC.2021.3050188","National Natural Science Foundation of China(grant numbers:62002222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317767","Vulnerability detection;password authentication;mobile security","Password;Authentication;Protocols;Servers;Tools;Automation;Mobile applications","","1","","51","IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"An Explainable Multi-Modal Hierarchical Attention Model for Developing Phishing Threat Intelligence","Y. Chai; Y. Zhou; W. Li; Y. Jiang","School of Management, Key Laboratory of Process Optimization and Intelligence Decision Making, Minister of Education, Hefei University of Technology, Hefei, Anhui, China; School of Management, Key Laboratory of Process Optimization and Intelligence Decision Making, Minister of Education, Hefei University of Technology, Hefei, Anhui, China; Department of Management Information Systems, University of Georgia, Athens, GA, USA; School of Management, Key Laboratory of Process Optimization and Intelligence Decision Making, Minister of Education, Hefei University of Technology, Hefei, Anhui, China","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","790","803","Phishing website attack, as one of the most persistent forms of cyber threats, evolves and remains a major cyber threat. Various detection methods (e.g., lookup systems, fraud cue-based methods) have been proposed to identify phishing websites. The limitations of lookup systems (e.g., failing to address newly created attacks) and the fraud cue-based methods (e.g., relying on feature engineering) motivated the development of deep representation-based methods capable of learning deep fraud cues for enhanced anti-phishing capacity. Focusing mostly on URLs, these methods fail to analyze other two important modalities of website content: textual information and visual design. Moreover, the interpretability of these deep learning based methods is limited, reducing model trustworthiness and preventing relevant and actionable intelligence. As such, we propose a multi-modal hierarchical attention model (MMHAM) which jointly learns the deep fraud cues from the three major modalities of website content for phishing website detection. Specifically, MMHAM features an innovative shared dictionary learning approach for aligning representations from different modalities in the attention mechanism. In our evaluation experiments, the proposed MMHAM not only learned improved deep cues for enhanced phishing detection, but provided a hierarchical interpretability system from which we could develop phishing threat intelligence to inform phishing websites detection at different levels.","1941-0018","","10.1109/TDSC.2021.3119323","National Natural Science Foundation of China(grant numbers:91846201,71722010,72101079,72171071,91746302); Research Project of Zhejiang Lab(grant numbers:2019KE0AB04); Shanghai Data Exchange Cooperative Program(grant numbers:W2021JSZX0052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568704","Antiphishing;machine learning;multimodal systems;security and protection","Phishing;Navigation;Visualization;Uniform resource locators;Deep learning;Protocols;Blacklisting","","12","","47","IEEE","12 Oct 2021","","","IEEE","IEEE Journals"
"Secure Protocols for Best Arm Identification in Federated Stochastic Multi-Armed Bandits","R. Ciucanu; A. Delabrouille; P. Lafourcade; M. Soare","University Grenoble Alpes & LIG, Saint-Martin-d'Heres, France; University Bordeaux, Talence, France; University Clermont Auvergne & LIMOS, Clermont-Ferrand, France; University Grenoble Alpes & LIG, Saint-Martin-d'Heres, France","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1378","1389","The stochastic multi-armed bandit is a classical reinforcement learning model, where a learning agent sequentially chooses an action (pull a bandit arm) and the environment responds with a stochastic reward drawn from an unknown distribution associated with the chosen action. A popular objective for the agent is to identify the arm having the maximum expected reward, also known as the best arm identification problem. We address the security concerns that occur in a cross-silo federated learning setting, where multiple data owners collaborate under the orchestration of a server to execute a best arm identification algorithm. We propose three secure protocols, which guarantee desirable security properties for the: input data (i.e., reward values), intermediate data (i.e., sums of rewards), and output data (i.e., ranking of arms and in particular the identified best arm). More precisely: (1) no data owner can learn the identified best arm; moreover, no data owner can learn local data pertaining to another data owner; (2) the orchestration participants cannot learn the identified best arm, any reward value, or any sum of rewards; (3) by analyzing the messages exchanged over the network, an external observer cannot learn the identified best arm, or any reward value, or any sum of rewards. Each protocol has a different architecture, uses different techniques, and proposes a different trade-off with respect to several criteria that we thoroughly analyze: number of participants, generality of the supported reward functions, cryptographic overhead, and communication cost. To build our protocols, we rely on secure multi-party computation, AES-CBC, and the additive homomorphic property of Paillier.","1941-0018","","10.1109/TDSC.2022.3154585","MIAI@Grenoble Alpes(grant numbers:ANR-19-P3IA-0003); EU H2020 Research and Innovation Programme(grant numbers:952215); INODE(grant numbers:863410); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721650","Best arm identification;federated learning;multi-armed bandits;secure protocol;machine learning security","Protocols;Cryptography;Servers;Collaborative work;Security;Stochastic processes;Testing","","1","","33","IEEE","25 Feb 2022","","","IEEE","IEEE Journals"
"Locally Differentially Private Personal Data Markets Using Contextual Dynamic Pricing Mechanism","M. Xiao; M. Li; J. J. Zhang","Department of Computer Science, California State Polytechnic University, Pomona, CA, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; College of Business, The University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","5043","5055","Data is becoming the world's most valuable asset and the ultimate renewable resource. This phenomenon has led to online personal data markets where data owners and collectors engage in the data sale and purchase. From the collector's standpoint, a key question is how to set a proper pricing rule that brings profitable tradings. One feasible solution is to set the price slightly above the owner's data cost. Nonetheless, data cost is generally unknown by the collector as being the owner's private information. To bridge this gap, we propose a novel learning algorithm, modified stochastic gradient descent (MSGD) that infers the owner's cost model from her interactions with the collector. To protect owners’ data privacy during trading, we employ the framework of local differential privacy (LDP) that allows owners to perturb their genuine data and trading behaviors. The vital challenge is how the collector can derive the accurate cost model from noisy knowledge gathered from owners. For this, MSGD relies on auxiliary parameters to correct biased gradients caused by noise. We formally prove that the proposed MSGD algorithm produces a sublinear regret of $\mathcal {O}(T^{\frac{5}{6}}\sqrt{\log (T^{\frac{1}{3}})})$O(T56log(T13)). The effectiveness of our design is further validated via a series of in-person experiments that involve 30 volunteers.","1941-0018","","10.1109/TDSC.2023.3239615","NSF(grant numbers:CNS-1943509); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025585","Local differential privacy;personal data market;private machine learning","Costs;Behavioral sciences;Data models;Noise measurement;Computational modeling;Privacy;Pricing","","","","67","IEEE","24 Jan 2023","","","IEEE","IEEE Journals"
"IoTFinder: Efficient Large-Scale Identification of IoT Devices via Passive DNS Traffic Analysis","R. Perdisci; T. Papastergiou; O. Alrawi; M. Antonakakis",Georgia Institute of Technology; University of Georgia; University of Georgia; University of Georgia,"2020 IEEE European Symposium on Security and Privacy (EuroS&P)","2 Nov 2020","2020","","","474","489","Being able to enumerate potentially vulnerable IoT devices across the Internet is important, because it allows for assessing global Internet risks and enables network operators to check the hygiene of their own networks. To this end, in this paper we propose IoTFinder, a system for efficient, large-scale passive identification of IoT devices. Specifically, we leverage distributed passive DNS data collection, and develop a machine learning-based system that aims to accurately identify a large variety of IoT devices based solely on their DNS fingerprints. Our system is independent of whether the devices reside behind a NAT or other middleboxes, or whether they are assigned an IPv4 or IPv6 address. We design IoTFinder as a multi-label classifier, and evaluate its accuracy in several different settings, including computing detection results over a third-party IoT traffic dataset and DNS traffic collected at a US-based ISP hosting more than 40 million clients. The experimental results show that our approach allows for accurately detecting many diverse IoT devices, even when they are hosted behind a NAT and their traffic is “mixed” with traffic generated by other IoT and non-IoT devices hosted in the same local network.","","978-1-7281-5087-1","10.1109/EuroSP48549.2020.00037","Defense Advanced Research Agency (DARPA)(grant numbers:2106EHP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9230403","IoT Security;Traffic Modeling;Passive DNS","Fingerprint recognition;Data collection;Middleboxes;Object recognition;Internet of Things","","45","","43","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"The Odyssey: Modeling Privacy Threats in a Brave New World","R. Galvez; S. Gurses","IMEC-COSIC KU, Leuven; IMEC-COSIC KU, Leuven","2018 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","9 Jul 2018","2018","","","87","94","In the upcoming General Data Protection Regulation (GDPR), privacy by design and privacy impact assessments are given an even more prominent role than before. It is now required that companies build privacy into the core of their technical products. Recently, researchers and industry players have proposed employing threat modeling methods, traditionally used in security engineering, as a way to bridge these two GDPR requirements in the process of engineering systems. Threat modeling, however, typically assumes a waterfall process and monolithic design, assumptions that are disrupted with the popularization of Agile methodologies and Service Oriented Architectures. Moreover, agile service environments make it easier to address some privacy problems, while complicating others. To date, the challenges of applying threat modeling for privacy in agile service environments remain understudied. This paper sets out to expose and analyze this gap. Specifically, we analyze what challenges and opportunities the shifts in software engineering practice introduce into traditional Threat Modeling activities; how they relate to the different Privacy Goals; and what Agile principles and Service properties have an impact on them. Our results show that both agile and services make the end-toend analysis of applications more difficult. At the same time, the former allows for more efficient communications and iterative progress, while the latter enables the parallelization of tasks and the documentation of some architecture decisions. Additionally, we open a new research avenue pointing to Amazon Macie as an example of Machine Learning applications that aim to provide a solution to the scalability and usability of Privacy Threat Modeling processes.","","978-1-5386-5445-3","10.1109/EuroSPW.2018.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406565","privacy by design;threat modeling;software development;agile;services;challenges;opportunities","Privacy;Security;Data privacy;Service-oriented architecture;Documentation","","10","","38","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Recessive Social Networking: Preventing Privacy Leakage against Reverse Image Search","J. Zhang; B. Zhang; J. Lin","Lancaster University, Lancashire, UK; Lancaster University, Lancashire, UK; Guangdong Technology Normal University, Guangzhou, China","2019 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","19 Aug 2019","2019","","","211","219","This work investigates the image privacy problem in the context of social networking under the threat of reverse image search. We introduce a new concept called recessive social networking. Unlike conventional privacy-preserving social networking, in our setting, the aim is to deceive machine learning algorithms that used in reverse image search, while still enabling unaffected ubiquitous social networking among humans. We, for the first time, ultilize adversarial example technique as a defensive mechanism to protect image privacy against content-based image search algorithms in the context of social networking. Finally, rigorous evaluations are conducted to demonstrate the effectiveness, transferability, and robustness of the proposed countermeasure.","","978-1-7281-3026-2","10.1109/EuroSPW.2019.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802379","Adversarial-examples;image-retrieval;privacy-preserving","","","1","","39","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"The Case of Adversarial Inputs for Secure Similarity Approximation Protocols","E. M. Kornaropoulos; P. Efstathopoulos",Brown University; Symantec Research Labs petros,"2019 IEEE European Symposium on Security and Privacy (EuroS&P)","22 Aug 2019","2019","","","247","262","Computing similarity between high-dimensional data is a fundamental problem in data mining and information retrieval, with numerous applications-such as e-discovery and patient similarity. To address the relevant performance and scalability challenges, approximation methods are employed. A common characteristic among all privacy-preserving approximation protocols based on sketching is that the sketching is performed locally and is based on common randomness. Inspired by the power of attacks on machine learning models, we introduce the study of adversarial inputs for secure similarity approximations. To formally capture the framework of this family of attacks we present a new threat model where a party is assumed to use the common randomness to perturb her input 1) offline, and 2) before the execution of any secure protocol, so as to steer the approximation result to a maliciously chosen output. We define perturbation attacks under this adversarial model and propose attacks for the techniques of minhash and cosine sketching. We demonstrate the simplicity and effectiveness of the attacks by measuring their success on synthetic and real data from the areas of e-discovery and patient similarity. To mitigate such perturbation attacks we propose a server-aided architecture, where an additional party, the server, assists in the secure similarity approximation by handling the common randomness as private data. We revise and introduce the necessary secure protocols so as to apply minhash and cosine sketching techniques in the server-aided architecture. Our implementation demonstrates that this new design can mitigate offline perturbation attacks without sacrificing the efficiency and scalability of the reconstruction protocol.","","978-1-7281-1148-3","10.1109/EuroSP.2019.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806727","Attacks;Secure Protocol;Adversarial Input;Homomorphic Encryption;Similarity","Protocols;Perturbation methods;Computational modeling;Servers;Approximation algorithms;Data models;Computer architecture","","","","92","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Detection of Information Hiding at Physical Layer in Wireless Communications","N. Xie; Z. Li; J. Tan; A. X. Liu","Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Qilu University of Technology, Jinan, China","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","1104","1117","This article concerns the problem of detecting the use of information hiding at the physical layer in wireless communications because they are harder to be detected than other layer-based information hiding schemes. Prior schemes for detecting physical layer based information hiding are either heuristic based or machine learning based. The key limitation of prior heuristics based information hiding detection schemes is that they do not answer the fundamental question of why the information hidden at the physical layer can be detected. The key limitation of prior machine learning based information hiding detection schemes is that they lack robustness because wireless signals at the physical layer are very much environmental dependent, and thus an information hiding detection scheme trained in one environment often does not work well in another environment. Our insight is that embedding information on wireless signals at the physical layer will inevitably have a negative impact on the decodability of the cover signals, such as the increase of the error probability at the receiver (as well as the monitor). Based on the above insight, in our approach, after the monitor demodulates and decodes the cover signals, it will re-encode and re-modulate the cover signals, and then compare the resulting recovered signals with the raw signals that it received from the sender. We further propose a new estimation scheme for calculating receiver noise variance and conducted theoretical analysis. Specifically, we propose two hidden information detection schemes, a noise grouped based detection scheme and a constellation distance based detection scheme, both taking estimation errors into consideration. In particular, our constellation distance based detection scheme is the first scheme that can pinpoint the exact location on the received signals that are embedded with hidden information. We implemented our schemes and conducted extensive performance comparison between our schemes and prior schemes. Our experimental results show that when the received SNR is more than 20 dB, our approach with the new estimation scheme has the probability of detection more than 0.95, and our constellation distance based detection scheme can correctly pinpoint all embedded locations.","1941-0018","","10.1109/TDSC.2020.3012461","Natural Science Foundations of China(grant numbers:61972262,61872082,61472184); Natural Science Foundation of Guangdong Province(grant numbers:2016A030313046); Guangdong Leading Talent Program(grant numbers:2016LJ06D658); Fundamental Research Programs of Shenzhen City(grant numbers:JCYJ20180305124648757); China Scholarship Council(grant numbers:201908440031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151255","Information hiding;physical layer;detection;noise estimation;embedded locations","Physical layer;Receivers;Wireless communication;Communication system security;Monitoring;Estimation;Machine learning","","2","","52","IEEE","28 Jul 2020","","","IEEE","IEEE Journals"
"Robust Deep Learning Ensemble Against Deception","W. Wei; L. Liu","School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1513","1527","Deep neural network (DNN) models are known to be vulnerable to maliciously crafted adversarial examples and to out-of-distribution inputs drawn sufficiently far away from the training data. How to protect a machine learning model against deception of both types of destructive inputs remains an open challenge. This article presents XEnsemble, a diversity ensemble verification methodology for enhancing the adversarial robustness of DNN models against deception caused by either adversarial examples or out-of-distribution inputs. XEnsemble by design has three unique capabilities. First, XEnsemble builds diverse input denoising verifiers by leveraging different data cleaning techniques. Second, XEnsemble develops a disagreement-diversity ensemble learning methodology for guarding the output of the prediction model against deception. Third, XEnsemble provides a suite of algorithms to combine input verification and output verification to protect the DNN prediction models from both adversarial examples and out of distribution inputs. Evaluated using 11 popular adversarial attacks and two representative out-of-distribution datasets, we show that XEnsemble achieves a high defense success rate against adversarial examples and a high detection success rate against out-of-distribution data inputs, and outperforms existing representative defense methods with respect to robustness and defensibility.","1941-0018","","10.1109/TDSC.2020.3024660","National Science Foundation(grant numbers:NSF 2038029,NSF 1564097); International Business Machines Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200713","Robust deep learning;adversarial attack and defense;ensemble method","Predictive models;Robustness;Machine learning;Prediction algorithms;Training;Neural networks;Data models","","18","","52","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"GCI: A GPU-Based Transfer Learning Approach for Detecting Cheats of Computer Game","M. S. Islam; B. Dong; S. Chandra; L. Khan; B. Thuraisingham","Computer Science Department, The University of Texas at Dallas, Richardson, TX, USA; Computer Science Department, The University of Texas at Dallas, Richardson, TX, USA; Computer Science Department, The University of Texas at Dallas, Richardson, TX, USA; Computer Science Department, The University of Texas at Dallas, Richardson, TX, USA; Computer Science Department, The University of Texas at Dallas, Richardson, TX, USA","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","804","816","Cheating in massive multiple online games (MMOGs) adversely affect the game’s popularity and reputation among its users. Therefore, game developers invest large amount of efforts to detect and prevent cheats that provide an unfair advantage to cheaters over other naive users during game play. Particularly, MMOG clients share data with the server during game play. Game developers leverage this data to detect cheating. However, detecting cheats is challenging mainly due to the limited client-side information, along with unknown and complex cheating techniques. In this article, we aim to leverage machine learning-based models to predict cheats over encrypted game traffic during game play. Concretely, network game traffic during game play from each player can be used to determine whether a cheat is employed. A major challenge in developing such a prediction model is the availability of sufficient training data, which is sparingly available in practice. Game traffic obtained from a few known players can be easily labeled. However, if such players are not a good representation of the population (i.e., other players), then a supervised model trained on labeled game traffic from these set of players may not generalize well for the population. Here, we propose a Graphics Processing Unit (GPU) based scalable transfer learning approach to overcome the constraints of limited labeled data. Our empirical evaluation on a popular MMOG demonstrates significant improvement in cheat prediction compared to other competing methods.","1941-0018","","10.1109/TDSC.2020.3013817","National Science Foundation(grant numbers:DMS-1737978,DGE-2039542,MRI-1828467); AFOSR(grant numbers:FA9550-14-1-0173); FAIN(grant numbers:1906630); IBM faculty Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154512","Computer games;cheat identification;bias correction;relative density ratio;GPU","Games;Cryptography;Servers;Machine learning;Data models;Graphics processing units;Predictive models","","3","","53","IEEE","3 Aug 2020","","","IEEE","IEEE Journals"
"Anonymity Services Tor, I2P, JonDonym: Classifying in the Dark (Web)","A. Montieri; D. Ciuonzo; G. Aceto; A. Pescapé","University of Napoli Federico II, Napoli, Italy; NM2 s.r.l., Napoli, Italy; University of Napoli Federico II, Napoli, Italy; University of Napoli Federico II, Napoli, Italy","IEEE Transactions on Dependable and Secure Computing","29 Apr 2020","2020","17","3","662","675","Traffic Classification (TC) is an important tool for several tasks, applied in different fields (security, management, traffic engineering, R&D). This process is impaired or prevented by privacy-preserving protocols and tools, that encrypt the communication content, and (in case of anonymity tools) additionally hide the source, the destination, and the nature of the communication. In this paper, leveraging a public dataset released in 2017, we provide classification results with the aim of investigating to which degree the specific anonymity tool (and the traffic it hides) can be identified, when compared to the traffic of other considered anonymity tools, using five machine learning classifiers. Initially, flow-based TC is considered, and the effects of feature importance and temporal-related features to the network are investigated. Additionally, the role of finer-grained features, such as the (joint) histogram of packet lengths (and inter-arrival times), is determined. Successively, “early” TC of anonymous networks is analyzed. Results show that the considered anonymity networks (Tor, I2P, JonDonym) can be easily distinguished (with an accuracy of 99.87% and 99.80%, in case of flow-based and early-TC, respectively), telling even the specific application generating the traffic (with an accuracy of 73.99% and 66.76%, in case of flow-based and early-TC, respectively).","1941-0018","","10.1109/TDSC.2018.2804394","art. 11 DM 593/2000; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288657","Dark web;dark net;Tor;I2P;JonDonym;traffic classification;anonymity;privacy;security","Cryptography;Privacy;Internet;Bayes methods;Telecommunication traffic","","64","","49","IEEE","9 Feb 2018","","","IEEE","IEEE Journals"
"Design, Analysis and Application of Embedded Resistive RAM Based Strong Arbiter PUF","R. Govindaraj; S. Ghosh; S. Katkoori","Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Computer Science and Engineering, Pennsylvania State University, Pennsylvania, USA; Computer Science and Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Dependable and Secure Computing","6 Nov 2020","2020","17","6","1232","1242","Resistive Random Access Memory (RRAM) based Physical Unclonable Function (PUF) designs exploit either the probabilistic switching or the resistance variability during forming, SET and RESET processes of RRAM. Memory PUFs using RRAM are typically weak PUFs due to fewer number of challenge response pairs. We propose a strong arbiter PUF based on 1T-1R bit cell which is designed from conventional RRAM memory array with minimally invasive changes. Conventional voltage sense amplifier is repurposed to act like an arbiter and generate the response. Similarly, address and data lines are repurposed to act as challenge and response bits respectively. The PUF is simulated using 65 nm predictive technology models for CMOS and Verilog-A model for a hafnium oxide based RRAM. The proposed PUF architecture is evaluated for uniqueness, uniformity and reliability for various number of stages. It demonstrates mean intra-die Hamming Distance (HD) of 0.135 percent and inter-die HD of 51.4 percent, and passes the NIST tests. We study the vulnerability of proposed PUF to machine learning attacks. We also present an application of proposed PUF for data attestation in the internet of things. Proposed PUF-based data attestation consumes 9.88pJ of total energy per data block of 64-bits and offers a speed of 120.7 kbps.","1941-0018","","10.1109/TDSC.2018.2866425","Semiconductor Research Corporation(grant numbers:#2442.001); National Science Foundation(grant numbers:CNS-1722557,CNS-1814710,CCF-1718474,DGE-1723687); Defense Advanced Research Projects Agency(grant numbers:#D15AP00089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443101","Physical unclonable function;resistive RAM;hardware security;non-volatile memory;arbiter PUF;ML attacks;data attestation","Resistance;Mathematical model;Computer architecture;Random access memory;Security;Hardware","","18","","44","IEEE","21 Aug 2018","","","IEEE","IEEE Journals"
"Decision Tree Evaluation on Sensitive Datasets for Secure e-Healthcare Systems","M. Zhang; Y. Chen; W. Susilo","School of Computers, Hubei University of Technology, Wuhan, China; College of Computer, National University of Defense Technology, Changsha, China; School of Computing and Information Technology, Faculty of Engineering and Information Sciences, University of Wollongong, Wollongong, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","3988","4001","By collecting and analyzing patients' e-healthcare data in Medical Internet-of-Things (MIOT), e-Healthcare providers can offer alternative and helpful evaluation services of the risk of diseases to patients. However, e-Healthcare providers cannot cope with the huge volumes of data and respond to this online service. Providers typically outsource medical data to powerful medical cloud servers. Since outsourced servers are not fully trusted, a direct evaluation service will inevitably result in privacy risks concerning the patient's identity or original medical data. It is hard to hide the results of an evaluation from the single-server model unless a fully homomorphic cryptosystem is used or the patients must communicate online with the cloud multiple times in an inefficient manner. With regards to these issues, this article proposes a Secure and Privacy-Preserving Decision Tree Evaluation scheme (namely SPP-DTE) to achieve secure disease diagnosis classification under e-Healthcare systems without revealing the sensitive information of patients such as physiological data or the private data of medical providers such as the structure of decision trees. Our proposed scheme uses modified KNN computation to match the similarity and preserve the confidentiality of raw data and also applies matrix randomization and monotonically increasing and one-way functions to confuse the intermediate results. The experiment is conducted in data sets from UCI machine learning repository of medical health data. Our analysis indicates that the proposed SPP-DTE scheme is efficient in terms of computational cost and communication overhead that is practical and efficient for privacy protection in e-Healthcare classification and diagnosis system.","1941-0018","","10.1109/TDSC.2022.3219849","National Natural Science Foundation of China(grant numbers:62072134,U2001205); Key projects of Guangxi Natural Science Foundation(grant numbers:2019JJD170020); Key Research and Development Program of Hubei Province(grant numbers:2021BEA163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941370","Decision trees evaluation;e-Healthcare;modified KNN computation;privacy protection","Entropy;Decision trees;Medical diagnostic imaging;Servers;Computational modeling;Privacy;Physiology","","18","","44","IEEE","8 Nov 2022","","","IEEE","IEEE Journals"
"Improving Cryptocurrency Crime Detection: CoinJoin Community Detection Approach","A. Wahrstätter; J. Gomes; S. Khan; D. Svetinovic","Department of Information Systems and Operations Management, Research Institute for Cryptoeconomics, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Research Institute for Cryptoeconomics, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Research Institute for Cryptoeconomics, Vienna University of Economics and Business, Vienna, Austria; Department of Information Systems and Operations Management, Research Institute for Cryptoeconomics, Vienna University of Economics and Business, Vienna, Austria","IEEE Transactions on Dependable and Secure Computing","13 Nov 2023","2023","20","6","4946","4956","The potential of Bitcoin for money laundering and terrorist financing represents a significant challenge in law enforcement. In recent years, the use of privacy-improving CoinJoin transactions has grown significantly and helped criminal actors obfuscate Bitcoin money flows. In this study, we use unsupervised machine learning to analyze the complete Bitcoin user graph in order to identify suspicious actors potentially involved in illegal activities. In contrast to the existing studies, we introduce a novel set of features that we use to identify potential criminal activity more accurately. Furthermore, we apply our clustering algorithm to a CoinJoin-adjusted variant of the Bitcoin user graph, which enables us to analyze the network at a more detailed, user-centric level while still offering opportunities to address advanced privacy-enhancing techniques at a later stage. By comparing the results with our ground truth data set, we find that our improved clustering method is able to capture significantly more illicit activity within the most suspicious clusters. Finally, we find that users associated with illegal activities commonly have significant short paths to CoinJoin wallets and show tendencies toward outlier behavior. Our results have potential contributions to anti-money laundering efforts and combating the financing of terrorism and other illegal activities.","1941-0018","","10.1109/TDSC.2023.3238412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023950","Cryptocurrency;bitcoin;crime detection;Coin Joins;unsupervised learning","Bitcoin;Blockchains;Privacy;Unsupervised learning;Support vector machines;Economics;Computer hacking","","8","","51","IEEE","20 Jan 2023","","","IEEE","IEEE Journals"
"CheckShake: Passively Detecting Anomaly in Wi-Fi Security Handshake Using Gradient Boosting Based Ensemble Learning","A. Agrawal; U. Chatterjee; R. R. Maiti","Department of CSIS, BITS Pilani, Hyderabad Campus, Secunderabad, Telangana, India; Department of CSE, IIT Kanpur, Kanpur, Uttar Pradesh, India; Department of CSIS, BITS Pilani, Hyderabad Campus, Secunderabad, Telangana, India","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4868","4880","Recently, a number of attacks have been demonstrated (like key reinstallation attack, called KRACK) on WPA2 protocol suite in Wi-Fi WLAN, for which a patching is often challenging. In this article, we design and implement a system, called CheckShake, to passively detect anomalies in the handshake of Wi-Fi security protocols, in particular WPA2, between a client and an AP using COTS radios. Our proposed system works without decrypting any traffic and sniffing on multiple channels in parallel. It uses a state machine model for grouping Wi-Fi handshake packets and then perform deep packet inspection to identify the symptoms of the anomaly in specific stages of a handshake session. Our implementation of CheckShake does not require any modification to the firmware of the client or the AP or the COTS devices, it only requires to be physically placed within the range of the AP and its clients. We use both the publicly available dataset and our own data set for performance analysis of CheckShake. Using gradient boosting-based supervised machine learning (ML) models, we show that an accuracy around 98.50% with no false positive can be achieved using CheckShake in open sourced data that has non-zero probability of missing packets per group of packets.","1941-0018","","10.1109/TDSC.2023.3236355","Science and Engineering Research Board(grant numbers:CRG/2020/005855); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016219","Wi-Fi;WPA2;KRACK;ensemble model","Wireless fidelity;Security;Protocols;Authentication;Standards;Wireless communication;Communication system security","","5","","39","IEEE","12 Jan 2023","","","IEEE","IEEE Journals"
"Fraud-Agents Detection in Online Microfinance: A Large-Scale Empirical Study","Y. Wu; Z. Xie; S. Ji; Z. Liu; X. Zhang; C. Lin; S. Deng; J. Zhou; T. Wang; R. Beyah","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Department of Computer Science, Zhejiang Gongshang University, Hangzhou, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, Zhejiang, China; Binjiang Institute of Zhejiang University, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Ant Group, Hangzhou, Zhejiang, China; College of Information Sciences and Technology, Pennsylvania State University, State College, PA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1169","1185","Online Microlending, a new financial service, focuses on small loans without any sort of collateral. It provides more flexible and quicker funding for borrowers, as well as higher interest rates of return. For platforms that provide such services, an essential task is to adequately evaluate each loan’s risk so as to minimize the possible financial loss. However, there exists a special group of borrowers, namely fraud-agents, who gain illegal profits from inciting other borrowers to cheat, i.e., they help the high-risk borrowers evade the risk evaluation by crafting fake personal information. The existence of fraud-agents poses a severe threat to the risk management systems and results in a huge financial loss for lending platforms. In this article, we present the first machine learning-based solution to detect fraud-agents in online microlending. The key challenge of this decade-long problem is that it is unclear how to construct effective features from multiple behavior logs such as phone call history, address book, loan history and activity logs of borrowers. To address this problem, we first conduct an empirical study on over 600K borrowers to gain some insights on the adversarial behaviors of fraud-agents comparing to normal borrowers and benign-agents. Based on the study, we are able to design a total of 26 features, falling into four groups, for fraud agent detection. Then, we propose a two-stage detection model to address the challenge of limited number of labeled fraud agent examples. The evaluation results show that our method can achieve a precision of 94.30%. We deploy our method on a real large online microlending platform with 11,953,273 borrowers, and we identify 29,727 fraud-agents from them. The domain experts from the platform confirm that 95.59% of them are real fraud-agents, and have added them to the platform’s internal blacklist. We further conduct a measurement study on those fraud-agents to share deeper insights on their adversarial behaviors.","1941-0018","","10.1109/TDSC.2022.3151132","Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:LR19F020003); National Natural Science Foundation of China(grant numbers:U20A20173,62102360,62125206,62102363); Natural Science Foundation of Zhejiang Province(grant numbers:LQ21F020010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713669","Fraud-agents detection;online lending;empirical study","History;Feature extraction;Wireless fidelity;Social networking (online);Systematics;Peer-to-peer computing;Computer science","","5","","48","IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"Attack Hypotheses Generation Based on Threat Intelligence Knowledge Graph","F. K. Kaiser; U. Dardik; A. Elitzur; P. Zilberman; N. Daniel; M. Wiens; F. Schultmann; Y. Elovici; R. Puzis","Institute for Industrial Production (IIP), Competence Center for Applied Security Technology and Institute of Information Security and Dependability (KASTEL), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Cyber@BGU, Beersheba, Israel; Cyber@BGU, Beersheba, Israel; Cyber@BGU, Beersheba, Israel; Cyber@BGU, Beersheba, Israel; TU Bergakademie Freiberg, Freiberg, Germany; Institute for Industrial Production (IIP), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Cyber@BGU, Beersheba, Israel; Cyber@BGU, Beersheba, Israel","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4793","4809","Cyber threat intelligence on past attacks may help with attack reconstruction and the prediction of the course of an ongoing attack by providing deeper understanding of the tools and attack patterns used by attackers. Therefore, cyber security analysts employ threat intelligence, alert correlations, machine learning, and advanced visualizations in order to produce sound attack hypotheses. In this article, we present AttackDB, a multi-level threat knowledge base that combines data from multiple threat intelligence sources to associate high-level ATT&CK techniques with low-level telemetry found in behavioral malware reports. We also present the Attack Hypothesis Generator which relies on knowledge graph traversal algorithms and a variety of link prediction methods to automatically infer ATT&CK techniques from a set of observable artifacts. Results of experiments performed with 53K VirusTotal reports indicate that the proposed algorithms employed by the Attack Hypothesis Generator are able to produce accurate adversarial technique hypotheses with a mean average precision greater than 0.5 and area under the receiver operating characteristic curve of over 0.8 when it is implemented on the basis of AttackDB. The presented toolkit will help analysts to improve the accuracy of attack hypotheses and to automate the attack hypothesis generation process.","1941-0018","","10.1109/TDSC.2022.3233703","Cyber Security Research Center at BGU; Helmholtz Information & Data Science Academy; NRF National Research Foundation(grant numbers:NRF2016NCR-NCR001-012); U.S.-Israel Energy Center; BIRD Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005832","Attack hypotheses;cyber threat intelligence;data fusion;link prediction","Cyber threat intelligence;Security;Computer crime;Knowledge based systems;Task analysis;Organizations;Generators","","4","","61","IEEE","4 Jan 2023","","","IEEE","IEEE Journals"
"Achieving Privacy-Preserving Discrete Fréchet Distance Range Queries","Y. Guan; R. Lu; Y. Zheng; S. Zhang; J. Shao; G. Wei","Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Faculty of Computer Science, University of New Brunswick, Fredericton, Canada; Zhejiang Gongshang University, Hangzhou, Zhejiang, China; Zhejiang Gongshang University, Hangzhou, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2097","2110","The advances in Internet of Things, Big Data, and machine learning technologies have greatly transformed our daily lives into much more intelligent ones by offering various promising services. Among those services, the discrete Fréchet distance (DFD) range query, which aims to obtain a set of trajectories whose distances to a given query trajectory do not exceed a given threshold, has been widely applied to support applications such as vehicle trajectory clustering and other data processing tasks. Meanwhile, due to the huge data volume issue in the Big Data era, there is a trend towards outsourcing various query services to the cloud for achieving a better performance. However, since the cloud is not fully trustable, designing privacy-preserving query services becomes a research focus. Over the past years, many schemes focusing on privacy-preserving trajectory analysis have been proposed, but none of them can well support privacy-preserving DFD range queries. Aiming at addressing this challenge, this paper proposes a novel privacy-preserving DFD range query scheme, in which queries are conducted in a filtration-and-verification manner and the privacy of the dataset and queries can be preserved. Specifically, by indexing the dataset with two R-trees, a query can be conducted by i) querying the two R-trees to obtain a candidate set and ii) verifying each trajectory in the set, which involve two basic operations, namely, rectangle intersection detection and proximity detection. To preserve the privacy of the dataset and queries, we build the two basic operations upon a novel Inner-Product Preserving Encryption (IPPE) scheme, which is proved to be selectively secure with trivial leakages. Besides, extensive experiments are conducted, and the results demonstrate that our proposed scheme can significantly reduce the computational cost by effectively reducing the candidate set’s size.","1941-0018","","10.1109/TDSC.2022.3171980","NSERC(grant numbers:04009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767631","Trajectory similarity;discrete Fréchet distance;range query;privacy-preserving;outsourced encrypted data","Trajectory;Servers;Data privacy;Data models;Cloud computing;Task analysis;Internet of Things","","3","","29","IEEE","3 May 2022","","","IEEE","IEEE Journals"
"PAD: Towards Principled Adversarial Malware Detection Against Evasion Attacks","D. Li; S. Cui; Y. Li; J. Xu; F. Xiao; S. Xu","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Engineering, Nanjing Institute of Technology, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computer Science, University of Colorado Colorado Springs, Colorado Springs, CO, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","920","936","Machine Learning (ML) techniques can facilitate the automation of malicious software (malware for short) detection, but suffer from evasion attacks. Many studies counter such attacks in heuristic manners, lacking theoretical guarantees and defense effectiveness. In this article, we propose a new adversarial training framework, termed Principled Adversarial Malware Detection (PAD), which offers convergence guarantees for robust optimization methods. PAD lays on a learnable convex measurement that quantifies distribution-wise discrete perturbations to protect malware detectors from adversaries, whereby for smooth detectors, adversarial training can be performed with theoretical treatments. To promote defense effectiveness, we propose a new mixture of attacks to instantiate PAD to enhance deep neural network-based measurements and malware detectors. Experimental results on two Android malware datasets demonstrate: (i) the proposed method significantly outperforms the state-of-the-art defenses; (ii) it can harden ML-based malware detection against 27 evasion attacks with detection accuracies greater than 83.45%, at the price of suffering an accuracy decrease smaller than 2.16% in the absence of attacks; (iii) it matches or outperforms many anti-malware scanners in VirusTotal against realistic adversarial malware.","1941-0018","","10.1109/TDSC.2023.3265665","NJUPT Research(grant numbers:XK0040922014); National Natural Science Foundation of China(grant numbers:61772284,61872193,62072254,62272237); National Science Fund for Distinguished Young Scholars of China(grant numbers:62125203); National Natural Science Foundation of China(grant numbers:61932013); National Science Foundation(grant numbers:#2122631,#2115134); Colorado State Bill 18-086; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097506","Adversarial example;deep neural network;evasion attack;malware detection;provable defense","Malware;Training;Detectors;Feature extraction;Convergence;Perturbation methods;Robustness","","2","","78","IEEE","7 Apr 2023","","","IEEE","IEEE Journals"
"Privacy-Preserving Collaborative Learning Through Feature Extraction","A. Sarmadi; H. Fu; P. Krishnamurthy; S. Garg; F. Khorrami","Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","486","498","We propose a framework in which multiple entities collaborate to build a machine learning model while preserving privacy of their data. The approach utilizes feature embeddings from shared/per-entity feature extractors transforming data into a feature space for cooperation between entities. We propose two specific methods and compare them with a baseline method. In Shared Feature Extractor (SFE) Learning, the entities use a shared feature extractor to compute feature embeddings of samples. In Locally Trained Feature Extractor (LTFE) Learning, each entity uses a separate feature extractor, and models are trained using concatenated features from all entities. As a baseline, in Cooperatively Trained Feature Extractor (CTFE) Learning, the entities train models by sharing raw data. Secure multi-party algorithms are utilized to train models without revealing data or features in plain text. We investigate the trade-offs among SFE, LTFE, and CTFE in regard to performance, privacy leakage (using an off-the-shelf membership inference attack), and computational cost. LTFE provides the most privacy, followed by SFE, and then CTFE. Computational cost is lowest for SFE and the relative speed of CTFE and LTFE depends on network architecture. CTFE and LTFE provide the best accuracy. We use three different datasets for evaluations.","1941-0018","","10.1109/TDSC.2023.3263507","Defense Advanced Research Projects Agency(grant numbers:#HR00112090103); National Science Foundation(grant numbers:#1565396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089424","Collaborative learning;privacy-preserving training;secure multiparty computation;neural networks;feature extractor","Feature extraction;Cryptography;Training;Servers;Computational modeling;Protocols;Data models","","","","65","IEEE","31 Mar 2023","","","IEEE","IEEE Journals"
"CD-VulD: Cross-Domain Vulnerability Discovery Based on Deep Domain Adaptation","S. Liu; G. Lin; L. Qu; J. Zhang; O. De Vel; P. Montague; Y. Xiang","School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Data61, Docklands, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Defence Science &amp; Technology Group, Edinburgh, SA, Australia; Defence Science &amp; Technology Group, Edinburgh, SA, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","438","451","A major cause of security incidents such as cyber attacks is rooted in software vulnerabilities. These vulnerabilities should ideally be found and fixed before the code gets deployed. Machine learning-based approaches achieve state-of-the-art performance in capturing vulnerabilities. These methods are predominantly supervised. Their prediction models are trained on a set of ground truth data where the training data and test data are assumed to be drawn from the same probability distribution. However, in practice, the test data often differs from the training data in terms of distribution because they are from different projects or they differ in the types of vulnerability. In this article, we present a new system for <underline>C</underline>ross <underline>D</underline>omain Software <underline>Vul</underline>nerability <underline>D</underline>iscovery (<italic>CD-VulD</italic>) using deep learning (DL) and domain adaptation (DA). We employ DL because it has the capacity of automatically constructing high-level abstract feature representations of programs, which are likely of more cross-domain useful than the handcrafted features driven by domain knowledge. The divergence between distributions is reduced by learning cross-domain representations. First, given software program representations, CD-VulD converts them into token sequences and learns the token embeddings for generalization across tokens. Next, CD-VulD employs a deep feature model to build abstract high-level presentations based on those sequences. Then, the metric transfer learning framework (MTLF) technique is employed to learn cross-domain representations by minimizing the distribution divergence between the source domain and the target domain. Finally, the cross-domain representations are used to build a classifier for vulnerability detection. Experimental results show that CD-VulD outperforms the state-of-the-art vulnerability detection approaches by a wide margin. We make the new datasets publicly available so that our work is replicable and can be further improved.","1941-0018","","10.1109/TDSC.2020.2984505","Defence Science and Technology Group&#x2018;s Next Generation Technologies Program(grant numbers:DP200100886); Linkage(grant numbers:LP180100170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054952","Cross-domain;vulnerability detection/discovery;deep learning;machine learning;domain adaptation","Software;Deep learning;Training data;Training;Predictive models;Security;Data models","","31","","44","IEEE","2 Apr 2020","","","IEEE","IEEE Journals"
"Optimizing Privacy-Preserving Outsourced Convolutional Neural Network Predictions","M. Li; S. S. M. Chow; S. Hu; Y. Yan; C. Shen; Q. Wang","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong; Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, National Engineering Research Center for Big Data Technology and System, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, Hubei, China; MOE Key Laboratory for Intelligent Networks and Network Security, School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1592","1604","Convolutional neural networks (CNN) is a popular architecture in machine learning for its predictive power, notably in computer vision and medical image analysis. Its great predictive power requires extensive computation, which encourages model owners to host the prediction service in a cloud platform. This article proposes a CNN prediction scheme that preserves privacy in the outsourced setting, i.e., the model-hosting server cannot learn the query, (intermediate) results, and the model. Similar to SecureML (S&P’17), a representative work that provides model privacy, we employ two non-colluding servers with secret sharing and triplet generation to minimize the usage of heavyweight cryptography. We made the following optimizations for both overall latency and accuracy. 1) We adopt asynchronous computation and SIMD for offline triplet generation and parallelizable online computation. 2) As MiniONN (CCS’17) and its improvement by the generic EzPC compiler (EuroS&P’19), we use a garbled circuit for the non-polynomial ReLU activation to keep the same accuracy as the underlying network (instead of approximating it in SecureML prediction). 3) For the pooling in CNN, we employ (linear) average-pooling, which achieves almost the same accuracy as the (non-linear, and hence less efficient) max-pooling exhibited by MiniONN and EzPC. Considering both offline and online costs, our experiments on the MNIST dataset show a latency reduction of $122\times$122×, $14.63\times$14.63×, and $36.69\times$36.69× compared to SecureML, MiniONN, and EzPC; and a reduction of communication costs by $1.09\times$1.09×, $36.69\times$36.69×, and $31.32\times$31.32×, respectively. On the CIFAR dataset, our scheme achieves a lower latency by $7.14\times$7.14× and $3.48\times$3.48× and lower communication costs by $13.88\times$13.88× and $77.46\times$77.46× when compared with MiniONN and EzPC, respectively.","1941-0018","","10.1109/TDSC.2020.3029899","National Key Research and Development Program of China(grant numbers:2020AAA0107700); National Natural Science Foundation of China(grant numbers:61822207,U1636219); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); General Research Fund(grant numbers:CUHK 14210319); Research Grants Council, University Grants Committee; Fundamental Research Funds for the Central Universities(grant numbers:2020kfyXJJS075); National Natural Science Foundation of China(grant numbers:61822309,61773310); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9219246","Secure outsourcing;machine learning;convolutional neural network;homomorphic encryption","Cryptography;Computational modeling;Servers;Predictive models;Privacy;Convolutional neural networks","","28","","46","IEEE","9 Oct 2020","","","IEEE","IEEE Journals"
"“Seeing is Not Always Believing”: Detecting Perception Error Attacks Against Autonomous Vehicles","J. Liu; J. -M. Park","Department of Electrical and Computer Engineering, Virginia Tech, Arlington, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Arlington, VA, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2209","2223","Due to the great achievements in artificial intelligence, it is predicted that autonomous vehicles with little or even no human involvement will come to market in the near future. Autonomous vehicles are equipped with multiple types of sensors. An autonomous vehicle relies on its sensors to perceive its environment, and this sensory information plays a key role in the vehicle's driving decisions. Hence, ensuring the trustworthiness of the sensor data is crucial for drivers' safety. In this article, we discuss the impact of perception error attacks (PEAs) on autonomous vehicles, and propose a countermeasure called LIFE (LIDAR and Image data Fusion for detecting perception Errors). LIFE detects PEAs by analyzing the consistency between camera image data and LIDAR data using novel machine learning and computer vision algorithms. The performance of LIFE has been evaluated extensively using the KITTI dataset.","1941-0018","","10.1109/TDSC.2021.3078111","National Science Foundation(grant numbers:1822173); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425012","Autonomous vehicle;machine learning;computer vision;stereo camera;LIDAR;perception error attack","Laser radar;Sensors;Cameras;Autonomous vehicles;Sensor fusion;Feature extraction;Sensor phenomena and characterization","","13","","43","IEEE","6 May 2021","","","IEEE","IEEE Journals"
"How About Bug-Triggering Paths? - Understanding and Characterizing Learning-Based Vulnerability Detectors","X. Cheng; X. Nie; N. Li; H. Wang; Z. Zheng; Y. Sui","University of Technology Sydney, Ultimo, NSW, Australia; Beijing University of Posts and Telecommunications, Beijing, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Beihang University, Beijing, China; University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","542","558","Machine learning and its promising branch deep learning have proven to be effective in a wide range of application domains. Recently, several efforts have shown success in applying deep learning techniques for automatic vulnerability discovery, as alternatives to traditional static bug detection. In principle, these learning-based approaches are built on top of classification models using supervised learning. Depending on the different granularities to detect vulnerabilities, these approaches rely on learning models which are typically trained with well-labeled source code to predict whether a program method, a program slice, or a particular code line contains a vulnerability or not. The effectiveness of these models is normally evaluated against conventional metrics including precision, recall and F1 score. In this paper, we show that despite yielding promising numbers, the above evaluation strategy can be insufficient and even misleading when evaluating the effectiveness of current learning-based approaches. This is because the underlying learning models only produce the classification results or report individual/isolated program statements, but are unable to pinpoint bug-triggering paths, which is an effective way for bug fixing and the main aim of static bug detection. Our key insight is that a program method or statement can only be stated as vulnerable in the context of a bug-triggering path. In this work, we systematically study the gap between recent learning-based approaches and conventional static bug detectors in terms of fine-grained metrics called BTP metrics using bug-triggering paths. We then characterize and compare the quality of the prediction results of existing learning-based detectors under different granularities. Finally, our comprehensive empirical study reveals several key issues and challenges in developing classification models to pinpoint bug-triggering paths and calls for more advanced learning-based bug detection techniques.","1941-0018","","10.1109/TDSC.2022.3192419","Australian Research(grant numbers:DP200101328,DP210101348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833339","Software vulnerabilities;machine learning;bug-triggering paths;empirical study","Computer bugs;Codes;Measurement;Feature extraction;Detectors;Training;Predictive models","","13","","71","IEEE","19 Jul 2022","","","IEEE","IEEE Journals"
"MsDroid: Identifying Malicious Snippets for Android Malware Detection","Y. He; Y. Liu; L. Wu; Z. Yang; K. Ren; Z. Qin","Department of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2025","2039","Machine learning has shown promise for improving the accuracy of Android malware detection in the literature. However, it is challenging to (1) stay robust towards real-world scenarios and (2) provide interpretable explanations for experts to analyse. In this article, we propose MsDroid, an Android malware detection system that makes decisions by identifying malicious snippets with interpretable explanations. We mimic a common practice of security analysts, i.e., filtering APIs before looking through each method, to focus on local snippets around sensitive APIs instead of the whole program. Each snippet is represented with a graph encoding both code attributes and domain knowledge and then classified by Graph Neural Network (GNN). The local perspective helps the GNN classifier to concentrate on code highly correlated with malicious behaviors, and the information contained in graphs benefit in better understanding of the behaviors. Hence, MsDroid is more robust and interpretable in nature. To identify malicious snippets, we present a semi-supervised learning approach that only requires app labeling. The key insight is that malicious snippets only exist in malwares and appear at least once in a malware. To make malicious snippets less opaque, we design an explanation mechanism to show the importance of control flows and to retrieve similarly implemented snippets from known malwares. A comprehensive comparison with 5 baseline methods is conducted on a dataset of more than 81K apps in 3 real-world scenarios, including zero-day, evolution, and obfuscation. The experimental results show that MsDroid is more robust than state-of-the-art systems in all cases, with 5.37% to 49.52% advantage in F1-score. Besides, we demonstrate that the provided explanations are effective and illustrate how the explanations facilitate malware analysis.","1941-0018","","10.1109/TDSC.2022.3168285","National Key Research and Development Program of China(grant numbers:2020AAA0107705); National Natural Science Foundation of China(grant numbers:U20A20178); National Key Research and Development Program of China(grant numbers:2020AAA0107705); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762803","Android malware detection;graph neural network;explainable machine learning;static code analysis","Malware;Codes;Task analysis;Feature extraction;Security;Semantics;Labeling","","8","","66","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"PILE: Robust Privacy-Preserving Federated Learning Via Verifiable Perturbations","X. Tang; M. Shen; Q. Li; L. Zhu; T. Xue; Q. Qu","School of Information Engineering, Minzu University of China, Beijing, China; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Blockchain Lab, Huawei Cloud Tech Co., Ltd., Shenzhen, China; Blockchain Lab, Huawei Cloud Tech Co., Ltd., Shenzhen, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","5005","5023","Federated learning (FL) protects training data in clients by collaboratively training local machine learning models of clients for a global model, instead of directly feeding the training data to the server. However, existing studies show that FL is vulnerable to various attacks, resulting in training data leakage or interfering with the model training. Specifically, an adversary can analyze local gradients and the global model to infer clients’ data, and poison local gradients to generate an inaccurate global model. It is extremely challenging to guarantee strong privacy protection of training data while ensuring the robustness of model training. None of the existing studies can achieve the goal. In this paper, we propose a robust privacy-preserving federated learning framework (PILE), which protects the privacy of local gradients and global models, while ensuring their correctness by gradient verification where the server verifies the computation process of local gradients. In PILE, we develop a verifiable perturbation scheme that makes confidential local gradients verifiable for gradient verification. In particular, we build two building blocks of zero-knowledge proofs for the gradient verification without revealing both local gradients and global models. We perform rigorous theoretical analysis that proves the security of PILE and evaluate PILE on both passive and active membership inference attacks. The experiment results show that the attack accuracy under PILE is between $[50.3\%,50.9\%]$[50.3%,50.9%], which is close to the random guesses. Particularly, compared to prior defenses that incur the accuracy losses ranging from 2% to 13%, the accuracy loss of PILE is negligible, i.e., only $\pm 0.3\%$±0.3% accuracy loss.","1941-0018","","10.1109/TDSC.2023.3239007","National Key R&D Program of China(grant numbers:2020YFB1005600); NSFC Projects(grant numbers:62132011,62222201,61972039); Beijing Nova Program(grant numbers:Z201100006820006,20220484174); Beijing Natural Science Foundation(grant numbers:L222098); CCF-Huawei Populus Euphratica Blockchain Foundation(grant numbers:CCF-HuaweiC2021007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024795","Distributed machine learning;federated learning;privacy protection;verifiability","Computational modeling;Servers;Privacy;Data models;Training data;Training;Perturbation methods","","6","","60","IEEE","23 Jan 2023","","","IEEE","IEEE Journals"
"Gotham Testbed: A Reproducible IoT Testbed for Security Experiments and Dataset Generation","X. Sáez-de-Cámara; J. L. Flores; C. Arellano; A. Urbieta; U. Zurutuza","Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Arrasate-Mondragón, Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Arrasate-Mondragón, Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Arrasate-Mondragón, Spain; Ikerlan Technology Research Centre, Basque Research and Technology Alliance (BRTA), Arrasate-Mondragón, Spain; Mondragon Unibertsitatea, Arrasate-Mondragón, Spain","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","186","203","The growing adoption of the Internet of Things (IoT) has brought a significant increase in attacks targeting those devices. Machine learning (ML) methods have shown promising results for intrusion detection; however, the scarcity of IoT datasets remains a limiting factor in developing ML-based security systems for IoT scenarios. Static datasets get outdated due to evolving IoT architectures and threat landscape; meanwhile, the testbeds used to generate them are rarely published. This article presents the Gotham testbed, a reproducible and flexible security testbed extendable to accommodate new emulated devices, services or attackers. Gotham is used to build an IoT scenario composed of 100 emulated devices communicating via MQTT, CoAP and RTSP protocols, among others, in a topology composed of 30 switches and 10 routers. The scenario presents three threat actors, including the entire Mirai botnet lifecycle and additional red-teaming tools performing DoS, scanning, and attacks targeting IoT protocols. The testbed has many purposes, including a cyber range, testing security solutions, and capturing network and application data to generate datasets. We hope that researchers can leverage and adapt Gotham to include other devices, state-of-the-art attacks and topologies to share scenarios and datasets that reflect the current IoT settings and threat landscape.","1941-0018","","10.1109/TDSC.2023.3247166","Horizon Europe program(grant numbers:101021911); Ayudas Cervera para Centros Tecnológicos(grant numbers:CER-20191012); Basque Country Government(grant numbers:KK-2021/00091); Intelligent Systems for Industrial Systems research group of Mondragon Unibertsitatea(grant numbers:IT1676-22); Department of Education, Universities and Research of the Basque Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049670","Botnet;emulation;Internet of Things;machine learning;network security;testbed","Internet of Things;Botnet;Protocols;Security;Malware;Servers;Sensors","","3","","77","CCBY","22 Feb 2023","","","IEEE","IEEE Journals"
"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization","S. Li; M. Xue; B. Z. H. Zhao; H. Zhu; X. Zhang","Shanghai Jiao Tong University, Shanghai, China; University of Adelaide, Adelaide, SA, Australia; University of Adelaide, Adelaide, SA, Australia; University of New South Wales, Sydney, NSW, Australia; Data61 CSIRO, Canberra, ACT, Australia","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2088","2105","Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this article, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches.","1941-0018","","10.1109/TDSC.2020.3021407","National Natural Science Foundation of China(grant numbers:61972453,61672350,U1936214,U1636206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186317","Backdoor attacks;steganography;deep neural networks","Training;Data models;Machine learning;Perturbation methods;Neural networks;Inspection;Image color analysis","","88","","55","IEEE","3 Sep 2020","","","IEEE","IEEE Journals"
"Taking Care of the Discretization Problem: A Comprehensive Study of the Discretization Problem and a Black-Box Adversarial Attack in Discrete Integer Domain","L. Bu; Z. Zhao; Y. Duan; F. Song","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Information Science and Technology, Shanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech University, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3200","3217","Neural network based (NN-based) classifiers are known vulnerable against adversarial examples, namely, adding slight perturbations to a benign image cause a classifier to make a false prediction. To evaluate the robustness of NN-based classifiers against adversarial examples, numerous adversarial attacks with high success rates have been proposed recently. NN-based image classifiers usually normalize valid images (e.g., RGB image where the value at each coordinate is an integer between 0 and 255) into a real continuous domain (e.g., 3-dimensional matrix where the value at each coordinate is a real number between 0 and 1) and make classification decisions on the normalized images. However, adversarial examples crafted in a real continuous domain may become benign once they are denormalized back into the corresponding discrete integer domain, known as the discretization problem. This problem has been mentioned in some prior works but received relatively limited attention. In this work, we report the first comprehensive study of existing works to understand the impacts of the discretization problem. By analyzing 35 representative methods and empirically studying 20 representative open source tools, we found 29/35 (theoretically) and 14/20 (empirically) are affected by the discretization problem, e.g., the success rate could dramatically drop from 100 to 10 percent after the domain transformation. As the first step towards addressing this problem in a black-box scenario, we propose a novel derivative-free optimization method, which can directly craft adversarial examples in the discrete integer domain. Experimental results show that the method achieves nearly 100 percent attack success rates for both targeted and untargeted attacks, comparable to the most popular white-box methods (FGSM, BIM and C&W), and significantly outperforms representative black-box methods (ZOO, AutoZOOM, NES-PGD, Bandits, FD, FD-PSO and GenAttack). Our results suggest that the discretization problem should be treated more seriously, and the discrete optimization algorithms show a promising future in crafting effective black-box attacks.","1941-0018","","10.1109/TDSC.2021.3088661","Jiangsu Natural Science Fundation(grant numbers:BK20202001); National Natural Science Foundation of China(grant numbers:62032010); National Natural Science Foundation of China(grant numbers:62072309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453106","Adversarial examples;deep neural networks;discretization;black-box attacks;derivative-free optimization","Tools;Estimation;Tuning;Machine learning algorithms;Artificial neural networks;Search problems;Robustness","","7","","121","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Anchor Link Prediction for Privacy Leakage via De-Anonymization in Multiple Social Networks","H. Wang; W. Yang; D. Man; W. Wang; J. Lv","College of Computer Science and Technology, Harbin Engineering University, Harbin, Heilongjiang, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, Heilongjiang, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, Heilongjiang, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, Heilongjiang, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, Heilongjiang, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","5197","5213","Anchor link prediction exacerbates the risk of privacy leakage via the de-anonymization of social network data. Embedding-based methods for anchor link prediction are limited by the excessive similarity of the associated nodes in a latent feature space and the variation between latent feature spaces caused by the semantics of different networks. In this article, we propose a novel method which reduces the impact of semantic discrepancies between different networks in the latent feature space. The proposed method consists of two phases. First, graph embedding focuses on the network structural roles of nodes and increases the distinction between the associated nodes in the embedding space. Second, a federated adversarial learning framework which performs graph embedding on each social network and an adversarial learning model on the server according to the observable anchor links is used to associate independent graph embedding approaches on different social networks. The combination of distinction enhancement and the association of graph embedding approaches alleviates variance between the latent feature spaces caused by the semantics of different social networks. Extensive experiments on real social networks demonstrate that the proposed method significantly outperforms the state-of-the-art methods in terms of both precision and robustness.","1941-0018","","10.1109/TDSC.2023.3242009","NSFC-Xinjiang Joint Fund Key Program(grant numbers:U2003206); National Natural Science Foundation of China(grant numbers:61831007); Basic Scientific Research projects(grant numbers:JCKY20200604B004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036110","Anchor link prediction;de-anonymization;federated adversarial learning;graph embedding;privacy leakage;social network","Social networking (online);Semantics;Blogs;Privacy;Adversarial machine learning;Predictive models;Servers","","","","49","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"Social Fingerprinting: Detection of Spambot Groups Through DNA-Inspired Behavioral Modeling","S. Cresci; R. D. Pietro; M. Petrocchi; A. Spognardi; M. Tesconi","Institute of Informatics and Telematics, IIT-CNR, Pisa, Italy; Nokia Bell Labs, Paris, France; Institute of Informatics and Telematics, IIT-CNR, Pisa, Italy; DTU Compute, Technical University of Denmark, Kgs. Lyngby, Denmark; Institute of Informatics and Telematics, IIT-CNR, Pisa, Italy","IEEE Transactions on Dependable and Secure Computing","9 Jul 2018","2018","15","4","561","576","Spambot detection in online social networks is a long-lasting challenge involving the study and design of detection techniques capable of efficiently identifying ever-evolving spammers. Recently, a new wave of social spambots has emerged, with advanced human-like characteristics that allow them to go undetected even by current state-of-the-art algorithms. In this paper, we show that efficient spambots detection can be achieved via an in-depth analysis of their collective behaviors exploiting the digital DNA technique for modeling the behaviors of social network users. Inspired by its biological counterpart, in the digital DNA representation the behavioral lifetime of a digital account is encoded in a sequence of characters. Then, we define a similarity measure for such digital DNA sequences. We build upon digital DNA and the similarity between groups of users to characterize both genuine accounts and spambots. Leveraging such a characterization, we design the Social Fingerprinting technique, which is able to discriminate among spambots and genuine accounts in both a supervised and an unsupervised fashion. We also evaluate the effectiveness of Social Fingerprinting and we compare it with three state-of-the-art detection showing the superiority of our solution. Finally, among the peculiarities of our approach is the possibility to apply off-the-shelf DNA analysis techniques to study online users behaviors and to efficiently rely on a limited number of lightweight account characteristics.","1941-0018","","10.1109/TDSC.2017.2681672","EU H2020 Program(grant numbers:INFRAIA-1-2014-2015); Research Infrastructures(grant numbers:#654024); MSCA-ITN-2015-ETN(grant numbers:#675320); SmartNews(grant numbers:PAR-FAS 2007-2013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876716","Spambot detection;social bots;online social networks;Twitter;behavioral modeling;digital DNA","DNA;Twitter;Fingerprint recognition;Biological system modeling;Computational modeling;Informatics","","36","","56","IEEE","13 Mar 2017","","","IEEE","IEEE Journals"
"SEAR: Secure and Efficient Aggregation for Byzantine-Robust Federated Learning","L. Zhao; J. Jiang; B. Feng; Q. Wang; C. Shen; Q. Li","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; Khoury College of Computer Sciences, Northeastern University, Boston, MA, USA; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; MOE Key Laboratory for Intelligent Networks and Network Security, School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3329","3342","Federated learning facilitates the collaborative training of a global model among distributed clients without sharing their training data. Secure aggregation, a new security primitive for federated learning, aims to preserve the confidentiality of both local models and training data. Unfortunately, existing secure aggregation solutions fail to defend against Byzantine failures that are common in distributed computing systems. In this work, we propose a new secure and efficient aggregation framework, SEAR, for Byzantine-robust federated learning. Relying on the trusted execution environment, i.e., Intel SGX, SEAR protects clients’ private models while enabling Byzantine resilience. Considering the limitation of the current Intel SGX's architecture (i.e., the limited trusted memory), we propose two data storage modes to efficiently implement aggregation algorithms efficiently in SGX. Moreover, to balance the efficiency and performance of aggregation, we propose a sampling-based method to efficiently detect Byzantine failures without degrading the global model's performance. We implement and evaluate SEAR in a LAN environment, and the experiment results show that SEAR is computationally efficient and robust to Byzantine adversaries. Compared to the previous practical secure aggregation framework, SEAR improves aggregation efficiency by 4-6 times while supporting Byzantine resilience at the same time.","1941-0018","","10.1109/TDSC.2021.3093711","National Key R&D Program of China(grant numbers:2020AAA0107701); National Natural Science Foundation of China(grant numbers:U20B2049,61822207,61822309,61773310,U1736205); BNRist(grant numbers:BNR2020RC0101); Fundamental Research Funds for Central Universities(grant numbers:2042021gf0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468910","Federated learning;secure aggregation;trusted execution environment","Servers;Computational modeling;Collaborative work;Data models;Privacy;Cryptography;Training data","","28","","51","CCBY","30 Jun 2021","","","IEEE","IEEE Journals"
"LARGen: Automatic Signature Generation for Malwares Using Latent Dirichlet Allocation","S. Lee; S. Kim; S. Lee; J. Choi; H. Yoon; D. Lee; J. -R. Lee","Korea National University of Transportation, Uiwang-si, Korea; National Security Research Institute, Daejeon, Korea; National Security Research Institute, Daejeon, Korea; Gachon University, Seongnam-si, Korea; National Security Research Institute, Daejeon, Korea; National Security Research Institute, Daejeon, Korea; Kangwon National University, Samcheok-si, Korea","IEEE Transactions on Dependable and Secure Computing","30 Aug 2018","2018","15","5","771","783","As the quantity and complexity of network threats grow, Intrusion Detection Systems (IDSs) have become critical for securing networks. Achieving computer network intrusion detection with these IDSs requires high-level information technology and security expertise because malicious traffic has to be rigorously analyzed and the appropriate IDS rules written to effectively detect vulnerabilities that may potentially be exploited. However, incorrect IDS rules may produce numerous false positives, thereby degrading the performance of the IDS, and even worse, paralyzing the network. In this paper, we present a novel approach that exploits the Latent Dirichle Allocation (LDA) algorithm to generate IDS rules. Our proposed method, called LDA-based Automatic Rule Generation (LARGen), automatically performs an analysis of the malicious traffic and extracts the appropriate attack signatures that will be used for IDS rules. LARGen first extracts multiple signature strings embedded in network flows. Then, the flows are classified based on the extracted signature strings, and key content strings for malicious traffic are identified through the LDA inferential topic model. Those key content strings are the core of an IDS rule that can detect malicious traffic. We study the effectiveness of LDA in the context of network attack signature generation via extensive experiments with real network trace data, consisting of both benign and malicious traffic. Experimental results confirm that threat rules generated from LARGen accurately detect every cyber attack with high accuracy.","1941-0018","","10.1109/TDSC.2016.2609907","Korea National University of Transportation; Basic Science Research Program of the National Research Foundation of Korea (NRF)(grant numbers:NRF-2014R1A1A2053456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569096","Intrusion detection system;automated threat rule generation;latent Dirichlet allocation;system design and implementation","Malware;Security;Resource management;Context;Internet;Protocols;Semantics","","22","","56","IEEE","15 Sep 2016","","","IEEE","IEEE Journals"
"Hierarchical Temporal Memory-Based One-Pass Learning for Real-Time Anomaly Detection and Simultaneous Data Prediction in Smart Grids","A. Barua; D. Muthirayan; P. P. Khargonekar; M. A. Al Faruque","Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA; Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA","IEEE Transactions on Dependable and Secure Computing","12 May 2022","2022","19","3","1770","1782","A neuro-cognitive inspired architecture based on the Hierarchical Temporal Memory (HTM) is proposed for anomaly detection and simultaneous data prediction in real-time for smart grid $\mu$μPMU data. The key technical idea is that the HTM learns a sparse distributed temporal representation of sequential data that turns out to be very useful for anomaly detection and simultaneous data prediction in real-time. Our results show that the proposed HTM can predict anomalies within 83–90 percent accuracy for three different application profiles, namely Standard, Reward Few False Positive, Reward Few False Negative for two different datasets. We show that the HTM is competitive to five state-of-the-art algorithms for anomaly detection. Moreover, for the multi-step prediction in the online setting, the same HTM achieves a low 0.0001 normalized mean square error, a low negative log-likelihood score of 1.5 and is also competitive to six state-of-the-art prediction algorithms. We demonstrate that the same HTM model can be used for both the tasks and can learn online in one-pass, in an unsupervised fashion and adapt to changing statistics. The other state-of-the-art algorithms are either less accurate or are limited to one of the tasks or cannot learn online in one-pass, and adapt to changing statistics.","1941-0018","","10.1109/TDSC.2020.3037054","University of California(grant numbers:LFR-18-548175); National Science Foundation(grant numbers:ECCS-1839429,ECCS-2028269,CMMI-1739503); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253615","Smart grid;anomaly detection;simultaneous prediction;hierarchical temporal memory;sparse distributed representation","Anomaly detection;Smart grids;Neurons;Phasor measurement units;Real-time systems;Prediction algorithms;Computer architecture","","10","","44","IEEE","10 Nov 2020","","","IEEE","IEEE Journals"
"Open Source Intelligence for Malicious Behavior Discovery and Interpretation","Y. -T. Huang; C. Y. Lin; Y. -R. Guo; K. -C. Lo; Y. S. Sun; M. C. Chen","Academia Sinica, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; Academia Sinica, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; Academia Sinica, Taipei, Taiwan","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","776","789","Cyber threats are one of the most pressing issues in the digital age. There has been a consensus on deploying a proactive defense to effectively detect and respond to adversary threats. The key to success is understanding the characteristics of malware, including their activities and manipulated resources on the target machines. The MITRE ATT&CK framework (ATT&CK), a popular source of open source intelligence (OSINT), provides rich information and knowledge about adversary lifecycles and attack behaviors. The main challenges of this study involve knowledge collection from ATT&CK, malicious behavior identification using deep learning, and the identification of associated API calls. A MITRE ATT&CK based Malicious Behavior Analysis system (MAMBA) for Windows malware is proposed, which incorporates ATT&CK knowledge and considers attentions on manipulated resources and malicious activities in the neural network model. To synchronize ATT&CK updates in a timely manner, knowledge collection can be an automatic and incremental process. Given these features, MAMBA achieves the best performance of malicious behavior discovery among all the compared learning-based methods and rule-based approaches on all datasets; it also yields a highly interpretable mapping from the discovered malicious behaviors to relevant ATT&CK techniques, as well as to the related API calls.","1941-0018","","10.1109/TDSC.2021.3119008","CITI; Academia Sinica; MOST(grant numbers:110-2218-E-001-001-MBK,109-2221-E-001-010-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566808","Cyber threat intelligence;dynamic analysis;malware behavior analysis;MITRE ATT&CK framework","Malware;Analytical models;Neural networks;Semantics;Computer security;Special issues and sections;Labeling","","10","","67","IEEE","11 Oct 2021","","","IEEE","IEEE Journals"
"Accurate Detection of IoT Sensor Behaviors in Legitimate, Faulty and Compromised Scenarios","K. Sood; M. R. Nosouhi; N. Kumar; A. Gaddam; B. Feng; S. Yu","Deakin University, Geelong, VIC, Australia; Deakin University, Geelong, VIC, Australia; Thapar University, Patiala, Punjab, India; Deakin University, Geelong, VIC, Australia; Beijing Jiaotong University, Beijing, China; University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","288","300","In smart farming sector, Internet of Things (IoT) based smart sensing systems are vulnerable to failure, malfunction, and malicious attacks. Also, sensors are deployed often in an alien and harsh environment. Here, the conditions are not well supportive which either causes the sensor to fail prematurely or gives unusual and erroneous readings, known as outliers. This effects the smart network's performance and decision-making ability in many ways. Therefore, it is important to accurately detect the IoT sensor behaviour in legitimate, faulty, and compromised or attack scenarios. To distinguish the sensor behaviour in different scenarios we have proposed a feasible approach using spatial correlation theory which is validated using Moran's I index tool. We have used Classification and Regression Trees (CART), Random Forest (RF), and Support Vector Machine (SVM) models to test our approach. For real-time anomaly detection we have used an edge computing technology. We have compared the proposed approach, using Forest Fire real dataset, with the three existing recent works. Our results are promising in terms of accurate detection of IoT sensor behaviours in real-time. This will assist the precision farming industry in making better decisions to securely manage IoT field network, increase productivity, and improves operational efficiency.","1941-0018","","10.1109/TDSC.2021.3131991","ARC(grant numbers:200101374); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632348","Internet of Things;IoT behaviour;outlier detection;security;precision farming","Internet of Things;Wireless sensor networks;Sensors;Real-time systems;Digital agriculture;Monitoring;Anomaly detection","","9","","57","IEEE","1 Dec 2021","","","IEEE","IEEE Journals"
"Privacy-Aware Personal Data Storage (P-PDS): Learning how to Protect User Privacy from External Applications","B. C. Singh; B. Carminati; E. Ferrari","Department of Information and Communication Engineering, Islamic University, Kushtia, Bangladesh; DISTA, University of Insubria, Varese, VA, Italy; DISTA, University of Insubria, Varese, VA, Italy","IEEE Transactions on Dependable and Secure Computing","8 Mar 2021","2021","18","2","889","903","Recently, Personal Data Storage (PDS) has inaugurated a substantial change to the way people can store and control their personal data, by moving from a service-centric to a user-centric model. PDS offers individuals the capability to keep their data in a unique logical repository, that can be connected and exploited by proper analytical tools, or shared with third parties under the control of end users. Up to now, most of the research on PDS has focused on how to enforce user privacy preferences and how to secure data when stored into the PDS. In contrast, in this paper we aim at designing a Privacy-aware Personal Data Storage (P-PDS), that is, a PDS able to automatically take privacy-aware decisions on third parties access requests in accordance with user preferences. The proposed P-PDS is based on preliminary results presented in [1] , where it has been demonstrated that semi-supervised learning can be successfully exploited to make a PDS able to automatically decide whether an access request has to be authorized or not. In this paper, we have deeply revised the learning process in order to have a more usable P-PDS, in terms of reduced effort for the training phase, as well as a more conservative approach w.r.t. users privacy, when handling conflicting access requests. We run several experiments on a realistic dataset exploiting a group of 360 evaluators. The obtained results show the effectiveness of the proposed approach.","1941-0018","","10.1109/TDSC.2019.2903802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662576","Personal data storage (PDS);active learning;personalized privacy preferences","Training;Data privacy;Privacy;Memory;Data models;Semisupervised learning;Handheld computers","","9","","36","IEEE","7 Mar 2019","","","IEEE","IEEE Journals"
"Golden Grain: Building a Secure and Decentralized Model Marketplace for MLaaS","J. Weng; J. Weng; C. Cai; H. Huang; C. Wang","College of Information Science and Technology/College of Cyber security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, Guangdong, China; College of Information Science and Technology/College of Cyber security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, Guangdong, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; College of Information Science and Technology/College of Cyber security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, Guangdong, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3149","3167","ML-as-a-service (MLaaS) becomes increasingly popular and revolutionizes the lives of people. A natural requirement for MLaaS is, however, to provide highly accurate prediction services. To achieve this, current MLaaS systems integrate and combine multiple well-trained models in their services. Yet, in reality, there is no easy way for MLaaS providers, especially for startups, to collect sufficiently well-trained models from individual developers, due to the lack of incentives. In this article, we aim to fill this gap by building up a model marketplace, called as Golden Grain, to facilitate model sharing, which enforces the fair model-money swapping process between individual developers and MLaaS providers. Specifically, we deploy the swapping process on the blockchain, and further introduce a blockchain-empowered model benchmarking process for transparently determining the model prices according to their authentic performances, so as to motivate the faithful contributions of well-trained models. Especially, to ease the blockchain overhead for model benchmarking, our marketplace carefully offloads the heavy computation and designs a secure off-chain on-chain interaction protocol based on a trusted execution environment (TEE), for ensuring both the integrity and authenticity of benchmarking. We implement a prototype of our Golden Grain on the Ethereum blockchain, and conduct extensive experiments using standard benchmark datasets to demonstrate the practically affordable performance of our design.","1941-0018","","10.1109/TDSC.2021.3085988","National Key R&D Plan of China(grant numbers:2020YFB1005600); National Natural Science Foundation of China(grant numbers:U1736203,61825203,61732021); Major Program of Guangdong Basic and Applied Research Project(grant numbers:2019B030302008); Guangdong Provincial Science and Technology Project(grant numbers:2017B010111005); Research Grants Council of Hong Kong(grant numbers:CityU 11217819,CityU 11217620); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); National Natural Science Foundation of China(grant numbers:61572412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445602","ML-as-a-service;blockchain;marketplace;trusted execution environment","Benchmark testing;Blockchain;Computational modeling;Data models;Smart contracts;Predictive models;Urban areas","","9","","88","IEEE","2 Jun 2021","","","IEEE","IEEE Journals"
"Cyber Threat Intelligence Sharing for Co-Operative Defense in Multi-Domain Entities","S. Purohit; R. Neupane; N. R. Bhamidipati; V. Vakkavanthula; S. Wang; M. Rockey; P. Calyam","Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri-Columbia, Columbia, MO, USA","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","4273","4290","Cloud-hosted applications are prone to targeted attacks such as DDoS, advanced persistent threats, Cryptojacking which threaten service availability. Recently, methods for threat information sharing and defense require cooperation and trust between multiple domains/entities. There is a need for mechanisms that establish distributed trust to allow for such a collective defense. In this paper, we present a novel threat intelligence sharing and defense system, namely “DefenseChain,” to allow organizations to have incentive-based and trustworthy cooperation to mitigate the impact of cyber attacks. Our solution approach features a consortium Blockchain platform and an economic model to obtain threat data and select suitable peers to help with attack detection and mitigation. We apply DefenseChain in the financial technology industry for an insurance claim processing use case to demonstrate the effectiveness of DefenseChain in a real-world application setting. Our evaluation experiments with DefenseChain implementation are performed on an Open Cloud testbed with Hyperledger Composer and in a simulation environment. Our results show that the DefenseChain system overall performs better than state-of-the-art decision making schemes in choosing the most appropriate detector and mitigator peers. Lastly, we validate how DefenseChain helps mitigate the threat risk of incidents relating to potential fraudulent insurance claims or cyber attacks.","1941-0018","","10.1109/TDSC.2022.3214423","National Security Agency(grant numbers:H98230-20-1-0297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917547","Consortium blockchain;cyber defense;threat intelligence sharing;reputation system","Blockchains;Organizations;Insurance;Cyberattack;Automation;Computer architecture;Measurement","","5","","58","IEEE","13 Oct 2022","","","IEEE","IEEE Journals"
"Towards Attack-Resistant Service Function Chain Migration: A Model-Based Adaptive Proximal Policy Optimization Approach","T. Zhang; C. Xu; B. Zhang; X. Li; X. Kuang; L. A. Grieco","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; National University of Singapore, Singapore; State Key Laboratory of Science and Technology on Information System Security, Beijing, China; Department of Electrical and Information Engineering and CNIT, Politecnico di Bari, Bari, Italy","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4913","4927","Network function virtualization (NFV) supports the rapid development of service function chain (SFC), which efficiently connects a sequence of network virtual function instances (VNFIs) placed into physical infrastructures. Current SFC migration mechanisms usually keep static SFC deployment after finishing certain objectives, and deployment methods mostly provide static resource allocation for VNFIs. Therefore, the adversary has enough time to plan for devastating attacks for in-service SFCs. Fortunately, moving target defense (MTD) was proposed as a game-changing solution to dynamically adjust network configurations. However, existing MTD methods mostly depend on attack-defense models, and lack adaptive mutation period. In this article, we propose an Intelligence-Driven Service Function Chain Migration (ID-SFCM) scheme. First, we model a Markov decision process (MDP) to formulate the dynamic arrival or departure of SFCs. To remove infeasible actions from the action space of MDP, we formalize the SFC deployment as a constrained satisfaction problem. Then, we design a deep reinforcement learning (DRL) algorithm named model-based adaptive proximal policy optimization (MA-PPO) to enable attack-resistant migration decisions and adaptive migration period. Finally, we evaluate the defense performance by multiple attack strategies and two realistic datasets called CICIDS-2017 and LYCOS-IDS2017 respectively. Simulation results highlight the effectiveness of ID-SFCM compared with representative solutions.","1941-0018","","10.1109/TDSC.2023.3237604","National Natural Science Foundation of China(grant numbers:62225105,61871048,61872253); Italian MIUR PRIN(grant numbers:2017NS9FEY); Italian MIUR PON projects AGREED(grant numbers:ARS01 00254); BUPT Excellent Ph.D. Students Foundation(grant numbers:CX2020123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018506","Deep reinforcement learning;moving target defense;service function chain;virtual network function","Virtual links;Service function chaining;Adaptation models;Computer crime;Routing;Resource management;Quality of service","","4","","63","IEEE","17 Jan 2023","","","IEEE","IEEE Journals"
"CryptoRec: Novel Collaborative Filtering Recommender Made Privacy-Preserving Easy","J. Wang; C. Jin; Q. Tang; Z. Liu; K. M. M. Aung","International Electronics & Engineering, Bissen, Luxembourg; I2R, A*Star, Singapore, Singapore; Luxembourg Institute of Science and Technology, Esch-sur-Alzette, Luxembourg; Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; I2R, A*Star, Singapore, Singapore","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2622","2634","With the explosive growth of user data, recommenders have become increasingly complicated. State-of-the-art algorithms often have high computational complexity and heavily use non-linear transformations. This fact makes the privacy-preserving problem more challenging, despite the significant advances in cryptography. To alleviate this problem, we propose a privacy-friendly recommender, CryptoRec. It only relies on additions and multiplications, which are efficiently supported by most cryptographic primitives. Different from others, in CryptoRec, the parameter space only contains item features (user features can be directly computed from the item features). This property allows CryptoRec to, (1) naturally achieve transferability if two datasets share the same item entries, which can benefit differential privacy protection; (2) directly estimate the preference of new users whose data is not included in the training set, drastically improving recommendation efficiency. We first evaluate CryptoRec on three real-world datasets. The evaluation results show that the accuracy is competitive with state-of-the-art. Then, we build differential privacy into CryptoRec and leverage its transferability property to reduce the overall privacy loss. Lastly, we demonstrate the simplicity and efficiency of using CryptoRec to construct secure recommendation protocols based on homomorphic encryption schemes. Our results show that CryptoRec outperforms existing solutions in terms of both accuracy and efficiency.","1941-0018","","10.1109/TDSC.2021.3065752","Agency for Science, Technology and Research; RIE2020 Advanced Manufacturing and Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376980","Privacy-preserving;recommender system;differential privacy;homomorphic encryption","Encryption;Differential privacy;Privacy;Neural networks;Data models;Computational modeling;Training","","4","","64","IEEE","12 Mar 2021","","","IEEE","IEEE Journals"
"Optimizing Secure Decision Tree Inference Outsourcing","Y. Zheng; C. Wang; R. Wang; H. Duan; S. Nepal","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong; Data61, CSIRO, Marsfield, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","3079","3092","Outsourcing decision tree inference services to the cloud is highly beneficial, yet raises critical privacy concerns on the proprietary decision tree of the model provider and the private input data of the client. In this paper, we design, implement, and evaluate a new system that allows highly efficient outsourcing of decision tree inference. Our system significantly improves upon prior art in the overall online end-to-end secure inference service latency at the cloud as well as the local-side performance of the model provider. We first present a new scheme which securely shifts most of the processing of the model provider to the cloud, resulting in a substantial reduction on the model provider's performance complexities. We further devise a scheme which substantially optimizes the performance for secure decision tree inference at the cloud, particularly the communication round complexities. The synergy of these techniques allows our new system to achieve up to $8 \times$8× better overall online end-to-end secure inference latency at the cloud side over realistic WAN environment, as well as bring the model provider up to $19 \times$19× savings in communication and $18 \times$18× savings in computation.","1941-0018","","10.1109/TDSC.2022.3194048","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110027); Shenzhen Science and Technology Program(grant numbers:RCBS20210609103056041); Research Council of Hong Kong(grant numbers:CityU 11217819,11217620,RFS2122-1S04,N_CityU139/21,C2004-21GF,R1012-21,R6021-20F); Shenzhen Municipality Science and Technology Innovation Commission(grant numbers:SGDX20201103093004019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841036","Cloud computing;decision trees;inference service;privacy preservation;secure outsourcing","Decision trees;Cloud computing;Computational modeling;Servers;Complexity theory;Cryptography;Outsourcing","","3","","48","IEEE","26 Jul 2022","","","IEEE","IEEE Journals"
"T-Counter: Trustworthy and Efficient CPU Resource Measurement Using SGX in the Cloud","C. Dong; Q. Shen; X. Ding; D. Yu; W. Luo; P. Wu; Z. Wu","School of Software and Microelectronics, Peking University, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; School of Information Systems, Singapore Management University, Singapore; School of Software and Microelectronics, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Computing, National University of Singapore, Singapore; School of Software and Microelectronics, Peking University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","867","885","As cloud services have become popular, and their adoption is growing, consumers are becoming more concerned about the cost of cloud services. Cloud Service Providers (CSPs) generally use a pay-per-use billing scheme in the cloud services model: consumers use resources as they needed and are billed for their resource usage. However, CSPs are untrusted and privileged; they have full control of the entire operating system (OS) and may tamper with bills to cheat consumers. So, how to provide a trusted solution that can keep track of and verify the consumers’ resource usage has been a challenging problem. In this article, we propose a T-Counter framework based on Intel SGX. The T-Counter allows applications to construct a trusted solution to measure its CPU usage by itself in cloud computing. These constructed applications are instrumented with counters in basic blocks and added three components in trusted parts to count instructions and defend against malicious CSPs’ manipulations. We propose two algorithms which selectively instrument counters in the CFG. T-Counter is implemented as an extension of the LLVM framework and integrated with the SGX SDK. Theoretical analyses and evaluations show that T-Counter can effectively measure CPU usage and defend against malicious CSPs’ manipulations.","1941-0018","","10.1109/TDSC.2022.3145814","National Natural Science Foundation of China(grant numbers:61672062,61232005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693116","Resource measurement;instruction accounting;SGX;cloud services;control flow graph (CFG)","Instruments;Cloud computing;Software;Observers;Switches;Clocks;Security","","3","","53","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"A Fine-Grained Approach for Anomaly Detection in File System Accesses With Enhanced Temporal User Profiles","S. Mehnaz; E. Bertino","Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Cyber Center, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2021","2021","18","6","2535","2550","Protecting sensitive data from theft, exfiltration, and other kinds of abuses by malicious insiders is a challenging problem. While access control mechanisms cannot always prevent the insiders from misusing sensitive data (since, in most of the cases, authorized users within organizations are granted access permissions), malicious outsiders also pose severe threats due to different security vulnerabilities in the systems, e.g., phishing attacks, memory corruptions, etc., which enable them to steal the credentials of the authorized users who have access to the data. To protect sensitive data from such attackers, anomaly detection techniques are often combined with other existing security measures, e.g., access control and encryption. An anomaly detection technique for identifying anomalies in file system accesses is based on the key idea that there should be significant differences between the file access behaviors of a benign user and an attacker. In this article, we propose an approach to create fine-grained profiles of the users’ regular file access activities while extensively analyzing the timestamp information of the file accesses. According to our observation, even if a user’s access to a file seems benign, only a fine-grained analysis of the access (such as the size of access, the timestamp of access) can determine the original intention of the user. We exploit the users’ file access information at the block level to model their regular file access behaviors (user profiles) which are then securely stored and used for identifying anomalous file system accesses in the detection phase. We are also able to automatically profile new files and new users added to the system dynamically. Finally, our performance evaluations demonstrate that our proposed approach has an accuracy of 98.7 percent in detecting anomalies while incurring an overhead of only 2 percent.","1941-0018","","10.1109/TDSC.2019.2954507","Schlumberger Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907413","Insider attacks;anomaly detection;file system access","Cyberattack;Anomaly detection;Access control;Data protection;File systems;Feature extraction","","2","","60","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Towards Class-Balanced Privacy Preserving Heterogeneous Model Aggregation","X. Pang; Z. Wang; Z. He; P. Sun; M. Luo; J. Ren; K. Ren","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; School of Cyber Science and Technology, Zhejiang Provincial Key Laboratory of Blockchain and Cyberspace Governance, Zhejiang University, Hangzhou, Zhejiang, China; School of Cyber Science and Technology, Zhejiang Provincial Key Laboratory of Blockchain and Cyberspace Governance, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; School of Cyber Science and Technology, Zhejiang Provincial Key Laboratory of Blockchain and Cyberspace Governance, Zhejiang University, Hangzhou, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2421","2432","Heterogeneous model aggregation (HMA) is an effective paradigm that integrates on-device trained models heterogeneous in architecture and target task into a comprehensive model. Recent works adopt knowledge distillation to amalgamate the knowledge of learned features and predictions from heterogeneous on-device models to realize HMA. However, most of them ignore that the disclosure of learned features exposes on-device models to privacy attacks. Moreover, the aggregated model may suffer from the imbalanced supervision caused by the uneven distribution of amalgamated knowledge about each class and show class bias. In this article, to address these issues, we propose a response-based class-balanced heterogeneous model aggregation mechanism, called CBHMA. It can effectively achieve HMA in a privacy-preserving manner and alleviate class bias in the aggregated model. Specifically, CBHMA aggregates on-device models by using only their response information to reduce their privacy leakage risk. To mitigate the impact of imbalanced supervision, CBHMA quantitatively measures the imbalanced supervision level for each class. Based on that, CBHMA customizes fine-grained misclassification costs for each class and utilizes such costs to adjust the importance of each class (more importance to classes with weaker supervision) in the response-based HMA algorithm. Extensive experiments on two real-world datasets demonstrate the effectiveness of CBHMA.","1941-0018","","10.1109/TDSC.2022.3183170","National Key R&D Program of China(grant numbers:2021ZD0112803); National Natural Science Foundation of China(grant numbers:62122066,U20A20182,61872274,62122095,U19A2067); Key R&D Program of Zhejiang(grant numbers:2022C01018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796594","","Computational modeling;Training;Privacy;Data models;Costs;Training data;Task analysis","","1","","38","IEEE","15 Jun 2022","","","IEEE","IEEE Journals"
"Slowing Down the Aging of Learning-Based Malware Detectors With API Knowledge","X. Zhang; M. Zhang; Y. Zhang; M. Zhong; X. Zhang; Y. Cao; M. Yang","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; School of Computer Science, Fudan University, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","902","916","Learning-based malware detectors are widely used in practice to safeguard real-world computers. One major challenge is known as model aging, where the effectiveness of these models drops drastically as malware variants keep evolving. To tackle model aging, most existing works choose to label new samples to retrain the aged models. However, such data-perspective methods often require excessive costs in labeling and retraining. In this article, we observe that during evolution, malware samples often preserve similar malicious semantics while switching to new implementations with semantically equivalent APIs. Such observation enables us to look into the problem from a different perspective: feature space. More specifically, if the models can capture the intrinsic semantics of malware variants from feature space, it will help slow down the aging of learning-based detectors. Based on this insight, we design APIGraph to automatically extract API knowledge from API documentation and incorporate these knowledge into the training of malware detection models. We use APIGraph to enhance 5 state-of-the-art malware detectors, covering both Android and Windows platforms and various learning algorithms. Experiments on large-scale, evolutionary datasets with nearly 340K samples show that APIGraph can help slow down the aging of these models by 5.9% to 19.6%, as well as reduce labeling efforts from 33.07% to 96.30% on top of data-perspective methods.","1941-0018","","10.1109/TDSC.2022.3144697","National Natural Science Foundation of China(grant numbers:62102091,U1636204,U1836210,U1836213,U1736208,61972099); China Postdoctoral Science Foundation(grant numbers:BX2021079,2021M690706); Natural Science Foundation of Shanghai(grant numbers:19ZR1404800); National Program on Key Basic Research(grant numbers:2015CB358800); National Science Foundation(grant numbers:CNS-18-54000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693181","Malware detection;model aging;API knowledge;learning-based detection","Malware;Detectors;Aging;Semantics;Sockets;Servers;Computational modeling","","1","","69","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"STD-NET: Search of Image Steganalytic Deep-Learning Architecture via Hierarchical Tensor Decomposition","S. Tan; Q. Li; L. Li; B. Li; J. Huang","Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen University, Shenzhen, China","IEEE Transactions on Dependable and Secure Computing","16 May 2023","2023","20","3","2657","2673","Steganalysis aims to reveal covert communication established via steganography. In the arm race with steganography, steganalysis has evolved from the old-style hand-crafted features set to deep-learning architectures. However, recent studies show that the majority of existing deep steganalysis models have a large amount of redundancy, which leads to a huge waste of storage and computing resources. The existing model compression method cannot flexibly compress the convolutional layer in residual shortcut block so that a satisfactory shrinking rate cannot be obtained. In this article, we propose STD-NET, an unsupervised deep-learning architecture search approach via hierarchical tensor decomposition for image steganalysis. Our proposed strategy will not be restricted by various residual connections, since this strategy does not change the number of input and output channels of the convolution block. We propose a normalized distortion threshold to evaluate the sensitivity of each involved convolutional layer of the base model to guide STD-NET to compress target network in an efficient and unsupervised approach, and obtain two network structures of different shapes with low computation cost and similar performance compared with the original one. Extensive experiments have confirmed that, on one hand, our model can achieve comparable or even better detection performance in various steganalytic scenarios due to the great adaptivity of the obtained network architecture. On the other hand, the experimental results also demonstrate that our proposed strategy is more efficient and can remove more redundancy compared with previous steganalytic network compression methods.","1941-0018","","10.1109/TDSC.2023.3267065","NSFC(grant numbers:U19B2022,62272314,U22B2047); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2019B151502001); Shenzhen R&D Program(grant numbers:JCYJ20200109105008228); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102544","Steganalysis;steganography;deep learning;convolutional neural network;tensor decomposition","Tensors;Kernel;Convolution;Steganography;Computational modeling;Distortion;Computer architecture","","","","59","IEEE","14 Apr 2023","","","IEEE","IEEE Journals"
"A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code","L. Cen; C. S. Gates; L. Si; N. Li","Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN","IEEE Transactions on Dependable and Secure Computing","7 Jul 2015","2015","12","4","400","412","Mobile devices are an important part of our everyday lives, and the Android platform has become a market leader. In recent years a number of approaches for Android malware detection have been proposed, using permissions, source code analysis, or dynamic analysis. In this paper, we propose to use a probabilistic discriminative model based on regularized logistic regression for Android malware detection. Through extensive experimental evaluation, we demonstrate that it can generate probabilistic outputs with highly accurate classification results. In particular, we propose to use Android API calls as features extracted from decompiled source code, and analyze and explore issues in feature granularity, feature representation, feature selection, and regularization. We show that the probabilistic discriminative model also works well with permissions, and substantially outperforms the state-of-the-art methods for Android malware detection with application permissions. Furthermore, the discriminative learning model achieves the best detection results by combining both decompiled source code and application permissions. To the best of our knowledge, this is the first research that proposes probabilistic discriminative model for Android malware detection with a thorough study of desired representation of decompiled source code and is the first research work for Android malware detection task that combines both analysis of decompiled source code and application permissions.","1941-0018","","10.1109/TDSC.2014.2355839","National Science Foundation(grant numbers:1314688); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894210","Android;malicious application;machine learning;discriminative model","Feature extraction;Malware;Androids;Humanoid robots;Smart phones;Probabilistic logic;Measurement","","112","","32","IEEE","8 Sep 2014","","","IEEE","IEEE Journals"
"Real-Time Multistep Attack Prediction Based on Hidden Markov Models","P. Holgado; V. A. Villagrá; L. Vázquez","Departamento de Ingeniería y Sistemas Telemáticos, Universidad Politécnica de Madrid Avenida Complutense, 30, Madrid, Spain; Departamento de Ingeniería y Sistemas Telemáticos, Universidad Politécnica de Madrid, Madrid, Spain; Departamento de Ingeniería y Sistemas Telemáticos, Universidad Politécnica de Madrid, Madrid, Spain","IEEE Transactions on Dependable and Secure Computing","15 Jan 2020","2020","17","1","134","147","A novel method based on the Hidden Markov Model is proposed to predict multistep attacks using IDS alerts. We consider the hidden states as similar phases of a particular type of attack. As a result, it can be easily adapted to multistep attacks and foresee the next steps of an attacker. To achieve this goal, a preliminary off-line training phase based on observations will be required. These observations are obtained by matching the IDS alert information with a database previously built for this purpose using a clusterization method from the CVE global database to avoid overfitting. The training model is performed using both unsupervised and supervised algorithms. Once the training is completed and probability matrices are computed, the prediction module compute the best state sequence based on the state probability for each step of the multistep attack in progress using the Viterbi and forward-backward algorithms. The training model includes the mean number of alerts and the number of alerts in progress to assist in obtaining the final attack probability. The model is analyzed for DDoS phases because it is a great problem in all Internet services. The proposed method is validated into a virtual DDoS scenario using current vulnerabilities. The results proving the system's ability to perform real-time prediction.","1941-0018","","10.1109/TDSC.2017.2751478","Spanish MINECO(grant numbers:TIN2014-59023-C2-2-R); European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031986","Multistep attack prediction;hidden Markov model;distributed denial of service;proactive response;machine learning","Hidden Markov models;Training;Computer crime;Proposals;Predictive models;Mathematical model;Prediction algorithms","","72","","36","IEEE","12 Sep 2017","","","IEEE","IEEE Journals"
"Efficient Cyber Attack Detection in Industrial Control Systems Using Lightweight Neural Networks and PCA","M. Kravchik; A. Shabtai","Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be'er Sheva, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Be'er Sheva, Israel","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2179","2197","Industrial control systems (ICSs) are widely used and vital to industry and society. Their failure can have severe impact on both the economy and human life. Hence, these systems have become an attractive target for physical and cyber attacks alike. In this article, we examine an attack detection method based on simple and lightweight neural networks, namely, 1D convolutional neural networks and autoencoders. We apply these networks to both the time and frequency domains of the data and discuss the pros and cons of each representation approach. The suggested method is evaluated on three popular public datasets, and detection rates matching or exceeding previously published detection results are achieved, while demonstrating a small footprint, short training and detection times, and generality. We also show the effectiveness of PCA, which, given proper data preprocessing and feature selection, can provide high attack detection rates in many settings. Finally, we study the proposed method’s robustness against adversarial attacks that exploit inherent blind spots of neural networks to evade detection while achieving their intended physical effect. Our results show that the proposed method is robust to such evasion attacks: in order to evade detection, the attacker is forced to sacrifice the desired physical impact on the system.","1941-0018","","10.1109/TDSC.2021.3050101","European Union's Horizon 2020 Research and Innovation Programme(grant numbers:830927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317834","Anomaly detection;industrial control systems;convolutional neural networks;autoencoders;frequency analysis;adversarial machine learning;adversarial robustness","Integrated circuits;Sensors;Actuators;Cyberattack;Time-domain analysis;Process control;Feature extraction","","69","","62","IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"Leveraging Compression-Based Graph Mining for Behavior-Based Malware Detection","T. Wüchner; A. Cisłak; M. Ochoa; A. Pretschner","Computer Science Department, Technical University of Munich, München, Germany; Faculty of Mathematics and Information Science, Warsaw University of Technology, Warsaw, Poland; Information Systems Technology and Design Department, Singapore University of Technology and Design, Singaporem; Computer Science Department, Technical University of Munich, München, Germany","IEEE Transactions on Dependable and Secure Computing","15 Jan 2019","2019","16","1","99","112","Behavior-based detection approaches commonly address the threat of statically obfuscated malware. Such approaches often use graphs to represent process or system behavior and typically employ frequency-based graph mining techniques to extract characteristic patterns from collections of malware graphs. Recent studies in the molecule mining domain suggest that frequency-based graph mining algorithms often perform sub-optimally in finding highly discriminating patterns. We propose a novel malware detection approach that uses so-called compression-based mining on quantitative data flow graphs to derive highly accurate detection models. Our evaluation on a large and diverse malware set shows that our approach outperforms frequency-based detection models in terms of detection effectiveness by more than 600 percent.","1941-0018","","10.1109/TDSC.2017.2675881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867799","Malware detection;quantitative data flow analysis;data mining;graph mining;machine learning","Malware;Data mining;Data models;Labeling;Sockets;Mathematical model;Frequency-domain analysis","","49","","41","IEEE","1 Mar 2017","","","IEEE","IEEE Journals"
"VerSA: Verifiable Secure Aggregation for Cross-Device Federated Learning","C. Hahn; H. Kim; M. Kim; J. Hur","Department of Electrical and Information Engineering, Seoul National University of Science and Technology, Seoul, Korea; Department of Computer Science and Engineering, Korea University, Seoul, Korea; Department of Computer Science and Engineering, Korea University, Seoul, Korea; Department of Computer Science and Engineering, Korea University, Seoul, Korea","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","36","52","In privacy-preserving cross-device federated learning, users train a global model on their local data and submit encrypted local models, while an untrusted central server aggregates the encrypted models to obtain an updated global model. Prior work has demonstrated how to verify the correctness of aggregation in such a setting. However, such verification relies on strong assumptions, such as a trusted setup among all users under unreliable network conditions, or it suffers from expensive cryptographic operations, such as bilinear pairing. In this paper, we scrutinize the verification mechanism of prior work and propose a model recovery attack, demonstrating that most local models can be leaked within a reasonable time (e.g., $98\%$98% of encrypted local models are recovered within 21 h). Then, we propose VerSA, a verifiable secure aggregation protocol for cross-device federated learning. VerSA does not require any trusted setup for verification between users while minimizing the verification cost by enabling both the central server and users to utilize only a lightweight pseudorandom generator to prove and verify the correctness of model aggregation. We experimentally confirm the efficiency of VerSA under diverse datasets, demonstrating that VerSA is orders of magnitude faster than verification in prior work.","1941-0018","","10.1109/TDSC.2021.3126323","National Research Foundation of Korea(grant numbers:2021R1F1A1061420); Institute of Information & communications Technology Planning & Evaluation(grant numbers:IITP-2021-0-01810,IITP-2021-2020-0-01819); National Research Foundation of Korea(grant numbers:NRF-2021R1A6A1A13044830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609695","Federated learning;distributed machine learning;security;privacy","Computational modeling;Servers;Data models;Training;Cryptography;Codes;Data privacy","","17","","72","CCBY","9 Nov 2021","","","IEEE","IEEE Journals"
"LP-SBA-XACML: Lightweight Semantics Based Scheme Enabling Intelligent Behavior-Aware Privacy for IoT","M. Chehab; A. Mourad","Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon; Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","161","175","The broad applicability of Internet of Things (IoT) would truly enable the pervasiveness of smart devices for sensing data. In this context, achieving service personalization requires collecting sensitive data about users. That yields to privacy concerns due to the possibility of abusing the data through unauthorized access. Moreover, IoT devices have limited computing resources, making them difficult to perform heavy protection mechanisms. Despite several existing solutions for privacy protection, they were not designed to run on limited resources in large scale environment. In addition, existing access control solutions, including XACML, are heavy to run on resource constraint devices and lack behavior-based customization of user privacy where users have little to no control over their private data. In this regard, we address the aforementioned problems by proposing LP-SBA-XACML, which embeds an efficient and lightweight semantics-based scheme targeting user privacy and providing efficient policy evaluation. LP-SBA-XACML is a scalable and lightweight solution suitable for the IoT context while preserving the assumptions of XACML. Moreover, an intelligent model for real-time behavior/activity prediction is integrated to systematically customize user’s privacy and services. Experiments conducted on synthetic and real-life scenarios demonstrate the feasibility and relevance of our proposed framework within a mobile IoT resource-constrained environment.","1941-0018","","10.1109/TDSC.2020.2999866","Lebanese American University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107456","Machine learning;deep learning, access control;customized user privacy;behavior based privacy;IoT;XACML;limited resource devices","Privacy;Internet of Things;Data privacy;Authorization;Performance evaluation","","13","","49","IEEE","3 Jun 2020","","","IEEE","IEEE Journals"
"An Automated Multi-Tab Website Fingerprinting Attack","Q. Yin; Z. Liu; Q. Li; T. Wang; Q. Wang; C. Shen; Y. Xu","Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, and Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; School of Computer Science, Simon Fraser University, Burnaby, BC, Canada; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","3656","3670","In Website Fingerprinting (WF) attack, a local passive eavesdropper utilizes network flow information to identify which web pages a user is browsing. Previous researchers have demonstrated the feasibility and effectiveness of WF attacks under a strong Single Page Assumption: the network flow extracted by the adversary belongs to a single web page. In reality, the assumption may not hold because users tend to open multiple tabs simultaneously (or within a short period of time) so that their network traffic is mixed. In this article, we propose an automated multi-tab Website Fingerprinting attack that is able to accurately classify websites regardless of the number of simultaneously opened pages. Our design is powered by two innovative designs. First, we develop a split point classification method to dynamically identify the split point between the first page and its subsequent pages. As a result, the network traffic before the split point is solely generated for the first page. Then, we propose a new chunk-based WF classifier to infer the websites based on the initial chunk of clean traffic. For both classifiers, we apply automated feature selection to select a concise yet representative feature set. We implement a prototype of our design and perform extensive evaluations using SSH and Tor-based datasets to demonstrate the effectiveness of both our system components individually and the integrated system as a whole.","1941-0018","","10.1109/TDSC.2021.3104869","National Key R&D Program of China(grant numbers:2018YFB1800304); National Natural Science Foundation of China(grant numbers:62132011,61572278,U20B2049,61822207,61822309,61773310,U1736205); BNRist(grant numbers:BNR2020RC0101); Huawei; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9514394","Website fingerprinting attack;machine learning;feature selection;traffic analysis","Feature extraction;Fingerprint recognition;Web pages;Cryptography;Loading;Prototypes;Load modeling","","12","","33","IEEE","16 Aug 2021","","","IEEE","IEEE Journals"
"Alert-Driven Attack Graph Generation Using S-PDFA","A. Nadeem; S. Verwer; S. Moskal; S. J. Yang","Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands; Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Dependable and Secure Computing","11 Mar 2022","2022","19","2","731","746","Ideal cyber threat intelligence (CTI) includes insights into attacker strategies that are specific to a network under observation. Such CTI currently requires extensive expert input for obtaining, assessing, and correlating system vulnerabilities into a graphical representation, often referred to as an attack graph (AG). Instead of deriving AGs based on system vulnerabilities, this work advocates the direct use of intrusion alerts. We propose SAGE, an explainable sequence learning pipeline that automatically constructs AGs from intrusion alerts without a priori expert knowledge. SAGE exploits the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton (S-PDFA) — a model that brings infrequent severe alerts into the spotlight and summarizes paths leading to them. Attack graphs are extracted from the model on a per-victim, per-objective basis. SAGE is thoroughly evaluated on three open-source intrusion alert datasets collected through security testing competitions in order to analyze distributed multi-stage attacks. SAGE compresses over 330k alerts into 93 AGs that show how specific attacks transpired. The AGs are succinct, interpretable, and provide directly relevant insights into strategic differences and fingerprintable paths. They even show that attackers tend to follow shorter paths after they have discovered a longer one in 84.5% of the cases.","1941-0018","","10.1109/TDSC.2021.3117348","National Science Foundation(grant numbers:1742789); RIT Global Cybersecurity Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557854","Alert-driven attack graphs;explainable machine learning;suffix automaton model;attacker strategy;intrusion alerts","Security;Pipelines;Probabilistic logic;Learning automata;Testing;Special issues and sections;Markov processes","","10","","68","IEEE","4 Oct 2021","","","IEEE","IEEE Journals"
"DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks","H. Huang; W. Luo; G. Zeng; J. Weng; Y. Zhang; A. Yang","College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China; College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China; College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China; College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China; College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China; College of Information Science and Technology/College of Cyber Security, National Joint Engineering Research Center of Network Security Detection and Protection Technology, and Guangdong Key Laboratory of Data Security and Privacy Preserving, Jinan University, Guangzhou, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2022","2022","19","5","3183","3199","Deep Learning (DL) techniques allow ones to train models from a dataset to solve tasks. DL has attracted much interest given its fancy performance and potential market value, while security issues are amongst the most colossal concerns. However, the DL models may be prone to the membership inference attack, where an attacker determines whether a given sample is from the training dataset. Efforts have been made to hinder the attack but unfortunately, they may lead to a major overhead or impaired usability. In this article, we propose and implement DAMIA, leveraging Domain Adaptation (DA) as a defense aginist membership inference attacks. Our observation is that during the training process, DA obfuscates the dataset to be protected using another relate and similar dataset, and derives a model that underlyingly extracts the features from both datasets. Seeing that the model is obfuscated, membership inference fails, while the extracted features provide supports for usability. Extensive experiments have been conducted to validates our intuition. The model trained by DAMIA has a negligible footprint to the usability and introduces slight overhead compared with other defenses. Our experiment also excludes factors that may hinder the performance of DAMIA, and comparisons with other defenses, providing a potential guideline to vendors and researchers to benefit from our solution in a timely manner.","1941-0018","","10.1109/TDSC.2021.3088480","National Natural Science Foundation of China(grant numbers:61872153); National Natural Science Foundation of China(grant numbers:61877029); Guangdong Provincial Special Funds for Applied Technology Research and Development and Transformation of Important Scientific and Technological Achieve(grant numbers:2017B010124002); National Key R&D Plan of China(grant numbers:2020YFB1005600); National Natural Science Foundation of China(grant numbers:U1736203,61825203,61732021); Major Program of Guangdong Basic and Applied Research Project(grant numbers:2019B030302008); Guangdong Provincial Science and Technology Project(grant numbers:2017B010111005); National Natural Science Foundation of China(grant numbers:61877029); National Natural Science Foundation of China(grant numbers:62072215); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452807","Privacy-preserving machine learning;membership inference attack;domain adaptation;deep learning","Adaptation models;Training;Data models;Mathematical model;Feature extraction;Brain modeling;Training data","","9","","57","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Privacy Leakage in Wireless Charging","J. Liu; X. Zou; L. Zhao; Y. Tao; S. Hu; J. Han; K. Ren","Zhejiang University and Key Laboratory of Blockchain and Cyberspace Governance of Zhejiang Province, Hangzhou, China; Xi'an Jiaotong University, Xi'an, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University and Key Laboratory of Blockchain and Cyberspace Governance of Zhejiang Province, Hangzhou, China; Zhejiang University, Key Laboratory of Blockchain and Cyberspace Governance of Zhejiang Province, Hangzhou, China","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","501","514","Wireless charging is becoming an essential power supply pattern for electronic devices. Currently, mainstream smartphones are almost compatible with wireless charging. However, when the charging efficiency is continuously improved, its security challenge still remains open yet overlooked. In this paper, we reveal that severe security flaws exist in the wireless charging procedure of off-the-shelf commodity smartphones. Specifically, we find that an attacker can utilize the electromagnetic induction effect between the wireless charger and the smartphone to detect the activities and operations performed on the smartphone. We term such attack as EM-Surfing side-channel attack and build a theoretical model to show its feasibility. To explore the hazard of EM-Surfing, we propose a three-module attack method, with which we conduct real-world experiments over three mainstream models of smartphones. The results show that the attacker can achieve over 99%, 96%, 94%, and 97% accuracy when inferring the passcode, keystroke, App information, and speech content, respectively. We also design an App named SecCharging to prevent smartphones from EM-Surfing attacks. The defense experiment results demonstrate that SecCharging can mitigate the threats posed by EM-Surfing effectively.","1941-0018","","10.1109/TDSC.2022.3173063","National Key R&D Program of China(grant numbers:2021QY0703); National Natural Science Foundation of China(grant numbers:U21A20462,61872285,52177199,62032021,61772236,61972348); Research Institute of Cyberspace Governance in Zhejiang University; Leading Innovative and Entrepreneur Team Introduction Program of Zhejiang(grant numbers:2018R01005); Ant Group Funding(grant numbers:Z51202000234); Alibaba-Zhejiang University Joint Institute of Frontier Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770365","Side-channel;wireless charging;machine learning","Smart phones;Privacy;Wireless communication;Communication system security;Inductive charging;Wireless sensor networks;Voltage","","4","","50","IEEE","6 May 2022","","","IEEE","IEEE Journals"
"Differentially Private $k$k-Means Clustering With Convergence Guarantee","Z. Lu; H. Shen","Department of Computing, Macquarie University, Sydney, NSW, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1541","1552","Iterative clustering around representative points is an effective technique for clustering and helps us learn insights behind data to support various important applications. Unfortunately, it also provides security holes which may allow adversaries to infer the privacy of individuals with some background knowledge. To protect individual privacy against such inference attacks, preserving differential privacy for iterative clustering algorithms has been extensively studied. Existing differentially private clustering algorithms adopt the same framework to compute differentially private centroids iteratively by running Lloyd's k-means algorithm to obtain the actual centroids, then perturbing them with a differential privacy mechanism. These algorithms suffer from the problem of no convergence guarantee, i.e., they provide no guarantee of termination at a solution of Lloyd's algorithm within a bounded number of iterations. This problem severely impacts their clustering quality and execution efficiency. To address this problem, this article follows the same centroid updating pattern as existing work in interactive settings; however we propose a novel framework for injecting differential privacy into the actual centroids. Specifically, to ensure convergence, we maintain the perturbed centroids of the previous iteration t-1 to compute a convergence zone for each cluster in the current iteration t, where we inject differential privacy noise. To achieve a satisfactory convergence rate, we further control the orientation of centroid movement in each cluster using two strategies: one takes the orientation of centroid movement from iteration t-1 to iteration t (past knowledge); the other uses the additional information of the orientation from iteration t+1 (future knowledge). We prove that, in the expected case, our algorithm (in both strategies) converges to a solution of Lloyd's algorithm in at most twice as many iterations as Lloyd's algorithm. Furthermore, when using both past and future knowledge, we prove that our algorithm converges to the same solution as Lloyd's algorithm (for the same initial centroids) with high probability, at the cost of a slower convergence speed compared to using only past knowledge due to duplicated operations in each iteration required for computing the future knowledge. We perform experimental evaluations on seven widely used real-world datasets. The experimental results show that our algorithm outperforms the state-of-the-art methods for interactive differentially private clustering with a guaranteed convergence and better clustering quality whilst meeting the same differential privacy requirements.","1941-0018","","10.1109/TDSC.2020.3043369","Australian Government Research Training Program Scholarship; Australian Research Council(grant numbers:DP150104871); National Key Research and Development Program of China(grant numbers:#2017YFB0203201); University of Adelaide; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286725","Differential privacy;machine learning; $k$   k    -Means clustering","Clustering algorithms;Convergence;Differential privacy;Inference algorithms;Privacy;Sensitivity;Computer science","","4","","32","IEEE","8 Dec 2020","","","IEEE","IEEE Journals"
"PRISM: A Hierarchical Intrusion Detection Architecture for Large-Scale Cyber Networks","Y. Javed; M. A. Khayat; A. A. Elghariani; A. Ghafoor","Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; Department of Computer Engineering, Umm Al-Qura University, Makkah, Saudi Arabia; XCOM-Labs, San Diego, CA, USA; Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","5070","5086","The increase in scale of cyber networks and the rise in sophistication of cyber-attacks have introduced several challenges in intrusion detection. The primary challenge is the requirement to detect complex multi-stage attacks in realtime by processing the immense amount of traffic produced by present-day networks. In this paper we present PRISM, a hierarchical intrusion detection architecture that uses a novel attacker behavior model-based sampling technique to minimize the realtime traffic processing overhead. PRISM has a unique multi-layered architecture that monitors network traffic distributedly to provide efficiency in processing and modularity in design. PRISM employs a Hidden Markov Model-based prediction mechanism to identify multi-stage attacks and ascertain the attack progression for a proactive response. Furthermore, PRISM introduces a stream management procedure that rectifies the issue of alert reordering when collected from distributed alert reporting systems. To evaluate the performance of PRISM, multiple metrics have been proposed, and various experiments have been conducted on multi-stage attack datasets. The results exhibit up to 7.5x improvement in processing overhead as compared to a standard centralized IDS without the loss of prediction accuracy while demonstrating the ability to predict different attack stages promptly.","1941-0018","","10.1109/TDSC.2023.3240315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027167","Network security;intrusion detection;threat forecasting;network traffic sampling;machine learning;stream processing","Hidden Markov models;Intrusion detection;Telecommunication traffic;Security;Computer architecture;Behavioral sciences;Surveillance","","3","","55","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"Generic and Sensitive Anomaly Detection of Network Covert Timing Channels","H. Li; T. Song; Y. Yang","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","4085","4100","Network covert timing channels can be maliciously used to exfiltrate secrets, coordinate attacks and propagate malwares, posing serious threats to cybersecurity. Current covert timing channels normally conduct small-volume transmission under the covers of various disguising techniques, making them hard to detect especially when a detector has little priori knowledge of their traffic features. In this article, we propose a generic and sensitive detection approach, which can simultaneously (i) identify various types of channels without their traffic knowledge and (ii) maintain reasonable performance on small traffic samples. The basis of our approach is the finding that the short-term timing behavior of covert and legitimate traffic is significantly different from the perspective of inter-packet delays’ variation. This phenomenon can be a generic reference to detect various channels because it is resistant to major channel disguising techniques which only mimic long-term traffic features, while it is also a sensitive reference to spot small-volume covert transmission since it can capture traffic anomalies in a fine-grained manner. To obtain the inner patterns of inter-packet delays’ variation, we design a context-sensitive feature-extraction technique. This technique transforms each raw inter-packet delay into a discrete counterpart based on its contextual properties, thus extracting its variation features and reducing traffic data complexity. Then we learn legitimate variation patterns using a neural network model, and identify samples showing anomalous variation as covert. The experimental results show that our approach effectively detects all currently representative channels in the absence of their knowledge, presenting once to twice higher sensitivity than the state-of-the-art solutions.","1941-0018","","10.1109/TDSC.2022.3207573","National Key Research and Development Program of China(grant numbers:2020YFB1806000); National Natural Science Foundation of China(grant numbers:92067203,61672101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894661","Cyber security;covert timing channel;anomaly detection;short-term traffic behavior;machine learning","Feature extraction;Behavioral sciences;Delays;Sensitivity;Anomaly detection;Transforms;Receivers","","3","","47","IEEE","19 Sep 2022","","","IEEE","IEEE Journals"
"Stealthy 3D Poisoning Attack on Video Recognition Models","S. Xie; Y. Yan; Y. Hong","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1730","1743","Deep Neural Networks (DNNs) have been proven to be vulnerable to poisoning attacks that poison the training data with a trigger pattern and thus manipulate the trained model to misclassify data instances. In this article, we study the poisoning attacks on video recognition models. We reveal the major limitations of the state-of-the-art poisoning attacks on stealthiness and attack effectiveness: (i) the frame-by-frame poisoning trigger may cause temporal inconsistency among the video frames which can be leveraged to easily detect the attack; (ii) the feature collision-based method for crafting poisoned videos could lack both generalization and transferability. To address these limitations, we propose a novel stealthy and efficient poisoning attack framework which has the following advantages: (i) we design a 3D poisoning trigger as natural-like textures, which can maintain temporal consistency and human-imperceptibility; (ii) we formulate an ensemble attack oracle as the optimization objective to craft poisoned videos, which could construct convex polytope-like adversarial subspaces in the feature space and thus gain more generalization; (iii) our poisoning attack can be readily extended to the black-box setting with good transferability. We have experimentally validated the effectiveness of our attack (e.g., up to $95\%$95% success rates with only less than $\sim 0.5\%$∼0.5% poisoned dataset).","1941-0018","","10.1109/TDSC.2022.3163397","National Science Foundation(grant numbers:CNS-2046335,CNS-2034870); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745374","Poisoning attack;video recognition;machine learning security","Three-dimensional displays;Training;Data models;Computational modeling;Solid modeling;Feature extraction;Neural networks","","2","","56","IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"Exploring the Effect of Randomness on Transferability of Adversarial Samples Against Deep Neural Networks","Y. Zhou; M. Kantarcioglu; B. Xi","Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Statistics, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","83","99","We investigate the transferability of adversarial attacks against deep neural networks (DNNs)—the contagion effect of adversarial attacks that, once deceiving one DNN model, can easily deceive other DNN models built on similar data. We demonstrate that introducing randomness to DNN models can break the curse of the transferability of adversarial attacks, given that the adversary does not have an unlimited attack budget. Two randomization schemes are explored: 1.) a random selection—single or ensemble—from a set of DNNs is surprisingly more robust against the strongest form of complete-knowledge attacks (a.k.a, white box attacks); 2.) after a small Gaussian random noise is added to its learned weights, a DNN model can potentially increase its resilience to adversarial attacks by as much as 74.2%. We compare the two randomization techniques to the Ensemble Adversarial Training technique and show that our randomization techniques are superior under different attack budget constraints. Furthermore, we explore the relationship between attack severity and decision boundary robustness in the version space. Finally, we connect the dots between the effectiveness of randomization to prevent attack transferability and the variability of DNN models through analyzing the differential entropy of sample hypotheses in the hypothesis space.","1941-0018","","10.1109/TDSC.2021.3127439","National Science Foundation(grant numbers:CNS-1837627,OAC-1828467,IIS-1939728,DMS-1925346,CNS-2029661,OAC-2115094); ARO(grant numbers:W911NF-17-1-0356); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612007","Adversarial attacks;adversarial machine learning;deep neural networks","Computational modeling;Training;Perturbation methods;Robustness;Deep learning;Neural networks;Data models","","1","","78","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"Lore a Red Team Emulation Tool","H. Holm","Department of Information Security, Swedish Defence Research Agency (FOI), Stockholm, Sweden","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1596","1608","This article presents the red team emulation tool Lore, which uses boolean logic and trained models to automatically select and execute red team actions. Lore improves the current state of red team automation, and is the first such tool shown to provide a more fun and educational experience than a manual red team during a cyber defence exercise. In addition to the cyber defence exercise, empirical tests are performed to examine the accuracy of Lore's trained models. The results show that application of these models lead to two times more compromised machines than when applying expert-defined models, and five times more compromised machines than when randomly selecting actions.","1941-0018","","10.1109/TDSC.2022.3160792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740000","Network-level security and protection;security and protection;machine learning","Servers;Computational modeling;Planning;Databases;Proposals;Automation;Manuals","","","","38","IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Dataset Characteristics for Reliable Code Authorship Attribution","F. Abazari; E. Branca; N. Ridley; N. Stakhanova; M. Dalla Preda","Computer Science, University of Saskatchewan, Saskatoon, SK, Canada; Computer Science, University of Saskatchewan, Saskatoon, SK, Canada; Computer Science, University of Saskatchewan, Saskatoon, SK, Canada; Computer Science, University of Saskatchewan, Saskatoon, SK, Canada; Informatica, University of Verona, Verona, Italy","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","506","521","Code authorship attribution aims to identify the author of software source code according to the author’s unique coding style characteristics. The lack of benchmark data in the field, forced researchers to employ various resources that often did not reflect real programming practices. Throughout the years, research studies have used textbook examples, students’ programming assignments, faculty code samples, code from programming competitions and files retrieved from open-source repositories as research objects. The diversity of the data raised concerns about the feasibility of capturing the appropriate data characteristics to reliably evaluate code attribution. In this paper, we investigate these concerns and analyze the effect of the dataset characteristics and feature elimination techniques on the accuracy of code attribution. Unlike the majority of the work done in this field, which mainly concentrates on designing new features, we explore the nature of the data used in previous studies and assess the factors that influence the attribution task. Within this analysis, we investigate the robustness of three feature sets regarded as reliable benchmarks in the attribution research. Based on our findings, we define a process for deriving a reduced set of features for accurate and predictable attribution and make recommendations on the dataset characteristics.","1941-0018","","10.1109/TDSC.2021.3138700","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664378","Source code attribution;machine learning;feature selection;authorship attribution;GitHub","Codes;Java;Programming;C++ languages;Software development management;Software;Radio frequency","","","","64","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-Based Incentive","J. Weng; J. Weng; J. Zhang; M. Li; Y. Zhang; W. Luo","Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China; Guangdong/Guangzhou Key Laboratory of Data Security and Privacy Preserving, and National-Local Joint Engineering Research Center of Cyber Security Detection and Protection Technology, College of Information Science and Technology, Jinan University, Guangzhou, China","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2438","2455","Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this article, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a prototype of DeepChain and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.","1941-0018","","10.1109/TDSC.2019.2952332","National Natural Science Foundation of China(grant numbers:61825203,U1736203,61732021); Guangdong Provincial Special Funds for Applied Technology Research and Development and Transformation of Key Scientific and Technological Achievements(grant numbers:2016B010124009); Science and Technology Program of Guangzhou of China(grant numbers:201802010061); Guangdong Provincial Basic and Applied Research Major Programme(grant numbers:2019B030302008); National Key Research and Development Program of China(grant numbers:2018YFB1402600); National Key Research and Development Program of China(grant numbers:2018YFB1003701); National Natural Science Foundation of China(grant numbers:61972177); Communication and Computer Network Lab of Guangdong(grant numbers:CCNL201903); National Key Research and Development Program of China(grant numbers:2017YFB0802203); Jinan University; National Natural Science Foundation of China(grant numbers:61872153); National Natural Science Foundation of China(grant numbers:61877029); Applied Technology Research and Development and Transformation of Key Scientific and Technological Achievements(grant numbers:2017B010124002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894364","Deep learning;privacy-preserving training;blockchain;incentive","Deep learning;Training;Servers;Blockchain;Collaboration;Training data;Data models","","259","","73","IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Software Vulnerability Discovery via Learning Multi-Domain Knowledge Bases","G. Lin; J. Zhang; W. Luo; L. Pan; O. De Vel; P. Montague; Y. Xiang","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Department of Defence, Defence Science and Technology Group (DSTG), Canberra, ACT, Australia; Department of Defence, Defence Science and Technology Group (DSTG), Canberra, ACT, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2469","2485","Machine learning (ML) has great potential in automated code vulnerability discovery. However, automated discovery application driven by off-the-shelf machine learning tools often performs poorly due to the shortage of high-quality training data. The scarceness of vulnerability data is almost always a problem for any developing software project during its early stages, which is referred to as the cold-start problem. This article proposes a framework that utilizes transferable knowledge from pre-existing data sources. In order to improve the detection performance, multiple vulnerability-relevant data sources were selected to form a broader base for learning transferable knowledge. The selected vulnerability-relevant data sources are cross-domain, including historical vulnerability data from different software projects and data from the Software Assurance Reference Database (SARD) consisting of synthetic vulnerability examples and proof-of-concept test cases. To extract the information applicable in vulnerability detection from the cross-domain data sets, we designed a deep-learning-based framework with Long-short Term Memory (LSTM) cells. Our framework combines the heterogeneous data sources to learn unified representations of the patterns of the vulnerable source codes. Empirical studies showed that the unified representations generated by the proposed deep learning networks are feasible and effective, and are transferable for real-world vulnerability detection. Our experiments demonstrated that by leveraging two heterogeneous data sources, the performance of our vulnerability detection outperformed the static vulnerability discovery tool Flawfinder. The findings of this article may stimulate further research in ML-based vulnerability detection using heterogeneous data sources.","1941-0018","","10.1109/TDSC.2019.2954088","NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906156","Vulnerability discovery;representation learning;deep learning","Software;Feature extraction;Deep learning;Feeds;Task analysis;Neural networks;Data mining","","53","","53","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities","Z. Li; D. Zou; S. Xu; H. Jin; Y. Zhu; Z. Chen","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Big Data Security Engineering Research Center, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Big Data Security Engineering Research Center, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, University of Colorado Colorado Springs, Colorado Springs, CO, USA; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Big Data Security Engineering Research Center, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Big Data Security Engineering Research Center, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Big Data Security Engineering Research Center, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2244","2258","The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with four software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, seven are unknown and have been reported to the vendors, and the other eight have been “silently” patched by the vendors when releasing newer versions of the pertinent software products.","1941-0018","","10.1109/TDSC.2021.3051525","Huazhong University of Science and Technology; Hebei University; National Natural Science Foundation of China(grant numbers:U1936211,61802106); Natural Science Foundation of Hebei Province(grant numbers:F2020201016); ARO(grant numbers:W911NF-17-1-0566); National Science Foundation(grant numbers:1814825,1736209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321538","Vulnerability detection;security;deep learning;program analysis;program representation","Deep learning;Syntactics;Software;Semantics;Proposals;Image processing;Big Data","","177","","52","IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"Generating Summary Risk Scores for Mobile Applications","C. S. Gates; N. Li; H. Peng; B. Sarma; Y. Qi; R. Potharaju; C. Nita-Rotaru; I. Molloy","Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; IBM TJ Watson Research Center, Yorktown Heights, NY","IEEE Transactions on Dependable and Secure Computing","9 May 2014","2014","11","3","238","251","One of Android's main defense mechanisms against malicious apps is a risk communication mechanism which, before a user installs an app, warns the user about the permissions the app requires, trusting that the user will make the right decision. This approach has been shown to be ineffective as it presents the risk information of each app in a “stand-alone” fashion and in a way that requires too much technical knowledge and time to distill useful information. We discuss the desired properties of risk signals and relative risk scores for Android apps in order to generate another metric that users can utilize when choosing apps. We present a wide range of techniques to generate both risk signals and risk scores that are based on heuristics as well as principled machine learning techniques. Experimental results conducted using real-world data sets show that these methods can effectively identify malware as very risky, are simple to understand, and easy to use.","1941-0018","","10.1109/TDSC.2014.2302293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720107","Risk;mobile;malware;data mining","Androids;Humanoid robots;Malware;Google;Computational modeling;Smart phones;Biological system modeling","","37","","36","IEEE","23 Jan 2014","","","IEEE","IEEE Journals"
"LAW: Learning Automatic Windows for Online Payment Fraud Detection","C. Wang; C. Wang; H. Zhu; J. Cui","Department of Computer Science and Engineering, Tongji University, Jiading District, Shanghai, China; Department of Computer Science and Engineering, Tongji University, Jiading District, Shanghai, China; Department of Computer Science and Engineering, Tongji University, Jiading District, Shanghai, China; Department of Computer Science and Engineering, Tongji University, Jiading District, Shanghai, China","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2122","2135","The rapid development of internet finance has caused increasing concern in online payment fraud due to its great threat. It is typical to employ rule systems or machine learning-based techniques to detect frauds. For the most significant features of such fraudulent transactions are exhibited in a sequential form, the sliding time window is a widely-recognized effective tool for this problem. With a sliding time window, features about the transaction characteristics can be extracted, and the latent patterns hidden in transaction records can be captured. However, the adaptive setting of sliding time window is really a big challenge, since the transaction patterns in real-life application scenarios are often too elusive to be captured. As a matter of fact, the practical setting usually needs to be updated and refined with manual intervention regularly. This is time-consuming indeed. In this article, we pursue an adaptive learning approach to detect fraudulent online payment transactions with automatic sliding time windows. Accordingly, we make efforts on optimizing the setting of windows and improving the adaptability. We design an intelligent window, called learning automatic window (LAW). It utilizes the learning automata to learn the proper parameters of time windows and adjust them dynamically and regularly according to the variation and oscillation of fraudulent transaction patterns. By the experiments over a real-world dataset of the online payment service from a commercial bank, we validate the gain of LAW in terms of detection effectiveness and robustness. To the best of our knowledge, this is the first work to make a sliding time window for fraud detection capable of learning its proper size in changing situations.","1941-0018","","10.1109/TDSC.2020.3037784","National Natural Science Foundation of China(grant numbers:61972287); Ministry of Industry and Information Technology of China(grant numbers:TC200H01J); Municipal Human Resources Development Program for Outstanding Young Talents in Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258411","Fraud detection;online payment;learning automata;sliding time window","Learning automata;Predictive models;Feature extraction;Adaptation models;Computational modeling;Robustness;Heuristic algorithms","","11","","56","IEEE","12 Nov 2020","","","IEEE","IEEE Journals"
"A Comparative Analysis of Software Aging in Image Classifiers on Cloud and Edge","E. Andrade; R. Pietrantuono; F. Machida; D. Cotroneo","Department of Computing, Federal Rural University of Pernambuco, Recife, Brazil; University of Naples Federico II, Naples, Italy; Department of Computer Science, University of Tsukuba, Ibaraki, Japan; University of Naples Federico II, Naples, Italy","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","563","573","Image classifiers for recognizing real-world objects are widely used in the Internet of Things (IoT) and Cyber-Physical Systems(CPSs). A classifier is trained offline by machine learning algorithms with training data sets, and then it is deployed on a cloud or an edge computing system for online label predictions. As the classifier's performance depends on the underlying software infrastructure, it may degrade over time due to software faults causing software aging. In this paper, we address this issue and experimentally investigate software aging observed in an image classification system that continuously runs on cloud and edge computing environments. We apply several statistical techniques to analyze degradation trends in the systems under stress tests. Our statistical trend analysis confirms the degradation trends in the throughput as well as the available memory resources both in the cloud and the edge environments. Contrary to our expectation, the edge computing environment under test had much less impact on the performance degradation than our cloud environment when the workload is high, although the latter one has four times larger allocated memory resources. We also show that the observed performance degradation trends are associated with the memory usage of specific processes by performing correlation analysis.","1941-0018","","10.1109/TDSC.2021.3139201","CNPq - Brazil(grant numbers:406263/2018-3); University of Tsukuba Basic Research Support Program Type S; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665276","Cloud computing;Edge computing;image classifiers;performance analysis;software aging","Aging;Software;Cloud computing;Degradation;Market research;Image edge detection;Edge computing","","8","","48","IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"PersonaIA: A Lightweight Implicit Authentication System Based on Customized User Behavior Selection","Y. Yang; J. Sun; L. Guo","University of Tennessee, Knoxville, TN; University of Tennessee, Knoxville, TN; Binghamton University, Binghamton, NY","IEEE Transactions on Dependable and Secure Computing","17 Jan 2019","2019","16","1","113","126","Motivated by the great potential of implicit and seamless user authentication, we attempt to build an implicit authentication (IA) system with adaptive sampling that automatically selects dynamic sets of activities for user behavior extraction. Various activities, such as user location, application usage, user motion, and battery usage have been popular choices to generate behaviors, the soft biometrics, for implicit authentication. Unlike password-based or hard biometric-based authentication, implicit authentication does not require explicit user action or expensive hardware. However, user behaviors can change unpredictably which renders it more challenging to develop systems that depend on them. In addition to dynamic behavior extraction, the proposed implicit authentication system differs from the existing systems in terms of energy efficiency for battery-powered mobile devices. Since implicit authentication systems including the proposed one rely on machine learning, the expensive training process needs be outsourced to the remote server. However, mobile devices may not always have reliable network connections to send real-time data to the server for training. We overcome this limitation by proposing a W-layer, an overlay that provides a practical and energy-efficient solution for implicit authentication on mobile devices. We implemented partially labeled Dirichlet allocation (PLDA) on the server side for more accurate feature extraction, and achieved 93.3 percent precision and 98.6 percent accuracy in the synthetic dataset. Furthermore, we tested the power consumption of the smartphones used for our experiments and found that our method consumed 14.5 percent of the devices' total battery usage.","1941-0018","","10.1109/TDSC.2016.2645208","US National Science Foundation(grant numbers:CNS-1422665); Army Research Office(grant numbers:66270-CS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797513","Implicit authentication;user behavior;topic model;mobile security;energy efficiency","Authentication;Feature extraction;Smart phones;Biometrics (access control);Servers;Reliability;Computational modeling","","8","","39","IEEE","26 Dec 2016","","","IEEE","IEEE Journals"
"SkillVet: Automated Traceability Analysis of Amazon Alexa Skills","J. S. Edu; X. Ferrer-Aran; J. Such; G. Suarez-Tangil","King's College (KCL) London, London, U.K; King's College (KCL) London, London, U.K; King's College (KCL) London, London, U.K.; IMDEA Networks Institute, Leganés, Spain","IEEE Transactions on Dependable and Secure Computing","13 Jan 2023","2023","20","1","161","175","Skills, are essential components in Smart Personal Assistants (SPA). The number of skills has grown rapidly, dominated by a changing environment that has no clear business model. Skills can access personal information and this may pose a risk to users. However, there is little information about how this ecosystem works, let alone the tools that can facilitate its study. In this article, we present the largest systematic measurement of the Amazon Alexa skill ecosystem to date. We study developers’ practices in this ecosystem, including how they collect and justify the need for sensitive information, by designing a methodology to identify over-privileged skills with broken privacy policies. We collect 199,295 Alexa skills and uncover that around 43% of the skills (and 50% of the developers) that request these permissions follow bad privacy practices, including (partially) broken data permissions traceability. In order to perform this kind of analysis at scale, we present SkillVet that leverages machine learning and natural language processing techniques, and generates high-accuracy prediction sets. We report several concerning practices, including how developers can bypass Alexa's permission system through account linking and conversational skills, and offer recommendations on how to improve transparency, privacy and security. Resulting from the responsible disclosure we did, 13% of the reported issues no longer pose a threat at submission time.","1941-0018","","10.1109/TDSC.2021.3129116","Engineering and Physical Sciences Research Council(grant numbers:EP/T026723/1); Ramon y Cajal(grant numbers:RYC-2020-029401-I); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619970","Voice assistants;smart personal assistants;alexa;smart home;privacy;permission;personal data;access control","Virtual assistants;Privacy;Ecosystems;Systematics;Data privacy;Uniform resource locators;Speech recognition","","7","","57","IEEE","18 Nov 2021","","","IEEE","IEEE Journals"
"Scaling Camouflage: Content Disguising Attack Against Computer Vision Applications","Y. Chen; C. Shen; C. Wang; Q. Xiao; K. Li; Y. Chen","Ministry of Education Key Laboratory for Intelligent Networks and Network Security, Faculty of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Ministry of Education Key Laboratory for Intelligent Networks and Network Security, Faculty of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, University of Georgia, Athens, GA, USA; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, University of Georgia, Athens, GA, USA","IEEE Transactions on Dependable and Secure Computing","27 Aug 2021","2021","18","5","2017","2028","Recently, deep neural networks have achieved state-of-the-art performance in multiple computer vision tasks, and become core parts of computer vision applications. In most of their implementations, a standard input preprocessing component called image scaling is embedded, in order to resize the original data to match the input size of pre-trained neural networks. This article demonstrates content disguising attacks by exploiting the image scaling procedure, which cause machine's extracted content to be dramatically dissimilar with that before scaled. Different from previous adversarial attacks, our attacks happen in the data preprocessing stage, and hence they are not subject to specific machine learning models. To achieve a better deceiving and disguising effect, we propose and implement three feasible attack approaches with L0- and L∞-norm distance metrics. We have conducted a comprehensive evaluation on various image classification applications, including three local demos and two remote proprietary services. We also investigate the attack effects on a YOLO-v3 object detection demo. Our experimental results demonstrate successful content disguising against all of them, which validate our approaches are practical.","1941-0018","","10.1109/TDSC.2020.2971601","National Key Research and Development Program of China(grant numbers:2019YFB1312000); National Natural Science Foundation of China(grant numbers:61822309,61773310,U1736205); Natural Science Foundation of Shaanxi Province(grant numbers:2019JQ-084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982037","Content disguising;image scaling;adversarial examples;deep learning;computer vision","Computer vision;Neural networks;Visualization;Task analysis;Measurement;Object detection;Computational modeling","","5","","46","IEEE","4 Feb 2020","","","IEEE","IEEE Journals"
"Adaptive Secure Nearest Neighbor Query Processing Over Encrypted Data","R. Li; A. X. Liu; H. Xu; Y. Liu; H. Yuan","College of Cyberspace Security, Dongguan University of Technology, Dongguan, Guangdong, China; College of Cyberspace Security, Dongguan University of Technology, Dongguan, Guangdong, China; College of Cyberspace Security, Dongguan University of Technology, Dongguan, Guangdong, China; College of Cyberspace Security, Dongguan University of Technology, Dongguan, Guangdong, China; College of Cyberspace Security, Dongguan University of Technology, Dongguan, Guangdong, China","IEEE Transactions on Dependable and Secure Computing","14 Jan 2022","2022","19","1","91","106","Nearest neighbor query processing is a fundamental problem that arises in many fields such as spatial databases and machine learning. This article aims to address the Secure Nearest Neighbor (SNN) problem in cloud computing. Prior SNN schemes are both insecure and inefficient. In this article, we formally prove and experimentally demonstrate that the SNN scheme ASPE is actually insecure against even ciphertext only attacks. Although prior work proved that it is impossible to construct an SNN scheme even in much relaxed standard security models, we point out the flaws of the hardness proof. We propose an SNN scheme and prove that it is secure against adaptive chosen keyword attacks. Our scheme is efficient as its query processing complexity is logarithmic. To evaluate the efficiency of our SNN scheme, we implemented our scheme in C++ and compared its performance with a plain text scheme, binary scheme, and a PIR scheme on a large set of over 10 million real-world data points. Experimental results show that our scheme is fast (0.124 millisecond per query when data set size is 10 million) and scalable in terms of the number of data points.","1941-0018","","10.1109/TDSC.2020.2998039","National Natural Science Foundation of China(grant numbers:61972089,61672156,61872082,61802060,61972090); Dongguan University of Technology(grant numbers:KCYKYQD2017009,KCYKYQD2017002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103106","Cloud computing;secure nearest neighbor queries;adaptive IND-CKA security","Cloud computing;Indexes;Adaptation models;Encryption;Data models","","4","","43","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"Improving the Security of Audio CAPTCHAs With Adversarial Examples","P. Wang; H. Gao; X. Guo; Z. Yuan; J. Nian","Department of Computer Science and Technology, Xidian University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xidian University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xidian University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xidian University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xidian University, Xi'an, Shaanxi, China","IEEE Transactions on Dependable and Secure Computing","13 Mar 2024","2024","21","2","650","667","CAPTCHAs (completely automated public Turing tests to tell computers and humans apart) have been the main protection against malicious attacks on public systems for many years. Audio CAPTCHAs, as one of the most important CAPTCHA forms, provide an effective test for visually impaired users. However, in recent years, most of the existing audio CAPTCHAs have been successfully attacked by machine learning-based audio recognition algorithms, showing their insecurity. In this article, a generative adversarial network (GAN)-based method is proposed to generate adversarial audio CAPTCHAs. This method is implemented by using a generator to synthesize noise, a discriminator to make it similar to the target and a threshold function to limit the size of the perturbation; then, the synthetic perturbation is combined with the original audio to generate the adversarial audio CAPTCHA. The experimental results demonstrate that the addition of adversarial examples can greatly reduce the recognition accuracy of automatic models and improve the robustness of different types of audio CAPTCHAs. We also explore ensemble learning strategies to improve the transferability of the proposed adversarial audio CAPTCHA methods. To investigate the effect of adversarial CAPTCHAs on human users, a user study is also conducted.","1941-0018","","10.1109/TDSC.2023.3236367","National Natural Science Foundation of China(grant numbers:61972306); SongShan Laboratory(grant numbers:YYJC012022005); Zhejiang Lab(grant numbers:2021KD0AB03); Zhejiang Lab's International Talent Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015851","Audio CAPTCHA;reCAPTCHA;adversarial examples;generative adversarial networks","CAPTCHAs;Perturbation methods;Security;Image recognition;Training;Internet;Task analysis","","3","","97","IEEE","12 Jan 2023","","","IEEE","IEEE Journals"
"Pervasive Micro Information Flow Tracking","S. Mallissery; K. -Y. Chiang; C. -A. Bau; Y. -S. Wu","Department of Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4957","4975","Detection of advanced security attacks that exploit zero-day vulnerabilities or application-specific logic loopholes has been challenging due to the lack of attack signatures or substantial deviations in the overall system behavior. One has to zoom in to the affected code regions and look for local anomalies distinguishable from the benign workload to detect such attacks. We propose pervasive micro information flow tracking (PerMIT) that realizes variable-level online dynamic information flow tracking (DIFT) as a means to detect the attacks. The system uses hardware virtualization extension to monitor access to taint source variables and performs asynchronous code emulation to infer the local information flow. We demonstrate that the pervasive micro information flow can sufficiently capture the attacks and incurs only a small overhead. Given the program source code, the system can further enrich the semantics of micro information flow by embedding the variable names. We have integrated the system with machine learning algorithms to demonstrate the effectiveness of anomaly detection for zero-day attacks with pervasive micro information flow.","1941-0018","","10.1109/TDSC.2023.3238547","National Science and Technology Council of the Republic of China(grant numbers:111-2628-E-A49-007-MY2,111-2218-E-A49-013-MBK); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023951","Anomaly detection;dynamic information flow tracking;online taint analysis;production system;zero-day attacks","Target tracking;Security;Codes;Malware;Source coding;Emulation;Anomaly detection","","3","","51","IEEE","20 Jan 2023","","","IEEE","IEEE Journals"
"Optimizing the Numbers of Queries and Replies in Convex Federated Learning With Differential Privacy","Y. Zhou; X. Liu; Y. Fu; D. Wu; J. H. Wang; S. Yu","Department of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, NSW, Australia; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia","IEEE Transactions on Dependable and Secure Computing","10 Nov 2023","2023","20","6","4823","4837","Federated learning (FL) empowers distributed clients to collaboratively train a shared machine learning model through exchanging parameter information. Despite the fact that FL can protect clients’ raw data, malicious users can still crack original data with disclosed parameters. To amend this flaw, differential privacy (DP) is incorporated into FL clients to disturb original parameters, which however can significantly impair the accuracy of the trained model. In this work, we study an imperative question which has been vastly overlooked by existing works: what are the optimal numbers of queries and replies in FL with DP so that the final model accuracy is maximized. In FL, the parameter server (PS) needs to query participating clients for multiple global iterations to complete training. Each client responds a query from the PS by conducting a local iteration. We consider FL that will uniformly and randomly select participating clients to conduct local iterations with the FedSGD algorithm. Our work investigates how many times the PS should query clients and how many times each client should reply the PS by incorporating two most extensively used DP mechanisms (i.e., the Laplace mechanism and Gaussian mechanisms). Through conducting convergence rate analysis, we can determine the optimal numbers of queries and replies in FL with DP so that the final model accuracy can be maximized. Finally, extensive experiments are conducted with publicly available datasets: MNIST and FEMNIST, to verify our analysis and the results demonstrate that properly setting the numbers of queries and replies can significantly improve the final model accuracy in FL with DP.","1941-0018","","10.1109/TDSC.2023.3234599","National Natural Science Foundation of China(grant numbers:U1911201,U2001209,62072269); Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Shenzhen Fundamental Research Program(grant numbers:20200814105901001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008087","Federated learning;query and reply;differential privacy;convergence rate","Convergence;Differential privacy;Computational modeling;Privacy;Servers;Data models;Training","","2","","45","IEEE","6 Jan 2023","","","IEEE","IEEE Journals"
"Generative Pre-Trained Transformer-Based Reinforcement Learning for Testing Web Application Firewalls","H. Liang; X. Li; D. Xiao; J. Liu; Y. Zhou; A. Wang; J. Li","Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Trusted Software and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Nation Key Laboratory of Science and Technology on Information System Security, Beijing, China","IEEE Transactions on Dependable and Secure Computing","16 Jan 2024","2024","21","1","309","324","Web Application Firewalls (WAFs) are widely deployed to protect key web applications against multiple security threats, so it is important to test WAFs regularly to prevent attackers from bypassing them easily. Machine-learning-based black-box WAF testing is gaining more attention, though existing learning-based approaches have strict requirements on the source and scale of payload data and suffer from the local optimum problem, limiting their effectiveness and practical application. We propose GPTFuzzer, a practical and effective generation-based approach to test WAFs by generating attack payloads token-by-token. Specifically, we fine-tune a Generative Pre-trained Transformer language model with reinforcement learning to make GPTFuzzer have the least restrictions on payload data and thus more applicable in practice, and we use reward modeling and KL-divergence penalty to improve the effectiveness of our approach and mitigate the local optimum issue. We implement GPTFuzzer and evaluate it on two well-known open-source WAFs against three kinds of common attacks. Experimental results show that GPTFuzzer significantly outperforms state-of-the-art approaches, i.e., ML-Driven and RAT, finding up to 7.8× (3.2× on average) more bypassing payloads within 1,250,000 requests, or finding out all bypassing payloads using up to 8.1× (3.3× on average) fewer requests.","1941-0018","","10.1109/TDSC.2023.3252523","CNKLSTISS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059237","Black-box testing;reinforcement learning;transformer;web application firewall","Payloads;Grammar;Testing;Reinforcement learning;Data models;Adaptation models;Security","","2","","34","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"Understanding Security Risks of Embedded Devices Through Fine-Grained Firmware Fingerprinting","Q. Li; D. Tan; X. Ge; H. Wang; Z. Li; J. Liu","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Institute of Information Engineering, Chinese Academy of Sciences (CAS), Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Dependable and Secure Computing","10 Nov 2022","2022","19","6","4099","4112","An increasing number of embedded devices are connecting to the Internet, ranging from cameras, routers to printers, while an adversary can exploit security flaws already known to compromise those devices. Security patches are usually associated with the device firmware, which relies on the device vendors and products. Due to compatibility and release-time issues, many embedded devices are still using outdated firmware with known vulnerabilities or flaws. In this article, we conduct a systematic study on device vulnerabilities by leveraging firmware fingerprints. Specifically, we use a web crawler to gather 9,716 firmware images from official websites of device vendors, and 347,685 security reports scattered across data archives, blogs, and forums. We propose to generate fine-grained fingerprints based on the subtle differences between the filesystems of various firmware images. Furthermore, machine learning algorithms and regex are used to identify device vulnerabilities and corresponding device firmware fingerprints. We perform real-world experiments to validate the performance of the firmware fingerprint, which yields high accuracy of 91% precision and 90% recall. We reveal that 6,898 reports have the firmware and related vulnerability information, and there are more than 10% of firmware vulnerabilities without any patches or solutions for mitigating underlying security risks.","1941-0018","","10.1109/TDSC.2021.3119970","National Key R&D Program of China(grant numbers:2018YFB0803402); National Natural Science Foundation of China(grant numbers:61972024); National Key R&D Program of China(grant numbers:2020YFB2103802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573428","Firmware;fingerprinting;embedded device;vulnerability","Security;Internet;Microprogramming;Blogs;Performance evaluation;Fingerprint recognition;Software","","","","38","CCBY","14 Oct 2021","","","IEEE","IEEE Journals"
"Mangling Rules Generation With Density-Based Clustering for Password Guessing","S. Li; Z. Wang; R. Zhang; C. Wu; H. Luo","Intelligent Network Research Institute, Zhejiang Laboratory, Hangzhou, Zhejiang, China; Intelligent Network Research Institute, Zhejiang Laboratory, Hangzhou, Zhejiang, China; Intelligent Network Research Institute, Zhejiang Laboratory, Hangzhou, Zhejiang, China; College of computer science and technology, Zhejiang University, Hangzhou, Zhejiang, China; Intelligent Network Research Institute, Zhejiang Laboratory, Hangzhou, Zhejiang, China","IEEE Transactions on Dependable and Secure Computing","31 Aug 2023","2023","20","5","3588","3600","Rule-based password generation is one of the most effective and often employed techniques in the highly compute-intensive password recovery process. However, it is challenging to design and maintain a practical password mangling ruleset, which is a time-consuming task requiring specialized expertise. This paper therefore introduced MDBSCAN (Modified Density-Based Spatial Clustering of Applications with Noise), a novel density-based cluster approach in machine learning, to build an automatic password mangling rule generator. To evaluate the proposed method, cross-checks across 4 different real-world password datasets leaked from popular Internet services and applications are adopted. The results indicate that the proposed generator could produce high-quality mangling rules with a better hit rate and enhance current mangling rules by identifying hidden or omitted rules. The proposed approach also shows strong interpretability and computational efficiency. When examining the RockYou password dataset with the top 77 rules, the hit rate may rise by 11% to 104% proportionally to other well-known solutions. Furthermore, by combining the top 77 rules generated by MDBSCAN with those from other rulesets, 3–12.67% more real-world passwords can be retrieved.","1941-0018","","10.1109/TDSC.2022.3217002","National Key R&D Program of China(grant numbers:2018YFB2100400); Key Research Project of Zhejiang Lab(grant numbers:2021LE0AC01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928405","Density-based clustering;password mangling rule generation;password recovery","Passwords;Semantics;Clustering algorithms;Syntactics;Generators;Probabilistic logic;Markov processes","","","","48","IEEE","25 Oct 2022","","","IEEE","IEEE Journals"
"STRisk: A Socio-Technical Approach to Assess Hacking Breaches Risk","H. Hammouchi; N. Nejjari; G. Mezzour; M. Ghogho; H. Benbrahim","College of Engineering & Architecture (TICLab), Université Internationale de Rabat, Rabat, Morocco; College of Engineering & Architecture (TICLab), Université Internationale de Rabat, Rabat, Morocco; College of Engineering & Architecture (TICLab), Université Internationale de Rabat, Rabat, Morocco; College of Engineering & Architecture (TICLab), Université Internationale de Rabat, Rabat, Morocco; ENSIAS, Université Mohammed V, Rabat, Morocco","IEEE Transactions on Dependable and Secure Computing","13 Mar 2023","2023","20","2","1074","1087","Data breaches have begun to take on new dimensions and their prediction is becoming of great importance to organizations. Prior work has addressed this issue mainly from a technical perspective and neglected other interfering aspects such as the social media dimension. To fill this gap, we propose STRisk which is a predictive system where we expand the scope of the prediction task by bringing into play the social media dimension. We study over 3800 US organizations including both victim and non-victim organizations. For each organization, we design a profile composed of a variety of externally measured technical indicators and social factors. In addition, to account for unreported incidents, we consider the non-victim sample to be noisy and propose a noise correction approach to correct mislabeled organizations. We then build several machine learning models to predict whether an organization is exposed to experience a hacking breach. By exploiting both technical and social features, we achieve a Area Under Curve (AUC) score exceeding 98%, which is 12% higher than the AUC achieved using only technical features. Furthermore, our feature importance analysis reveals that open ports and expired certificates are the best technical predictors, while spreadability and agreeability are the best social predictors.","1941-0018","","10.1109/TDSC.2022.3149208","NATO Science for Peace and Security(grant numbers:SPS G5319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706308","Data breach;hacking breach;Twitter;cyber risk assessment;cyber risk prediction","Social networking (online);Data breach;Organizations;Blogs;Security;Predictive models;Noise measurement","","","","58","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"UC Secure Private Branching Program and Decision Tree Evaluation","K. Ji; B. Zhang; T. Lu; L. Li; K. Ren","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Ant Group, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Dependable and Secure Computing","10 Jul 2023","2023","20","4","2836","2848","Branching program (BP) is a DAG-based non-uniform computational model for L/poly class. It has been widely used in formal verification, logic synthesis, and data analysis. As a special BP, a decision tree is a popular machine learning classifier for its effectiveness and simplicity. In this work, we propose a UC-secure efficient 3-party computation platform for outsourced branching program and/or decision tree evaluation. We construct a constant-round protocol and a linear-round protocol. In particular, the overall (online + offline) communication cost of our linear-round protocol is $O(d(\ell + \log m+\log n))$O(d(ℓ+logm+logn)) and its round complexity is $2d-1$2d-1, where $m$m is the DAG size, $n$n is the number of features, $\ell$ℓ is the feature length, and $d$d is the longest path length. To enable efficient oblivious hopping among the DAG nodes, we propose a lightweight 1-out-of-$N$N shared OT protocol with logarithmic communication in both online and offline phase. This partial result may be of independent interest to some other cryptographic protocols. Our benchmark shows, compared with the state-of-the-arts, the proposed constant-round protocol is up to 10X faster in the WAN setting, while the proposed linear-round protocol is up to 15X faster in the LAN setting.","1941-0018","","10.1109/TDSC.2022.3202916","National Key R&D Program of China(grant numbers:2021YFB3101601); National Natural Science Foundation of China(grant numbers:62072401); Key Laboratory of Blockchain and Cyberspace Governance of Zhejiang Province; Input Output (iohk.io); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870545","Decision tree;privacy preserving;branching program","Protocols;Decision trees;Servers;Costs;Complexity theory;Privacy;Computational modeling","","","","26","IEEE","30 Aug 2022","","","IEEE","IEEE Journals"
"Pagoda: A Hybrid Approach to Enable Efficient Real-Time Provenance Based Intrusion Detection in Big Data Environments","Y. Xie; D. Feng; Y. Hu; Y. Li; S. Sample; D. Long","Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology, Wuhan, P.R. China; Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology, Wuhan, P.R. China; Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology, Wuhan, P.R. China; TuneUp.ai in San Francisco Bay Area, CA, USA; Jack Baskin School of Engineering, University of California, Santa Cruz, CA, USA; Jack Baskin School of Engineering, University of California, Santa Cruz, CA, USA","IEEE Transactions on Dependable and Secure Computing","6 Nov 2020","2020","17","6","1283","1296","Efficient intrusion detection and analysis of the security landscape in big data environments present challenge for today's users. Intrusion behavior can be described by provenance graphs that record the dependency relationships between intrusion processes and the infected files. Existing intrusion detection methods typically analyze and identify the anomaly either in a single provenance path or the whole provenance graph, neither of which can achieve the benefit on both detection accuracy and detection time. We propose Pagoda, a hybrid approach that takes into account the anomaly degree of both a single provenance path and the whole provenance graph. It can identify intrusion quickly if a serious compromise has been found on one path, and can further improve the detection rate by considering the behavior representation in the whole provenance graph. Pagoda uses a persistent memory database to store provenance and aggregates multiple similar items into one provenance record to maximumly reduce unnecessary I/O during the detection analysis. In addition, it encodes duplicate items in the rule database and filters noise that does not contain intrusion information. The experimental results on a wide variety of real-world applications demonstrate its performance and efficiency.","1941-0018","","10.1109/TDSC.2018.2867595","National Natural Science Foundation of China(grant numbers:U1705261,61821003); CCF-NSFOCUS Kun Peng research fund; Wuhan Application Basic Research Program(grant numbers:2017010201010104); Hubei Natural Science and Technology Foundation(grant numbers:2017CFB304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8450016","Provenance;intrusion detection;big data;real-time","Intrusion detection;Databases;Noise measurement;Big Data;Real-time systems;Aggregates","","26","","58","IEEE","29 Aug 2018","","","IEEE","IEEE Journals"
"Emerging Embedded and Cyber Physical System Security Challenges and Innovations","K. -K. R. Choo; M. M. Kermani; R. Azarderakhsh; M. Govindarasu","University of Texas at San Antonio, San Antonio, TX; Rochester Institute of Technology, Rochester, NY; Florida Atlantic University, Boca Raton, FL; Iowa State University, Ames, IA","IEEE Transactions on Dependable and Secure Computing","11 May 2017","2017","14","3","235","236","The papers in this special issue focus on deeply embedded systems and the challenges of cyper-physical security systems. Deeply-embedded systems (deployed in human body, with computer programs sending and receiving sensitive data and performing data mining for the decisions) are increasingly popular, but the security and privacy issues are not fully understood and studied. For example, issues relating to the conﬁdentiality/integrity/availability/privacy of implantable and wearable medical devices, secure and private big data analytics, acquisition, and storage, privacy-preserving data mining, secure machine learning, cyber physical systems security, and security of hardware and software systems used for databases (with diverse societal contexts) are critical, and can be challenging to address due to their unique constraints and usage model. Existing systems for such computations would need to be transparently integrated into sensitive environments-the consequent size and energy constraints imposed on any security solutions are demanding. Thus, unique challenges arise due to the sensitivity of computation processing, need for security in implementations, and assurance “gaps.” This special issue is dedicated to the identiﬁcation of techniques designed for embedded systems and cyber-physical systems, such as emerging cryptographic solutions applicable to extremely-constrained, sensitive infrastructures.","1941-0018","","10.1109/TDSC.2017.2664183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926472","","Special issues and sections;Cyber-physical systems;Embedded systems;Computer security;Cryptography;Data mining;Big data;Data analysis;Data privacy;Machine learning","","26","","","IEEE","11 May 2017","","","IEEE","IEEE Journals"
"Guest Editors’ Introduction to the Joint Special Section on Secure and Emerging Collaborative Computing and Intelligent Systems","Y. Hong; V. Issarny; S. Nepal; M. Srivatsa","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Inria, Talence, France; CSIRO‘s Data61, Sydney, NSW, Australia; IBM Research, Yorktown Heights, NY, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2021","2021","18","4","1511","1512","The papers in this special section focus on secure and emerging collaborative computing and intelligent systems. The Internet, coupled with recent advances in computing and information technologies, such as IoT, mobile edge/ cloud computing, cyber-physical-social systems, and artificial intelligence/machine learning/deep learning, have paved the way for creating next-generation smart and intelligent systems and applications that can have transformative impact in our society while accelerating rapid scientific discoveries and innovations. Unprecedented cyber-social and cyber-physical infrastructures and systems that span geographic boundaries are possible because of the Internet and the growing number of collaboration-enabling technologies. With newer technologies and paradigms getting increasingly embedded in the computing platforms and networked information systems/ infrastructures that form the digital foundation for our personal, organizational, and social processes and activities, it is increasingly becoming critical that the trust, privacy, and security issues in such digital environments are holistically addressed to ensure the safety and well-being of individuals as well as our society.","1941-0018","","10.1109/TDSC.2021.3076120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478313","","Special issues and sections;Deep learning;Blockchain;Privacy;Differential privacy;Intelligent systems;Smart devices;Predictive models;Intrusion detection","","","","0","IEEE","8 Jul 2021","","","IEEE","IEEE Journals"
