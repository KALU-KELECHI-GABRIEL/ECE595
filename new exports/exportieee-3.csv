"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Practitioners’ insights on machine-learning software engineering design patterns: a preliminary study","H. Washizaki; H. Takeuchi; F. Khomh; N. Natori; T. Doi; S. Okuda","Waseda University; Musashi University; Polytechnique Montréal; AISIN SEIKI Co., Ltd.; Lifematics Inc.; Japan Advanced Institute of Science and Technology","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","797","799","Machine-learning (ML) software engineering design patterns encapsulate reusable solutions to commonly occurring problems within the given contexts of ML systems and software design. These ML patterns should help develop and maintain ML systems and software from the design perspective. However, to the best of our knowledge, there is no study on the practitioners' insights on the use of ML patterns for design of their ML systems and software. Herein we report the preliminary results of a literature review and a questionnaire-based survey on ML system developers' state-of-practices with concrete ML patterns.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240692","Machine Learning;Design Patterns;Systematic Literature Review;Questionnaire Survey","Software maintenance;Software design;Conferences;Bibliographies;Machine learning;Software engineering","","8","","26","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"MLCatchUp: Automated Update of Deprecated Machine-Learning APIs in Python","S. A. Haryono; F. Thung; D. Lo; J. Lawall; L. Jiang","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; Inria, France; School of Information Systems, Singapore Management University, Singapore","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","584","588","Machine learning (ML) libraries are gaining vast popularity, especially in the Python programming language. Using the latest version of such libraries is recommended to ensure the best performance and security. When migrating to the latest version of a machine learning library, usages of deprecated APIs need to be updated, which is a time-consuming process. In this paper, we propose MLCatchUp, an automated API usage update tool for deprecated APIs of popular ML libraries written in Python. MLCatchUp automatically infers the required transformation to migrate usages of deprecated API through the differences between the deprecated and updated API signatures. MLCatchUp offers a readable transformation rule in the form of a domain specific language (DSL). We evaluate MLCatchUp using a dataset of 267 real-world Python code containing 551 usages of 68 distinct deprecated APIs, where MLCatchUp achieves 90.7% accuracy. A video demonstration of MLCatchUp is available at https://youtu.be/5NjOPNt5iaA.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609153","Python;Program Transformation;Automatic Update;Deprecated API","Software maintenance;Codes;Conferences;Machine learning;Tools;Libraries;Security","","2","","19","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Defuse: A Data Annotator and Model Builder for Software Defect Prediction","S. D. Palma; D. Di Nucci; D. Tamburri",Tilburg University/JADS; University of Salerno; Eindhoven University/JADS,"2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","479","483","We propose a language-agnostic tool for software defect prediction, called DEFUSE. The tool automatically collects and classifies failure data, enables the correction of those classifications, and builds machine learning models to detect defects based on those data. We instantiated the tool in the scope of Infrastructure-as-Code, the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files. We present its architecture and provide examples of its application.Demo video: https://youtu.be/37mmLdCX3jU.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978237","defect prediction;machine learning;mining software repositories","Software maintenance;Costs;Instruments;Machine learning;Computer architecture;Predictive models;Maintenance engineering","","1","","19","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Ticket Tagger: Machine Learning Driven Issue Classification","R. Kallis; A. Di Sorbo; G. Canfora; S. Panichella","Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy; School of Engineering, Zurich University of Applied Sciences, Zurich, Switzerland","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","406","409","Software maintenance is crucial for software projects evolution and success: code should be kept up-to-date and error-free, this with little effort and continuous updates for the end-users. In this context, issue trackers are essential tools for creating, managing and addressing the several (often hundreds of) issues that occur in software systems. A critical aspect for handling and prioritizing issues involves the assignment of labels to them (e.g., for projects hosted on GitHub), in order to determine the type (e.g., bug report, feature request and so on) of each specific issue. Although this labeling process has a positive impact on the effectiveness of issue processing, the current labeling mechanism is scarcely used on GitHub. In this demo, we introduce a tool, called Ticket Tagger, which leverages machine learning strategies on issue titles and descriptions for automatically labeling GitHub issues. Ticket Tagger automatically predicts the labels to assign to issues, with the aim of stimulating the use of labeling mechanisms in software projects, this to facilitate the issue management and prioritization processes. Along with the presentation of the tool's architecture and usage, we also evaluate its effectiveness in performing the issue labeling/classification process, which is critical to help maintainers to keep control of their workloads by focusing on the most critical issue tickets.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918993","Software maintenance and evolution;Issue Processing;Unstructured Data Labeling","Computer bugs;Labeling;Software maintenance;Pattern classification;Software systems","","52","","14","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"RepoQuester: A Tool Towards Evaluating GitHub Projects","K. Boyalakuntla; M. Nagappan; S. Chimalakonda; N. Munaiah","Indian Institute of Technology, Tirupati, India; University of Waterloo, Canada; Indian Institute of Technology, Tirupati, India; Rochester Institute of Technology, USA","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","509","513","Given the drastic rise of repositories on GitHub, it is often hard for developers to find relevant projects meeting their requirements as analyzing source code and other artifacts is effort-intensive. In our prior work, we proposed Repo Reaper (or simply Reaper) that assesses GitHub projects based on seven metrics spanning across project collaboration, quality, and maintenance. Reaper identified 1.4 million projects out of nearly 1.8 million projects to have no purpose for collaboration or software development by classifying them into ‘engineered’ and ‘non-engineered’ software projects. While Reaper can be used to assess millions of repositories based on GHTorrent, it is not designed to be used by developers for standalone repositories on local machines and is dependent on GHTorrent. Hence, in this paper, we propose a re-engineered and extended command-line tool named RepoQuester that aims to assist developers in evaluating GitHub projects on their local machines. RepoQuester computes metrics for projects and does not classify projects into ‘engineered’ and ‘non-engineered’ ones. However, to demonstrate the correctness of metric scores produced by RepoQuester, we have performed the project classification on the Reaper’s training and validation datasets by updating them with the latest metric scores (as reported by RepoQuester). These datasets have their ground truth manually established. During the analysis, we observed that the machine learning classifiers built on the updated datasets produced an F1 score of 72%. During the evaluation, for each project, we found that RepoQuester can analyze metric scores in less than 10 seconds. A demo video explaining the tool highlights and usage is available at https://youtu.be/Q8OdmNzUfN0, and source code at https://github.com/Kowndinya2000/Repoquester.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978187","Mining Software Repositories;GHTorrent;GitHub;Repo Reaper","Measurement;Training;Visualization;Software maintenance;Source coding;Collaboration;Machine learning","","","","17","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Design Smell Detection and Analysis for Open Source Java Software","A. Imran","Computer Science and Engineering, University at Buffalo, Buffalo, New York","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","644","648","Software design smells have gained significant importance in recent years since those directly lead to the increase of design debts and drastically affect software quality. Although the impact of design smells is manifold, techniques to detect design smells using both rule based and data mining approaches have been explored to a limited extent. This research aims to provide a tool which uses software metrics as a guide to detect smells and also deploys Spectral Clustering to mine the software repositories and group similar smells. The tool has been partially implemented till now and initial experiments on 2,59,509 Lines of Code (LoC) covering 3,306 classes of real life open source Java software show 2,220 occurrences of four types of design smells.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919054","Design Smell Detection, Design Smell Analysis, Software Engineering","Software;Tools;Java;Software metrics;Machine learning;Unsupervised learning;Data mining","","4","","31","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Why Don’t XAI Techniques Agree? Characterizing the Disagreements Between Post-hoc Explanations of Defect Predictions","S. Roy; G. Laberge; B. Roy; F. Khomh; A. Nikanjam; S. Mondal","University of Saskatchewan, Canada; Polytechnique Montréal, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","444","448","Machine Learning (ML) based defect prediction models can be used to improve the reliability and overall quality of software systems. However, such defect predictors might not be deployed in real applications due to the lack of transparency. Thus, recently, application of several post-hoc explanation methods (e.g., LIME and SHAP) have gained popularity. These explanation methods can offer insight by ranking features based on their importance in black box decisions. The explainability of ML techniques is reasonably novel in the Software Engineering community. However, it is still unclear whether such explainability methods genuinely help practitioners make better decisions regarding software maintenance. Recent user studies show that data scientists usually utilize multiple post-hoc explainers to understand a single model decision because of the lack of ground truth. Such a scenario causes disagreement between explainability methods and impedes drawing a conclusion. Therefore, our study first investigates three disagreement metrics between LIME and SHAP explanations of 10 defect-predictors, and exposes that disagreements regarding the rankings of feature importance are most frequent. Our findings lead us to propose a method of aggregating LIME and SHAP explanations that puts less emphasis on these disagreements while highlighting the aspect on which explanations agree.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978217","Empirical;Defect Prediction;eXplainable AI;LIME;SHAP;Software Maintenance","Measurement;Software maintenance;Closed box;Machine learning;Predictive models;Maintenance engineering;Software systems","","4","","23","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Leveraging Textual and Non-Textual Features for Documentation Decluttering","G. Colavito; P. Basile; N. Novielli","Dept. of Computer Science, University of Bari Aldo Moro, Bari, Italy; Dept. of Computer Science, University of Bari Aldo Moro, Bari, Italy; Dept. of Computer Science, University of Bari Aldo Moro, Bari, Italy","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","862","863","This paper describes the participation of a team from the University of Bari in the Decluttering Challenge organized in the scope of the DocGen2 workshop. We propose a supervised approach relying on a minimal set of non-textual features (length, overlapping between the comment text and the source code, code block type, tags, comment type) and classical textual features (bag-of-words). Our system ranked 2nd in the documentation decluttering task.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240676","Machine Learning;Source Code Comments","Software maintenance;Conferences;Documentation;Task analysis","","","","3","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Engineering Accessible Software","A. Krishnavajjala; K. Moran","Department of Computer Science, George Mason University, Fairfax, United States; Department of Computer Science, George Mason University, Fairfax, United States","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","597","600","This paper discusses a research agenda at the intersection of Machine Learning, Software Engineering, and Human-Computer Interaction aimed at creating intelligent tools that enable developers to build more accessible software. First, we discuss the creation of MOTOREASE, a novel approach that utilizes computer vision and text processing techniques to identify accessibility issues in mobile app UIs for motor-impaired users. The tool detects four motor-impaired user-focused UI design guidelines: touch target size, expanding sections, persisting elements, and adjacent icon distance, with an average accuracy of around 90%. This represents a significant step towards improving software accessibility for all users. Finally, we discuss our future work for better-supporting developers with tools to engineer accessible software.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336281","Accessibility;Deep Learning;Mobile Apps;UI Understanding","Human computer interaction;Software maintenance;Computer vision;Design methodology;Machine learning;Mobile applications;Text processing","","","","34","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"An Empirical Study on the Usage of Automated Machine Learning Tools","F. Majidi; M. Openja; F. Khomh; H. Li","Polytechnique Montréal, Montréal, Quebec, Canada; Polytechnique Montréal, Montréal, Quebec, Canada; Polytechnique Montréal, Montréal, Quebec, Canada; Polytechnique Montréal, Montréal, Quebec, Canada","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","59","70","The popularity of automated machine learning (AutoML) tools in different domains has increased over the past few years. Machine learning (ML) practitioners use AutoML tools to automate and optimize the process of feature engineering, model training, and hyperparameter optimization and so on. Recent work performed qualitative studies on practitioners’ experiences of using AutoML tools and compared different AutoML tools based on their performance and provided features, but none of the existing work studied the practices of using AutoML tools in real-world projects at a large scale. Therefore, we conducted an empirical study to understand how ML practitioners use AutoML tools in their projects. To this end, we examined the top 10 most used AutoML tools and their respective usages in a large number of open-source project repositories hosted on GitHub. The results of our study show 1) which AutoML tools are mostly used by ML practitioners and 2) the characteristics of the repositories that use these AutoML tools. Also, we identified the purpose of using AutoML tools (e.g. model parameter sampling, search space management, model evaluation/error-analysis, Data/ feature transformation, and data labeling) and the stages of the ML pipeline (e.g. feature engineering) where AutoML tools are used. Finally, we report how often AutoML tools are used together in the same source code files. We hope our results can help ML practitioners learn about different AutoML tools and their usages, so that they can pick the right tool for their purposes. Besides, AutoML tool developers can benefit from our findings to gain insight into the usages of their tools and improve their tools to better fit the users’ usages and needs.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00014","Natural Sciences and Engineering Research Council of Canada; Canadian Institute for Advanced Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978249","Empirical;Automated machine learning;Au-toML tools;GitHub repository","Training;Software maintenance;Source coding;Pipelines;Machine learning;Data models;Labeling","","6","","64","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"On the Impact of Multi-language Development in Machine Learning Frameworks","M. Grichi; E. E. Eghan; B. Adams","GIGL department, Polytechnique Montreal, Quebec, Canada; GIGL department, Polytechnique Montreal, Quebec, Canada; GIGL department, Polytechnique Montreal, Quebec, Canada","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","546","556","The role of machine learning frameworks in software applications has exploded in recent years. Similar to non-machine learning frameworks, those frameworks need to evolve to incorporate new features, optimizations, etc., yet their evolution is impacted by the interdisciplinary development teams needed to develop them: scientists and developers. One concrete way in which this shows is through the use of multiple programming languages in their code base, enabling the scientists to write optimized low-level code while developers can integrate the latter into a robust framework. Since multi-language code bases have been shown to impact the development process, this paper empirically compares ten large open-source multi-language machine learning frameworks and ten large open-source multi-language traditional systems in terms of the volume of pull requests, their acceptance ratio i.e., the percentage of accepted pull requests among all the received pull requests, review process duration i.e., period taken to accept or reject a pull request, and bug-proneness. We find that multi-language pull request contributions present a challenge for both machine learning and traditional systems. Our main findings show that in both machine learning and traditional systems, multi-language pull requests are likely to be less accepted than mono-language pull requests; it also takes longer for both multi- and mono-language pull requests to be rejected than accepted. Machine learning frameworks take longer to accept/reject a multi-language pull request than traditional systems. Finally, we find that mono-language pull requests in machine learning frameworks are more bug-prone than traditional systems.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240642","Machine learning;Framework;Open Source;Software engineering;Multi-language;Traditional systems","Software maintenance;Computer languages;Conferences;Machine learning;Open source software;Optimization","","6","","36","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Sine-Cosine Algorithm for Software Fault Prediction","T. Sharma; O. P. Sangwan","Deptt. of Computer Science and Engineering, G J University of Science and Technology, Hisar, India; Deptt. of Computer Science and Engineering, G J University of Science and Technology, Hisar, India","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","701","706","For developing an efficient and quality Software Fault Prediction (SFP) model, redundant and irrelevant features need to be removed. This task can be achieved, to a significant extent, with Feature Selection (FS) methods. Many empirical studies have been proposed on FS methods (Filter and Wrapper-based) and have shown effective results in reducing the problem of high dimensionality in metrics-based SFP models. This study evaluates the performance of novel wrapper-based Sine Cosine Algorithm (SCA) on five datasets of the AEEEM repository and compares the results with two metaheuristic techniques Genetic Algorithm (GA) and Cuckoo Search algorithm (CSA) on four different Machine Learning (ML) classifiers - Random Forest (RF), Support Vector Machine (SVM), Naïve Bayes (NB), and K-Nearest Neighbor (KNN). We found that the application of FS methods (SCA, GA & CSA) has improved the classifier performance. SCA has proved to be more efficient than GA and CSA in terms of lesser convergence time with the smallest subset of selected features and equivalent performance.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609148","software fault prediction;feature selection;sine cosine algorithm;metaheuristic techniques","Support vector machines;Software maintenance;Machine learning algorithms;Software algorithms;Stochastic processes;Predictive models;Prediction algorithms","","","","32","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"A Practical Approach to the Automatic Classification of Security-Relevant Commits","A. Sabetta; M. Bezzi",SAP Security Research; SAP Security Research,"2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","579","582","The lack of reliable sources of detailed information on the vulnerabilities of open-source software (OSS) components is a major obstacle to maintaining a secure software supply chain and an effective vulnerability management process. Standard sources of advisories and vulnerability data, such as the National Vulnerability Database (NVD), are known to suffer from poor coverage and inconsistent quality. To reduce our dependency on these sources, we propose an approach that uses machine-learning to analyze source code repositories and to automatically identify commits that are security-relevant (i.e., that are likely to fix a vulnerability). We treat the source code changes introduced by commits as documents written in natural language, classifying them using standard document classification methods. Combining independent classifiers that use information from different facets of commits, our method can yield high precision (80%) while ensuring acceptable recall (43%). In particular, the use of information extracted from the source code changes yields a substantial improvement over the best known approach in state of the art, while requiring a significantly smaller amount of training data and employing a simpler architecture.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530068","machine learning;open source software;vulnerabilities;NVD;CVE;commit classification;source code repositories;code change classification","Security;Standards;Open source software;Databases;Predictive models;Machine learning","","36","","8","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Sorrel: an IDE Plugin for Managing Licenses and Detecting License Incompatibilities","D. Pogrebnoy; I. Kuznetsov; Y. Golubev; V. Tankov; T. Bryksin",JetBrains; Saint Petersburg Polytechnic University; JetBrains Research; Higher School of Economics; JetBrains Research,"2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","574","578","Software development is a complex process that includes many different tasks besides just writing code. One of the aspects of software engineering is selecting and managing licenses for the given project. In this paper, we present SORREL-a plugin for managing licenses and detecting potential incompatibilities for IntelliJ IDEA, a popular Java IDE. The plugin scans the project in search of information about the project license and the licenses of its libraries. If the project does not yet have a license, the plugin provides the developer with recommendations for choosing the most suitable open license, and if there is a license, it informs the programmer about potential licensing violations. The tool makes it easier for developers to choose a proper license for a project and avoid most of the licensing errors-all inside the familiar IDE editor. The plugin and its source code are available online on GitHub: https://github.com/JetBrains-Research/sorrel. A demonstration video can be found at https://youtu.be/doUeAwPjcPE.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609150","license detection;license management;license incompatibilities","Java;Software maintenance;Codes;Machine learning;Licenses;Tools;Writing","","1","","24","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"flexfringe: A Passive Automaton Learning Package","S. Verwer; C. A. Hammerschmidt",Delft University of Technology Cyber Security group; Delft University of Technology Cyber Security group,"2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","638","642","Finite state models, such as Mealy machines or state charts, are often used to express and specify protocol and software behavior. Consequently, these models are often used in verification, testing, and for assistance in the development and maintenance process. Reverse engineering these models from execution traces and log files, in turn, can accelerate and improve the software development and inform domain experts about the processes actually executed in a system. We present name, an open-source software tool to learn variants of finite state automata from traces using a state-of-the-art evidence-driven state-merging algorithm at its core. We embrace the need for customized models and tailored learning heuristics in different application domains by providing a flexible, extensible interface.","","978-1-5386-0992-7","10.1109/ICSME.2017.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094471","machine learning tool;automata learning;software package;finite state machines","Learning automata;Tools;Software algorithms;Software;Heuristic algorithms;Machine learning algorithms;Algorithm design and analysis","","14","","37","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Elevating Jupyter Notebook Maintenance Tooling by Identifying and Extracting Notebook Structures","Y. Jiang; C. Kästner; S. Zhou",Carnegie Mellon University; Carnegie Mellon University; University of Toronto,"2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","399","403","Data analysis is an exploratory, interactive, and often collaborative process. Computational notebooks have become a popular tool to support this process, among others because of their ability to interleave code, narrative text, and results. However, notebooks in practice are often criticized as hard to maintain and being of low code quality, including problems such as unused or duplicated code and out-of-order code execution. Data scientists can benefit from better tool support when maintaining and evolving notebooks. We argue that central to such tool support is identifying the structure of notebooks. We present a lightweight and accurate approach to extract notebook structure and outline several ways such structure can be used to improve maintenance tooling for notebooks, including navigation and finding alternatives.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00047","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978200","Jupyter notebook;maintenance tooling;notebook structure;static analysis;classification;navigation","Out of order;Software maintenance;Codes;Runtime;Navigation;Prototypes;Documentation","","","","20","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Automatic Identification of Rollback Edit with Reasons in Stack Overflow Q&A Site","S. Mondal; G. Uddin; C. K. Roy","University of Saskatchewan, Canada; University of Calgary, Canada; University of Saskatchewan, Canada","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","856","856","Crowd-sourced developer forums, such as Stack Overflow (SO), rely on edits from users to improve the quality of the shared knowledge. Unfortunately, suggested edits in SO are frequently rejected by rollbacks due to undesired edits or violation of editing guidelines. Such rollbacks could frustrate and demotivate users to provide future suggestions. We thus need to warn a user of a potential rollback so that he can improve the suggested edit and thus increase its likelihood of acceptance. This study proposes to help users with an automated machine learning classification model that can warn them of potential rollbacks to their suggested edits. We present the conceptual design of EditEx, an online tool that can guide SO users during their editing by highlighting the potential causes of rollback. We offer details of an empirical study to assess the accuracy of the classifiers and a user study to evaluate the effectiveness of EditEx.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240665","Stack Overflow;Rollback edits;Classification Model;User Study","Software maintenance;Conferences;Machine learning;Tools;Guidelines","","","","5","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"DebtViz: A Tool for Identifying, Measuring, Visualizing, and Monitoring Self-Admitted Technical Debt","Y. Li; M. Soliman; P. Avgeriou; M. Van Ittersum","Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, The Netherlands; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, The Netherlands; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, The Netherlands; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, The Netherlands","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","558","562","Technical debt, specifically Self-Admitted Technical Debt (SATD), remains a significant challenge for software developers and managers due to its potential to adversely affect long-term software maintainability. Although various approaches exist to identify SATD, tools for its comprehensive management are notably lacking. This paper presents DebtViz, an innovative SATD tool designed to automatically detect, classify, visualize and monitor various types of SATD in source code comments and issue tracking systems. DebtViz employs a Convolutional Neural Network-based approach for detection and a deconvolution technique for keyword extraction. The tool is structured into a back-end service for data collection and pre-processing, a SATD classifier for data categorization, and a front-end module for user interaction. DebtViz not only makes the management of SATD more efficient but also provides in-depth insights into the state of SATD within software systems, fostering informed decision-making on managing it. The scalability and deployability of DebtViz also make it a practical tool for both developers and managers in diverse software development environments. The source code of DebtViz is available at https://github.com/yikun-li/visdom-satd-management-system and the demo of DebtViz is at https://youtu.be/QXH6Bj0HQew.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336277","self-admitted technical debt;technical debt management;technical debt visualization","Visualization;Software maintenance;Deconvolution;Source coding;Scalability;Decision making;Refining","","","","18","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Automatic Traceability Maintenance via Machine Learning Classification","C. Mills; J. Escobar-Avila; S. Haiduc","Department of Computer Science, Florida State University, Tallahassee, FL, USA; Department of Computer Science, Florida State University, Tallahassee, FL, USA; Department of Computer Science, Florida State University, Tallahassee, FL, USA","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","369","380","Previous studies have shown that software traceability, the ability to link together related artifacts from different sources within a project (e.g., source code, use cases, documentation, etc.), improves project outcomes by assisting developers and other stakeholders with common tasks such as impact analysis, concept location, etc. Establishing traceability links in a software system is an important and costly task, but only half the struggle. As the project undergoes maintenance and evolution, new artifacts are added and existing ones are changed, resulting in outdated traceability information. Therefore, specific steps need to be taken to make sure that traceability links are maintained in tandem with the rest of the project. In this paper we address this problem and propose a novel approach called TRAIL for maintaining traceability information in a system. The novelty of TRAIL stands in the fact that it leverages previously captured knowledge about project traceability to train a machine learning classifier which can then be used to derive new traceability links and update existing ones. We evaluated TRAIL on 11 commonly used traceability datasets from six software systems and compared it to seven popular Information Retrieval (IR) techniques including the most common approaches used in previous work. The results indicate that TRAIL outperforms all IR approaches in terms of precision, recall, and F-score.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530044","traceability link recovery;classification;machine learning;information retrieval;traceability maintenance;query quality","Feature extraction;Maintenance engineering;Machine learning;Task analysis;Software systems;Matrix decomposition","","29","","57","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"LDA Analyzer: A Tool for Exploring Topic Models","C. Zou; D. Hou","Department of Electrical and Computer Engineering, Clarkson University, Potsdam, New York; Department of Electrical and Computer Engineering, Clarkson University, Potsdam, New York","2014 IEEE International Conference on Software Maintenance and Evolution","6 Dec 2014","2014","","","593","596","Online technical forums are valuable sources for mining useful software engineering information. LDA (Latent Dirichlet Allocation) is an unsupervised machine learning method which can be used for extracting underlying topics out of such large forums. However, the main output of LDA forum learning are usually huge matrices that contain millions of numbers, which is impossible for researchers to directly scrutinize the numerical distribution and semantically evaluate the relationship between the extracted topics and large collection of unorganized documents. In this paper, we present LDAAnalyzer, an LDA visualization tool that makes the hidden topic-document structures rise to the surface. From the functionality point of view, LDA Analyzer consists of (1) LDA modeling (2) LDA output analysis and (3) new corpus training. With the help of LDAAnalyzer, our semantic topic-modeling evaluation based on large technical forums becomes feasible.","1063-6773","978-1-4799-6146-7","10.1109/ICSME.2014.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976147","LDA;topic modeling;visualization;forum","Data visualization;Semantics;Visualization;Training;Load modeling;Mathematical model;Standards","","15","","13","IEEE","6 Dec 2014","","","IEEE","IEEE Conferences"
"A Conceptual Antifragile Microservice Framework for Reshaping Critical Infrastructures","H. Bangui; B. Rossi; B. Buhnova","Faculty of Informatics, Masaryk University, Brno, Czech Republic; Faculty of Informatics, Masaryk University, Brno, Czech Republic; Faculty of Informatics, Masaryk University, Brno, Czech Republic","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","364","368","Recently, microservices have been examined as a solution for reshaping and improving the flexibility, scalability, and maintainability of critical infrastructure systems. However, microservice systems are also suffering from the presence of a substantial number of potentially vulnerable components that may threaten the protection of critical infrastructures. To address the problem, this paper proposes to leverage the concept of antifragility built in a framework for building self-learning microservice systems that could be strengthened by faults and threats instead of being deteriorated by them. To illustrate the approach, we instantiate the proposed approach of autonomous machine learning through an experimental evaluation on a benchmarking dataset of microservice faults.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9977445","Critical Infrastructures;Microservices;Antifragility;Machine Learning;Generative Adversarial Network","Software maintenance;Autonomous systems;Scalability;Microservice architectures;Termination of employment;Elasticity;Generative adversarial networks","","","","17","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Exploring the use of deep learning for feature location","C. S. Corley; K. Damevski; N. A. Kraft","The University of Alabama, Tuscaloosa, AL, USA; Virginia Commonwealth University, Richmond, VA, USA; ABB Corporate Research, Raleigh, NC, USA","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","23 Nov 2015","2015","","","556","560","Deep learning models can infer complex patterns present in natural language text. Relative to n-gram models, deep learning models can capture more complex statistical patterns based on smaller training corpora. In this paper we explore the use of a particular deep learning model, document vectors (DVs), for feature location. DVs seem well suited to use with source code, because they both capture the influence of context on each term in a corpus and map terms into a continuous semantic space that encodes semantic relationships such as synonymy. We present preliminary results that show that a feature location technique (FLT) based on DVs can outperform an analogous FLT based on latent Dirichlet allocation (LDA) and then suggest several directions for future work on the use of deep learning models to improve developer effectiveness in feature location.","","978-1-4673-7532-0","10.1109/ICSM.2015.7332513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332513","deep learning;neural networks;document vectors;feature location","Semantics;Machine learning;Natural languages;Voltage control;Neural networks;Training;Context","","29","","23","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"Source Code based On-demand Class Documentation Generation","M. Liu; X. Peng; X. Meng; H. Xu; S. Xing; X. Wang; Y. Liu; G. Lv","School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","864","865","In this paper, we present OpenAPIDocGen2, a tool that generates on-demand class documentation based on source code and documentation analysis. For a given class, OpenAPIDocGen2 generates a combined documentation for it, which includes functionality descriptions, directives, domain concepts, usage examples, class/method roles, key methods, relevant classes/methods, characteristics and concepts classification, and usage scenarios.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00114","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240703","Documentation Generation;Knowledge Extraction;Information Seeking","Software maintenance;Conferences;Documentation;Tools","","3","","9","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"How Developers Implement Property-Based Tests","A. L. Corgozinho; M. T. Valente; H. Rocha","Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil; Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil; Department of Computer Science, Loyola University Maryland, Baltimore, USA","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","380","384","Property-based testing (PBT) is an interesting alternative to example-based testing where the inputs are randomly generated by the testing tool. In PBT, we check properties that always hold for any input. Despite being a promising testing category, to the best of our knowledge, we still lack studies that investigate in the wild how developers are using PBT in practice. In this paper, we report the preliminary results of a study we are conducting on the usage of PBT. We created a dataset of 30 popular Python repositories using Hypothesis (a PBT tool) and selected a random sample of 86 tests. We manually analyzed these tests to understand the most commonly implemented properties and also to reveal the most used features to create them.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00049","EMI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336336","Property-Based Testing;Hypothesis;Testing","Software maintenance;Testing;Python","","","","9","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Characterizing Performance Regression Introducing Code Changes","D. A. ALShoaibi","Software Engineering Department, Rochester Institute of Technology, NY, USA","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","634","638","Performance regression testing is highly expensive as it delays system development when optimally conducted after each code change. As a result, performance regression testing should be devoted to code changes highly probably encountering regression. In this context, recent studies focus on the early identification of potentially problematic code changes through characterizing them using static and dynamic metrics. The aim of my research thesis is to support performance regression by better identifying and characterizing performance regression introducing code changes. Our first contribution has tackled the detection of these changes as an optimization problem. Our proposed approach used a combination of static and dynamic metrics and built using evolutionary computation, a detection rule, which was shown to outperform recent state-of-the-art studies. To extend our research, we are planning to increase metrics used, to better profile problematic code changes. We also plan on reducing the identification cost by searching for a traedeoff that reduces the use of dynamic metrics, while maintaining the detection performance. In addition, we would like to prioritize test case based on code changes characteristics to be conducted when regression predicted.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919139","Performance regression;genetic programming","Measurement;Benchmark testing;Decision trees;Software;History;Optimization","","","","16","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Improving Software Maintenance Using Process Mining and Predictive Analytics","M. Gupta; A. Serebrenik; P. Jalote","IIIT Delhi, India; Eindhoven University of Technology, The Netherlands; IIIT Delhi, India","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","681","686","This research focuses on analyzing and improving maintenance process by exploring novel applications of process mining and predictive analytics. We analyze the software maintenance process by applying process mining on software repositories, and address the identified inefficiencies using predictive analytics. To drive our research, we engage with practitioners from large, global IT companies and emphasize on the practical usability of the proposed solution approaches, which are evaluated by conducting a series of case studies on open source and commercial projects.","","978-1-5386-0992-7","10.1109/ICSME.2017.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094481","Process Mining;Qualitative Study;Machine Learning;IT Support Process;Bug Detection","Data mining;Software maintenance;Companies;Maintenance engineering;Object recognition;Clocks","","7","","25","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Cost Reduction on Testing Evolving Cancer Registry System","E. Isaku; H. Sartaj; C. Laaber; T. Yue; S. Ali; T. Schwitalla; J. F. Nygård","Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Cancer Registry of Norway, Oslo, Norway; Cancer Registry of Norway, Oslo, Norway","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","508","518","The Cancer Registration Support System (CaReSS), built by the Cancer Registry of Norway (CRN), is a complex real-world socio-technical software system that undergoes continuous evolution in its implementation. Consequently, continuous testing of CaReSS with automated testing tools is needed such that its dependability is always ensured. Towards automated testing of a key software subsystem of CaReSS, i.e., GURI, we present a real-world application of an extension to the open-source tool EvoMaster, which automatically generates test cases with evolutionary algorithms. We named the extension EvoClass, which enhances EvoMaster with a machine learning classifier to reduce the overall testing cost. This is imperative since testing with EvoMaster involves sending many requests to GURI deployed in different environments, including the production environment, whose performance and functionality could potentially be affected by many requests. The machine learning classifier of EvoClass can predict whether a request generated by EvoMaster will be executed successfully or not; if not, the classifier filters out such requests, consequently reducing the number of requests to be executed on GURI. We evaluated EvoClass on ten GURI versions over four years in three environments: development, testing, and production. Results showed that EvoClass can significantly reduce the testing cost of evolving GURI without reducing testing effectiveness (measured as rule coverage) across all three environments, as compared to the default EvoMaster. Overall, EvoClass achieved ≈31% of overall cost reduction. Finally, we report our experiences and lessons learned that are equally valuable for researchers and practitioners.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336343","Software Evolution;Testing;Machine Learning","Software maintenance;Costs;Machine learning;Production;Evolutionary computation;Software systems;Test pattern generators","","","","35","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"A Machine Learning Approach to Convert Pseudo-Code to Domain-Specific Programming Language","J. Neal; S. Rogers; E. Parra","Binary Evolution, Fayetteville, TN, USA; Binary Evolution, Fayetteville, TN, USA; Department of Math and Computer Science, Belmont University, Nashville, TN, USA","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","442","452","The California Department of Energy oversees a set of standards and rules that new constructions (i.e., buildings) must adhere to within the State of California (the California Energy Code). Energy Code Ace (ECA) is a website that helps users plan their future construction in California and see if the plans for their structure comply with the California Energy Code. However, there are thousands of details and sections in the California Energy Code that need to be translated into questions and fields on the website for the user to fill out. To do this, the Department of Energy provides a web development team at Binary Evolution with XML schematics that contain details of each field and pseudo-code of the logic that fields should follow compared to the other fields on the ECA website. A very large portion of the developers’ job involves reading the pseudo-code and writing real code in a language developed by Binary Evolution. Reading and handwriting the code as described in the schematics can take up to hours of a developer’s time.This paper presents ECAi, a new approach for automatically generating executable code from pseudo-code schematics. ECAi uses a Decision Tree Classifier trained on 23,864 lines of pseudo-code and had to classify a line of pseudo-code into one of 13 categories achieving an 89.87% F1-score. Leveraging the classifier category, ECAi then calls a code-writing algorithm to parse the line of pseudo-code and writes the executable code that corresponds with the given pseudo-code line. The implementation of a prototype of the system shows a statistically significant (p=0.0116) decrease in the time it takes a developer to obtain an executable piece of code. ECAi has saved countless hours of development work during the maintenance workflow of ECA.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336307","automatic code generation;pseudo-code;machine learning","Software maintenance;Computer languages;Codes;XML;Prototypes;Machine learning;Writing","","","","23","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"What do Developers Know About Machine Learning: A Study of ML Discussions on StackOverflow","A. A. Bangash; H. Sahar; S. Chowdhury; A. W. Wong; A. Hindle; K. Ali","Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","260","264","Machine learning, a branch of Artificial Intelligence, is now popular in software engineering community and is successfully used for problems like bug prediction, and software development effort estimation. Developers' understanding of machine learning, however, is not clear, and we require investigation to understand what educators should focus on, and how different online programming discussion communities can be more helpful. We conduct a study on Stack Overflow (SO) machine learning related posts using the SOTorrent dataset. We found that some machine learning topics are significantly more discussed than others, and others need more attention. We also found that topic generation with Latent Dirichlet Allocation (LDA) can suggest more appropriate tags that can make a machine learning post more visible and thus can help in receiving immediate feedback from sites like SO.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816808","stackoverflow, machine learning, topic modeling","Machine learning;Machine learning algorithms;Software;Classification algorithms;Training;Programming;Tagging","","24","","10","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Characterizing the Complexity and Its Impact on Testing in ML-Enabled Systems : A Case Sutdy on Rasa","J. Cao; B. Chen; L. Hu; J. Gao; K. Huang; X. Song; X. Peng","School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; Singapore University of Technology and Design, Singapore; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","258","270","Machine learning (ML) enabled systems are emerging with recent breakthroughs in ML. A model-centric view is widely taken by the literature to focus only on the analysis of ML models. However, only a small body of work takes a system view that looks at how ML components work with the system and how they affect software engineering for ML-enabled systems. In this paper, we adopt this system view, and conduct a case study on Rasa 3.0, an industrial dialogue system that has been widely adopted by various companies around the world. Our goal is to characterize the complexity of such a large-scale ML-enabled system and to understand the impact of the complexity on testing. Our study reveals practical implications for software engineering for ML-enabled systems.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336332","machine learning system;empirical study;machine learning testing","Analytical models;Software maintenance;Source coding;Machine learning;Companies;Complexity theory;Testing","","","","97","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Improving Clone Detection Precision Using Machine Learning Techniques","V. Arammongkolvichai; R. Koschke; C. Ragkhitwetsagul; M. Choetkiertikul; T. Sunetnanta","Faculty of Information and Communication Technology (ICT), Mahidol University, Thailand; University of Bremen, Germany; Faculty of Information and Communication Technology (ICT), Mahidol University, Thailand; Faculty of Information and Communication Technology (ICT), Mahidol University, Thailand; Faculty of Information and Communication Technology (ICT), Mahidol University, Thailand","2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)","30 Dec 2019","2019","","","31","315","Code clones or similar segments of code in a software project can be detected by using a clone detection tool. Due to modifications applied after copying and pasting of the cloned code, the current code clone detection tools face challenges to accurately detect clones with heavy modifications (i.e., Type-3 clones or clones with added/deleted/modified statements). One challenge is because the clone results contain several false positives. In this paper, we propose an approach for increasing the precision of code clone detection using machine learning techniques. By training a decision tree on 19 clone class metrics, we use the trained decision tree as a clone filter by placing it in the last step in the clone detection pipeline. This aims to remove false positive clone classes reported by a clone detection tool. We found that the decision tree clone filter is helpful for decreasing the number of false positive clone classes in iClones, a well-known code clone detector. After training the decision tree on 537 clone classes in JFreeChart and evaluating it on the test data set, it could improve iClone's precision from 0.94 to 0.98. The findings show that decision tree can be used effectively for filtering false positive clones. Nonetheless, we found that the filter is only effective for Java and does not offer satisfying performance when running on a Django Python project.","2573-2021","978-1-7281-5590-6","10.1109/IWESEP49350.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945086","code clones;machine learning;decision tree","Cloning;Decision trees;Measurement;Training;Machine learning;Detectors;Data models","","1","","22","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Automatic Clone Recommendation for Refactoring Based on the Present and the Past","R. Yue; Z. Gao; N. Meng; Y. Xiong; X. Wang; J. D. Morgenthaler","EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China; Department of Computer Science, Virginia Tech, Blacksburg, VA; EECS, Peking University, Beijing, China; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX; Google, Mountain View, CA","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","115","126","When many clones are detected in software programs, not all clones are equally important to developers. To help developers refactor code and improve software quality, various tools were built to recommend clone-removal refactorings based on the past and the present information, such as the cohesion degree of individual clones or the co-evolution relations of clone peers. The existence of these tools inspired us to build an approach that considers as many factors as possible to more accurately recommend clones. This paper introduces CREC, a learning-based approach that recommends clones by extracting features from the current status and past history of software projects. Given a set of software repositories, CREC first automatically extracts the clone groups historically refactored (R-clones) and those not refactored (NR-clones) to construct the training set. CREC extracts 34 features to characterize the content and evolution behaviors of individual clones, as well as the spatial, syntactical, and co-change relations of clone peers. With these features, CREC trains a classifier that recommends clones for refactoring. We designed the largest feature set thus far for clone recommendation, and performed an evaluation on six large projects. The results show that our approach suggested refactorings with 83% and 76% F-scores in the within-project and cross-project settings. CREC significantly outperforms a state-of-the-art similar approach on our data set, with the latter one achieving 70% and 50% F-scores. We also compared the effectiveness of different factors and different learning algorithms.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530022","clone refactoring;clone recommendation;machine learning","Cloning;Feature extraction;History;Software;Tools;Training;Machine learning","","20","","52","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Predicting Developers' IDE Commands with Machine Learning","T. Bulmer; L. Montgomery; D. Damian","University of Victoria, Victoria, Canada; University of Victoria, Victoria, Canada; University of Victoria, Victoria, Canada","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","82","85","When a developer is writing code they are usually focused and in a state-of-mind which some refer to as flow. Breaking out of this flow can cause the developer to lose their train of thought and have to start their thought process from the beginning. This loss of thought can be caused by interruptions and sometimes slow IDE interactions. Predictive functionality has been harnessed in user applications to speed up load times, such as in Google Chrome's browser which has a feature called ""Predicting Network Actions"". This will pre-load web-pages that the user is most likely to click through. This mitigates the interruption that load times can introduce. In this paper we seek to make the first step towards predicting user commands in the IDE. Using the MSR 2018 Challenge Data of over 3000 developer session and over 10 million recorded events, we analyze and cleanse the data to be parsed into event series, which can then be used to train a variety of machine learning models, including a neural network, to predict user induced commands. Our highest performing model is able to obtain a 5 cross-fold validation prediction accuracy of 64%.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595185","IDE Monitoring;Machine Learning;Neural Network;Developer Commands","Computational modeling;Predictive models;Machine learning;Data models;Neural networks;Load modeling;Feature extraction","","","","21","","30 Dec 2018","","","IEEE","IEEE Conferences"
"RMove: Recommending Move Method Refactoring Opportunities using Structural and Semantic Representations of Code","D. Cui; S. Wang; Y. Luo; X. Li; J. Dai; L. Wang; Q. Li","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","281","292","Incorrect placement of methods within classes is a typical code smell called Feature Envy, which causes additional maintenance and cost during evolution. To remove this design flaw, several Move Method refactoring tools have been proposed. To the best of our knowledge, state-of-the-art related techniques can be broadly divided into two categories: the first line is non-machine-learning-based approaches built on software measurement, while the selection and thresholds of software metrics heavily rely on expert knowledge. The second line is machine learning-based approaches, which suggest Move Method refactoring by learning to extract features from code information. However, most approaches in this line treat different forms of code information identically, disregarding their significant variation on data analysis. In this paper, we propose an approach to recommend Move Method refactoring named RMove by automatically learning structural and semantic representation from code fragment respectively. We concatenate these representations together and further train the machine learning classifiers to guide the movement of method to suitable classes. We evaluate our approach on two publicly available datasets. The results show that our approach outperforms three state-of-the-art refactoring tools including PathMove, JDeodorant, and JMove in effectiveness and usefulness. The results also unveil useful findings and provide new insights that benefit other types of feature envy refactoring techniques.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00033","Research and Development; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978256","","Software maintenance;Codes;Data analysis;Costs;Software metrics;Semantics;Machine learning","","1","","65","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Semantics-Aware Machine Learning for Function Recognition in Binary Code","S. Wang; P. Wang; D. Wu","College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","388","398","Function recognition in program binaries serves as the foundation for many binary instrumentation and analysis tasks. However, as binaries are usually stripped before distribution, function information is indeed absent in most binaries. By far, identifying functions in stripped binaries remains a challenge. Recent research work proposes to recognize functions in binary code through machine learning techniques. The recognition model, including typical function entry point patterns, is automatically constructed through learning. However, we observed that as previous work only leverages syntax-level features to train the model, binary obfuscation techniques can undermine the pre-learned models in real-world usage scenarios. In this paper, we propose FID, a semantics-based method to recognize functions in stripped binaries. We leverage symbolic execution to generate semantic information and learn the function recognition model through well-performing machine learning techniques.FID extracts semantic information from binary code and, therefore, is effectively adapted to different compilers and optimizations. Moreover, we also demonstrate that FID has high recognition accuracy on binaries transformed by widely-used obfuscation techniques. We evaluate FID with over four thousand test cases. Our evaluation shows that FID is comparable with previous work on normal binaries and it notably outperforms existing tools on obfuscated code.","","978-1-5386-0992-7","10.1109/ICSME.2017.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094438","reverse engineering;machine learning;function recognition","Semantics;Tools;Syntactics;Binary codes;Registers;Reverse engineering;Instruments","","16","","44","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Characterization and Automatic Updates of Deprecated Machine-Learning API Usages","S. A. Haryono; F. Thung; D. Lo; J. Lawall; L. Jiang","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; Inria, France; School of Information Systems, Singapore Management University, Singapore","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","137","147","Due to the rise of AI applications, machine learning (ML) libraries, often written in Python, have become far more accessible. ML libraries tend to be updated periodically, which may deprecate existing APIs, making it necessary for application developers to update their usages. In this paper, we build a tool to automate deprecated API usage updates. We first present an empirical study to better understand how updates of deprecated ML API usages in Python can be done. The study involves a dataset of 112 deprecated APIs from Scikit-Learn, TensorFlow, and PyTorch. Guided by the findings of our empirical study, we propose MLCatchUp, a tool to automate the updates of Python deprecated API usages, that automatically infers the API migration transformation through comparison of the deprecated and updated API signatures. These transformations are expressed in a Domain Specific Language (DSL). We evaluate MLCatchUp using a dataset containing 267 files with 551 API usages that we collected from public GitHub repositories. In our dataset, MLCatchUp can detect deprecated API usages with perfect accuracy, and update them correctly for 80.6% of the cases. We further improve the accuracy of MLCatchUp in performing updates by adding a feature that allows it to accept an additional user input that specifies the transformation constraints in the DSL for context-dependent API migration. Using this addition, MLCatchUp can make correct updates for 90.7% of the cases","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609172","Python;Program Transformation;Automatic Update;Deprecated API","Software maintenance;Conferences;Machine learning;Tools;Libraries;DSL;Python","","5","","26","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Revisiting Machine Learning based Test Case Prioritization for Continuous Integration","Y. Zhao; D. Hao; L. Zhang","Ministry of Education, Key Laboratory of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Laboratory of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Laboratory of High Confidence Software Technologies (Peking University), Beijing, China","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","232","244","To alleviate the cost of regression testing in continuous integration (CI), a large number of machine learning-based (ML-based) test case prioritization techniques have been proposed. However, it is yet unknown how they perform under the same experimental setup, because they are evaluated on different datasets with different metrics. To bridge this gap, we conduct the first comprehensive study on these ML-based techniques in this paper. We investigate the performance of 11 representative ML-based prioritization techniques for CI on 11 open-source subjects and obtain a series of findings. For example, the performance of the techniques changes across CI cycles, mainly resulting from the changing amount of training data, instead of code evolution and test removal/addition. Based on the findings, we give some actionable suggestions on enhancing the effectiveness of ML-based techniques, e.g., pretraining a prioritization technique with cross-subject data to get it thoroughly trained and then finetuning it with within-subject data dramatically improves its performance. In particular, the pretrained MART achieves state-of-the-art performance, producing the optimal sequence on 80% subjects, while the existing best technique, the original MART, only produces the optimal sequence on 50% subjects.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00032","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336260","test prioritization;machine learning;continuous integration","Measurement;Bridges;Software maintenance;Costs;Codes;Training data;Machine learning","","","","50","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference","A. M. Mir; E. Latoškinas; G. Gousios","Department of Software Technology, Delft University of Technology, Delft, The Netherlands; Department of Software Technology, Delft University of Technology, Delft, The Netherlands; Department of Software Technology, Delft University of Technology, Delft, The Netherlands","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","585","589","In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a light-weight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463150","Type Inference;Machine Learning;Python;Type Annotations;Static Analysis","Training;Analytical models;Annotations;Pipelines;Tools;Benchmark testing;Syntactics","","14","","16","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Software Architecture Challenges for ML Systems","G. A. Lewis; I. Ozkaya; X. Xu","Carnegie Mellon Software Engineering Institute, Pittsburgh, PA, USA; Carnegie Mellon Software Engineering Institute, Pittsburgh, PA, USA; CSIRO Data61 Computer Science and Engineering, UNSW, Sydney, Australia","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","634","638","Developing machine learning (ML) systems, just like any other system, requires architecture thinking. However, there are characteristics of ML components that create challenges and unique quality attribute (QA) concerns for software architecture and design activities, such as data-dependent behavior, detecting and responding to drift over time, and timely capture of ground truth to inform retraining. This paper presents four categories of software architecture challenges that need to be addressed to support ML system development, maintenance and evolution: software architecture practices for ML systems, architecture patterns and tactics for ML-important QAs, monitorability as a driving QA, and co-architecting and co-versioning. These challenges were collected from targeted workshops, practitioner interviews, and industry engagements. The goal of our work is to encourage further research in these areas and use the information presented in this paper to guide the development of empirically-validated practices for architecting ML systems.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609199","machine learning;software architecture;software maintenance and evolution","Industries;Software maintenance;Software architecture;Conferences;Systems architecture;Machine learning;Computer architecture","","20","","26","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Tracing with Less Data: Active Learning for Classification-Based Traceability Link Recovery","C. Mills; J. Escobar-Avila; A. Bhattacharya; G. Kondyukov; S. Chakraborty; S. Haiduc","Department of Computer Science, Florida State University, Tallahassee, USA; Department of Computer Science, Florida State University, Tallahassee, USA; Department of Computer Science, Florida State University, Tallahassee, USA; Department of Computer Science, Florida State University, Tallahassee, USA; Department of Computer Science, Florida State University, Tallahassee, USA; Department of Computer Science, Florida State University, Tallahassee, USA","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","103","113","Previous work has established both the importance and difficulty of establishing and maintaining adequate software traceability. While it has been shown to support essential maintenance and evolution tasks, recovering traceability links between related software artifacts is a time consuming and error prone task. As such, substantial research has been done to reduce this barrier to adoption by at least partially automating traceability link recovery. In particular, recent work has shown that supervised machine learning can be effectively used for automating traceability link recovery, as long as there is sufficient data (i.e., labeled traceability links) to train a classification model. Unfortunately, the amount of data required by these techniques is a serious limitation, given that most software systems rarely have traceability information to begin with. In this paper we address this limitation of previous work and propose an approach based on active learning, which substantially reduces the amount of training data needed by supervised classification approaches for traceability link recovery while maintaining similar performance.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919048","software traceability;machine learning;active learning;classification","Machine learning;Data models;Training data;Predictive models;Software;Task analysis;Training","","12","","39","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services","A. Cummaudo; R. Vasa; J. Grundy; M. Abdelrazek; A. Cain","Applied Artificial Intelligence Institute, Deakin University, Geelong, Australia; Applied Artificial Intelligence Institute, Deakin University, Geelong, Australia; Faculty of Information Technology, Monash University, Clayton, Australia; School of Information Technology, Deakin University, Geelong, Australia; School of Information Technology, Deakin University, Geelong, Australia","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","333","342","The following topics are dealt with: software maintenance; public domain software; program testing; source code (software); program debugging; software quality; program diagnostics; learning (artificial intelligence); mobile computing; data mining.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919078","machine learning;intelligent service;computer vision;quality assurance;evolution risk;documentation","Computer vision;Dogs;Testing;Cloud computing;Documentation;Software quality","","14","","63","Crown","5 Dec 2019","","","IEEE","IEEE Conferences"
"Semi-Automated Feature Traceability with Embedded Annotations","H. Abukwaik; A. Burger; B. K. Andam; T. Berger","ABB Corporate Research Center, Ladenburg, Germany; ABB Corporate Research Center, Ladenburg, Germany; Chalmers | University of Gothenburg, Gothenburg, Sweden; Chalmers | University of Gothenburg, Gothenburg, Sweden","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","529","533","Engineering software amounts to implementing and evolving features. While some engineering approaches advocate the explicit use of features, developers usually do not record feature locations in software artifacts. However, when evolving or maintaining features - especially in long-living or variant-rich software with many developers - the knowledge about features and their locations quickly fades and needs to be recovered. While automated or semi-automated feature-location techniques have been proposed, their accuracy is usually too low to be useful in practice. We propose a semi-automated, machine-learning-assisted feature-traceability technique that allows developers to continuously record feature-traceability information while being supported by recommendations about missed locations. We show the accuracy of our proposed technique in a preliminary evaluation, simulating the engineering of an open-source web application that evolved in different, cloned variants.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530059","software evolution, clone&own, variability, feature annotations, feature traceability, recommendation system, feature location, machine","Recommender systems;Software;History;Prediction algorithms;Measurement;Software algorithms;Tools","","23","","33","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Improving Bug Triaging with High Confidence Predictions at Ericsson","A. Sarkar; P. C. Rigby; B. Bartalos","Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Ericsson, Budapest, Hungary","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","81","91","Correctly assigning bugs to the right developer or team, i.e. bug triaging, is a costly activity. A concerted effort at Ericsson has been done to adopt automated bug triaging to reduce development costs. In this work, we replicate the research approaches that have been widely used in the literature. We apply them on over 10k bug reports for 9 large products at Ericsson. We find that a logistic regression classifier including the simple textual and categorical attributes of the bug reports has the highest precision and recall of 78.09% and 79.00%, respectively. Ericsson's bug reports often contain logs that have crash dumps and alarms. We add this information to the bug triage models. We find that this information does not improve the precision and recall of bug triaging in Ericsson's context. Although our models perform as well as the best ones reported in the literature, a criticism of bug triaging at Ericsson is that the accuracy is not sufficient for regular use. We develop a novel approach where we only triage bugs when the model has high confidence in the triage prediction. We find that we improve the accuracy to 90%, but we can make predictions for 62% of the bug reports.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919115","Bug Triaging;Machine Learning;Log Analysis;Incremental Learning","Computer bugs;Support vector machines;Machine learning;Software;Manuals;Social network services","","19","","31","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"A Simple NLP-Based Approach to Support Onboarding and Retention in Open Source Communities","C. Stanik; L. Montgomery; D. Martens; D. Fucci; W. Maalej","University of Hamburg, Germany; University of Hamburg, Germany; University of Hamburg, Germany; University of Hamburg, Germany; University of Hamburg, Germany","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","172","182","Successful open source communities are constantly looking for new members and helping them become active developers. A common approach for developer onboarding in open source projects is to let newcomers focus on relevant yet easy-to-solve issues to familiarize themselves with the code and the community. The goal of this research is twofold. First, we aim at automatically identifying issues that newcomers can resolve by analyzing the history of resolved issues by simply using the title and description of issues. Second, we aim at automatically identifying issues, that can be resolved by newcomers who later become active developers. We mined the issue trackers of three large open source projects and extracted natural language features from the title and description of resolved issues. In a series of experiments, we optimized and compared the accuracy of four supervised classifiers to address our research goals. Random Forest, achieved up to 91% precision (F1-score 72%) towards the first goal while for the second goal, Decision Tree achieved a precision of 92% (F1-score 91%). A qualitative evaluation gave insights on what information in the issue description is helpful for newcomers. Our approach can be used to automatically identify, label, and recommend issues for newcomers in open source software projects based only on the text of the issues.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530027","open source software;onboarding;task selection;newcomers;machine learning;natural language processing","Feature extraction;Machine learning;Computer bugs;Open source software;Natural language processing;History;Forestry","","17","","41","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques","J. Deshmukh; K. M. Annervaz; S. Podder; S. Sengupta; N. Dubash",Accenture Technology Labs; Accenture Technology Labs; Accenture Technology Labs; Accenture Technology Labs; Accenture Technology Labs,"2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","115","124","Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available.","","978-1-5386-0992-7","10.1109/ICSME.2017.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094414","Information Retrieval;Duplicate Bug Detection;Deep Learning;Natural Language Processing;Word Embeddings;Siamese Networks;Convolutional Neural Networks;Long Short Term Memory","Computer bugs;Machine learning;Neural networks;Training;Computational modeling;Sun","","55","","40","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Automatic Refactoring Candidate Identification Leveraging Effective Code Representation","I. Palit; G. Shetty; H. Arif; T. Sharma","Dalhousie University, Canada; Dalhousie University, Canada; Dalhousie University, Canada; Dalhousie University, Canada","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","369","374","The use of machine learning to automate the detection of refactoring candidates is a rapidly evolving research area. The majority of work in this direction uses source code metrics and commit messages to predict refactoring candidates and do not exploit the rich semantics of source code. This paper proposes a new approach for extract method refactoring candidates identification. First, we propose a novel mechanism to identify negative samples for the refactoring candidate identification task. We then employ a self-supervised autoencoder to acquire a compact representation of source code generated by a pre-trained large language model. Subsequently, we train a binary classifier to predict extract method refactoring candidates. Experiments show that our new approach outperforms the state of the art by 30% in terms of F1 score. The proposed work has implications for researchers and practitioners. Software developers may use the proposed automated approach to predict refactoring candidates better. This study will facilitate the development of improved refactoring candidate identification methods that the researchers in the field could use and extend.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336318","extract method refactoring;deep learning;code representation","Measurement;Software maintenance;Codes;Source coding;Semantics;Machine learning;Task analysis","","","","35","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"PathMiner: A Library for Mining of Path-Based Representations of Code","V. Kovalenko; E. Bogomolov; T. Bryksin; A. Bacchelli","Delft University of Technology, Delft, The Netherlands; Higher School of Economics, JetBrains Research, Saint Petersburg, Russia; Higher School of Economics, JetBrains Research, Saint Petersburg, Russia; University of Zurich, Switzerland","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","13","17","One recent, significant advance in modeling source code for machine learning algorithms has been the introduction of path-based representation - an approach consisting in representing a snippet of code as a collection of paths from its syntax tree. Such representation efficiently captures the structure of code, which, in turn, carries its semantics and other information. Building the path-based representation involves parsing the code and extracting the paths from its syntax tree; these steps build up to a substantial technical job. With no common reusable toolkit existing for this task, the burden of mining diverts the focus of researchers from the essential work and hinders newcomers in the field of machine learning on code. In this paper, we present PathMiner - an open-source library for mining path-based representations of code. PathMiner is fast, flexible, well-tested, and easily extensible to support input code in any common programming language. Preprint [https://doi.org/10.5281/zenodo.2595271]; released tool [https://doi.org/10.5281/zenodo.2595257].","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816777","mining tool;ast path;path based representation;machine learning on code;code2vec","Syntactics;Task analysis;Grammar;Data mining;Machine learning;Python;Libraries","","23","","21","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"GPTCloneBench: A comprehensive benchmark of semantic clones and cross-language clones using GPT-3 model and SemanticCloneBench","A. I. Alam; P. R. Roy; F. Al-Omari; C. K. Roy; B. Roy; K. A. Schneider","Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","1","13","With the emergence of Machine Learning, there has been a surge in leveraging its capabilities for problem-solving across various domains. In the code clone realm, the identification of type-4 or semantic clones has emerged as a crucial yet challenging task. Researchers aim to utilize Machine Learning to tackle this challenge, often relying on the Big-CloneBench dataset. However, it’s worth noting that BigCloneBench, originally not designed for semantic clone detection, presents several limitations that hinder its suitability as a comprehensive training dataset for this specific purpose. Furthermore, CLCDSA dataset suffers from a lack of reusable examples aligning with real-world software systems, rendering it inadequate for cross-language clone detection approaches. In this work, we present a comprehensive semantic clone and cross-language clone benchmark, GPTCloneBench 1 by exploiting SemanticCloneBench and OpenAI’s GPT-3 model. In particular, using code fragments from SemanticCloneBench as sample inputs along with appropriate prompt engineering for GPT-3 model, we generate semantic and cross-language clones for these specific fragments and then conduct a combination of extensive manual analysis, tool-assisted filtering, functionality testing and automated validation in building the benchmark. From 79,928 clone pairs of GPT-3 output, we created a benchmark with 37,149 true semantic clone pairs, 19,288 false semantic pairs(Type-1/Type-2), and 20,770 cross-language clones across four languages (Java, C, C#, and Python). Our benchmark is 15-fold larger than SemanticCloneBench, has more functional code examples for software systems and programming language support than CLCDSA, and overcomes BigCloneBench’s qualities, quantification, and language variety limitations. GPTCloneBench can be found here1.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00013","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336319","Software Clone;SemanticCloneBench;GPT-3;Language Model;Machine Learning;Cross Language Clone;Semantic Clone;BigCloneBench","Training;Codes;Semantics;Buildings;Cloning;Machine learning;Benchmark testing","","2","","58","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Deep Learning Anti-Patterns from Code Metrics History","A. Barbez; F. Khomh; Y. -G. Guéhéneuc","Polytechnique Montreal, Canada; Polytechnique Montreal, Canada; Concordia University, Montreal, QC, Canada","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","114","124","Anti-patterns are poor solutions to recurring design problems. Number of empirical studies have highlighted the negative impact of anti-patterns on software maintenance which motivated the development of various detection techniques. Most of these approaches rely on structural metrics of software systems to identify affected components while others exploit historical information by analyzing co-changes occurring between code components. By relying solely on one aspect of software systems (i.e., structural or historical), existing approaches miss some precious information which limits their performances. In this paper, we propose CAME (Convolutional Analysis of code Metrics Evolution), a deep-learning based approach that relies on both structural and historical information to detect anti-patterns. Our approach exploits historical values of structural code metrics mined from version control systems and uses a Convolutional Neural Network classifier to infer the presence of anti-patterns from this information. We experiment our approach for the widely know God Class anti-pattern and evaluate its performances on three software systems. With the results of our study, we show that: (1) using historical values of source code metrics allows to increase the precision; (2) CAME outperforms existing static machine-learning classifiers; and (3) CAME outperforms existing detection tools.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919037","Anti-patterns;Deep learning;Mining Software Repositories","Measurement;History;Software systems;Machine learning;Feature extraction;Open source software;Software maintenance","","11","","40","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"DeepOrder: Deep Learning for Test Case Prioritization in Continuous Integration Testing","A. Sharif; D. Marijan; M. Liaaen","Simula Research Laboratory, Norway; Simula Research Laboratory, Norway; Cisco Systems Norway","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","525","534","Continuous integration testing is an important step in the modern software engineering life cycle. Test prioritization is a method that can improve the efficiency of continuous integration testing by selecting test cases that can detect faults in the early stage of each cycle. As continuous integration testing produces voluminous test execution data, test history is a commonly used artifact in test prioritization. However, existing test prioritization techniques for continuous integration either cannot handle large test history or are optimized for using a limited number of historical test cycles. We show that such a limitation can decrease fault detection effectiveness of prioritized test suites. This work introduces DeepOrder, a deep learning-based model that works on the basis of regression machine learning. DeepOrder ranks test cases based on the historical record of test executions from any number of previous test cycles. DeepOrder learns failed test cases based on multiple factors including the duration and execution status of test cases. We experimentally show that deep neural networks, as a simple regression model, can be efficiently used for test case prioritization in continuous integration testing. DeepOrder is evaluated with respect to time-effectiveness and fault detection effectiveness in comparison with an industry practice and the state of the art approaches. The results show that DeepOrder outperforms the industry practice and state-of-the-art test prioritization approaches in terms of these two metrics.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609187","Regression testing;Test case prioritization;Test case selection;Deep Learning;Machine Learning;Continuous Integration","Industries;Deep learning;Measurement;Software maintenance;Fault detection;Conferences;History","","12","","38","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Predicting and Evaluating Software Model Growth in the Automotive Industry","J. Schroeder; C. Berger; A. Knauss; H. Preenja; M. Ali; M. Staron; T. Herpel","Department of Computer Science and Engineering, Chalmers and University of Gothenburg, Gothenburg, Sweden; Dept. of Comput. Sci. & Eng., Chalmers & Univ. of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers and University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers and University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers and University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers and University of Gothenburg, Gothenburg, Sweden; Automotive Safety Technologies GmbH, Ingolstadt, Germany","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","584","593","The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical.Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations.Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.","","978-1-5386-0992-7","10.1109/ICSME.2017.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094464","software size;time series;prediction;machine learning;measurement;model-based-software","Software;Mathematical model;Time series analysis;Predictive models;Automotive engineering;Measurement;Complexity theory","","4","","30","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Predicting Software Maintainability in Object-Oriented Systems Using Ensemble Techniques","H. Alsolai; M. Roper; D. Nassar","University of Strathclyde, Glasgow, United Kingdom; Comput. & Inf. Sci., University of Strathclyde, Glasgow, Glasgow, GB; Comput. Sci. & Inf. Syst., Princess Nourah Bint Abdulrahman Univ., Riyadh, Saudi Arabia","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","716","721","Prediction of the maintainability of classes in object-oriented systems is a significant factor for software success, however it is a challenging task to achieve. To date, several machine learning models have been applied with variable results and no clear indication of which techniques are more appropriate. With the goal of achieving more consistent results, this paper presents the first set of results in an extensive empirical study designed to evaluate the capability of bagging models to increase accuracy prediction over individual models. The study compares two major machine learning based approaches for predicting software maintainability: individual models (regression tree, multilayer perceptron, k-nearest neighbors and m5rules), and an ensemble model (bagging) that are applied to the QUES data set. The results obtained from this study indicate that k-nearest neighbors model outperformed all other individual models. The bagging ensemble model improved accuracy prediction significantly over almost all individual models, and the bagging ensemble models with k-nearest neighbors as a base model achieved superior accurate prediction. This paper also provides a description of the planned programme of research which aims to investigate the performance over various datasets of advanced (ensemble-based) machine learning models.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530091","individual models, bagging ensemble model, software maintainability, prediction, Object-oriented systems","Predictive models;Object oriented modeling;Software;Bagging;Maintenance engineering;Measurement;Data models","","6","","42","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Repairing Intricate Faults in Code Using Machine Learning and Path Exploration","D. Gopinath; K. Wang; J. Hua; S. Khurshid","The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin; The University of Texas, Austin","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","16 Jan 2017","2016","","","453","457","Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.","","978-1-5090-3806-0","10.1109/ICSME.2016.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816493","program repair;semi-supervised learning;decisiontree learning;JPF;data-structures;condition faults","Maintenance engineering;Support vector machines;Data structures;Systematics;Semisupervised learning;Debugging;Space exploration","","2","","21","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"How does Machine Learning Change Software Development Practices?","Z. Wan; X. Xia; D. Lo; G. C. Murphy","College of Computer Science and Technology, Ningbo Research Institute, Zhejiang University, Hangzhou, China; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Systems, Singapore Management University, Singapore","IEEE Transactions on Software Engineering","16 Sep 2021","2021","47","9","1857","1871","Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.","1939-3520","","10.1109/TSE.2019.2937083","National Key Research and Development Program of China(grant numbers:2018YFB1003904); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812912","Software engineering;machine learning;practitioner;empirical study","Software;Interviews;Data models;Machine learning;Testing;Task analysis;Software engineering","","65","","40","IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Automating Software Development for Mobile Computing Platforms","K. Moran","Department of Computer Science, College of William & Mary, Williamsburg, Virginia","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","749","754","Mobile devices such as smartphones and tablets have become ubiquitous in today's modern computing landscape. The applications that run on these mobile devices (often referred to as ""apps"") have become a primary means of computing for millions of users and, as such, have garnered immense developer interest. These apps allow for unique, personal software experiences through touch-based UIs and a complex assortment of sensors. However designing and implementing high quality mobile apps can be a difficult process. This is primarily due to challenges unique to mobile development including change-prone APIs and platform fragmentation, just to name a few. This paper presents the motivation and an overview of a dissertation which presents new approaches for automating and improving mobile app design and development practices. Additionally, this paper discusses potential avenues for future research based upon the work conducted, as well as general lessons learned during the author's tenure as a doctoral student in the general areas of software engineering, maintenance, and evolution.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530097","Mobile;apps;Android;Machine Learning;testing;prototyping;software development","Software;Graphical user interfaces;Computer crashes;Testing;Mobile applications;Software engineering;Computational modeling","","2","","20","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"The Open-Closed Principle of Modern Machine Learning Frameworks","H. Ben Braiek; F. Khomh; B. Adams","SWAT Lab., Montréal; SWAT Lab., Montréal; MCIS, Montréal","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","353","363","Recent advances in computing technologies and the availability of huge volumes of data have sparked a new machine learning (ML) revolution, where almost every day a new headline touts the demise of human experts by ML models on some task. Open source software development is rumoured to play a significant role in this revolution, with both academics and large corporations such as Google and Microsoft releasing their ML frameworks under an open source license. This paper takes a step back to examine and understand the role of open source development in modern ML, by examining the growth of the open source ML ecosystem on GitHub, its actors, and the adoption of frameworks over time. By mining LinkedIn and Google Scholar profiles, we also examine driving factors behind this growth (paid vs. voluntary contributors), as well as the major players who promote its democratization (companies vs. communities), and the composition of ML development teams (engineers vs. scientists). According to the technology adoption lifecycle, we find that ML is in between the stages of early adoption and early majority. Furthermore, companies are the main drivers behind open source ML, while the majority of development teams are hybrid teams comprising both engineers and professional scientists. The latter correspond to scientists employed by a company, and by far represent the most active profiles in the development of ML applications, which reflects the importance of a scientific background for the development of ML frameworks to complement coding skills. The large influence of cloud computing companies on the development of open source ML frameworks raises the risk of vendor lock-in. These frameworks, while open source, could be optimized for specific commercial cloud offerings.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595219","Machine Learning;Open Source;Framework;Technology adoption","Companies;Machine learning;Google;LinkedIn;Technological innovation;Data collection;Maximum likelihood estimation","","","","21","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Machine Learning Testing: Survey, Landscapes and Horizons","J. M. Zhang; M. Harman; L. Ma; Y. Liu","CREST, University College London, London, U.K; CREST, University College London, London, U. K.; Kyushu University, Fukuoka, Japan; Nanyang Technological University, Singapore","IEEE Transactions on Software Engineering","10 Jan 2022","2022","48","1","1","36","This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.","1939-3520","","10.1109/TSE.2019.2962027","ERC(grant numbers:741278); JSPS KAKENHI(grant numbers:19K24348,19H04086); Qdai-jump Research Program(grant numbers:01277); Nvidia; National Research Foundation Singapore(grant numbers:NRF2018NCR-NCR005-0001); National Satellite of Excellence in Trustworthy Software System(grant numbers:NRF2018NCR-NSOE003-0001); NTU(grant numbers:NGF-2019-06-024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000651","Machine learning;software testing;deep neural network","Machine learning;Software testing;Software engineering;Training data;Data models;Robustness","","350","","292","IEEE","17 Feb 2020","","","IEEE","IEEE Journals"
"Identifying Auto-Generated Code by Using Machine Learning Techniques","K. Shimonaka; S. Sumi; Y. Higo; S. Kusumoto","Graduate School of Information Science and Technology, Osaka University, Japan; Osaka Daigaku, Suita, Osaka, JP; Graduate School of Information Science and Technology, Osaka University, Japan; Graduate School of Information Science and Technology, Osaka University, Japan","2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP)","5 May 2016","2016","","","18","23","Recently, many researchers have conducted mining source code repositories to retrieve useful information about software development. Source code repositories often include auto-generated code, and auto-generated code is usually removed in a preprocessing phase because the presence of auto-generated code is harmful to source code analysis. A usual way to removeauto-generated code is searching particular comments which existamong auto-generated code. However, we cannot identify auto-generated code automatically with such a way if comments have disappeared. In addition, it takes too much time to identify auto-generated code manually. Therefore, we propose a techniqueto identify auto-generated code automatically by using machinelearning techniques. In our proposed technique, we can identifywhether source code is auto-generated code or not by utilizingsyntactic information of source code. In order to evaluate theproposed technique, we conducted experiments on source codegenerated by four kinds of code generators. As a result, weconfirmed that the proposed technique was able to identify auto-generated code with high accuracy.","","978-1-5090-1851-2","10.1109/IWESEP.2016.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464547","auto-generated code;machine learning techniques;source code analysis","Syntactics;Generators;Data models;Predictive models;Machine learning algorithms;Cloning;Decision trees","","9","1","16","IEEE","5 May 2016","","","IEEE","IEEE Conferences"
"Boa Meets Python: A Boa Dataset of Data Science Software in Python Language","S. Biswas; M. J. Islam; Y. Huang; H. Rajan","Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","577","581","The popularity of Python programming language has surged in recent years due to its increasing usage in Data Science. The availability of Python repositories in Github presents an opportunity for mining software repository research, e.g., suggesting the best practices in developing Data Science applications, identifying bug-patterns, recommending code enhancements, etc. To enable this research, we have created a new dataset that includes 1,558 mature Github projects that develop Python software for Data Science tasks. By analyzing the metadata and code, we have included the projects in our dataset which use a diverse set of machine learning libraries and managed by a variety of users and organizations. The dataset is made publicly available through Boa infrastructure both as a collection of raw projects as well as in a processed form that could be used for performing large scale analysis using Boa language. We also present two initial applications to demonstrate the potential of the dataset that could be leveraged by the community.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816757","MSR;Boa;AST;machine learning;data science;open source repositories;program analysis","Python;Data science;Libraries;Metadata;Machine learning;Data mining","","15","","18","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"How can i improve my app? Classifying user reviews for software maintenance and evolution","S. Panichella; A. Di Sorbo; E. Guzman; C. A. Visaggio; G. Canfora; H. C. Gall","University of Zurich, Switzerland; Universita degli Studi del Sannio, Benevento, Campania, IT; +Technische Universitat Munchen, Garchinz, Germanv; Universita degli Studi del Sannio, Benevento, Campania, IT; Tuniversity of Sannio, Benevento, Italy; Tuniversity of Sannio, Benevento, Italy","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","23 Nov 2015","2015","","","281","290","App Stores, such as Google Play or the Apple Store, allow users to provide feedback on apps by posting review comments and giving star ratings. These platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. Previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. However, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task. In this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) Natural Language Processing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify app reviews into the proposed categories. We show that the combined use of these techniques allows to achieve better results (a precision of 75% and a recall of 74%) than results obtained using each technique individually (precision of 70% and a recall of 67%).","","978-1-4673-7532-0","10.1109/ICSM.2015.7332474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332474","User Reviews;Mobile Applications;Natural Language Processing;Sentiment Analysis;Text classification","Taxonomy;Software maintenance;Feature extraction;Natural language processing;Mobile communication;Maintenance engineering;Text analysis","","288","2","39","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"Why are Features Deprecated? An Investigation Into the Motivation Behind Deprecation","A. A. Sawant; G. Huang; G. Vilen; S. Stojkovski; A. Bacchelli","Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; University of Zurich, Zurich, Switzerland","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","13","24","In this study, we investigate why API producers deprecate features. Previous work has shown us that knowing the rationale behind deprecation of an API aids a consumer in deciding to react, thus hinting at a diversity of deprecation reasons. We manually analyze the Javadoc of 374 deprecated methods pertaining four mainstream Java APIs to see whether the reason behind deprecation is mentioned. We find that understanding the rationale from just the Javadoc is insufficient; hence we add other data sources such as the source code, issue tracker data and commit history. We observe 12 reasons that trigger API producers to deprecate a feature. We evaluate an automated approach to classify these motivations.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529833","API;documentation;deprecation","Java;Software;Documentation;Tools;History;Libraries;Taxonomy","","15","","57","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Hurdles for Developers in Cryptography","M. Hazhirpasand; O. Nierstrasz; M. Shabani; M. Ghafari","University of Bern, Bern, Switzerland; University of Bern, Bern, Switzerland; Azad University, Rasht, Iran; School of Computer Science, University of Auckland, Auckland, New Zealand","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","659","663","Prior research has shown that cryptography is hard to use for developers. We aim to understand what cryptography issues developers face in practice. We clustered 91 954 cryptography-related questions on the Stack Overflow website, and manually analyzed a significant sample (i.e., 383) of the questions to comprehend the crypto challenges developers commonly face in this domain. We found that either developers have a distinct lack of knowledge in understanding the fundamental concepts, e.g., OpenSSL, public-key cryptography or password hashing, or the usability of crypto libraries undermined developer performance to correctly realize a crypto scenario. This is alarming and indicates the need for dedicated research to improve the design of crypto APIs.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00076","Swiss National Science Foundation(grant numbers:200020–181973); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609232","Security;cryptography;developer issues","Software maintenance;Conferences;Passwords;Public key cryptography;Libraries;Complexity theory;Cryptography","","7","","20","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Community Smell Detection and Refactoring in SLACK: The CADOCS Project","G. Voria; V. Pentangelo; A. D. Porta; S. Lambiase; G. Catolino; F. Palomba; F. Ferrucci","Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Jheronimus Academy of Data Science, Tilburg University, ’s-Hertogenbosch, Netherlands; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy; Department of Computer Science, Software Engineering (SeSa) Lab, University of Salerno, Salerno, Italy","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","469","473","Software engineering is a human-centered activity involving various stakeholders with different backgrounds that have to communicate and collaborate to reach shared objectives. The emergence of conflicts among stakeholders may lead to undesired effects on software maintainability, yet it is often unavoidable in the long run. Community smells, i.e., sub-optimal communication and collaboration practices, have been defined to map recurrent conflicts among developers. While some community smell detection tools have been proposed in the recent past, these can be mainly used for research purposes because of their limited level of usability and user engagement. To facilitate a wider use of community smell-related information by practitioners, we present CADOCS, a client-server conversational agent that builds on top of a previous community smell detection tool proposed by Almarini et al. to (1) make it usable within a well-established communication channel like Slack and (2) augment it by providing initial support to software analytics instruments useful to diagnose and refactor community smells. We describe the features of the tool and the preliminary evaluation conducted to assess and improve robustness and usability.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00061","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978263","Conversational Agents;Community Smells;Socio-Technical Analysis","Software maintenance;Instruments;Collaboration;Communication channels;Robustness;Stakeholders;Usability","","1","","34","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"The State of the ML-universe: 10 Years of Artificial Intelligence & Machine Learning Software Development on GitHub","D. Gonzalez; T. Zimmermann; N. Nagappan","Rochester Institute of Technology, Rochester, NY, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","431","442","In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI & ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI & ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI & ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends. In this paper, we conducted a large-scale empirical study of AI & ML Tool (700) and Application (4,524) repositories hosted on GitHub to develop such a characterization. While not the only platform hosting AI & ML development, GitHub facilitates collecting a rich data set for each repository with high traceability between issues, commits, pull requests and users. To compare the AI & ML community to the wider population of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance this characterization with an elaborate study of developer workflow that measures collaboration and autonomy within a repository. We've captured key insights of this community's 10 year history such as it's primary language (Python) and most popular repositories (Tensorflow, Tesseract). Our findings show the AI & ML community has unique characteristics that should be accounted for in future research.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148701","machine learning;artificial intelligence;mining software repositories;software engineering;Open Source;GitHub","Sociology;Collaboration;Machine learning;Software;Libraries;Artificial intelligence;Statistics","","11","","59","","20 Jun 2023","","","IEEE","IEEE Conferences"
"METHODS2TEST: A dataset of focal methods mapped to test cases","M. Tufano; S. K. Deng; N. Sundaresan; A. Svyatkovskiy","Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","299","303","Unit testing is an essential part of the software development process, which helps to identify issues with source code in early stages of development and prevent regressions. Machine learning has emerged as viable approach to help software developers generate automated unit tests. However, generating reliable unit test cases that are semantically correct and capable of catching software bugs or unintended behavior via machine learning requires large, metadata-rich, datasets. In this paper we present Methods2Test: a large, supervised dataset of test cases mapped to corresponding methods under test (i.e., focal methods). This dataset contains 780,944 pairs of JUnit tests and focal methods, extracted from a total of 91,385 Java open source projects hosted on GitHub with licenses permitting re-distribution. The main challenge behind the creation of the Methods2Test was to establish a reliable mapping between a test case and the relevant focal method. To this aim, we designed a set of heuristics, based on developers' best practices in software testing, which identify the likely focal method for a given test case. To facilitate further analysis, we store a rich set of metadata for each method-test pair in JSON-formatted files. Additionally, we extract textual corpus from the dataset at different context levels, which we provide both in raw and tokenized forms, in order to enable researchers to train and evaluate machine learning models for Automated Test Generation. Methods2Test is publicly available at: https://github.com/microsoft/methods2test","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796371","datasets;software testing","Software testing;Java;Machine learning;Metadata;Software reliability","","","","28","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Regression Testing of Massively Multiplayer Online Role-Playing Games","Y. Wu; Y. Chen; X. Xie; B. Yu; C. Fan; L. Ma","Fuxi AI Lab, Netease, Inc.; Fuxi AI Lab, Netease, Inc.; Nanyang Technological University; Kyushu University; Fuxi AI Lab, Netease, Inc.; Kyushu University","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","692","696","Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240641","Game Testing, Reinforcement Learning","Software maintenance;Computer bugs;Games;Manuals;Machine learning;Task analysis;Testing","","4","","29","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Capturing Contextual Relationships of Buggy Classes for Detecting Quality-Related Bugs","R. Krasniqi; H. Do","Dept. of Computer Science and Eng., University of North Texas, Denton, TX, USA; Dept. of Computer Science and Eng., University of North Texas, Denton, TX, USA","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","375","379","Quality concerns are critical for addressing system-wide issues related to reliability, security, and performance, among others. However, these concerns often become scattered across the codebase, making it challenging for software developers to effectively address quality bugs. In this paper, we propose a holistic approach to detecting and clustering quality-related content hidden within the codebase. By leveraging the Hierarchical Dirichlet Process (HDP) and complementary techniques such as information retrieval and machine learning, including structural and textual analysis, we create a meaningful hierarchy that detects classes containing relevant information for addressing quality bugs. This approach allows us to uncover rich synergies between complex structured artifacts and infer bug-fixing classes for repairing quality bugs. The reported results show that our approach improves over the state-of-the-art achieving a high precision of 83%, recall of 82%, and F1 score of 83%.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336297","Quality Bugs;Hierarchical Topic Modeling;Vector Space Model;Contextual Relationships","Software maintenance;Computer bugs;Machine learning;Information retrieval;Software reliability;Security;Context modeling","","","","34","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Studying Software Engineering Patterns for Designing Machine Learning Systems","H. Washizaki; H. Uchida; F. Khomh; Y. -G. Guéhéneuc","Waseda University, National Institute of Informatics SYSTEM INFORMATION eXmotion, Tokyo, Japan; Waseda University, Tokyo, Japan; Polytechnique Montréal, Montre’al, QC, Canada; Concordia University, Montreal, QC, Canada","2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)","30 Dec 2019","2019","","","49","495","Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.","2573-2021","978-1-7281-5590-6","10.1109/IWESEP49350.2019.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945075","Machine Learning;Architecture Patterns;Design Patterns;Anti-patterns;ML Patterns","Software;Computer architecture;Pipelines;Data models;Software engineering;Mathematics;Machine learning","","45","","47","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Why is Developing Machine Learning Applications Challenging? A Study on Stack Overflow Posts","M. Alshangiti; H. Sapkota; P. K. Murukannaiah; X. Liu; Q. Yu","Rochester Institute of Technology, Rochester NY, USA; Rochester Institute of Technology, Rochester NY, USA; Rochester Institute of Technology, Rochester NY, USA; Rochester Institute of Technology, Rochester NY, USA; Rochester Institute of Technology, Rochester NY, USA","2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","17 Oct 2019","2019","","","1","11","Background: As smart and automated applications pervade our lives, an increasing number of software developers are required to incorporate machine learning (ML) techniques into application development. However, acquiring the ML skill set can be nontrivial for software developers owing to both the breadth and depth of the ML domain. Aims: We seek to understand the challenges developers face in the process of ML application development and offer insights to simplify the process. Despite its importance, there has been little research on this topic. A few existing studies on development challenges with ML are outdated, small scale, or they do no involve a representative set of developers. Method: We conduct an empirical study of ML-related developer posts on Stack Overflow. We perform in-depth quantitative and qualitative analyses focusing on a series of research questions related to the challenges of developing ML applications and the directions to address them. Results: Our findings include: (1) ML questions suffer from a much higher percentage of unanswered questions on Stack Overflow than other domains; (2) there is a lack of ML experts in the Stack Overflow QA community; (3) the data preprocessing and model deployment phases are where most of the challenges lay; and (4) addressing most of these challenges require more ML implementation knowledge than ML conceptual knowledge. Conclusions: Our findings suggest that most challenges are under the data preparation and model deployment phases, i.e., early and late stages. Also, the implementation aspect of ML shows much higher difficulty level among developers than the conceptual aspect.","1949-3789","978-1-7281-2968-6","10.1109/ESEM.2019.8870187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870187","Machine Learning;Software Development;Stack Overflow;Data Mining","Machine learning;Software;Libraries;Face;Data models;Task analysis;Analytical models","","29","","32","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"Evaluation of Context-Aware Language Models and Experts for Effort Estimation of Software Maintenance Issues","M. Alhamed; T. Storer","School of Computing Science, University of Glasgow, Glassgow, UK; School of Computing Science, University of Glasgow, Glassgow, UK","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","129","138","Reflecting upon recent advances in Natural Language Processing (NLP), this paper evaluates the effectiveness of context-aware NLP models for predicting software task effort estimates. Term Frequency–Inverse Document Frequency (TF-IDF) and Bidirectional Encoder Representations from Transformers (BERT) were used as feature extraction methods; Random forest and BERT feed-forward linear neural networks were used as classifiers. Using three datasets drawn from open-source projects and one from a commercial project, the paper evaluates the models and compares the best performing model with expert estimates from both kinds of datasets. The results suggest that BERT as feature extraction and classifier shows slightly better performance than other combinations, but that there is no significant difference between the presented methods. On the other hand, the results show that expert and Machine Learning (ML) estimate performances are similar, with the experts’ performance being slightly better. Both findings confirmed existing literature, but using substantially different experimental settings.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978209","empirical software engineering;software effort estimation;software maintenance issues;machine learning;NLP;BERT;TF–IDF;datasets;Planing Poker","Maximum likelihood estimation;Software maintenance;Bit error rate;Feature extraction;Transformers;Natural language processing;Data models","","1","","35","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference","K. Jesse; P. T. Devanbu","University of California, Davis, Davis, CA, USA; University of California, Davis, Davis, CA, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","294","298","In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for Type-Script. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528507","NSF(grant numbers:1414172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796285","Type Inference;Machine Learning;TypeScript;Code Properties","Training;Measurement;Analytical models;Annotations;Machine learning;Software;Data mining","","1","","34","CCBY","21 Jun 2022","","","IEEE","IEEE Conferences"
"Linking Source Code to Untangled Change Intents","X. Liu; L. Huang; C. Li; V. Ng","Department of Computer Science and Engineering, Southern Methodist University, Dallas, TX, USA; Department of Computer Science and Engineering, Southern Methodist University, Dallas, TX, USA; Nanjing University, Nanjing, Jiangsu, CN; Human Language Technology Research Institute, University of Texas, Dallas, TX, USA","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","393","403","Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: the pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530046","Commit, Code change, Machine learning","Software;Task analysis;Computer bugs;Machine learning;Cognition;Gold;Additives","","2","","42","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Identifying Compiler and Optimization Options from Binary Code using Deep Learning Approaches","D. Pizzolotto; K. Inoue","Osaka University, Osaka, Japan; Osaka University, Osaka, Japan","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","232","242","When compiling a source file, several flags can be passed to the compiler. These flags, however, can vary between debug and release compilation. In the release compilation, in fact, smaller or faster executables are usually preferred, whereas for a debug one, ease-of-debug is preferred over speed and no optimization is involved. After the compilation, however, most of the flags used cannot be inferred from the compiled file. These flags could be useful in case we want to classify if an older build was made for release or debug purposes, or to check if the file was compiled with flags that could expose vulnerabilities. In this paper we present a deep learning network capable of automatically detecting, with function granularity, the compiler used and the presence of optimization with 99% accuracy. We also analyze the change in accuracy when submitting increasingly shorter amounts of data, from 2048 up to a single byte, obtaining competitive results with less than 100 bytes. We also present our process in the huge dataset creation and manipulation, along with a comparison with other less successful networks using functions of varying size.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240714","Static Analysis;Binary Analysis;Deep Learning;Compilers","Deep learning;Software maintenance;Conferences;Binary codes;Convolutional neural networks;Optimization","","8","","28","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Automated Characterization of Software Vulnerabilities","D. Gonzalez; H. Hastings; M. Mirakhorli","Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA","2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","135","139","Preventing vulnerability exploits is a critical software maintenance task, and software engineers often rely on Common Vulnerability and Exposure (CVEs) reports for information about vulnerable systems and libraries. These reports include descriptions, disclosure sources, and manually-populated vulnerability characteristics such as root cause from the NIST Vulnerability Description Ontology (VDO). This information needs to be complete and accurate so stakeholders of affected products can prevent and react to exploits of the reported vulnerabilities. In this study, we demonstrate that VDO characteristics can be automatically detected from the textual descriptions included in CVE reports. We evaluated the performance of 6 classification algorithms with a dataset of 365 vulnerability descriptions, each mapped to 1 of 19 characteristics from the VDO. This work demonstrates that it is feasible to train classification techniques to accurately characterize vulnerabilities from their descriptions. All 6 classifiers evaluated produced accurate results, and the Support Vector Machine classifier was the best-performing individual classifier. Automating the vulnerability characterization process is a step towards ensuring stakeholders have the necessary data to effectively maintain their systems.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918946","software maintainence;vulnerability characterization;text classification;CVE;VDO","Support vector machines;Software;Task analysis;Measurement;NIST;Ontologies;Stakeholders","","6","","17","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Peeler: Learning to Effectively Predict Flakiness without Running Tests","Y. Qin; S. Wang; K. Liu; B. Lin; H. Wu; L. Li; X. Mao; T. F. Bissyandé","National University of Defense Technology, China; National University of Defense Technology, China; Huawei Software Engineering Application Technology Lab, China; National University of Defense Technology, China; National University of Defense Technology, China; Monash University, Australia; National University of Defense Technology, China; University of Luxembourg, Luxembourg","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","257","268","Regression testing is a widely adopted approach to expose change-induced bugs as well as to verify the correctness/robustness of code in modern software development settings. Unfortunately, the occurrence of flaky tests leads to a significant increase in the cost of regression testing and eventually reduces the productivity of developers (i.e., their ability to find and fix real problems). State-of-the-art approaches leverage dynamic test information obtained through expensive re-execution of test cases to effectively identify flaky tests. Towards accounting for scalability constraints, some recent approaches have built on static test case features, but fall short on effectiveness. In this paper, we introduce Peeler, a new fully static approach for predicting flaky tests through exploring a representation of test cases based on the data dependency relations. The predictor is then trained as a neural network based model, which achieves at the same time scalability (because it does not require any test execution), effectiveness (because it exploits relevant test dependency features), and practicality (because it can be applied in the wild to find new flaky tests). Experimental validation on 17,532 test cases from 21 Java projects shows that Peeler outperforms the state-of-the-art FlakeFlagger by around 20 percentage points: we catch 22% more flaky tests while yielding 51% less false positives. Finally, in a live study with projects in-the-wild, we reported to developers 21 flakiness cases, among which 12 have already been confirmed by developers as being indeed flaky.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00031","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978221","Flaky tests;Deep learning;Program dependency","Productivity;Software maintenance;Java;Costs;Scalability;Neural networks;Computer bugs","","3","","44","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Understanding Quantum Software Engineering Challenges An Empirical Study on Stack Exchange Forums and GitHub Issues","M. R. El aoun; H. Li; F. Khomh; M. Openja","Department of Computer Engineering and Software Engineering Polytechnique Montréal, Montréal, QC, Canada; Department of Computer Engineering and Software Engineering Polytechnique Montréal, Montréal, QC, Canada; Department of Computer Engineering and Software Engineering Polytechnique Montréal, Montréal, QC, Canada; Department of Computer Engineering and Software Engineering Polytechnique Montréal, Montréal, QC, Canada","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","343","354","With the advance of quantum computing, quantum software becomes critical for exploring the full potential of quantum computing systems. Recently, quantum software engineering (QSE) becomes an emerging area attracting more and more attention. However, it is not clear what are the challenges and opportunities of quantum computing facing the software engineering community. This work aims to understand the QSE-related challenges perceived by developers. We perform an empirical study on Stack Exchange forums where developers post-QSE-related questions & answers and Github issue reports where developers raise QSE-related issues in practical quantum computing projects. Based on an existing taxonomy of question types on Stack Overflow, we first perform a qualitative analysis of the types of QSE-related questions asked on Stack Exchange forums. We then use automated topic modeling to uncover the topics in QSE-related Stack Exchange posts and GitHub issue reports. Our study highlights some particularly challenging areas of QSE that are different from that of traditional software engineering, such as explaining the theory behind quantum computing code, interpreting quantum program outputs, and bridging the knowledge gap between quantum computing and classical computing, as well as their associated opportunities.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609196","Quantum computing;Quantum software engineering;Topic modeling;Stack Exchange;Issue reports","Knowledge engineering;Software maintenance;Quantum computing;Codes;Computational modeling;Taxonomy;Quantum mechanics","","2","","84","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Automated Extraction of Requirement Entities by Leveraging LSTM-CRF and Transfer Learning","M. Li; Y. Yang; L. Shi; Q. Wang; J. Hu; X. Peng; W. Liao; G. Pi","Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Sciences, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; China Merchants Bank, Shenzhen, China; China Merchants Bank, Shenzhen, China; China Merchants Bank, Shenzhen, China","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","208","219","Requirement entities, ""explicit specification of concepts that define the primary function objects"", play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79% precision, 81% recall, and 80% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00029","National Science Foundation; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240689","Requirement Entity;Sequence Tagging;LSTM-CRF;Transfer Learning","Training;Adaptation models;Maintenance engineering;Software systems;Data models;Task analysis;Context modeling","","1","","56","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"An Empirical Study on Bugs Inside PyTorch: A Replication Study","S. C. Yin Ho; V. Majdinasab; M. Islam; D. E. Costa; E. Shihab; F. Khomh; S. Nadi; M. Raza",Concordia University; Polytechnique Montréal; University of Alberta; Université du Québec à Montréal; Concordia University; Polytechnique Montréal; University of Alberta; Queen’s University,"2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","220","231","Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch’s development, and assess their locality within the project, and extract patterns of bug fixes. Our results highlight that PyTorch bugs are more like traditional software projects bugs, than related to deep learning characteristics. Finally, we also compare our results with the study on TensorFlow, highlighting similarities and differences across the bug identification and fixing process.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336350","Deep Learning;Bug Analysis;Software Library Defect;PyTorch;Empirical Study","Deep learning;Software maintenance;Computer bugs;Software algorithms;Software systems;Libraries;Intelligent systems","","","","57","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Automatically Tagging the “AAA” Pattern in Unit Test Cases Using Machine Learning Models","C. Wei; L. Xiao; T. Yu; X. Chen; X. Wang; S. Wong; A. Clune","School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; Department of EECS, University of Cincinnati, Cincinnati, OH, USA; HSBC Software Development (Guangdong) Limited, Guangzhou, Guangdong Province, China; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; Envestnet, Inc., Berwyn, PA, USA; AGI, Ansys Company, Exton, PA, USA","IEEE Transactions on Software Engineering","15 May 2023","2023","49","5","3305","3324","The AAA pattern (i.e., Arrange-Act-Assert) is a common and natural layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The AAA structure of real-life test cases, however, may not be clear due to their high complexity. Manually labeling AAA statements in test cases is tedious. Thus, we envision that an automated approach for labeling AAA statements in existing test cases could benefit new developers and projects that practice collective code ownership and test-driven development. This paper contributes an automatic approach based on machine learning models. The “secret sauce” of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the AAA pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. To achieve the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify Arrangement, Action, and Assertion statements with a precision upwards of 92%, and recall up to 74%. We also summarize some experience based on our experiments—regarding the choice of machine learning models, data balancing algorithm, and feature engineering methods—which could potentially provide some reference to related future research.","1939-3520","","10.1109/TSE.2023.3252442","National Science Foundation(grant numbers:CCF-1909085,CCF-1909763); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10058578","AAA pattern;feature engineering;machine learning;natural language processing;software testing;unit testing","Codes;Machine learning;Tagging;Debugging;Production;Maintenance engineering;Computer bugs","","","","81","IEEE","3 Mar 2023","","","IEEE","IEEE Journals"
"NICHE: A Curated Dataset of Engineered Machine Learning Projects in Python","R. Widyasari; Z. Yang; F. Thung; S. Qin Sim; F. Wee; C. Lok; J. Phan; H. Qi; C. Tan; Q. Tay; D. Lo","School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University; School of Computing and Information System, Singapore Management University","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","62","66","Machine learning (ML) has gained much attention and has been incorporated into our daily lives. While there are numerous publicly available ML projects on open source platforms such as GitHub, there have been limited attempts in filtering those projects to curate ML projects of high quality. The limited availability of such a high-quality dataset poses an obstacle to understanding ML projects. To help clear this obstacle, we present NICHE, a manually labelled dataset consisting of 572 ML projects. Based on the evidence of good software engineering practices, we label 441 of these projects as engineered and 131 as non-engineered. This dataset can help researchers understand the practices that are adopted in high-quality ML projects. It can also be used as a benchmark for classifiers designed to identify engineered ML projects.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00022","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174042","Engineered Software Project;Machine Learning;Python;Open Source Projects","Filtering;Machine learning;Benchmark testing;Software;Data mining;Software engineering;Software development management","","","","24","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"A Machine Learning Approach to Improve the Detection of CI Skip Commits","R. Abdalkareem; S. Mujahid; E. Shihab","Department of Computer Science and Software Engineering, Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada","IEEE Transactions on Software Engineering","10 Dec 2021","2021","47","12","2740","2754","Continuous integration (CI) frameworks, such as Travis CI, are growing in popularity, encouraged by market trends towards speeding up the release cycle and building higher-quality software. A key facilitator of CI is to automatically build and run tests whenever a new commit is submitted/pushed. Despite the many advantages of using CI, it is known that the CI process can take a very long time to complete. One of the core causes for such delays is the fact that some commits (e.g., cosmetic changes) unnecessarily kick off the CI process. Therefore, the main goal of this paper is to automate the process of determining which commits can be CI skipped through the use of machine learning techniques. We first extracted 23 features from historical data of ten software repositories. Second, we conduct a study on the detection of CI skip commits using machine learning where we built a decision tree classifier. We then examine the accuracy of using the decision tree in detecting CI skip commits. Our results show that the decision tree can identify CI skip commits with an average AUC equal to 0.89. Furthermore, the top node analysis shows that the number of developers who changed the modified files, the CI-Skip rules, and commit message are the most important features to detect CI skip commits. Finally, we investigate the generalizability of identifying CI skip commits through applying cross-project validation, and our results show that the general classifier achieves an average 0.74 of AUC values.","1939-3520","","10.1109/TSE.2020.2967380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961089","Continuous integration;travis CI;build status;machine learning","Machine learning;Decision trees;Feature extraction;Message systems;Documentation;Buildings","","22","","69","IEEE","16 Jan 2020","","","IEEE","IEEE Journals"
"Test Input Prioritization for Machine Learning Classifiers","X. Dang; Y. Li; M. Papadakis; J. Klein; T. F. Bissyandé; Y. L. Traon","University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg","IEEE Transactions on Software Engineering","18 Mar 2024","2024","50","3","413","442","Machine learning has achieved remarkable success across diverse domains. Nevertheless, concerns about interpretability in black-box models, especially within Deep Neural Networks (DNNs), have become pronounced in safety-critical fields like healthcare and finance. Classical machine learning (ML) classifiers, known for their higher interpretability, are preferred in these domains. Similar to DNNs, classical ML classifiers can exhibit bugs that could lead to severe consequences in practice. Test input prioritization has emerged as a promising approach to ensure the quality of an ML system, which prioritizes potentially misclassified tests so that such tests can be identified earlier with limited manual labeling costs. However, when applying to classical ML classifiers, existing DNN test prioritization methods are constrained from three perspectives: 1) Coverage-based methods are inefficient and time-consuming; 2) Mutation-based methods cannot be adapted to classical ML models due to mismatched model mutation rules; 3) Confidence-based methods are restricted to a single dimension when applying to binary ML classifiers, solely depending on the model's prediction probability for one class. To overcome the challenges, we propose MLPrior, a test prioritization approach specifically tailored for classical ML models. MLPrior leverages the characteristics of classical ML classifiers (i.e., interpretable models and carefully engineered attribute features) to prioritize test inputs. The foundational principles are: 1) tests more sensitive to mutations are more likely to be misclassified, and 2) tests closer to the model's decision boundary are more likely to be misclassified. Building on the first concept, we design mutation rules to generate two types of mutation features (i.e., model mutation features and input mutation features) for each test. Drawing from the second notion, MLPrior generates attribute features of each test based on its attribute values, which can indirectly reveal the proximity between the test and the decision boundary. For each test, MLPrior combines all three types of features of it into a final vector. Subsequently, MLPrior employs a pre-trained ranking model to predict the misclassification probability of each test based on its final vector and ranks tests accordingly. We conducted an extensive study to evaluate MLPrior based on 185 subjects, encompassing natural datasets, mixed noisy datasets, and fairness datasets. The results demonstrate that MLPrior outperforms all the compared test prioritization approaches, with an average improvement of 14.74%$\sim$∼66.93% on natural datasets, 18.55%$\sim$∼67.73% on mixed noisy datasets, and 15.34%$\sim$∼62.72% on fairness datasets.","1939-3520","","10.1109/TSE.2024.3350019","Luxembourg National Research Fund AFR PhD(grant numbers:17036341); European Research Council (ERC)(grant numbers:949014); Luxembourg National Research Funds (FNR)(grant numbers:C20/IS/14761415/TestFlakes); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10382258","Test input prioritization;machine learning;mutation analysis;learning to rank;labelling","Predictive models;Adaptation models;Labeling;Machine learning;Testing;Noise measurement;Manuals","","","","120","CCBY","5 Jan 2024","","","IEEE","IEEE Journals"
"Comments on “Researcher Bias: The Use of Machine Learning in Software Defect Prediction”","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computing, Queen's University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","IEEE Transactions on Software Engineering","10 Nov 2016","2016","42","11","1092","1094","Shepperd et al. find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd et al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results.","1939-3520","","10.1109/TSE.2016.2553030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450669","Software quality assurance;defect prediction;researcher bias","Measurement;Interference;Analysis of variance;Predictive models;Analytical models;NASA;Data models","","60","","18","IEEE","11 Apr 2016","","","IEEE","IEEE Journals"
"Authors’ Reply to “Comments on ‘Researcher Bias: The Use of Machine Learning in Software Defect Prediction’”","M. Shepperd; T. Hall; D. Bowes","Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; University of Hertfordshire, Hatfield, United Kingdom","IEEE Transactions on Software Engineering","11 Nov 2018","2018","44","11","1129","1131","In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us who conduct and publish experiments comparing the performance of competing software defect prediction systems.","1939-3520","","10.1109/TSE.2017.2731308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990255","Software quality assurance;defect prediction;researcher bias","Software;NASA;Measurement;Analysis of variance;Data models;Predictive models;Analytical models","","8","","5","IEEE","24 Jul 2017","","","IEEE","IEEE Journals"
"You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems","A. Groce; T. Kulesza; C. Zhang; S. Shamasunder; M. Burnett; W. -K. Wong; S. Stumpf; S. Das; A. Shinsel; F. Bice; K. McIntosh","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; Centre for HCI Design, School of Informatics, City University London, London, United Kingdom; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon","IEEE Transactions on Software Engineering","31 Mar 2014","2014","40","3","307","323","How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a “gold standard” and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.","1939-3520","","10.1109/TSE.2013.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682887","Machine learning;end-user testing;test suite size","Testing;Software;Training;Training data;Electronic mail;Software algorithms;Machine learning algorithms","","42","","63","IEEE","12 Dec 2013","","","IEEE","IEEE Journals"
"LAGOON: An Analysis Tool for Open Source Communities","S. Dey; W. Woods","Galois, Inc., Portland, Oregon, USA; Galois, Inc., Portland, Oregon, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","717","721","This paper presents LAGOON - an open source platform for understanding the complex ecosystems of Open Source Software (OSS) communities. The platform currently utilizes spatiotemporal graphs to store and investigate the artifacts produced by these communities, and help analysts identify bad actors who might compromise an OSS project's security. LAGOON provides ingest of artifacts from several common sources, including source code repositories, issue trackers, mailing lists and scraping content from project websites. Ingestion utilizes a modular architecture, which supports incremental updates from data sources and provides a generic identity fusion process that can recognize the same community members across disparate accounts. A user interface is provided for visualization and exploration of an OSS project's complete sociotechnical graph. Scripts are provided for applying machine learning to identify pat-terns within the data. While current focus is on the identification of bad actors in the Python community, the platform's reusability makes it easily extensible with new data and analyses, paving the way for LAGOON to become a comprehensive means of assessing various OSS-based projects and their communities.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528504","Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR00112190092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796249","graph data;machine learning;open source software;social database;spatiotemporal data analysis;user interface","Soft sensors;Ecosystems;Data visualization;Machine learning;Computer architecture;Spatiotemporal phenomena;Security","","1","","36","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Grammar Based Directed Testing of Machine Learning Systems","S. Udeshi; S. Chattopadhyay","Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore","IEEE Transactions on Software Engineering","11 Nov 2021","2021","47","11","2487","2503","The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our Ogma approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. Ogma leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our Ogma approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare Ogma with a random test generation approach and observe that Ogma is more effective than such random test generation by up to 489 percent.","1939-3520","","10.1109/TSE.2019.2953066","President's Graduate Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907363","Software testing;machine learning;natural language processing","Machine learning;Grammar;Robustness;Systematics;Test pattern generators;Natural language processing","","6","","45","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Learning to Extract API Mentions from Informal Natural Language Discussions","D. Ye; Z. Xing; C. Y. Foo; J. Li; N. Kapre","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","16 Jan 2017","2016","","","389","399","When discussing programming issues on social platforms (e.g, Stack Overflow, Twitter), developers often mention APIs in natural language texts. Extracting API mentions in natural language texts is a prerequisite for effective indexing and searching for API-related information in software engineering social content. However, the informal nature of social discussions creates two fundamental challenges for API extraction: common-word polysemy and sentence-format variations. Common-word polysemy refers to the ambiguity between the API sense of a common word and the normal sense of the word (e.g., append, apply and merge). Sentence-format variations refer to the lack of consistent sentence writing format for inferring API mentions. Existing API extraction techniques fall short to address these two challenges, because they assume distinct API naming conventions (e.g., camel case, underscore) or structured sentence format (e.g., code-like phrase, API annotation, or full API name). In this paper, we propose a semi-supervised machine-learning approach that exploits name synonyms and rich semantic context of API mentions to extract API mentions in informal social text. The key innovation of our approach is to exploit two complementary unsupervised language models learned from the abundant unlabeled text to model sentence-format variations and to train a robust model with a small set of labeled data and an iterative self-training process. The evaluation of 1,205 API mentions of the three libraries (Pandas, Numpy, and Matplotlib) in Stack Overflow texts shows that our approach significantly outperforms existing API extraction techniques based on language-convention and sentence-format heuristics and our earlier machine-learning based method for named-entity recognition.","","978-1-5090-3806-0","10.1109/ICSME.2016.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816484","","Natural languages;Feature extraction;Libraries;Context;Standards;Software engineering;Joining processes","","36","","39","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"Characterizing and Understanding Software Security Vulnerabilities in Machine Learning Libraries","N. S. Harzevili; J. Shin; J. Wang; S. Wang; N. Nagappan","Lassonde School of Engineering, York University, Toronto, Canada; Lassonde School of Engineering, York University, Toronto, Canada; Institute of Software Chinese Academy of Sciences, Beijing, China; Lassonde School of Engineering, York University, Toronto, Canada; IIIT Delhi, New Delhi, India","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","27","38","The application of machine learning (ML) libraries has tremendously increased in many domains, including autonomous driving systems, medical, and critical industries. Vulnerabilities of such libraries could result in irreparable consequences. However, the characteristics of software security vulnerabilities have not been well studied. In this paper, to bridge this gap, we take the first step toward characterizing and understanding the security vulnerabilities of seven well-known ML libraries, including TensorFlow, PyTorch, Scikit-learn, Mlpack, Pandas, Numpy, and Scipy. To do so, we collected 683 security vulnerabilities to explore four major factors: 1) vulnerability types, 2) root causes, 3) symptoms, and 4) fixing patterns of security vulnerabilities in the studied ML libraries. The findings of this study can help developers and researchers understand the characteristics of security vulnerabilities across the studied ML libraries.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173858","Security vulnerability;machine learning libraries;empirical study","Industries;Machine learning;Debugging;Reliability engineering;Libraries;Software;Software reliability","","4","","52","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"A3Ident: A Two-phased Approach to Identify the Leading Authors of Android Apps","W. Wang; G. Meng; H. Wang; K. Chen; W. Ge; X. Li","Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; Chinese Academy of Sciences, Institute of Information Engineering, China; Beijing University of Posts and Telecommunications, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, China; Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","617","628","Authorship identification is the process of identifying and classifying authors through given codes. Authorship identification can be used in a wide range of software domains, e.g., code authorship disputes, plagiarism detection, exposure of attackers’ identity. Besides the inherent challenges from legacy software development, framework programming and crowdsourcing mode in Android raise the difficulties of authorship identification significantly. More specifically, widespread third party libraries and inherited components (e.g., classes, methods, and variables) dilute the primary code within the entire Android app and blur the boundaries of code written by different authors. However, prior research has not well addressed these challenges.To this end, we design a two-phased approach to attribute the primary code of an Android app to the specific developer. In the first phase, we put forward three types of strategies to identify the relationships between Java packages in an app, which consist of context, semantic and structural relationships. A package aggregation algorithm is developed to cluster all packages that are of high probability written by the same authors. In the second phase, we develop three types of features to capture authors’ coding habits and code stylometry. Based on that, we generate fingerprints for an author from its developed Android apps and employ several machine learning algorithms for authorship classification. We evaluate our approach in three datasets that contain 15,666 apps from 257 distinct developers and achieve a 92.5% accuracy rate on average. Additionally, we test it on 2,900 obfuscated apps and our approach can classify apps with an accuracy rate of 80.4%.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00064","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240707","authorship identification;authorship decoupling;android app;package relation graph;leading author","Support vector machines;Software maintenance;Machine learning algorithms;Semantics;Tools;Programming;Random forests","","4","","51","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"How (Not) to Find Bugs: The Interplay Between Merge Conflicts, Co-Changes, and Bugs","L. Amaral; M. C. Oliveira; W. Luz; J. Fortes; R. Bonifácio; D. Alencar; E. Monteiro; G. Pinto; D. Lo","University of Brasília, Brasília, Brazil; Brazilian Ministry of Economy, Brasília, Brazil; University of Brasília, Brasília, Brazil; University of Brasília, Brasília, Brazil; University of Brasília, Brasília, Brazil; Brazilian Ministry of Economy, Brasília, Brazil; University of Brasília, Brasília, Brazil; Federal University of Pará, Belém, Brazil; Singapore Management University, Singapore","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","441","452","Context: In a seminal work, Ball et al. [1] investigate if the information available in version control systems could be used to predict defect density, arguing that practitioners and researchers could better understand errors ""if [our] version control system could talk"". In the meanwhile, several research works have reported that conflict merge resolution is a time consuming and error-prone task, while other contributions diverge about the correlation between co-change dependencies and defect density. Problem: The correlation between conflicting merge scenarios and bugs has not been addressed before, whilst the correlation between co-change dependencies and bug density has been only investigated using a small number of case studies-which can compromise the generalization of the results. Goal: To address this gap in the literature, this paper presents the results of a comprehensive study whose goal is to understand whether or not (a) conflicting merge scenarios and (b) co-change dependencies are good predictors for bug density. Method: We first build a curated dataset comprising the source code history of 29 popular Java Apache projects and leverage the SZZ algorithm to collect the sets of bug-fixing and bug-introducing commits. We then combine the SZZ results with the set of past conflicting merge scenarios and co-change dependencies of the projects. Finally, we use exploratory data analysis and machine learning models to understand the strength of the correlation between conflict resolution and co-change dependencies with defect density. Findings: (a) conflicting merge scenarios are not more prone to introduce bugs than regular commits, (b) there is a negligible to a small correlation between co-change dependencies and defect density-contradicting previous studies in the literature.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240688","software defects;software integration;merge conflicts;co-change dependencies","Software maintenance;Correlation;Machine learning algorithms;Computer bugs;Control systems;Prediction algorithms;Task analysis","","1","","52","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Recommending when Design Technical Debt Should be Self-Admitted","F. Zampetti; C. Noiseux; G. Antoniol; F. Khomh; M. Di Penta","Dept. of Engineering, University of Sannio, Italy; DGIGL, Polytechnique Montreal, SOCCER-SWAT Labs., Canada; DGIGL, Polytechnique Montreal, SOCCER-SWAT Labs., Canada; DGIGL, Polytechnique Montreal, SOCCER-SWAT Labs., Canada; Dept. of Engineering, Universita degli Studi del Sannio, Benevento, Campania, IT","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","216","226","Previous research has shown how developers ""selfadmit"" technical debt introduced in the source code, commenting why such code represents a workaround or a temporary, incomplete solution. This paper investigates the extent to which previously self-admitted technical debt can be used to provide recommendations to developers when they write new source code, suggesting them when to ""self-admit"" design technical debt, or possibly when to improve the code being written. To achieve this goal, we have developed a machine learning approach named TEDIOUS (TEchnical Debt IdentificatiOn System), which leverages various kinds of method-level features as independent variables, including source code structural metrics, readability metrics and, last but not least, warnings raised by static analysis tools. We assessed TEDIOUS on data from nine open source projects for which there are available tagged self-admitted technical debt instances, also comparing the performances of different machine learners. Results of the study indicate that TEDIOUS achieves, when recommending self-admitted technical debts within a single project, an average precision of about 50% and a recall of 52%. When predicting cross-projects, TEDIOUS improves, achieving an average precision of 67% and a recall of 55%. Last, but not least, we noticed how TEDIOUS leverages readability, size and complexity metrics, as well as some warnings raised by static analysis tools.","","978-1-5386-0992-7","10.1109/ICSME.2017.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094423","Self-Admitted Technical Debt;Recommender Systems;Static Analysis Tools","Measurement;Tools;Feature extraction;Static analysis;Training;Complexity theory;Readability metrics","","22","","41","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Learning to Rank Improves IR in SE","D. Binkley; D. Lawrie","Loyola University Maryland, Baltimore, MD, USA; Loyola University Maryland, Baltimore, MD, USA","2014 IEEE International Conference on Software Maintenance and Evolution","6 Dec 2014","2014","","","441","445","Learning to Rank (LtR) encompasses a class of machine learning techniques developed to automatically learn how to better rank the documents returned for an information retrieval (IR) search. Such techniques offer great promise to software engineers because they better adapt to the wider range of differences in the documents and queries seen in software corpora. To encourage the greater use of LtR in software maintenance and evolution research, this paper explores the value that LtR brings to two common maintenance problems: feature location and traceability. When compared to the worst, median, and best models identified from among hundreds of alternative models for performing feature location, LtR ubiquitously provides a statistically significant improvement in MAP, MRR, and MnDCG scores. Looking forward a further motivation for the use of LtR is its ability to enable the development of software specific retrieval models.","1063-6773","978-1-4799-6146-7","10.1109/ICSME.2014.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976114","Information Retrieval;Software Specific Retrieval","Training;Software;Information retrieval;Stability analysis;Software engineering;Computational modeling;Robustness","","15","1","20","IEEE","6 Dec 2014","","","IEEE","IEEE Conferences"
"Deep Green: Modelling Time-Series of Software Energy Consumption","S. Romansky; N. C. Borle; S. Chowdhury; A. Hindle; R. Greiner","Department of Computing Science, University of Alberta Edmonton, Canada; Department of Computing Science, University of Alberta Edmonton, Canada; Department of Computing Science, University of Alberta Edmonton, Canada; Department of Computing Science, University of Alberta Edmonton, Canada; Department of Computing Science, University of Alberta Edmonton, Canada","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","273","283","Inefficient mobile software kills battery life. Yet, developers lack the tools necessary to detect and solve energy bugs in software. In addition, developers are usually tasked with the creation of software features and triaging existing bugs. This means that most developers do not have the time or resources to research, build, or employ energy debugging tools. We present a new method for predicting software energy consumption to help debug software energy issues. Our approach enables developers to align traces of software behavior with traces of software energy consumption. This allows developers to match run-time energy hot spots to the corresponding execution. We accomplish this by applying recent neural network models to predict time series of energy consumption given a software's behavior. We compare our time series models to prior state-of-the-art models that only predict total software energy consumption. We found that machine learning based time series based models, and LSTM based time series based models, can often be more accurate at predicting instantaneous power use and total energy consumption.","","978-1-5386-0992-7","10.1109/ICSME.2017.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094428","energy;software engineering;online model;profiling;green mining;modelling","Software;Energy consumption;Predictive models;Tools;Time series analysis;Energy measurement;Hardware","","14","","36","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"ICON: Inferring Temporal Constraints from Natural Language API Descriptions","R. Pandita; K. Taneja; L. Williams; T. Tung",North Carolina State University; Google Inc.; Accenture Technology Labs; North Carolina State University,"2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","16 Jan 2017","2016","","","378","388","Temporal constraints of an Application Programming Interface (API) are the allowed sequences of method invocations in the API governing the secure and robust operation of client software using the API. These constraints are typically described informally in natural language API documents, and therefore are not amenable to existing constraint-checking tools. Manually identifying and writing formal temporal constraints from API documents can be prohibitively time-consuming and error-prone. To address this issue, we propose ICON: an approach based on Machine Learning (ML) and Natural Language Processing (NLP) for identifying and inferring formal temporal constraints. To evaluate our approach, we use ICON to infer and formalize temporal constraints from the Amazon S3 REST API, the PayPal Payment REST API, and the java.io package in the JDK API. Our results indicate that ICON can effectively identify temporal constraint sentences (from over 4000 human annotated API sentences) with the average 79.0% precision and 60.0% recall. Furthermore, our evaluation demonstrates that ICON achieves an accuracy of 70% in inferring 77 formal temporal constraints from these APIs.","","978-1-5090-3806-0","10.1109/ICSME.2016.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816483","NLP;Temporal Specifications;API","Natural languages;Contracts;Tagging;Documentation;Semantics;Dictionaries;Syntactics","","10","","42","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"CounterFault: Value-Based Fault Localization by Modeling and Predicting Counterfactual Outcomes","A. Podgurski; Y. Küçük","Department of Computer and Data Sciences, Case Western Reserve University, Cleveland, OH, USA; Department of Computer and Data Sciences, Case Western Reserve University, Cleveland, OH, USA","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","382","393","This paper presents a new, flexible approach to automatically localizing faults in software, named CounterFault. It uses a form of causal inference called counterfactual prediction to predict the effect, on the success or failure of an execution Ex, of intervening at a statement s to set an assignment target A to a value a that is not actually assigned to A in Ex but that could be if s or Ex was modified. CounterFault generates this prediction without actually modifying s or Ex, by employing a very flexible non-parametric statistical or machine learning model (e.g., a random forest). CounterFault applies this basic idea to estimate, with minimal confounding bias, the average causal effects on program failures of different changes in the values assigned to program variables, and these estimates are then employed to derive suspiciousness scores, which are used to assist developers in localizing faults. This paper also reports on an empirical evaluation of CounterFault involving the widely used Defects4J evaluation framework, which contains real software faults, as well as several other Java numerical programs. CounterFault is compared empirically with two other value-based fault localization techniques and four of the best performing coverage-based techniques. The results indicate that CounterFault is more effective than the competing techniques.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240635","fault localization, causal inference, counterfactual outcome, causal effect, confounding bias, software engineering, statistical fault localization, causal inference methodology","Software maintenance;Java;Conferences;Predictive models;Numerical models;Random forests","","6","","51","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"On the Security of Python Virtual Machines: An Empirical Study","X. Lin; B. Hua; Q. Fan","School of Software Engineering, University of Science and Technology of China, China; School of Software Engineering, University of Science and Technology of China, China; School of Software Engineering, University of Science and Technology of China, China","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","223","234","Python continues to be one of the most popular programming languages and has been used in many safety-critical fields such as medical treatment, autonomous driving systems, and data science. These fields put forward higher security requirements to Python ecosystems. However, existing studies on machine learning systems in Python concentrate on data security, model security and model privacy, and just assume the underlying Python virtual machines (PVMs) are secure and trustworthy. Unfortunately, whether such an assumption really holds is still unknown.This paper presents, to the best of our knowledge, the first and most comprehensive empirical study on the security of CPython, the official and most deployed Python virtual machine. To this end, we first designed and implemented a software prototype dubbed PVMSCAN, then use it to scan the source code of the latest CPython (version 3.10) and other 10 versions (3.0 to 3.9), which consists of 3,838,606 lines of source code. Empirical results give relevant findings and insights towards the security of Python virtual machines, such as: 1) CPython virtual machines are still vulnerable, for example, PVMSCAN detected 239 vulnerabilities in version 3.10, including 55 null dereferences, 86 uninitialized variables and 98 dead stores; Python/C API-related vulnerabilities are very common and have become one of the most severe threats to the security of PVMs: for example, 70 Python/C API-related vulnerabilities are identified in CPython 3.10; 3) the overall quality of the code remained stable during the evolution of Python VMs with vulnerabilities per thousand line (VPTL) to be 0.50; and 4) automatic vulnerability rectification is effective: 166 out of 239 (69.46%) vulnerabilities can be rectified by a simple yet effective syntax-directed heuristics.We have reported our empirical results to the developers of CPython, and they have acknowledged us and already confirmed and fixed 2 bugs (as of this writing) while others are still being analyzed. This study not only demonstrates the effectiveness of our approach, but also highlights the need to improve the reliability of infrastructures like Python virtual machines by leveraging state-of-the-art security techniques and tools.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00028","University of Science and Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978245","Empirical;Python virtual machines;Security","Software maintenance;Source coding;Prototypes;Data science;Writing;Virtual machining;Data models","","","","110","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Keynote Abstract","",,"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","34","34","Provides an abstract of the keynote presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816771","","Industries;Codes;Privacy;Machine learning;Data science;Data models;User centered design","","","","","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Efficient Bug Triage For Industrial Environments","W. Zhang","Adobe Inc., Mclean, VA, USA","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","727","735","Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240673","Automatic bug triage;machine learning;text classification","Industries;Software maintenance;Computer bugs;Neural networks;Feature extraction;Real-time systems;Task analysis","","11","","26","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"An Empirical Study on Performance Bugs in Deep Learning Frameworks","T. Makkouk; D. J. Kim; T. -H. P. Chen","Software PEformance, Analysis and Reliability (SPEAR) Lab, Concordia University, Montreal, Canada; Software PEformance, Analysis and Reliability (SPEAR) Lab, Concordia University, Montreal, Canada; Software PEformance, Analysis and Reliability (SPEAR) Lab, Concordia University, Montreal, Canada","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","35","46","Machine Learning (ML) and Deep Learning (DL) applications are becoming more popular due to the availability of DL frameworks such as TensorFlow and PyTorch. Therefore, the quality of DL frameworks is essential to ensure DL/ML application quality. Given the computationally expensive nature of DL tasks (e.g., training), performance is a critical aspect of DL frameworks. However, optimizing DL frameworks may have its own unique challenges due to the peculiarities of DL (e.g., hardware integration and the nature of the computation). In this paper, we conduct an empirical study on the performance bugs in DL frameworks. We conduct our study on TensorFlow and PyTorch by identifying the performance and non-performance bugs by mining the GitHub repositories. We find that 1) the proportion of newly reported performance bugs increases faster than fixed performance bugs, and the ratio of performance bugs among all bugs increases over time; 2) performance bugs take more time to fix, have larger fix sizes, and more community engagement (e.g., discussion) compared to non-performance bugs; and 3) we manually derived a taxonomy of 12 categories and 19 sub-categories of the root causes of performance bugs by studying all performance bug fixes. Finally, we present some actionable implications for researchers and developers.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978257","empirical;software engineering;machine learning;deep learning;performance bugs","Deep learning;Training;Software maintenance;Computer bugs;Taxonomy;Hardware;Data mining","","2","","81","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"CCLearner: A Deep Learning-Based Clone Detection Approach","L. Li; H. Feng; W. Zhuang; N. Meng; B. Ryder","Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","249","260","Programmers produce code clones when developing software. By copying and pasting code with or without modification, developers reuse existing code to improve programming productivity. However, code clones present challenges to software maintenance: they may require consistent application of the same or similar bug fixes or program changes to multiple code locations. To simplify the maintenance process, various tools have been proposed to automatically detect clones [1], [2], [3], [4], [5], [6]. Some tools tokenize source code, and then compare the sequence or frequency of tokens to reveal clones [1], [3], [4], [5]. Some other tools detect clones using tree-matching algorithms to compare the Abstract Syntax Trees (ASTs) of source code [2], [6]. In this paper, we present CCLEARNER, the first solely token-based clone detection approach leveraging deep learning. CCLEARNER extracts tokens from known method-level code clones and nonclones to train a classifier, and then uses the classifier to detect clones in a given codebase. To evaluate CCLEARNER, we reused BigCloneBench [7], an existing large benchmark of real clones. We used part of the benchmark for training and the other part for testing, and observed that CCLEARNER effectively detected clones. With the same data set, we conducted the first systematic comparison experiment between CCLEARNER and three popular clone detection tools. Compared with the approaches not using deep learning, CCLEARNER achieved competitive clone detection effectiveness with low time cost.","","978-1-5386-0992-7","10.1109/ICSME.2017.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094426","deep learning;clone detection;empirical","Cloning;Feature extraction;Machine learning;Neural networks;Tools;Training;Testing","","108","","46","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Do as I Do, Not as I Say: Do Contribution Guidelines Match the GitHub Contribution Process?","O. Elazhary; M. -A. Storey; N. Ernst; A. Zaidman",University of Victoria; University of Victoria; University of Victoria; Delft University of Technology,"2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","286","290","Developer contribution guidelines are used in social coding sites like GitHub to explain and shape the process a project expects contributors to follow. They set standards for all participants and ""save time and hassle caused by improperly created pull requests or issues that have to be rejected and re-submitted"" (GitHub). Yet, we lack a systematic understanding of the content of a typical contribution guideline, as well as the extent to which these guidelines are followed in practice. Additionally, understanding how guidelines may impact projects that use Continuous Integration as part of the contribution process is of particular interest. To address this knowledge gap, we conducted a mixed-methods study of 53 GitHub projects with explicit contribution guidelines and coded the guidelines to extract key themes. We then created a process model using GitHub activity data (e.g., commit, new issue, new pull request) to compare the actual activity with the prescribed contribution guidelines. We show that approximately 68% of these projects diverge significantly from the expected process.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919187","code contributions, software engineering, automation","Guidelines;Tools;Documentation;Encoding;Testing;Software;Machine learning","","13","","21","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Constrained feature selection for localizing faults","T. -D. B. Le; D. Lo; M. Li","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","23 Nov 2015","2015","","","501","505","Developers often take much time and effort to find buggy program elements. To help developers debug, many past studies have proposed spectrum-based fault localization techniques. These techniques compare and contrast correct and faulty execution traces and highlight suspicious program elements. In this work, we propose constrained feature selection algorithms that we use to localize faults. Feature selection algorithms are commonly used to identify important features that are helpful for a classification task. By mapping an execution trace to a classification instance and a program element to a feature, we can transform fault localization to the feature selection problem. Unfortunately, existing feature selection algorithms do not perform too well, and we extend its performance by adding a constraint to the feature selection formulation based on a specific characteristic of the fault localization problem. We have performed experiments on a popular benchmark containing 154 faulty versions from 8 programs and demonstrate that several variants of our approach can outperform many fault localization techniques proposed in the literature. Using Wilcoxon rank-sum test and Cliff's d effect size, we also show that the improvements are both statistically significant and substantial.","","978-1-4673-7532-0","10.1109/ICSM.2015.7332502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332502","","Standards;Feature extraction;Software;Machine learning algorithms;Benchmark testing;Computer bugs;Information systems","","10","","14","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"TECCD: A Tree Embedding Approach for Code Clone Detection","Y. Gao; Z. Wang; S. Liu; L. Yang; W. Sang; Y. Cai",Tianjin University; Tianjin University; Tianjin University; Tianjin University; Tianjin University; Drexel University,"2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)","5 Dec 2019","2019","","","145","156","Clone detection techniques have been explored for decades. Recently, deep learning techniques has been adopted to improve the code representation capability, and improve the state-of-the-art in code clone detection. These approaches usually require a transformation from AST to binary tree to incorporate syntactical information, which introduces overheads. Moreover, these approaches conduct term-embedding, which requires large training datasets. In this paper, we introduce a tree embedding technique to conduct clone detection. Our approach first conducts tree embedding to obtain a node vector for each intermediate node in the AST, which captures the structure information of ASTs. Then we compose a tree vector from its involving node vectors using a lightweight method. Lastly Euclidean distances between tree vectors are measured to determine code clones. We implement our approach in a tool called TECCD and conduct an evaluation using the BigCloneBench (BCB) and 7 other large scale Java projects. The results show that our approach achieves good accuracy and recall and outperforms existing approaches.","2576-3148","978-1-7281-3094-1","10.1109/ICSME.2019.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918964","code clone detection;AST;Skip-gram","Cloning;Machine learning;Training;Tools;Task analysis;Natural language processing","","9","","72","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"You Look so Different: Finding Structural Clones and Subclones in Java Source Code","W. Amme; T. S. Heinze; A. Schäfer","Institute of Computer Science Friedrich Schiller University Jena, Jena, Germany; Institute of Data Science German Aerospace Center (DLR), Jena, Germany; Institute of Computer Science Friedrich Schiller University Jena, Jena, Germany","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","70","80","Code reuse and copying is a widespread practice in software development. Detecting code clones, i.e., identical or similar fragments of code, is thus an important task with many applications, ranging from code search to bug finding and malware detection. In this paper, we propose a new approach to detect code clones in source code. Instead of analyzing the code tokens or syntax, our technique is based upon control flow analysis and dominator trees. In this way, the technique not only detects exact and syntactically similar near-miss code clones but also two new types of clones, which we characterize as structural code clones and subclones. For implementation and evaluation, we have developed the tool StoneDetector, which finds code clones in Java source code. StoneDetector performs competitive with the state of the art as measured on the BigCloneBench benchmark and finds more structural clones and subclones.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609181","code clone;clone detection;subclone;structural clone;code duplication","Java;Software maintenance;Codes;Cloning;Machine learning;Tools;Syntactics","","9","","52","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Identifying Defect-Inducing Changes in Visual Code","K. Eng; A. Hindle; A. Senchenko","Quality, Verification & Standards, Electronic Arts, Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada; Quality, Verification & Standards, Electronic Arts, Vancouver, Canada","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","474","484","Defects, or bugs, often form during software development. Identifying the root cause of defects is essential to improve code quality, evaluate testing methods, and support defect prediction. Examples of defect-inducing changes can be found using the SZZ algorithm to trace the textual history of defect-fixing changes back to the defect-inducing changes that they fix in line-based code. The line-based approach of the SZZ method is ineffective for visual code that represents source code graphically rather than textually. In this paper we adapt SZZ for visual code and present the SZZ Visual Code (SZZ-VC) algorithm, that finds changes in visual code based on the differences of graphical elements rather than differences of lines to detect defect-inducing changes. We validated the algorithm for an industry-made AAA video game and 20 music visual programming defects across 12 open source projects. Our results show that SZZ-VC is feasible for detecting defects in visual code for 3 different visual programming languages.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336342","visual programming;visual code;bugs;defects;version control","Visualization;Video games;Software maintenance;Codes;Source coding;Software algorithms;Machine learning","","1","","51","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Making the Most of Small Software Engineering Datasets With Modern Machine Learning","J. A. Prenner; R. Robbes","Faculty of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy; Faculty of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy","IEEE Transactions on Software Engineering","9 Dec 2022","2022","48","12","5050","5067","This paper provides a starting point for Software Engineering (SE) researchers and practitioners faced with the problem of training machine learning models on small datasets. Due to the high costs associated with labeling data, in Software Engineering, there exist many small (< 5,000 samples) and medium-sized (<100,000 samples) datasets. While deep learning has set the state of the art in many machine learning tasks, it is only recently that it has proven effective on small-sized datasets, primarily thanks to pre-training, a semi-supervised learning technique that leverages abundant unlabelled data alongside scarce labelled data. In this work, we evaluate pre-trained Transformer models on a selection of 13 smaller datasets from the SE literature, covering both, source code and natural language. Our results suggest that pre-trained Transformers are competitive and in some cases superior to previous models, especially for tasks involving natural language; whereas for source code tasks, in particular for very small datasets, traditional machine learning methods often has the edge. In addition, we experiment with several techniques that ought to aid training on small datasets, including active learning, data augmentation, soft labels, self-training and intermediate-task fine-tuning, and issue recommendations on when they are effective. We also release all the data, scripts, and most importantly pre-trained models for the community to reuse on their own datasets.","1939-3520","","10.1109/TSE.2021.3135465","University of Bozen-Bolzano; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653849","Small datasets;transformer;BERT;RoBERTA;pre-training;fine-tuning;data augmentation;back translation;soft labels;active learning","Codes;Task analysis;Transformers;Training;Software;Machine learning;Support vector machines","","6","","97","IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"Learning to Combine Multiple Ranking Metrics for Fault Localization","J. Xuan; M. Monperrus","INRIA Lille - Nord Europe, Lille, France; University of Lille & INRIA, Lille, France","2014 IEEE International Conference on Software Maintenance and Evolution","6 Dec 2014","2014","","","191","200","Fault localization is an inevitable step in software debugging. Spectrum-based fault localization consists in computing a ranking metric on execution traces to identify faulty source code. Existing empirical studies on fault localization show that there is no optimal ranking metric for all faults in practice. In this paper, we propose Multric, a learning-based approach to combining multiple ranking metrics for effective fault localization. In Multric, a suspiciousness score of a program entity is a combination of existing ranking metrics. Multric consists two major phases: learning and ranking. Based on training faults, Multric builds a ranking model by learning from pairs of faulty and non-faulty source code elements. When a new fault appears, Multric computes the final ranking with the learned model. Experiments are conducted on 5386 seeded faults in ten open-source Java programs. We empirically compare Multric against four widely-studied metrics and three recently-proposed one. Our experimental results show that Multric localizes faults more effectively than state-of-art metrics, such as Tarantula, Ochiai, and Ample.","1063-6773","978-1-4799-6146-7","10.1109/ICSME.2014.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976085","Fault localization;learning to rank;multiple ranking metrics","Measurement;Training data;Training;Object oriented modeling;Computational modeling;Java;Debugging","","129","1","35","IEEE","6 Dec 2014","","","IEEE","IEEE Conferences"
"An Empirical Study of the Dependency Networks of Deep Learning Libraries","J. Han; S. Deng; D. Lo; C. Zhi; J. Yin; X. Xia","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information Systems, Singapore Management University, Singapore, Singapore; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Faculty of Information Technology, Monash University, Melbourne, Australia","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","868","878","Deep Learning techniques have been prevalent in various domains, and more and more open source projects in GitHub rely on deep learning libraries to implement their algorithms. To that end, they should always keep pace with the latest versions of deep learning libraries to make the best use of deep learning libraries. Aptly managing the versions of deep learning libraries can help projects avoid crashes or security issues caused by deep learning libraries. Unfortunately, very few studies have been done on the dependency networks of deep learning libraries. In this paper, we take the first step to perform an exploratory study on the dependency networks of deep learning libraries, namely, Tensorflow, PyTorch, and Theano. We study the project purposes, application domains, dependency degrees, update behaviors and reasons as well as version distributions of deep learning projects that depend on Tensorflow, PyTorch, and Theano. Our study unveils some commonalities in various aspects (e.g., purposes, application domains, dependency degrees) of deep learning libraries and reveals some discrepancies as for the update behaviors, update reasons, and the version distributions. Our findings highlight some directions for researchers and also provide suggestions for deep learning developers and users.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00116","Research and Development; National Science Foundation; Natural Science Foundation of Zhejiang Province; Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240645","","Deep learning;Software maintenance;Conferences;Libraries;Computer crashes;Security;Software development management","","15","","41","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"An Empirical Study of Challenges in Converting Deep Learning Models","M. Openja; A. Nikanjam; A. H. Yahmed; F. Khomh; Z. M. J. Jiang","Polytechnique Montréal, Montreal, Quebec, Canada; Polytechnique Montréal, Montreal, Quebec, Canada; Polytechnique Montréal, Montreal, Quebec, Canada; Polytechnique Montréal, Montreal, Quebec, Canada; York University, Toronto, Ontario, Canada","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","13","23","There is an increase in deploying Deep Learning (DL)-based software systems in real-world applications. Usually, DL models are developed and trained using DL frameworks like TensorFlow and PyTorch. Each framework has its own internal mechanisms/formats to represent and train DL models (deep neural networks), and usually those formats cannot be recognized by other frameworks. Moreover, trained models are usually deployed in environments different from where they were developed. To solve the interoperability issue and make DL models compatible with different frameworks/environments, some exchange formats are introduced for DL models, like ONNX and CoreML. However, ONNX and CoreML were never empirically evaluated by the community to reveal their prediction accuracy, performance, and robustness after conversion. Poor accuracy or non-robust behavior of converted models may lead to poor quality of deployed DL-based software systems. We conduct, in this paper, the first empirical study to assess ONNX and CoreML for converting trained DL models. In our systematic approach, two popular DL frameworks, Keras and PyTorch, are used to train five widely used DL models on three popular datasets. The trained models are then converted to ONNX and CoreML and transferred to two runtime environments designated for such formats, to be evaluated. We investigate the prediction accuracy before and after conversion. Our results unveil that the prediction accuracy of converted models are at the same level of originals. The performance (time cost and memory consumption) of converted models are studied as well. The size of models are reduced after conversion, which can result in optimized DL-based software deployment. We also study the adversarial robustness of converted models to make sure about the robustness of deployed DL-based software. Leveraging the state-of-the-art adversarial attack approaches, converted models are generally assessed robust at the same level of originals. However, obtained results show that CoreML models are more vulnerable to adversarial attacks compared to ONNX. The general message of our findings is that DL developers should be cautious on the deployment of converted models that may 1) perform poorly while switching from one framework to another, 2) have challenges in robust deployment, or 3) run slowly, leading to poor quality of deployed DL-based software, including DL-based software maintenance tasks, like bug prediction.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00010","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978197","Empirical;Deep Learning;Converting Trained Models;Deploying ML Models;Robustness","Deep learning;Analytical models;Software maintenance;Runtime environment;Systematics;Switches;Predictive models","","8","","40","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"An Experience Report on Applying Passive Learning in a Large-Scale Payment Company","R. Wieman; M. F. Aniche; W. Lobbezoo; S. Verwer; A. Van Deursen","Adyen B.V., The Netherlands; Delft University of Technology-The Netherlands, Delft, Netherlands; Adyen B.V., The Netherlands; Delft University of Technology-The Netherlands, Delft, Netherlands; Delft University of Technology, Delft, Netherlands","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","564","573","Passive learning techniques infer graph models on the behavior of a system from large trace logs. The research community has been dedicating great effort in making passive learning techniques more scalable and ready to use by industry. However, there is still a lack of empirical knowledge on the usefulness and applicability of such techniques in large scale real systems. To that aim, we conducted action research over nine months in a large payment company. Throughout this period, we iteratively applied passive learning techniques with the goal of revealing useful information to the development team. In each iteration, we discussed the findings and challenges to the expert developer of the company, and we improved our tools accordingly. In this paper, we present evidence that passive learning can indeed support development teams, a set of lessons we learned during our experience, a proposed guide to facilitate its adoption, and current research challenges.","","978-1-5386-0992-7","10.1109/ICSME.2017.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094462","passive learning;experience report;dfasat","Companies;Tools;Software;Automata;Inference algorithms;Merging;Industries","","7","","36","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Mining AndroZoo: A Retrospect","L. Li","SnT, University of Luxembourg, Luxembourg","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","675","680","This paper presents a retrospect of an Android app collection named AndroZoo and some research works conducted on top of the collection. AndroZoo is a growing collection of Android apps from various markets including the official Google Play. At the moment, over five million Android apps have been collected. Based on AndroZoo, we have explored several directions that mine Android apps for resolving various challenges. In this work, we summarize those resolved mining challenges in three research dimensions, including code analysis, app evolution analysis, malware analysis, and present in each dimension several case studies that experimentally demonstrate the usefulness of AndroZoo.","","978-1-5386-0992-7","10.1109/ICSME.2017.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094480","","Smart phones;Libraries;Androids;Humanoid robots;Malware;Privacy;Instruments","","7","","33","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Improving Traceability Link Recovery Using Fine-grained Requirements-to-Code Relations","T. Hey; F. Chen; S. Weigelt; W. F. Tichy","Karlsruhe Institute of Technology (KIT), Institute for Program Structures and Data Organization, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Institute for Program Structures and Data Organization, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Institute for Program Structures and Data Organization, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Institute for Program Structures and Data Organization, Karlsruhe, Germany","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","12","22","Traceability information is a fundamental prerequisite for many essential software maintenance and evolution tasks, such as change impact and software reusability analyses. However, manually generating traceability information is costly and error-prone. Therefore, researchers have developed automated approaches that utilize textual similarities between artifacts to establish trace links. These approaches tend to achieve low precision at reasonable recall levels, as they are not able to bridge the semantic gap between high-level natural language requirements and code. We propose to overcome this limitation by leveraging fine-grained, method and sentence level, similarities between the artifacts for traceability link recovery. Our approach uses word embeddings and a Word Mover's Distance-based similarity to bridge the semantic gap. The fine-grained similarities are aggregated according to the artifacts structure and participate in a majority vote to retrieve coarse-grained, requirement-to-class, trace links. In a comprehensive empirical evaluation, we show that our approach is able to outperform state-of-the-art unsupervised traceability link recovery approaches. Additionally, we illustrate the benefits of fine-grained structural analyses to word embedding-based trace link generation.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609109","Traceability;Traceability Link Recovery;Requirements Engineering;Word Embeddings;Natural Language Processing;Word Movers Distance","Bridges;Software maintenance;Conferences;Semantics;Natural languages;Knowledge based systems;Bit error rate","","5","","36","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"DRLgencert: Deep Learning-Based Automated Testing of Certificate Verification in SSL/TLS Implementations","C. Chen; W. Diao; Y. Zeng; S. Guo; C. Hu","Shandong University, Jinan, China; Jinan University, Guangzhou, China; China Mobile (Hangzhou) Information Technology Co., Ltd., Hangzhou, China; Shandong University, Jinan, China; Shandong University, Jinan, China","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Nov 2018","2018","","","48","58","The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are the foundation of network security. The certificate verification in SSL/TLS implementations is vital and may become the ""weak link"" in the whole network ecosystem. In previous works, some research focused on the automated testing of certificate verification, and the main approaches rely on generating massive certificates through randomly combining parts of seed certificates for fuzzing. Although the generated certificates could meet the semantic constraints, the cost is quite heavy, and the performance is limited due to the randomness. To fill this gap, in this paper, we propose DRLGENCERT, the first framework of applying deep reinforcement learning to the automated testing of certificate verification in SSL/TLS implementations. DRLGENCERT accepts ordinary certificates as input and outputs newly generated certificates which could trigger discrepancies with high efficiency. Benefited by the deep reinforcement learning, when generating certificates, our framework could choose the best next action according to the result of a previous modification, instead of simple random combinations. At the same time, we developed a set of new techniques to support the overall design, like new feature extraction method for X.509 certificates, fine-grained differential testing, and so forth. Also, we implemented a prototype of DRLGENCERT and carried out a series of real-world experiments. The results show DRLGENCERT is quite efficient, and we obtained 84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say around 46.5% effectiveness. Also, we evaluated six popular SSL/TLS implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification flaws, and most of them were previously unknown.","2576-3148","978-1-5386-7870-1","10.1109/ICSME.2018.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529836","Differential Testing;Deep Reinforcement Learning;SSL/TLS Implementations","Testing;Feature extraction;Security;Task analysis;Semantics","","5","","31","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Expanding the Number of Reviewers in Open-Source Projects by Recommending Appropriate Developers","A. Chueshev; J. Lawall; R. Bendraou; T. Ziadi","Sorbonne University/LIP6, Paris, France; Inria, Paris, France; Sorbonne University/LIP6, Paris, France; Sorbonne University/LIP6, Paris, France","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","499","510","Code review is an important part of the development of any software project. Recently, many open source projects have begun practicing lightweight and tool-based code review (a.k.a modern code review) to make the process simpler and more efficient. However, those practices still require reviewers, of which there may not be sufficiently many to ensure timely decisions. In this paper, we propose a recommender-based approach to be used by open-source projects to increase the number of reviewers from among the appropriate developers. We first motivate our approach by an exploratory study of nine projects hosted on GitHub and Gerrit. Secondly, we build the recommender system itself, which, given a code change, initially searches for relevant reviewers based on similarities between the reviewing history and the files affected by the change, and then augments this set with developers who have a similar development history as these reviewers but have little or no relevant reviewing experience. To make these recommendations, we rely on collaborative filtering, and more precisely, on matrix factorization. Our evaluation shows that all nine projects could benefit from our system by using it both to get recommendations of previous reviewers and to expand their number from among the appropriate developers.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240650","recommender systems;code review;collaborative filtering;matrix factorization","Software maintenance;Conferences;History;Open source software;Recommender systems;Software development management","","4","","68","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges","A. H. Yahmed; A. Allah Abbassi; A. Nikanjam; H. Li; F. Khomh","Polytechnique Montréal, Canada; Polytechnique Montréal, Canada; Polytechnique Montréal, Canada; Polytechnique Montréal, Canada; Polytechnique Montréal, Canada","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","26","38","Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. Then, we investigate the prevalence and difficulty of these challenges. Results show that the general interest in DRL deployment is growing, confirming the study’s relevance and importance. Results also show that DRL deployment is more difficult than other DRL issues. Additionally, we built a taxonomy of 31 unique challenges in deploying DRL to different platforms. On all platforms, RL environment-related challenges are the most popular, and communication-related challenges are the most difficult among practitioners. We hope our study inspires future research and helps the community overcome the most common and difficult challenges practitioners face when deploying DRL systems.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336261","Empirical;Deep Reinforcement Learning;Software Deployment;Taxonomy of Challenges;Stack Overflow","Deep learning;Video games;Software maintenance;Service robots;Soft sensors;Taxonomy;Reinforcement learning","","","","82","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"Stronger Together: On Combining Relationships in Architectural Recovery Approaches","E. Boerstra; J. Ahn; J. Rubin","Univ. of British Columbia, Canada; Univ. of British Columbia, Canada; Univ. of British Columbia, Canada","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","305","316","Architecture recovery is the process of obtaining the intended architecture of a software system by analyzing its implementation. Most existing architectural recovery approaches rely on extracting information about relationships between code entities and then use the extracted information to group closely related entities together. The approaches differ by the type of relationships they consider, e.g., method calls, data dependencies, and class name similarity. Prior work shows that combining multiple types of relationships during the recovery process is often beneficial as it leads to a better result than the one obtained by using the relationships individually. Yet, most, if not all, academic and industrial architecture recovery approaches simply unify the combined relationships to produce a more complete representation of the analyzed systems. In this paper, we propose and evaluate an alternative approach to combining information derived from multiple relationships, which is based on identifying agreements/disagreements between relationship types. We discuss advantages and disadvantages of both approaches and provide suggestions for future research in this area.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00035","IBM Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978268","architecture recovery;software re-engineering;relationships between code entities","Software maintenance;Codes;Microservice architectures;Computer architecture;Software systems;Data mining","","","","73","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Identifying Self-Admitted Technical Debts With Jitterbug: A Two-Step Approach","Z. Yu; F. M. Fahid; H. Tu; T. Menzies","Department of Software Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","16 May 2022","2022","48","5","1676","1691","Keeping track of and managing Self-Admitted Technical Debts (SATDs) are important to maintaining a healthy software project. This requires much time and effort from human experts to identify the SATDs manually. The current automated solutions do not have satisfactory precision and recall in identifying SATDs to fully automate the process. To solve the above problems, we propose a two-step framework called Jitterbug for identifying SATDs. Jitterbug first identifies the “easy to find” SATDs automatically with close to 100 percent precision using a novel pattern recognition technique. Subsequently, machine learning techniques are applied to assist human experts in manually identifying the remaining “hard to find” SATDs with reduced human effort. Our simulation studies on ten software projects show that Jitterbug can identify SATDs more efficiently (with less human effort) than the prior state-of-the-art methods.","1939-3520","","10.1109/TSE.2020.3031401","National Science Foundation(grant numbers:#1703487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226105","Technical debt;software engineering;machine learning;pattern recognition","Software;Machine learning;Pattern recognition;Training;Computer hacking;Machine learning algorithms;Estimation","","18","","52","IEEE","15 Oct 2020","","","IEEE","IEEE Journals"
"Confusion Detection in Code Reviews","F. Ebert; F. Castor; N. Novielli; A. Serebrenik","Eindhoven University of Technology, The Netherlands; Federal University of Pernambuco, Brazil; University of Bari, Italy; Eindhoven University of Technology, The Netherlands","2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)","7 Nov 2017","2017","","","549","553","Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context.Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.","","978-1-5386-0992-7","10.1109/ICSME.2017.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094460","code review;confusion;machine learning","Androids;Humanoid robots;Uncertainty;Software;Labeling;Training;Manuals","","24","","42","IEEE","7 Nov 2017","","","IEEE","IEEE Conferences"
"Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data","P. R. Mazrae; M. Izadi; A. Heydarnoori","Computer Engineering Department, Sharif University of Technology; Computer Engineering Department, Sharif University of Technology; Computer Engineering Department, Sharif University of Technology","2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)","24 Nov 2021","2021","","","263","273","An issue report documents the discussions around required changes in issue-tracking systems, while a commit contains the change itself in the version control systems. Recovering links between issues and commits can facilitate many software evolution tasks such as bug localization, defect prediction, software quality measurement, and software documentation. A previous study on over half a million issues from GitHub reports only about 42.2% of issues are manually linked by developers to their pertinent commits. Automating the linking of commit-issue pairs can contribute to the improvement of the said tasks. By far, current state-of-the-art approaches for automated commit-issue linking suffer from low precision, leading to unreliable results, sometimes to the point that imposes human supervision on the predicted links. The low performance gets even more severe when there is a lack of textual information in either commits or issues. Current approaches are also proven computationally expensive. We propose Hybrid-Linker, an enhanced approach that overcomes such limitations by exploiting two information channels; (1) a non-textual-based component that operates on non-textual, automatically recorded information of the commit-issue pairs to predict a link, and (2) a textual-based one which does the same using textual information of the commit-issue pairs. Then, combining the results from the two classifiers, Hybrid-Linker makes the final prediction. Thus, every time one component falls short in predicting a link, the other component fills the gap and improves the results. We evaluate Hybrid-Linker against competing approaches, namely FRLink and DeepLink on a dataset of 12 projects. Hybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and F-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and 41.3%, regarding the F-measure. Moreover, the proposed approach exhibits extensive improvements in terms of performance as well. Finally, our source code and data are publicly available.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609165","Link Recovery;Issue Report;Commit;Software Maintenance;Machine Learning;Ensemble Methods","Location awareness;Software maintenance;Current measurement;Conferences;Software quality;Documentation;Control systems","","5","","22","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Defining a Software Maintainability Dataset: Collecting, Aggregating and Analysing Expert Evaluations of Software Maintainability","M. Schnappinger; A. Fietzke; A. Pretschner","Technical University of Munich, Munich, Germany; itestra GmbH, Munich, Germany; Technical University of Munich, Munich, Germany","2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)","2 Nov 2020","2020","","","278","289","Before controlling the quality of software systems, we need to assess it. In the case of maintainability, this often happens with manual expert reviews. Current automatic approaches have received criticism because their results often do not reflect the opinion of experts or are biased towards a small group of experts. We use the judgments of a significantly larger expert group to create a robust maintainability dataset. In a large scale survey, 70 professionals assessed code from 9 open and closed source Java projects with a combined size of 1.4 million source lines of code. The assessment covers an overall judgment as well as an assessment of several subdimensions of maintainability. Among these subdimensions, we present evidence that understandability is valued the most by the experts. Our analysis also reveals that disagreement between evaluators occurs frequently. Significant dissent was detected in 17% of the cases. To overcome these differences, we present a method to determine a consensus, i.e. the most probable true label. The resulting dataset contains the consensus of the experts for more than 500 Java classes. This corpus can be used to learn precise and practical classifiers for software maintainability.","2576-3148","978-1-7281-5619-4","10.1109/ICSME46990.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240684","Software Maintenance;Software Quality;Machine Learning;Software Measurement","Java;Software quality;Tools;Software systems;Software;Complexity theory;Software reliability","","3","","55","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"""When the Code becomes a Crime Scene"" Towards Dark Web Threat Intelligence with Software Quality Metrics","G. Cascavilla; G. Catolino; F. Ebert; D. A. Tamburri; W. J. van den Heuvel","JADS - TU/e, Eindhoven University, The Netherlands; JADS, Tilburg University, The Netherlands; JADS, Tilburg University, The Netherlands; JADS - TU/e, Eindhoven University, The Netherlands; JADS, Tilburg University, The Netherlands","2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)","19 Dec 2022","2022","","","439","443","The increasing growth of illegal online activities in the so-called dark web—that is, the hidden collective of internet sites only accessible by a specialized web browsers—has challenged law enforcement agencies in recent years with sparse research efforts to help. For example, research has been devoted to supporting law enforcement by employing Natural Language Processing (NLP) to detect illegal activities on the dark web and build models for their classification. However, current approaches strongly rely upon the linguistic characteristics used to train the models, e.g., language semantics, which threatens their generalizability. To overcome this limitation, we tackle the problem of predicting illegal and criminal activities—a process defined as threat intelligence—on the dark web from a complementary perspective—that of dark web code maintenance and evolution— and propose a novel approach that uses software quality metrics and dark website appearance parameters instead of linguistic characteristics. We performed a preliminary empirical study on 10.367 web pages and collected more than 40 code metrics and website parameters using sonarqube. Results show an accuracy of up to 82% for predicting the three types of illegal activities (i.e., suspicious, normal, and unknown) and 66% for detecting 26 specific illegal activities, such as drugs or weapons trafficking. We deem our results can influence the current trends in detecting illegal activities on the dark web and put forward a completely novel research avenue toward dealing with this problem from a software maintenance and evolution perspective.","2576-3148","978-1-6654-7956-1","10.1109/ICSME55016.2022.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978266","Software Code metrics;Dark Web;Software Code Quality;Machine Learning","Measurement;Software maintenance;Dark Web;Codes;Law enforcement;Weapons;Web pages","","1","","19","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Summarize Me: The Future of Issue Thread Interpretation","A. Kumar; P. P. Das; P. Pratim Chakrabarti","Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India","2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)","11 Dec 2023","2023","","","341","345","Understanding issue threads is an essential aspect of software maintenance and development, aiding developers in effectively addressing and managing software-related issues. These threads typically contain an issue description, comments discussing possible solutions, and often culminate in a pull request where the proposed changes are elaborated. Even though they are crucial, understanding issue threads can be a lot of work because they are often long and complex, particularly in big projects. This paper, therefore, aims to automate the process of issue thread summarization using advanced AI models, specifically the GPT-3.5-Turbo, reducing the time spent and improving the efficiency of the interpretation process. Our approach taps into the potential of the zero-shot learning methodology, enabling the model to produce context-specific summaries without reliance on prior examples. Additionally, we have developed an algorithm that determines the most effective length for these summaries, which enhances their clarity and relevance. The performance of the model is assessed using automated metrics, including ROUGE and BART scores, for extractive and abstractive summary evaluation respectively. Further, we may like to add that summaries of around 30% to 40% of the total size of the issue thread appears to be sufficient, though it varies slightly from case to case. The model’s successful generation of brief, clear, and pertinent summaries not only boosts team communication and project management but also lays the groundwork for its future integration into a comprehensive tool for simplified exploration and comprehension of complex software repositories.","2576-3148","979-8-3503-2783-0","10.1109/ICSME58846.2023.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10336257","Issue Thread Summarization;Machine Learning;Software maintenance;Zero-shot learning;ROUGE and BART scores","Measurement;Software maintenance;Zero-shot learning;Project management;Artificial intelligence;Context modeling","","","","20","IEEE","11 Dec 2023","","","IEEE","IEEE Conferences"
"The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring","M. Aniche; E. Maziero; R. Durelli; V. H. S. Durelli","Delft University of Technology, Delft, CD, The Netherlands; Federal University of Lavras, Lavras, MG, Brazil; Federal University of Lavras, Lavras, MG, Brazil; Federal University of São João del Rei, São João del Rei, MG, Brazil","IEEE Transactions on Software Engineering","15 Apr 2022","2022","48","4","1432","1450","Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers’ expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90 percent. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.","1939-3520","","10.1109/TSE.2020.3021736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186715","Software engineering;software refactoring;machine learning for software engineering","Biological system modeling;Measurement;Tools;Software;Predictive models;Context modeling;Prediction algorithms","","24","","84","IEEE","4 Sep 2020","","","IEEE","IEEE Journals"
"PSIMiner: A Tool for Mining Rich Abstract Syntax Trees from Code","E. Spirin; E. Bogomolov; V. Kovalenko; T. Bryksin","JetBrains Research, Higher School of Economics, Saint Petersburg, Russia; JetBrains Research, Higher School of Economics, Saint Petersburg, Russia; JetBrains Research, JetBrains N.V., Amsterdam, The Netherlands; JetBrains Research, Saint Petersburg State University, Saint Petersburg, Russia","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","13","17","The application of machine learning algorithms to source code has grown in the past years. Since these algorithms are quite sensitive to input data, it is not surprising that researchers experiment with input representations. Nowadays, a popular starting point to represent code is abstract syntax trees (ASTs). Abstract syntax trees have been used for a long time in various software engineering domains, and in particular in IDEs. The API of modern IDEs allows to manipulate and traverse ASTs, resolve references between code elements, etc. Such algorithms can enrich ASTs with new data and therefore may be useful in ML-based code analysis. In this work, we present PSIMiner— a tool for processing PSI trees from the IntelliJ Platform. PSI trees contain code syntax trees as well as functions to work with them, and therefore can be used to enrich code representation using static analysis algorithms of modern IDEs. To showcase this idea, we use our tool to infer types of identifiers in Java ASTs and extend the code2seq model for the method name prediction problem.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463105","Software Engineering;Data mining;Code representation","Machine learning algorithms;Software algorithms;Static analysis;Tools;Syntactics;Predictive models;Data models","","7","","24","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Developer Interaction Traces Backed by IDE Screen Recordings from Think Aloud Sessions","A. Yamashita; F. Petrillo; F. Khomh; Y. -G. Guéhéneuc",Oslo Metropolitan University; Concordia University; Polytechnique Montréal; Oslo Metropolitan University,"2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","50","53","There are two well-known difficulties to test and interpret methodologies for mining developer interaction traces: first, the lack of enough large datasets needed by mining or machine learning approaches to provide reliable results; and second, the lack of ""ground truth"" or empirical evidence that can be used to triangulate the results, or to verify their accuracy and correctness. Moreover, relying solely on interaction traces limits our ability to take into account contextual factors that can affect the applicability of mining techniques in other contexts, as well hinders our ability to fully understand the mechanics behind observed phenomena. The data presented in this paper attempts to alleviate these challenges by providing 600+ hours of developer interaction traces, from which 26+ hours are backed with video recordings of the IDE screen and developer's comments. This data set is relevant to researchers interested in investigating program comprehension, and those who are developing techniques for interaction traces analysis and mining.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595177","empirical study;industrial data;interaction traces;log mining;program comprehension;programming flow","Data mining;Software;Companies;Programming;Java;Machine learning;Reliability","","1","","21","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Towards Automatically Identifying Paid Open Source Developers","M. Claes; M. Mäntylä; M. Kuutila; U. Farooq","M3S, ITEE, University of Oulu, Finland; M3S, ITEE, University of Oulu, Finland; M3S, ITEE, University of Oulu, Finland; M3S, ITEE, University of Oulu, Finland","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","437","441","Open source development contains contributions from both hired and volunteer software developers. Identification of this status is important when we consider the transferability of research results to the closed source software industry, as they include no volunteer developers. While many studies have taken the employment status of developers into account, this information is often gathered manually due to the lack of accurate automatic methods. In this paper, we present an initial step towards predicting paid and unpaid open source development using machine learning and compare our results with automatic techniques used in prior work. By relying on code source repository meta-data from Mozilla, and manually collected employment status, we built a dataset of the most active developers, both volunteer and hired by Mozilla. We define a set of metrics based on developers' usual commit time pattern and use different classification methods (logistic regression, classification tree, and random forest). The results show that our proposed method identify paid and unpaid commits with an AUC of 0.75 using random forest, which is higher than the AUC of 0.64 obtained with the best of the previously used automatic methods.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595227","machine learning;random forest;software repository mining;mozilla;open source;paid software development;volunteer","Employment;Measurement;Data mining;Electronic mail;Predictive models;Open source software","","","","20","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Automatically Prioritizing Pull Requests","E. van der Veen; G. Gousios; A. Zaidman","Delft Univ. of Technol, Delft, Zuid-Holland, NL; Radboud University, Nijmegen, The Netherlands; Delft University of Technology, The Netherlands","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","357","361","In previous work, we observed that in the pull-based development model integrators face challenges with regard to prioritizing work in the face of multiple concurrent pull requests. We present the design and initial implementation of a prototype pull request prioritisation tool called PRioritizer. PRioritizer works like a priority inbox for pull requests, recommending the top pull requests the project owner should focus on. A preliminary user study showed that Prioritize provides functionality that GitHub is currently lacking, even though users need more insight into how the priority ranking is established to make Prioritize really useful.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180094","pull based development;pull request;GitHub","Face;Electronic mail;Machine learning algorithms;Feature extraction;Sorting;Inspection;Software engineering","","41","","6","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Identifying Experts in Software Libraries and Frameworks Among GitHub Users","J. E. Montandon; L. Lourdes Silva; M. T. Valente","Technical College (COLTEC), Federal University of Minas Gerais, Belo Horizonte, Brazil; Department of Computer Science, Federal Institute of Minas Gerais, Ouro Branco, Brazil; Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","276","287","Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816776","expertise identification;technical expertise;software libraries;github;machine learning","Software;Feature extraction;Machine learning;Software libraries;LinkedIn;Data mining","","26","","55","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Varangian: A Git Bot for Augmented Static Analysis","S. Pujar; Y. Zheng; L. Buratti; B. Lewis; A. Morari; J. Laredo; K. Postlethwait; C. Görn","IBM Research, United States; IBM Research, United States; IBM Research, United States; IBM Research, United States; IBM Research, United States; IBM Research, United States; Red Hat, United States; Red Hat, United States","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","766","767","The complexity and scale of modern software programs often lead to overlooked programming errors and security vulnerabilities. Developers often rely on automatic tools, like static analysis tools, to look for bugs and vulnerabilities. Static analysis tools are widely used because they can understand nontrivial program behaviors, scale to millions of lines of code, and detect subtle bugs. However, they are known to generate an excess of false alarms which hinder their utilization as it is counterproductive for developers to go through a long list of reported issues, only to find a few true positives. One of the ways proposed to suppress false positives is to use machine learning to identify them. However, training machine learning models requires good quality labeled datasets. For this purpose, we developed D2A [3], a differential analysis based approach that uses the commit history of a code repository to create a labeled dataset of Infer [2] static analysis output.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796288","security;static analysis;git;bot;machine learning;bert","Training;Codes;Computer bugs;Static analysis;Machine learning;Programming;Software","","1","","3","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Understanding Development Process of Machine Learning Systems: Challenges and Solutions","E. d. S. Nascimento; I. Ahmed; E. Oliveira; M. P. Palheta; I. Steinmacher; T. Conte","Institute of Computing Universidade Federal do Amazonas, Manaus, Brazil; Department of Informatics, University of California, Irvine, USA; Department of IT, Secretaria de Estado da Fazenda, Manaus, Brazil; Executive Director Buritech Sistema Inteligentes, Manaus, Brazil; Computing and Cyber Systems, Northern Arizona University, Flagstaff, USA; Institute of Computing Universidade Federal do Amazonas, Manaus, Brazil","2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","17 Oct 2019","2019","","","1","6","Background: The number of Machine Learning (ML) systems developed in the industry is increasing rapidly. Since ML systems are different from traditional systems, these differences are clearly visible in different activities pertaining to ML systems software development process. These differences make the Software Engineering (SE) activities more challenging for ML systems because not only the behavior of the system is data dependent, but also the requirements are data dependent. In such scenario, how can Software Engineering better support the development of ML systems? Aim: Our objective is twofold. First, better understand the process that developers use to build ML systems. Second, identify the main challenges that developers face, proposing ways to overcome these challenges. Method: We conducted interviews with seven developers from three software small companies that develop ML systems. Based on the challenges uncovered, we proposed a set of checklists to support the developers. We assessed the checklists by using a focus group. Results: We found that the ML systems development follow a 4-stage process in these companies. These stages are: understanding the problem, data handling, model building, and model monitoring. The main challenges faced by the developers are: identifying the clients' business metrics, lack of a defined development process, and designing the database structure. We have identified in the focus group that our proposed checklists provided support during identification of the client's business metrics and in increasing visibility of the progress of the project tasks. Conclusions: Our research is an initial step towards supporting the development of ML systems, suggesting checklists that support developers in essential development tasks, and also serve as a basis for future research in the area.","1949-3789","978-1-7281-2968-6","10.1109/ESEM.2019.8870157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870157","Machine Learning Systems;data handling;software development;Software Engineering;challenges","Companies;Software;Interviews;Data handling;Task analysis;Industries;Measurement","","37","","14","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"A Dataset and an Approach for Identity Resolution of 38 Million Author IDs extracted from 2B Git Commits","T. Fry; T. Dey; A. Karnauch; A. Mockus","The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","518","522","The data collected from open source projects provide means to model large software ecosystems, but often suffer from data quality issues, specifically, multiple author identification strings in code commits might actually be associated with one developer. While many methods have been proposed for addressing this problem, they are either heuristics requiring manual tweaking, or require too much calculation time to do pairwise comparisons for 38M author IDs in, for example, the World of Code collection. In this paper, we propose a method that finds all author IDs belonging to a single developer in this entire dataset, and share the list of all author IDs that were found to have aliases. To do this, we first create blocks of potentially connected author IDs and then use a machine learning model to predict which of these potentially related IDs belong to the same developer. We processed around 38 million author IDs and found around 14.8 million IDs to have an alias, which belong to 5.4 million different developers, with the median number of aliases being 2 per developer. This dataset can be used to create more accurate models of developer behaviour at the entire OSS ecosystem level and can be used to provide a service to rapidly resolve new author IDs.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387500","NSF(grant numbers:CNS-1925615,IIS-1633437,IIS-1901102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148760","Identity Resolution;Git Commits;Heuristics;Machine Learning;Data Sharing","Training;Codes;Data integrity;Ecosystems;Manuals;Machine learning;Predictive models","","12","","20","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Cheating Death: A Statistical Survival Analysis of Publicly Available Python Projects","R. H. Ali; C. Parlett-Pelleriti; E. Linstead","Machine Learning and Assistive Technology Lab, Chapman University, Orange, CA, USA; Machine Learning and Assistive Technology Lab, Chapman University, Orange, CA, USA; Machine Learning and Assistive Technology Lab, Chapman University, Orange, CA, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","6","10","We apply survival analysis methods to a dataset of publicly-available software projects in order to examine the attributes that might lead to their inactivity over time. We ran a Kaplan-Meier analysis and fit a Cox Proportional-Hazards model to a subset of Software Heritage Graph Dataset, consisting of 3052 popular Python projects hosted on GitLab/GitHub, Debian, and PyPI, over a period of 165 months. We show that projects with repositories on multiple hosting services, a timeline of publishing major releases, and a good network of developers, remain healthy over time and should be worthy of the effort put in by developers and contributors.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148679","open source software projects;survival analysis;software repository health;hazard ratios","Analytical models;Publishing;Data mining;Open source software;Python","","1","","25","","20 Jun 2023","","","IEEE","IEEE Conferences"
"An Exploratory Study on Energy Consumption of Dataframe Processing Libraries","S. Shanbhag; S. Chimalakonda","Department of Computer Science & Engineering, RISHA Lab, Indian Institute of Technology Tirupati, India; Department of Computer Science & Engineering, RISHA Lab, Indian Institute of Technology Tirupati, India","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","284","295","The energy consumption of machine learning applications and their impact on the environment has recently gained attention as a research area, focusing on the model creation and training/inference phases. The data-oriented stages of the machine learning pipeline, which involve pre-processing, cleaning, and exploratory analysis, are critical components. However, energy consumption during these stages has received limited attention. Dataframe processing libraries play a significant role in these stages, and optimizing their energy consumption is important for reducing environmental impact and operational costs. Therefore, as a first step towards studying their energy efficiency, we investigate and compare the energy consumption of three popular dataframe processing libraries, namely Pandas, Vaex, and Dask. We perform experiments across 21 dataframe processing operations within four categories, utilizing three distinct datasets. Our results indicate that no single library is the most energy-efficient for all tasks, and the choice of a library can have a significant impact on energy consumption based on the types and frequencies of operations performed. The findings of this study suggest the potential for optimization of the energy consumption of data-oriented stages in the machine learning pipeline and warrant further research in this area.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174114","dataframe;data preprocessing;energy efficiency;machine learning pipeline","Training;Energy consumption;Pipelines;Machine learning;Libraries;Energy efficiency;Cleaning","","1","","48","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Keynote Abstract","L. Moonen","Simula Res. Lab., Norway","2017 8th International Workshop on Empirical Software Engineering in Practice (IWESEP)","11 May 2017","2017","","","xiii","xiii","Provides an abstract of the keynote presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","","978-1-5090-6699-5","10.1109/IWESEP.2017.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7925415","","Software;Data mining;Software systems;Software engineering;Machine learning;Testing;Task analysis","","","","","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"An Architectural Technical Debt Index Based on Machine Learning and Architectural Smells","D. Sas; P. Avgeriou","Bernoulli Institute for Mathematics, Computer Science, and Artificial Intelligence, University of Groningen, Groningen, Netherlands; Bernoulli Institute for Mathematics, Computer Science, and Artificial Intelligence, University of Groningen, Groningen, Netherlands","IEEE Transactions on Software Engineering","14 Aug 2023","2023","49","8","4169","4195","A key aspect of technical debt (TD) management is the ability to measure the amount of principal accumulated in a system. The current literature contains an array of approaches to estimate TD principal, however, only a few of them focus specifically on architectural TD, but none of them satisfies all three of the following criteria: being fully automated, freely available, and thoroughly validated. Moreover, a recent study has shown that many of the current approaches suffer from certain shortcomings, such as relying on hand-picked thresholds. In this article, we propose a novel approach to estimate architectural technical debt principal based on machine learning and architectural smells to address such shortcomings. Our approach can estimate the amount of technical debt principal generated by a single architectural smell instance. To do so, we adopt novel techniques from Information Retrieval to train a learning-to-rank machine learning model (more specifically, a gradient boosting machine) that estimates the severity of an architectural smell and ensure the transparency of the predictions. Then, for each instance, we statically analyse the source code to calculate the exact number of lines of code creating the smell. Finally, we combine these two values to calculate the technical debt principal. To validate the approach, we conducted a case study and interviewed 16 practitioners, from both open source and industry, and asked them about their opinions on the TD principal estimations for several smells detected in their projects. The results show that for 71% of instances, practitioners agreed that the estimations provided were representative of the effort necessary to refactor the smell.","1939-3520","","10.1109/TSE.2023.3286179","ITEA3 research project(grant numbers:17038 VISDOM); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152491","Machine learning;technical debt;architectural smells;arcan;learning-to-rank;case study","Codes;Estimation;Indexes;Couplings;Standards;Source coding;Shape","","1","","88","IEEE","14 Jun 2023","","","IEEE","IEEE Journals"
"AIMMX: Artificial Intelligence Model Metadata Extractor","J. Tsay; A. Braz; M. Hirzel; A. Shinnar; T. Mummert","IBM Research, New York, USA; IBM Research, São Paulo, Brazil; IBM Research, New York, USA; IBM Research, New York, USA; IBM Research, New York, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","81","92","Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148792","Artificial Intelligence;Machine Learning;Model Mining;Model Metadata;Model Catalog;Metadata Extraction","Analytical models;Adaptation models;Machine learning;Metadata;Software;Reproducibility of results;Libraries","","2","","39","","20 Jun 2023","","","IEEE","IEEE Conferences"
"An Empirical Study of Source Code Detection Using Image Classification","J. Hong; O. Mizuno; M. Kondo","Kyoto Institute of Technology, Kyoto, Japan; Kyoto Institute of Technology, Kyoto, Japan; Kyoto Institute of Technology, Kyoto, Japan","2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)","30 Dec 2019","2019","","","1","15","The detection of programming language for a source code file has achieved high accuracy using the machine learning techniques. On the other hand, for a piece of software (called snippet), the detection of programming language is required to append tags automatically in a question and answer site such as Stack Overflow. However, the detection of programming language for a snippet is still a challenge since snippets is not a complete source code. Usually, experienced developers can detect the language of such snippet at a glance. It is considered that such a task that a human being easily solves can be solved by the image classification method using deep learning technique. Therefore, we propose a programming language detection method using a deep learning based image classification method. By using the data from actual Q&A site, we evaluate our proposed model. The results of experiment demonstrate that we can successfully detect the correct programming language for snippets with over 90% accuracy.","2573-2021","978-1-7281-5590-6","10.1109/IWESEP49350.2019.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945090","Image classification;Source code snippet;Programming language detection","Computer languages;Convolution;Training;Task analysis;Data models;Dictionaries;Machine learning","","1","","15","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Machine Learning for Technical Debt Identification","D. Tsoukalas; N. Mittas; A. Chatzigeorgiou; D. Kehagias; A. Ampatzoglou; T. Amanatidis; L. Angelis","Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Department of Chemistry, International Hellenic University, Thessaloniki, Greece; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Centre for Research and Technology Hellas/Information Technologies Institute, Thessaloniki, Greece; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Computer Science Department, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Software Engineering","9 Dec 2022","2022","48","12","4892","4906","Technical Debt (TD) is a successful metaphor in conveying the consequences of software inefficiencies and their elimination to both technical and non-technical stakeholders, primarily due to its monetary nature. The identification and quantification of TD rely heavily on the use of a small handful of sophisticated tools that check for violations of certain predefined rules, usually through static analysis. Different tools result in divergent TD estimates calling into question the reliability of findings derived by a single tool. To alleviate this issue we use 18 metrics pertaining to source code, repository activity, issue tracking, refactorings, duplication and commenting rates of each class as features for statistical and Machine Learning models, so as to classify them as High-TD or not. As a benchmark we exploit 18,857 classes obtained from 25 Java projects, whose high levels of TD has been confirmed by three leading tools. The findings indicate that it is feasible to identify TD issues with sufficient accuracy and reasonable effort: a subset of superior classifiers achieved an F$_2$2-measure score of approximately 0.79 with an associated Module Inspection ratio of approximately 0.10. Based on the results a tool prototype for automatically assessing the TD of Java projects has been implemented.","1939-3520","","10.1109/TSE.2021.3129355","European Union's Horizon 2020 Research and Innovation Programme(grant numbers:801015); SmartCLIDE(grant numbers:871177); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622154","Machine learning;metrics/measurement;quality analysis and evaluation;software maintenance","Tools;Software;Java;Radio frequency;Codes;Support vector machines;Benchmark testing","","7","","47","IEEE","19 Nov 2021","","","IEEE","IEEE Journals"
"A Machine Learning Approach for Vulnerability Curation","Y. Chen; A. E. Santosa; A. M. Yi; A. Sharma; A. Sharma; D. Lo",Veracode; Veracode; Veracode; Veracode; Veracode; Singapore Management University,"2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","32","42","Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148736","application security;open-source software;machine learning;classifiers ensemble;self-training","Training;Sensitivity;Soft sensors;Pipelines;Machine learning;Predictive models;Data models","","12","","50","","20 Jun 2023","","","IEEE","IEEE Conferences"
"A Time Series Analysis of TravisTorrent Builds: To Everything There Is a Season","A. Atchison; C. Berardi; N. Best; E. Stevens; E. Linstead","Schmid College of Science and Technology, Chapman University, Orange, CA, USA; Schmid College of Science and Technology, Chapman University, Orange, CA, USA; Machine Learning & Assistive Technol. Lab., Chapman Univ. Orange, Orange, CA, USA; Schmid College of Science and Technology, Chapman University, Orange, CA, USA; Machine Learning & Assistive Technol. Lab., Chapman Univ. Orange, Orange, CA, USA","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","463","466","We apply a seasonal decomposition time series analysisto TravisTorrent data in order to examine growth trendsand periodic behavior related to number of builds ina continuous integration environment. We apply our techniquesat the macro level using the full TravisTorrent repository consisting of 1,283 projects, and at the micro level considering the Apache Drill project. Our results demonstrate strong seasonal behavior at both the large and small scale using an additive time series model. In addition to being able to accurately capture trend and periodicity in builddata, our techniques are also able to accurately forecast the expected number of builds for a future time interval.","","978-1-5386-1544-7","10.1109/MSR.2017.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962397","TravisTorrent;Time Series;Build Data;Data Seasonality","Time series analysis;Market research;Software;Additives;History;Tools;Forecasting","","7","","13","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Data Balancing Improves Self-Admitted Technical Debt Detection","M. Sridharan; M. Mantyla; L. Rantala; M. Claes","M3S, ITEE, University of Oulu, Oulu, Finland; M3S, ITEE, University of Oulu, Oulu, Finland; M3S, ITEE, University of Oulu, Oulu, Finland; M3S, ITEE, University of Oulu, Oulu, Finland","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","358","368","A high imbalance exists between technical debt and non-technical debt source code comments. Such imbalance affects Self-Admitted Technical Debt (SATD) detection performance, and existing literature lacks empirical evidence on the choice of balancing technique. In this work, we evaluate the impact of multiple balancing techniques, including Data level, Classifier level, and Hybrid, for SATD detection in Within-Project and Cross-Project setup. Our results show that the Data level balancing technique SMOTE or Classifier level Ensemble approaches Random Forest or XGBoost are reasonable choices depending on whether the goal is to maximize Precision, Recall, F1, or AUC-ROC. We compared our best-performing model with the previous SATD detection benchmark (cost-sensitive Convolution Neural Network). Interestingly the top-performing XGBoost with SMOTE sampling improved the Within-project F1 score by 10% but fell short in Cross-Project set up by 9%. This supports the higher generalization capability of deep learning in Cross-Project SATD detection, yet while working within individual projects, classical machine learning algorithms can deliver better performance. We also evaluate and quantify the impact of duplicate source code comments in SATD detection performance. Finally, we employ SHAP and discuss the interpreted SATD features. We have included the replication package1 and shared a web-based SATD prediction tool2 with the balancing techniques in this study.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00048","Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463080","Self-Admitted Technical Debt;data imbalance;classification;data sampling techniques;cost-sensitive technique;ensemble techniques","Deep learning;Machine learning algorithms;Transfer learning;Neural networks;Focusing;Tools;Software","","4","","44","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"On the Relationship between User Churn and Software Issues","O. E. Zarif; D. A. Da Costa; S. Hassan; Y. Zou","Queen's University, School Of Computing Kingston, Ontario, Canada; Department of Information Science, University of Otago, Dunedin, Otago, New Zealand; Queen's University, School Of Computing Kingston, Ontario, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, Ontario, Canada","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","339","349","The satisfaction of users is only part of the success of a software product, since a strong competition can easily detract users from a software product/service. User churn is the jargon used to denote when a user changes from a product / service to the one offered by the competition. In this study, we empirically investigate the relationship between the issues that are present in a software product and user churn. For this purpose, we investigate a new dataset provided by the alternativeto.net platform. Alternativeto.net has a unique feature that allows users to recommend alternatives for a specific software product, which signals the intention to switch from one software product to another. Through our empirical study, we observe that (i) the intention to change software is tightly associated to the issues that are present in these software; (ii) we can predict the rate of potential churn using machine learning models; (iii) the longer the issue takes to be fixed, the higher the chances of user churn; and (iv) issues within more general software modules are more likely to be associated with user churn. Our study can provide more insights on the prioritization of issues that need to be fixed to proactively minimize the chances of user churn.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148789","software issues;users churn;software alternatives;deep learning","Computer bugs;Machine learning;Switches;Documentation;Predictive models;Software;Web servers","","1","","0","","20 Jun 2023","","","IEEE","IEEE Conferences"
"SniP: An Efficient Stack Tracing Framework for Multi-threaded Programs","A. KP; S. Kumar; D. Mishra; B. Panda","Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Bombay, India","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","408","412","Usage of the execution stack at run-time captures the dynamic state of programs and can be used to derive useful insights into the program behaviour. The stack usage information can be used to identify and debug performance and security aspects of applications. Binary run-time instrumentation techniques are well known to capture the memory access traces during program execution. Tracing the program in entirety and filtering out stack specific accesses is a commonly used technique for stack related analysis. However, applying vanilla tracing techniques (using tools like Intel Pin) for multi-threaded programs has challenges such as identifying the stack areas to perform efficient run-time tracing. In this paper, we introduce SniP, an open-source stack tracing framework for multi-threaded programs built around Intel's binary instrumentation tool Pin. SniP provides a framework for efficient run-time tracing of stack areas used by multi-threaded applications by identifying the stack areas dynamically. The targeted tracing capability of SniP is demonstrated using a range of multi-threaded applications to show its efficacy in terms of trace size and time to trace. Compared to full program tracing using Pin, SniP achieves up to 75x reduction in terms of trace file size and up to 24x reduction in time to trace. SniP complements existing trace based stack usage analysis tools and we demonstrate that SniP can be easily integrated with the analysis framework through different use-cases.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796268","Multi-threaded programs;Run-time instrumentation;Stack tracing","Filtering;Instruments;Linux;Debugging;Pins;Performance analysis;Security","","1","","25","","21 Jun 2022","","","IEEE","IEEE Conferences"
"ConEx: Efficient Exploration of Big-Data System Configurations for Better Performance","R. Krishna; C. Tang; K. Sullivan; B. Ray","Department of Computer Science, Columbia University, New York, NY, USA; Walmart Labs, Mountain View, CA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, Columbia University, New York, NY, USA","IEEE Transactions on Software Engineering","15 Mar 2022","2022","48","3","893","909","Configuration space complexity makes the big-data software systems hard to configure well. Consider Hadoop, with over nine hundred parameters, developers often just use the default configurations provided with Hadoop distributions. The opportunity costs in lost performance are significant. Popular learning-based approaches to auto-tune software does not scale well for big-data systems because of the high cost of collecting training data. We present a new method based on a combination of Evolutionary Markov Chain Monte Carlo (EMCMC) sampling and cost reduction techniques to find better-performing configurations for big data systems. For cost reduction, we developed and experimentally tested and validated two approaches: using scaled-up big data jobs as proxies for the objective function for larger jobs and using a dynamic job similarity measure to infer that results obtained for one kind of big data problem will work well for similar problems. Our experimental results suggest that our approach promises to improve the performance of big data systems significantly and that it outperforms competing approaches based on random sampling, basic genetic algorithms (GA), and predictive model learning. Our experimental results support the conclusion that our approach strongly demonstrates the potential to improve the performance of big data systems significantly and frugally.","1939-3520","","10.1109/TSE.2020.3007560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134972","Performance optimization;MCMC;SBSE;machine learning","Big Data;Software systems;Machine learning;Markov processes;Monte Carlo methods;Predictive models","","9","","66","IEEE","7 Jul 2020","","","IEEE","IEEE Journals"
"Automatic Classification of Software Artifacts in Open-Source Applications","Y. Ma; S. Fakhoury; M. Christensen; V. Arnaoudova; W. Zogaan; M. Mirakhorli","School of Electrical Engineering and Computer Science, Washington State University; School of Electrical Engineering and Computer Science, Washington State University; School of Electrical Engineering and Computer Science, Washington State University; School of Electrical Engineering and Computer Science, Washington State University; Department of Software Engineering, Rochester Institute of Technology; Department of Software Engineering, Rochester Institute of Technology","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","414","425","With the increasing popularity of open-source software development, there is a tremendous growth of software artifacts that provide insight into how people build software. Researchers are always looking for large-scale and representative software artifacts to produce systematic and unbiased validation of novel and existing techniques. For example, in the domain of software requirements traceability, researchers often use software applications with multiple types of artifacts, such as requirements, system elements, verifications, or tasks to develop and evaluate their traceability analysis techniques. However, the manual identification of rich software artifacts is very labor-intensive. In this work, we first conduct a large-scale study to identify which types of software artifacts are produced by a wide variety of open-source projects at different levels of granularity. Then we propose an automated approach based on Machine Learning techniques to identify various types of software artifacts. Through a set of experiments, we report and compare the performance of these algorithms when applied to software artifacts.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595225","Open-source software;machine learning;software artifacts","Open source software;Software algorithms;Testing;Manuals;Classification algorithms;Machine learning","","3","","57","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Learning From Mistakes: Machine Learning Enhanced Human Expert Effort Estimates","F. Sarro; R. Moussa; A. Petrozziello; M. Harman","Department of Computer Science, University College London, London, U.K.; Department of Computer Science, University College London, London, U.K.; Department of Computer Science, University College London, London, U.K.; Department of Computer Science, University College London, London, U.K.","IEEE Transactions on Software Engineering","14 Jun 2022","2022","48","6","1868","1882","In this paper, we introduce a novel approach to predictive modeling for software engineering, named Learning From Mistakes (LFM). The core idea underlying our proposal is to automatically learn from past estimation errors made by human experts, in order to predict the characteristics of their future misestimates, therefore resulting in improved future estimates. We show the feasibility of LFM by investigating whether it is possible to predict the type, severity and magnitude of errors made by human experts when estimating the development effort of software projects, and whether it is possible to use these predictions to enhance future estimations. To this end we conduct a thorough empirical study investigating 402 maintenance and new development industrial software projects. The results of our study reveal that the type, severity and magnitude of errors are all, indeed, predictable. Moreover, we find that by exploiting these predictions, we can obtain significantly better estimates than those provided by random guessing, human experts and traditional machine learners in 31 out of the 36 cases considered (86 percent), with large and very large effect sizes in the majority of these cases (81 percent). This empirical evidence opens the door to the development of techniques that use the power of machine learning, coupled with the observation that human errors are predictable, to support engineers in estimation tasks rather than replacing them with machine-provided estimates.","1939-3520","","10.1109/TSE.2020.3040793","ERC Advanced fellowship(grant numbers:EPIC (741278)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272884","Software effort estimation;estimate errors;human expert estimates;human bias;human-competitive results","Software;Predictive models;Companies;Software engineering;Estimation error;Task analysis;Software measurement","","6","","104","IEEE","27 Nov 2020","","","IEEE","IEEE Journals"
"Natural Language or Not (NLoN) - A Package for Software Engineering Text Analysis Pipeline","M. Mäntylä; F. Calefato; M. Claes","M3S University of Oulu, Finland; Dipartimento Jonieo, University of Bari, Italy; M3S University of Oulu, Finland","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","387","391","The use of natural language processing (NLP) is gaining popularity in software engineering. In order to correctly perform NLP, we must pre-process the textual information to separate natural language from other information, such as log messages, that are often part of the communication in software engineering. We present a simple approach for classifying whether some textual input is natural language or not. Although our NLoN package relies on only 11 language features and character tri-grams, we are able to achieve an area under the ROC curve performances between 0.976-0.987 on three different data sources, with Lasso regression from Glmnet as our learner and two human raters for providing ground truth. Cross-source prediction performance is lower and has more fluctuation with top ROC performances from 0.913 to 0.980. Compared with prior work, our approach offers similar performance but is considerably more lightweight, making it easier to apply in software engineering text mining pipelines. Our source code and data are provided as an R-package for further improvements.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595222","natural language processing;preprocessing;filtering;machine learning;regular expressions;character n-grams;glmnet;lasso;logistic regression","Software engineering;Natural language processing;Predictive models;Task analysis;Tools;Machine learning","","","","23","","30 Dec 2018","","","IEEE","IEEE Conferences"
"DACOS—A Manually Annotated Dataset of Code Smells","H. Nandani; M. Saad; T. Sharma","Dalhousie University Halifax, Canada; Dalhousie University Halifax, Canada; Dalhousie University Halifax, Canada","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","446","450","Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we present DACOS, a manually annotated dataset containing 10, 267 annotations for 5, 192 code snippets. The dataset targets three kinds of code smells at different granularity–multifaceted abstraction, complex method, and long parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended dataset DACOSX that includes definitely benign and definitely smelly snippets by using the thresholds identified in the first phase. We have developed TAGMAN, a web application to help annotators view and mark the snippets one-by-one and record the provided annotations. We make the datasets and the web application accessible publicly. This dataset will help researchers working on smell detection techniques to build relevant and context-aware machine-learning models.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174039","Code smells;dataset;code annotation","Training;Measurement;Codes;Annotations;Machine learning;Benchmark testing;Software","","2","","37","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts","A. S. Yaraghi; M. Bagherzadeh; N. Kahani; L. C. Briand","School of EECS, University of Ottawa, Ottawa, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1615","1639","Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.","1939-3520","","10.1109/TSE.2022.3184842","Huawei Technologies Canada; Mitacs; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9801672","Machine learning;software testing;test case prioritization;test case selection;continuous integration","Feature extraction;Codes;Testing;History;Training;Data collection;Computational modeling","","9","","71","IEEE","20 Jun 2022","","","IEEE","IEEE Journals"
"Machine/Deep Learning for Software Engineering: A Systematic Literature Review","S. Wang; L. Huang; A. Gao; J. Ge; T. Zhang; H. Feng; I. Satyarth; M. Li; H. Zhang; V. Ng","Department of Computer Science, Southern Methodist University, Dallas, TX, USA; Department of Computer Science, Southern Methodist University, Dallas, TX, USA; Department of Computer Science, Southern Methodist University, Dallas, TX, USA; Nanjing University, Nanjing, Jiangsu, China; Nanjing University, Nanjing, Jiangsu, China; Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science, Southern Methodist University, Dallas, TX, USA; Nanjing University, Nanjing, Jiangsu, China; Nanjing University, Nanjing, Jiangsu, China; Human Language Technology Research Institute, University of Texas at Dallas, Richardson, TX, USA","IEEE Transactions on Software Engineering","14 Mar 2023","2023","49","3","1188","1231","Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Software Engineering (SE) and Machine Learning (ML)/Deep Learning (DL). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the applicability and generalizability of ML/DL-related SE studies, we conducted a 12-year Systematic Literature Review (SLR) on 1,428 ML/DL-related SE papers published between 2009 and 2020. Our trend analysis demonstrated the impacts that ML/DL brought to SE. We examined the complexity of applying ML/DL solutions to SE problems and how such complexity led to issues concerning the reproducibility and replicability of ML/DL studies in SE. Specifically, we investigated how ML and DL differ in data preprocessing, model training, and evaluation when applied to SE tasks, and what details need to be provided to ensure that a study can be reproduced or replicated. By categorizing the rationales behind the selection of ML/DL techniques into five themes, we analyzed how model performance, robustness, interpretability, complexity, and data simplicity affected the choices of ML/DL models.","1939-3520","","10.1109/TSE.2022.3173346","National Science Foundation(grant numbers:2034508); NSF of Jiangsu Province(grant numbers:BK20201250); Nanjing University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772253","Software engineering;machine learning;deep learning","Task analysis;Software;Data models;Complexity theory;Codes;Predictive models;Analytical models","","8","","402","IEEE","10 May 2022","","","IEEE","IEEE Journals"
"An Exploratory Study of Log Placement Recommendation in an Enterprise System","J. Cândido; J. Haesen; M. Aniche; A. van Deursen","Department of Software Technology, Delft University of Technology, The Netherlands; Adyen N.V., The Netherlands; Department of Software Technology, Delft University of Technology, The Netherlands; Department of Software Technology, Delft University of Technology, The Netherlands","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","143","154","Logging is a development practice that plays an important role in the operations and monitoring of complex systems. Developers place log statements in the source code and use log data to understand how the system behaves in production. Unfortunately, anticipating where to log during development is challenging. Previous studies show the feasibility of leveraging machine learning to recommend log placement despite the data imbalance since logging is a fraction of the overall code base. However, it remains unknown how those techniques apply to an industry setting, and little is known about the effect of imbalanced data and sampling techniques. In this paper, we study the log placement problem in the code base of Adyen, a large-scale payment company. We analyze 34,526 Java files and 309,527 methods that sum up +2M SLOC. We systematically measure the effectiveness of five models based on code metrics, explore the effect of sampling techniques, understand which features models consider to be relevant for the prediction, and evaluate whether we can exploit 388,086 methods from 29 Apache projects to learn where to log in an industry setting. Our best performing model achieves 79% of balanced accuracy, 81% of precision, 60% of recall. While sampling techniques improve recall, they penalize precision at a prohibitive cost. Experiments with open-source data yield under-performing models over Adyen's test set; nevertheless, they are useful due to their low rate of false positives. Our supporting scripts and tools are available to the community.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463118","Log Placement;Log Recommendation;Logging Practices;Supervised Learning","Industries;Measurement;Training;Biological system modeling;Training data;Machine learning;Production","","5","","50","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"AutoML from Software Engineering Perspective: Landscapes and Challenges","C. Wang; Z. Chen; M. Zhou","School of Computer Science, Peking University, Beijing, China; University College London, London, United Kingdom; School of Computer Science, Peking University, Beijing, China","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","39","51","Machine learning (ML) has been widely adopted in modern software, but the manual configuration of ML (e.g., hyper-parameter configuration) poses a significant challenge to software developers. Therefore, automated ML (AutoML), which seeks the optimal configuration of ML automatically, has received increasing attention from the software engineering community. However, to date, there is no comprehensive understanding of how AutoML is used by developers and what challenges developers encounter in using AutoML for software development. To fill this knowledge gap, we conduct the first study on understanding the use and challenges of AutoML from software developers’ perspective. We collect and analyze 1,554 AutoML downstream repositories, 769 AutoML-related Stack Overflow questions, and 1,437 relevant GitHub issues. The results suggest the increasing popularity of AutoML in a wide range of topics, but also the lack of relevant expertise. We manually identify specific challenges faced by developers for AutoML-enabled software. Based on the results, we derive a series of implications for AutoML framework selection, framework development, and research.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00019","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173951","AutoML;software engineering;application;challenge","Manuals;Machine learning;Market research;Software;Data mining;Software engineering;Software development management","","2","","110","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"QScored: A Large Dataset of Code Smells and Quality Metrics","T. Sharma; M. Kessentini","Siemens Technology, Charlotte, USA; University of Michigan, MI, USA","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","590","594","Code quality aspects such as code smells and code quality metrics are widely used in exploratory and empirical software engineering research. In such studies, researchers spend a substantial amount of time and effort to not only select the appropriate subject systems but also to analyze them to collect the required code quality information. In this paper, we present QScored dataset; the dataset contains code quality information of more than 86 thousand C# and Java GitHub repositories containing more than 1.1 billion lines of code. The code quality information contains seven kinds of detected architecture smells, 20 kinds of design smells, eleven kinds of implementation smells, and 27 commonly used code quality metrics computed at project, package, class, and method levels. Availability of the dataset will facilitate empirical studies involving code quality aspects by making the information readily available for a large number of active GitHub repositories.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463095","Code quality;code smells;quality metrics;maintainability;technical debt","Measurement;Java;Computer architecture;Software;C# languages;Data mining;Software development management","","5","","27","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Data Set of Program Invariants and Error Paths","D. Beyer","LMU, Munich, Germany","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","111","115","The analysis of correctness proofs and counterexamples of program source code is an important way to gain insights into methods that could make it easier in the future to find invariants to prove a program correct or to find bugs. The availability of high-quality data is often a limiting factor for researchers who want to study real program invariants and real bugs. The described data set provides a large collection of concrete verification results, which can be used in research projects as data source or for evaluation purposes. Each result is made available as verification witness, which represents either program invariants that were used to prove the program correct (correctness witness) or an error path to replay the actual bug (violation witness). The verification results are taken from actual verification runs on 10522 verification problems, using the 31 verification tools that participated in the 8th edition of the International Competition on Software Verification (SV-COMP). The collection contains a total of 125720 verification witnesses together with various meta data and a map to relate a witness to the C program that it originates from. Data set is available at: https://doi.org/10.5281/zenodo.2559175.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816801","Invariant Mining, Program Comprehension, Formal Verification, Model Checking, Program Analysis, Verification Witnesses, Program Invariants, Error Paths, Bugs","Software;Tools;Metadata;Computer bugs;Benchmark testing;Automata;Standards","","1","","43","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Public Git Archive: A Big Code Dataset for All","V. Markovtsev; W. Long","Source{d}, Madrid, Spain; Source{d}, Madrid, Spain","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","34","37","The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for ""Big Code"" research.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595173","source code;git;GitHub;software repositories;development history;open dataset","Software;Cloning;Pipelines;Java;Magnetic heads;Indexes","","","","31","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Bot Detection in GitHub Repositories","N. Chidambaram; P. R. Mazrae","Software Engineering Lab, University of Mons, Mons, Belgium; Software Engineering Lab, University of Mons, Mons, Belgium","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","726","728","Contemporary social coding platforms like GitHub promote collaborative development. Many open-source software repositories hosted in these platforms use machine accounts (bots) to automate and facilitate a wide range of effort-intensive and repetitive activities. Determining if an account corresponds to a bot or a human contributor is important for socio-technical development analytics, for example, to understand how humans collaborate and interact in the presence of bots, to assess the positive and negative impact of using bots, to identify the top project contributors, to identify potential bus factors, and so on. Our project aims to include the trained machine learning (ML) classifier from the BoDeGHa bot detection tool as a plugin to the GrimoireLab software development analytics platform. In this work, we present the procedure to form a pipeline for retrieving contribution and contributor data using Perceval, distinguishing bots from humans using BoDeGHa, and visualising the results using Kibana.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796354","","Bot (Internet);Pipelines;Data visualization;Machine learning;Organizations;Encoding;Data mining","","","","7","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Predictive Mutation Testing","J. Zhang; L. Zhang; M. Harman; D. Hao; Y. Jia; L. Zhang","Institute of Software, EECS, Peking University, Beijing, China; Department of Computer Science, University of Texas at Dallas, Richardson, TX; Facebook, London, United Kingdom; Institute of Software, EECS, Peking University, Beijing, China; Facebook, London, United Kingdom; Institute of Software, EECS, Peking University, Beijing, China","IEEE Transactions on Software Engineering","17 Sep 2019","2019","45","9","898","918","Test suites play a key role in ensuring software quality. A good test suite may detect more faults than a poor-quality one. Mutation testing is a powerful methodology for evaluating the fault-detection ability of test suites. In mutation testing, a large number of mutants may be generated and need to be executed against the test suite under evaluation to check how many mutants the test suite is able to detect, as well as the kind of mutants that the current test suite fails to detect. Consequently, although highly effective, mutation testing is widely recognized to be also computationally expensive, inhibiting wider uptake. To alleviate this efficiency concern, we propose Predictive Mutation Testing (PMT): the first approach to predicting mutation testing results without executing mutants. In particular, PMT constructs a classification model, based on a series of features related to mutants and tests, and uses the model to predict whether a mutant would be killed or remain alive without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss. It achieves above 0.80 AUC values for the majority of projects, indicating a good tradeoff between the efficiency and effectiveness of predictive mutation testing. Also, PMT is shown to perform well on different tools and tests, be robust in the presence of imbalanced data, and have high predictability (over 60 percent confidence) when predicting the execution results of the majority of mutants.","1939-3520","","10.1109/TSE.2018.2809496","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000801); National Natural Science Foundation of China(grant numbers:61522201,61529201); NSF(grant numbers:CCF-1566589); EPSRC grant DAASE Dynamic Adaptive Automated Software Engineering(grant numbers:EP/J017515/); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8304576","PMT;mutation testing;machine learning;binary classification","Predictive models;Pattern classification;Software testing;Sensitivity analysis;Software quality;Machine learning","","82","","105","IEEE","28 Feb 2018","","","IEEE","IEEE Journals"
"Developer-Driven Code Smell Prioritization","F. Pecorelli; F. Palomba; F. Khomh; A. De Lucia","SeSa Lab, University of Salerno, Italy; SeSa Lab, University of Salerno, Italy; École Polytechnique de Montréal, Canada; SeSa Lab, University of Salerno, Italy","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","220","231","Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148747","Code smells;Machine Learning for Software Engineering;Empirical Software Engineering","Codes;Machine learning algorithms;Software metrics;Social networking (online);Software;Distance measurement;Data mining","","13","","95","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Style-Analyzer: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms","V. Markovtsev; W. Long; H. Mougard; K. Slavnov; E. Bulychev","source{d}, Madrid, Spain; source{d}, Madrid, Spain; source{d}, Madrid, Spain; source{d}, Madrid, Spain; source{d}, Madrid, Spain","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","468","478","Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. We release style-analyzer as a reusable and extendable open source software package on GitHub for the benefit of the community.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816753","assisted code review, code style, decision tree forest, interpretable machine learning","Decision trees;Feature extraction;Tools;Machine learning;Forestry;Syntactics;Training","","5","","23","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Defectors: A Large, Diverse Python Dataset for Defect Prediction","P. Mahbub; O. Shuvo; M. Masudur Rahman",Dalhousie University; Dalhousie University; Dalhousie University,"2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","393","397","Defect prediction has been a popular research topic where machine learning (ML) and deep learning (DL) have found numerous applications. However, these ML/DL-based defect prediction models are often limited by the quality and size of their datasets. In this paper, we present Defectors, a large dataset for just-in-time and line-level defect prediction. Defectors consists of ≈ 213K source code files (≈ 93K defective and ≈ 120K defect- free) that span across 24 popular Python projects. These projects come from 18 different domains, including machine learning, automation, and internet-of-things. Such a scale and diversity make Defectors a suitable dataset for training ML/DL models, especially transformer models that require large and diverse datasets. We also foresee several application areas of our dataset including defect prediction and defect explanation.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174122","Defect Prediction;Just-in-Time;Dataset;Software Engineering","Training;Deep learning;Automation;Source coding;Predictive models;Transformers;Software","","1","","33","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"An Empirical Study on Log Level Prediction for Multi-Component Systems","Y. E. Ouatiti; M. Sayagh; N. Kerzazi; A. E. Hassan","School of Computing, Queen's University, Kingston, Canada; IT and Software Engineering, Université du Québec, Quebec, Canada; Informatique et Aide à la Décision, UM5R ENSIAS, Rabat, Morocco; School of Computing, Queen's University, Kingston, Canada","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","473","484","Logging statements are used to trace the execution of a software system. Practitioners leverage different logging information (e.g., the content of a log message) to decide for each logging statement an appropriate log level, which is leveraged to adjust the verbosity of logs so that only important log messages are traced. Deciding for the log level can be done differently from one to another component of a multi-component system, such as OpenStack and its 28 components. For example, a component might aim for increasing the verbosity of its log messages, while another component for the same multi-component system might aim at decreasing such a verbosity. Such different logging strategies can exist since each component can be developed and maintained by a different team. While a prior work leveraged an ordinal regression model to recommend the appropriate log level for a new logging statement, their evaluation did not consider the particularities that each component can have within a multi-component system. For instance, their model might not perform well at each component level of a multi-component system. The same model’s interpretability can mislead the developers of each component that has its unique logging strategy. In this paper, we quantify the impact of the particularities of each component of a multi-component system on the performance and interpretability of the log level prediction model of prior work. We observe that the performance of the log level prediction models that are trained at the whole project level (aka., global models) have lower performances (AUC) on 72% to 100% of the components of our five evaluated multi-component systems, compared to the same models when evaluated on the whole multi-component system. We observe that the models that are trained at the component level (aka., local models) statistically outperform the global model on 33% to 77% of the components of our evaluated multi-component systems. Furthermore, we observe that the rankings of the most important features that are obtained from the global models are statistically different from the feature importance rankings of 50% to 87% of the local models of our evaluated multi-component systems. Finally, we observe that 60% and 35% of the Spring and OpenStack components do not have enough data points to train their own local models (aka., data lacking components). Leveraging a peer-local model for such type of components is more promising than using the global model.","1939-3520","","10.1109/TSE.2022.3154672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721622","Log level prediction;machine learning;multi-component systems;software logging","Predictive models;Data models;Springs;Measurement;Software systems;Python;Java","","","","50","IEEE","25 Feb 2022","","","IEEE","IEEE Journals"
"Learning Assumptions for CompositionalVerification of Timed Systems","S. -W. Lin; É. André; Y. Liu; J. Sun; J. S. Dong","Temasek Laboratories, National University of Singapore, Singapore; Université Paris 13, Sorbonne Paris Cité, Laboratoire d’Informatique de Paris-Nord (LIPN), A204, Institut Galilée, 99 avenue Jean-Baptiste Clément, 93430 Villetaneuse, Villetaneuse, France; School of Computer Engineering, Nanyang Technological University, Singapore; BLK1, Level 3, Singapore University of Technology and Design, Singapore; Computer Science Department, School of Computing, National University of Singapore, Singapore","IEEE Transactions on Software Engineering","4 Mar 2014","2014","40","2","137","153","Compositional techniques such as assume-guarantee reasoning (AGR) can help to alleviate the state space explosion problem associated with model checking. However, compositional verification is difficult to be automated, especially for timed systems, because constructing appropriate assumptions for AGR usually requires human creativity and experience. To automate compositional verification of timed systems, we propose a compositional verification framework using a learning algorithm for automatic construction of timed assumptions for AGR. We prove the correctness and termination of the proposed learning-based framework, and experimental results show that our method performs significantly better than traditional monolithic timed model checking.","1939-3520","","10.1109/TSE.2013.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682903","Automatic assume-guarantee reasoning;model checking;timed systems","Model checking;Educational institutions;Explosions;Learning automata;Atomic clocks;Cognition","","19","","37","IEEE","12 Dec 2013","","","IEEE","IEEE Journals"
"Predicting Merge Conflicts in Collaborative Software Development","M. Owhadi-Kareshk; S. Nadi; J. Rubin","Dept. of Computing Science, University of Alberta, Edmonton, Canada; Dept. of Computing Science, University of Alberta, AB, Canada, Edmonton, Canada; Dept. of Electrical and Computer Engineering, University of British Columbia, BC, Canada, Vancouver, Canada","2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","17 Oct 2019","2019","","","1","11","Background. During collaborative software development, developers often use branches to add features or fix bugs. When merging changes from two branches, conflicts may occur if the changes are inconsistent. Developers need to resolve these conflicts before completing the merge, which is an error-prone and time-consuming process. Early detection of merge conflicts, which warns developers about resolving conflicts before they become large and complicated, is among the ways of dealing with this problem. Existing techniques do this by continuously pulling and merging all combinations of branches in the background to notify developers as soon as a conflict occurs, which is a computationally expensive process. One potential way for reducing this cost is to use a machine-learning based conflict predictor that filters out the merge scenarios that are not likely to have conflicts, i.e.safe merge scenarios.Aims. In this paper, we assess if conflict prediction is feasible.Method. We design a classifier for predicting merge conflicts, based on 9 light-weight Git feature sets. To evaluate our predictor, we perform a large-scale study on 267,657 merge scenarios from 744 GitHub repositories in seven programming languages.Results. Our results show that we achieve high f1-scores, varying from 0.95 to 0.97 for different programming languages, when predicting safe merge scenarios. The f1-score is between 0.57 and 0.68 for the conflicting merge scenarios.Conclusions. Predicting merge conflicts is feasible in practice, especially in the context of predicting safe merge scenarios as a pre-filtering step for speculative merging.","1949-3789","978-1-7281-2968-6","10.1109/ESEM.2019.8870173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870173","Conflict Prediction;Git;Software Merging","Merging;Computer languages;Feature extraction;Software;Tools;Machine learning;Correlation","","22","","42","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"Impact of Discretization Noise of the Dependent Variable on Machine Learning Classifiers in Software Engineering","G. K. Rajbahadur; S. Wang; Y. Kamei; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada; Principles of Software Languages (POSL) Lab, Graduate School and Faulty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","16 Jul 2021","2021","47","7","1414","1430","Researchers usually discretize a continuous dependent variable into two target classes by introducing an artificial discretization threshold (e.g., median). However, such discretization may introduce noise (i.e., discretization noise) due to ambiguous class loyalty of data points that are close to the artificial threshold. Previous studies do not provide a clear directive on the impact of discretization noise on the classifiers and how to handle such noise. In this paper, we propose a framework to help researchers and practitioners systematically estimate the impact of discretization noise on classifiers in terms of its impact on various performance measures and the interpretation of classifiers. Through a case study of 7 software engineering datasets, we find that: 1) discretization noise affects the different performance measures of a classifier differently for different datasets; 2) Though the interpretation of the classifiers are impacted by the discretization noise on the whole, the top 3 most important features are not affected by the discretization noise. Therefore, we suggest that practitioners and researchers use our framework to understand the impact of discretization noise on the performance of their built classifiers and estimate the exact amount of discretization noise to be discarded from the dataset to avoid the negative impact of such noise.","1939-3520","","10.1109/TSE.2019.2924371","JSPS KAKENHI(grant numbers:JP18H03222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744330","Discretization noise;discretization;classifiers;feature importance analysis;performance;random forest;logistic regression;decision trees;KNN","Software engineering;Computer bugs;Noise measurement;Software;Machine learning;Regression tree analysis;Logistics","","15","","76","IEEE","24 Jun 2019","","","IEEE","IEEE Journals"
"A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction","Q. Song; Y. Guo; M. Shepperd","Department of Computer Science & Technology, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science & Technology, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, Brunel University, Uxbridge, United Kingdom","IEEE Transactions on Software Engineering","10 Dec 2019","2019","45","12","1253","1269","Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results. Objective: We conduct a comprehensive experiment to explore (a) the basic characteristics of this problem; (b) the effect of imbalanced learning and its interactions with (i) data imbalance, (ii) type of classifier, (iii) input metrics and (iv) imbalanced learning method. Method: We systematically evaluate 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalanced learning methods (including doing nothing) using an experimental design that enables exploration of interactions between these factors and individual imbalanced learning algorithms. This yields 27 × 7 × 7 × 17 = 22491 results. The Matthews correlation coefficient (MCC) is used as an unbiased performance measure (unlike the more widely used F1 and AUC measures). Results: (a) we found a large majority (87 percent) of 106 public domain data sets exhibit moderate or low level of imbalance (imbalance ratio <; 10; median = 3.94); (b) anything other than low levels of imbalance clearly harm the performance of traditional learning for SDP; (c) imbalanced learning is more effective on the data sets with moderate or higher imbalance, however negative results are always possible; (d) type of classifier has most impact on the improvement in classification performance followed by the imbalanced learning method itself. Type of input metrics is not influential. (e) only 52% of the combinations of Imbalanced Learner and Classifier have a significant positive effect. Conclusion: This paper offers two practical guidelines. First, imbalanced learning should only be considered for moderate or highly imbalanced SDP data sets. Second, the appropriate combination of imbalanced method and classifier needs to be carefully chosen to ameliorate the imbalanced learning problem for SDP. In contrast, the indiscriminate application of imbalanced learning can be harmful.","1939-3520","","10.1109/TSE.2018.2836442","National Natural Science Foundation of China(grant numbers:61373046,61210004); Brunel University London; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359087","Software defect prediction;bug prediction;imbalanced learning;imbalance ratio;effect size","Software measurement;Boosting;Machine learning algorithms;Bagging;Computer bugs","","162","","92","IEEE","15 May 2018","","","IEEE","IEEE Journals"
"Cerebro: Static Subsuming Mutant Selection","A. Garg; M. Ojdanic; R. Degiovanni; T. T. Chekam; M. Papadakis; Y. Le Traon","University of Luxembourg, Esch-sur-Alzette, Luxembourg; University of Luxembourg, Esch-sur-Alzette, Luxembourg; University of Luxembourg, Esch-sur-Alzette, Luxembourg; SES, Betzdorf, Luxembourg; University of Luxembourg, Esch-sur-Alzette, Luxembourg; University of Luxembourg, Esch-sur-Alzette, Luxembourg","IEEE Transactions on Software Engineering","6 Jan 2023","2023","49","1","24","43","Mutation testing research has indicated that a major part of its application cost is due to the large number of low utility mutants that it introduces. Although previous research has identified this issue, no previous study has proposed any effective solution to the problem. Thus, it remains unclear how to mutate and test a given piece of code in a best effort way, i.e., achieving a good trade-off between invested effort and test effectiveness. To achieve this, we propose Cerebro, a machine learning approach that statically selects subsuming mutants, i.e., the set of mutants that resides on the top of the subsumption hierarchy, based on the mutants’ surrounding code context. We evaluate Cerebro using 48 and 10 programs written in C and Java, respectively, and demonstrate that it preserves the mutation testing benefits while limiting application cost, i.e., reduces all cost application factors such as equivalent mutants, mutant executions, and the mutants requiring analysis. We demonstrate that Cerebro has strong inter-project prediction ability, which is significantly higher than two baseline methods, i.e., supervised learning on features proposed by state-of-the-art, and random mutant selection. More importantly, our results show that Cerebro’s selected mutants lead to strong tests that are respectively capable of killing 2 times higher than the number of subsuming mutants killed by the baselines when selecting the same number of mutants. At the same time, Cerebro reduces the cost-related factors, as it selects, on average, 68% fewer equivalent mutants, while requiring 90% fewer test executions than the baselines.","1939-3520","","10.1109/TSE.2022.3140510","Luxembourg National Research Funds(grant numbers:INTER/ANR/18/12632675/SATOCROSS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9677967","Mutant;mutation;mutation testing;subsuming mutant;mutant prediction;static selection;static mutant selection;static subsuming mutant selection;static subsuming mutant prediction;encoder-decoder;machine translation;tf-seq2seq","Testing;Java;Codes;Machine learning;Costs;Supervised learning;Reliability","","7","","69","CCBY","11 Jan 2022","","","IEEE","IEEE Journals"
"Formal Equivalence Checking for Mobile Malware Detection and Family Classification","F. Mercaldo; A. Santone","Department of Medicine and Health Sciences “Vincenzo Tiberio”, University of Molise, Campobasso, Italy; Department of Medicine and Health Sciences “Vincenzo Tiberio”, University of Molise, Campobasso, Italy","IEEE Transactions on Software Engineering","15 Jul 2022","2022","48","7","2643","2657","Several techniques to overcome the weaknesses of the current signature based detection approaches adopted by free and commercial antimalware have been proposed by industrial and research communities. These techniques are mainly supervised machine learning based, requiring optimal class balance to generate good predictive models. In this paper, we propose a method to infer mobile application maliciousness by detecting the belonging family, exploiting formal equivalence checking. We introduce a set of heuristics to reduce the number of mobile application comparisons and we define a metric reflecting the application maliciousness. Real-world experiments on 35 Android malware families (ranging from 2010 to 2018) confirm the effectiveness of the proposed method in mobile malware detection and family identification.","1939-3520","","10.1109/TSE.2021.3067061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381654","Equivalence checking;formal methods;android;malware;security","Malware;Operating systems;Machine learning;Training;Tools;Smart phones;Automata","","6","","33","IEEE","18 Mar 2021","","","IEEE","IEEE Journals"
"Program Repair With Repeated Learning","L. Chen; Y. Pei; M. Pan; T. Zhang; Q. Wang; C. A. Furia","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; State Key Laboratory for Novel Software Technology, Software Institute of Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Software Institute of USI, Università della Svizzera italiana, Lugano, Switzerland","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","831","848","A key challenge in generate-and-validate automated program repair is directing the search for fixes so that it can efficiently find those that are more likely to be correct. To this end, several techniques use machine learning to capture the features of programmer-written fixes. In existing approaches, fitting the model typically takes place before fix generation and is independent of it: the fix generation process uses the learned model as one of its inputs. However, the intermediate outcomes of an ongoing fix generation process often provide valuable information about which candidate fixes were “better”; this information could profitably be used to retrain the model, so that each new iteration of the fixing process would also learn from the outcome of previous ones. In this paper, we propose the Liana technique for automated program repair, which is based on this idea of repeatedly learning the features of generated fixes. To this end, Liana uses a fine-grained model that combines information about fix characteristics, their relations to the fixing context, and the results of test execution. The model is initially trained offline, and then repeatedly updated online as the fix generation process unravels; at any step, the most up-to-date model is used to guide the search for fixes—prioritizing those that are more likely to include the right ingredients. In an experimental evaluation on 732 real-world Java bugs from 3 popular benchmarks, Liana built correct fixes for 134 faults (83 ranked as first in its output)— improving over several other generate-and-validate program repair tools according to various measures.","1939-3520","","10.1109/TSE.2022.3164662","Hong Kong Research Grants Council(grant numbers:PolyU 152002/18E); National Natural Science Foundation of China(grant numbers:61972193); Hong Kong RGC Theme-Based Research Scheme(grant numbers:T22-505/19-N (P0031331, RBCR, P0031259, RBCP)); RGC GRF(grant numbers:PolyU 152002/18E (P0005550, Q67V),PolyU 152164/14E (P0004750, Q44B)); RGC Germany/HK(grant numbers:G-PolyU503/16); Hong Kong Polytechnic University Fund(grant numbers:P0033695 (ZVRD),P0013879 (BBWH),P0031950 (ZE1N),P0036469 (CDA8),8B2V); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:Hi-Fi 200021-182060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749899","Automated program repair (APR);generate-and-validate APR;learning-to-rank;repeated learning","Java;Fitting;Computer bugs;Machine learning;Maintenance engineering;Benchmark testing;Context modeling","","2","","84","IEEE","5 Apr 2022","","","IEEE","IEEE Journals"
"PTMTorrent: A Dataset for Mining Open-source Pre-trained Model Packages","W. Jiang; N. Synovic; P. Jajal; T. R. Schorlemmer; A. Tewari; B. Pareek; G. K. Thiruvathukal; J. C. Davis","Purdue University; Loyola University, Chicago; Purdue University; Purdue University; Purdue University; Purdue University; Loyola University, Chicago; Purdue University","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","57","61","Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and fine-tune them for downstream tasks. PTM registries known as “model hubs” support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difficult — there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data.We present an open-source dataset, PTMTorrent, to facilitate the evaluation and understanding of PTM packages. This paper describes the creation, structure, usage, and limitations of the dataset. The dataset includes a snapshot of 5 model hubs and a total of 15,913 PTM packages. These packages are represented in a uniform data schema for cross-hub mining. We describe prior uses of this data and suggest research opportunities for mining using our dataset.The PTMTorrent dataset (v1) is available at: https://app.globus.org/file-manager?origin_id=55e17a6e-9d8f-11ed-a2a2-8383522b48d9&origin_path=%2F%7E%2F.Our dataset generation tools are available on GitHub: https://doi.org/10.5281/zenodo.7570357","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00021","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173952","Open-Source Software;Data Mining;Machine learning;Empirical software engineering","Deep learning;Training;Limiting;Documentation;Metadata;Software;Data mining","","3","","47","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Empirical Study in using Version Histories for Change Risk Classification","M. Kiehn; X. Pan; F. Camci","Advanced Micro Devices, Inc., Canada; Advanced Micro Devices, Inc., Canada; Amazon, Austin, USA","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","58","62","Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816740","Change risk;code ownership;file metrics;machine learning","Measurement;Feature extraction;Predictive models;Computer bugs;Analytical models;Software quality","","","","14","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Verification of Fuzzy Decision Trees","J. H. Good; N. Gisolfi; K. Miller; A. Dubrawski","Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","15 May 2023","2023","49","5","3277","3288","In recent years, there have been major strides in the safety verification of machine learning models such as neural networks and tree ensembles. However, fuzzy decision trees (FDT), also called soft or differentiable decision trees, are yet unstudied in the context of verification. They present unique verification challenges resulting from multiplications of input values; in the simplest case with a piecewise-linear splitting function, an FDT is piecewise-polynomial with degree up to the depth of the tree. We propose an abstraction-refinement algorithm for verification of properties of FDTs. We show that the problem is NP-Complete, like many other machine learning verification problems, and that our algorithm is complete in a finite precision setting. We benchmark on a selection of public data sets against an off-the-shelf SMT solver and a baseline variation of our algorithm that uses a refinement strategy from similar methods for neural network verification, finding the proposed method to be the fastest. Code for our algorithm along with our experiments and demos are available on GitHub at https://github.com/autonlab/fdt_verification.","1939-3520","","10.1109/TSE.2023.3251858","Space Technology Research Institutes; Defense Advanced Research Projects Agency(grant numbers:FA8750-17-2-0130); U.S. Department of Homeland Security Countering Weapons of Mass Destruction Office(grant numbers:HSHQDC-16-X-00001,18-DN-ARI-00031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057998","Decision tree;formal methods;fuzzy set;robustness;verification","Neural networks;Decision trees;Machine learning;Stars;Robustness;Machine learning algorithms;Training","","","","34","IEEE","2 Mar 2023","","","IEEE","IEEE Journals"
"Easy-to-Deploy API Extraction by Multi-Level Feature Embedding and Transfer Learning","S. Ma; Z. Xing; C. Chen; C. Chen; L. Qu; G. Li","Faculty of Information Technology, Monash University, Clayton, VIC, Australia; College of Engineering & Computer Science, Australian National University, Canberra, ACT, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; PricewaterhouseCoopers Firm, Shanghai, China; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Software, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Software Engineering","14 Oct 2021","2021","47","10","2296","2311","Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazetteers) beyond the input texts. We also propose to adopt transfer learning to adapt a source-library-trained model to a target-library, thus reducing the overhead of manual training-data labeling when the software text of multiple programming languages and libraries need to be processed. We conduct extensive experiments with six libraries of four programming languages which support diverse functionalities and have different API-naming and API-mention characteristics. Our experiments investigate the performance of our neural architecture for API extraction in informal software texts, the importance of different features, the effectiveness of transfer learning. Our results confirm not only the superior performance of our neural architecture than existing machine learning based methods for API extraction in informal software texts, but also the easy-to-deploy characteristic of our neural architecture.","1939-3520","","10.1109/TSE.2019.2946830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865646","API extraction;CNN;word embedding;LSTM;transfer learning","Libraries;Feature extraction;Machine learning;Software;Computer architecture;Training data;Manuals","","23","","61","IEEE","14 Oct 2019","","","IEEE","IEEE Journals"
"FutureWare: Designing a Middleware for Anticipatory Mobile Computing","A. Mehrotra; V. Pejovic; M. Musolesi","Department of Geography, University College London, London, United Kingdom; Faculty of Computing and Information Science, University of Ljubljana, Ljubljana, Slovenia; Department of Geography, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","14 Oct 2021","2021","47","10","2107","2124","Ubiquitous computing is moving from context-awareness to context-prediction. In order to build truly anticipatory systems developers have to deal with many challenges, from multimodal sensing to modeling context from sensed data, and, when necessary, coordinating multiple predictive models across devices. Novel expressive programming interfaces and paradigms are needed for this new class of mobile and ubiquitous applications. In this paper we present FutureWare, a middleware for seamless development of mobile applications that rely on context prediction. FutureWare exposes an expressive API to lift the burden of mobile sensing, individual and group behavior modeling, and future context querying, from an application developer. We implement FutureWare as an Android library, and through a scenario-based testing and a demo app we show that it represents an efficient way of supporting anticipatory applications, reducing the necessary coding effort by two orders of magnitude.","1939-3520","","10.1109/TSE.2019.2943554","Engineering and Physical Sciences Research Council(grant numbers:EP/P016278/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847471","Anticipatory computing;mobile middleware;mobile sensing;prediction","Middleware;Sensors;Predictive models;Context modeling;Machine learning;Servers;Mobile applications","","1","","64","IEEE","24 Sep 2019","","","IEEE","IEEE Journals"
"Quality Questions Need Quality Code: Classifying Code Fragments on Stack Overflow","M. Duijn; A. Kucera; A. Bacchelli","Delft University of Technology, The Netherlands; Czech Technical University in Prague, Czech Republic; Delft University of Technology, The Netherlands","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","410","413","Stack Overflow (SO) is a question and answers (Q&A) web platform on software development that is gaining in popularity. With increasing popularity often comes a very unwelcome side effect: A decrease in the average quality of a post. To keep Q&A websites like SO useful it is vital that this side effect is countered. Previous research proved to be reasonably successful in using properties of questions to help identify low quality questions to be later reviewed and improved. We present an approach to improve the classification of high and low quality questions based on a novel source of information: the analysis of the code fragments in SO questions. We show that we get similar performance to classification based on a wider set of metrics thus potentially reaching a better overall classification.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180105","","Measurement;Correlation;Java;Classification algorithms;Algorithm design and analysis;Accuracy;Decision trees","","30","","10","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Mining StackOverflow to Filter Out Off-Topic IRC Discussion","S. A. Chowdhury; A. Hindle","Department of Computing Science, University of Alberta, Edmonton, Canada; Department of Computing Science, University of Alberta, Edmonton, Canada","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","422","425","Internet Relay Chat (IRC) is a commonly used tool by Open Source developers. Developers use IRC channels to discuss programming related problems, but much of the discussion is irrelevant and off-topic. Essentially if we treat IRC discussions like email messages, and apply spam filtering, we can try to filter out the spam (the off-topic discussions) from the ham (the programming discussions). Yet we need labelled data that unfortunately takes time to curate. To avoid costly cur ration in order to filter out off-topic discussions, we need positive and negative data-sources. On-line discussion forums, such as Stack Overflow, are very effective for solving programming problems. By engaging in open-data, Stack Overflow data becomes a powerful source of labelled text regarding programming. This work shows that we can train classifiers using Stack Overflow posts as positive examples of on-topic programming discussion. You Tube video comments, notorious for their lack of quality, serve as training set of off-topic discussion. By exploiting these datasets, accurate classifiers can be built, tested and evaluated that require very little effort for end-users to deploy and exploit.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180108","IRC message filtering;Text classification;Stackoverflow mining;YouTube video comments;SVM;Naive Bayes","Support vector machines;Programming;YouTube;Training;Accuracy;Mathematical model;Data mining","","26","","13","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Impact of Continuous Integration on Code Reviews","M. M. Rahman; C. K. Roy","Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","499","502","Peer code review and continuous integration often interleave with each other in the modern software quality management. Although several studies investigate how non-technical factors (e.g., reviewer workload), developer participation and even patch size affect the code review process, the impact of continuous integration on code reviews is not yet properly understood. In this paper, we report an exploratory study using 578K automated build entries where we investigate the impact of automated builds on the code reviews. Our investigation suggests that successfully passed builds are more likely to encourage new code review participation in a pull request. Frequently built projects are found to be maintaining a steady level of reviewing activities over the years, which was quite missing from the rarely built projects. Experiments with 26,516 automated build entries reported that our proposed model can identify 64% of the builds that triggered new code reviews later.","","978-1-5386-1544-7","10.1109/MSR.2017.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962406","Automated build status;build frequency;code review quality;review participation","Correlation;Software quality;Manuals;Encoding;Standards;Frequency measurement","","12","","10","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Search4Code: Code Search Intent Classification Using Weak Supervision","N. Rao; C. Bansal; J. Guan",Microsoft Research; Microsoft Research; Microsoft,"2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","575","579","Developers use search for various tasks such as finding code, documentation, debugging information, etc. In particular, web search is heavily used by developers for finding code examples and snippets during the coding process. Recently, natural language based code search has been an active area of research. However, the lack of real-world large-scale datasets is a significant bottleneck. In this work, we propose a weak supervision based approach for detecting code search intent in search queries for C# and Java programming languages. We evaluate the approach against several baselines on a real-world dataset comprised of over 1 million queries mined from Bing web search engine and show that the CNN based model can achieve an accuracy of 77% and 76% for C# and Java respectively. Furthermore, we are also releasing Search4Code, the first large-scale real-world dataset of code search queries mined from Bing web search engine. We hope that the dataset will aid future research on code search.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463144","code search;weak supervision","Java;Natural languages;Documentation;Debugging;Software;Encoding;C# languages","","6","","30","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"SecretBench: A Dataset of Software Secrets","S. K. Basak; L. Neil; B. Reaves; L. Williams","North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","347","351","According to GitGuardian’s monitoring of public GitHub repositories, the exposure of secrets (API keys and other credentials) increased two-fold in 2021 compared to 2020, totaling more than six million secrets. However, no benchmark dataset is publicly available for researchers and tool developers to evaluate secret detection tools that produce many false positive warnings. The goal of our paper is to aid researchers and tool developers in evaluating and improving secret detection tools by curating a benchmark dataset of secrets through a systematic collection of secrets from open-source repositories. We present a labeled dataset of source codes containing 97,479 secrets (of which 15,084 are true secrets) of various secret types extracted from 818 public GitHub repositories. The dataset covers 49 programming languages and 311 file types.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00053","National Science Foundation; Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174157","software_secrets;credentials;github","Computer languages;Systematics;Source coding;Benchmark testing;Feature extraction;Software;Data mining","","1","","34","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Dazzle: Using Optimized Generative Adversarial Networks to Address Security Data Class Imbalance Issue","R. Shu; T. Xia; L. Williams; T. Menzies","North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","144","155","Background: Machine learning techniques have been widely used and demonstrate promising performance in many software security tasks such as software vulnerability prediction. However, the class ratio within software vulnerability datasets is often highly imbalanced (since the percentage of observed vulnerability is usually very low). Goal: To help security practitioners address software security data class imbalanced issues and further help build better prediction models with resampled datasets. Method: We introduce an approach called Dazzle which is an optimized version of conditional Wasserstein Generative Adversarial Networks with gradient penalty (cWGAN-GP). Dazzle explores the architecture hyperparameters of cWGAN-GP with a novel optimizer called Bayesian Optimization. We use Dazzle to generate minority class samples to resample the original imbalanced training dataset. Results: We evaluate Dazzle with three software security datasets, i.e., Moodle vulnerable files, Ambari bug reports, and JavaScript function code. We show that Dazzle is practical to use and demonstrates promising improvement over existing state-of-the-art oversampling techniques such as SMOTE (e.g., with an average of about 60% improvement rate over SMOTE in recall among all datasets). Conclusion: Based on this study, we would suggest the use of optimized GANs as an alternative method for security vulnerability data class imbalanced issues.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796337","Security Vulnerability Prediction;Class Imbalance;Hyperparameter Optimization;Generative Adversarial Networks","Training;Computer architecture;Predictive models;Generative adversarial networks;Software;Data models;Security","","3","","69","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Built to Last or Built Too Fast? Evaluating Prediction Models for Build Times","E. Bisong; E. Tran; O. Baysal","Department of Computer Science, Carleton University, Ottawa, Canada; Carleton University, Ottawa, ON, CA; Department of Computer Science, Carleton University, Ottawa, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","487","490","Automated builds are integral to the Continuous Integration (CI) software development practice. In CI, developers are encouraged to integrate early and often. However, long build times can be an issue when integrations are frequent. This research focuses on finding a balance between integrating often and keeping developers productive. We propose and analyze models that can predict the build time of a job. Such models can help developers to better manage their time and tasks. Also, project managers can explore different factors to determine the best setup for a build job that will keep the build wait time to an acceptable level. Software organizations transitioning to CI practices can use the predictive models to anticipate build times before CI is implemented. The research community can modify our predictive models to further understand the factors and relationships affecting build times.","","978-1-5386-1544-7","10.1109/MSR.2017.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962403","Machine learning;continuous integration;builds;build time","Predictive models;Software;Data models;Tools;Prediction algorithms;Computational modeling;Switches","","8","","10","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"GiveMeLabeledIssues: An Open Source Issue Recommendation System","J. Vargovich; F. Santos; J. Penney; M. A. Gerosa; I. Steinmacher","School of Informatics, Computing, and Cyber Systems, Northern Arizona University, Flagstaff, United States; School of Informatics, Computing, and Cyber Systems, Northern Arizona University, Flagstaff, United States; School of Informatics, Computing, and Cyber Systems, Northern Arizona University, Flagstaff, United States; School of Informatics, Computing, and Cyber Systems, Northern Arizona University, Flagstaff, United States; School of Informatics, Computing, and Cyber Systems, Northern Arizona University, Flagstaff, United States","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","402","406","Developers often struggle to navigate an Open Source Software (OSS) project’s issue-tracking system and find a suitable task. Proper issue labeling can aid task selection, but current tools are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines project repositories and labels issues based on the skills required to solve them. We leverage the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills. GiveMeLabeledIssues facilitates matching developers’ skills to tasks, reducing the burden on project maintainers. The tool obtained a precision of 83.9% when predicting the API domains involved in the issues. The replication package contains instructions on executing the tool and including new projects. A demo video is available at https://www.youtube.com/watch?v=ic2quUue7i8","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00061","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174101","Open Source Software;Machine Learning;Label;Tag;Task;Issue Tracker","Navigation;Databases;Computer bugs;User interfaces;Labeling;Data mining;Task analysis","","2","","26","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Detecting Privacy-Sensitive Code Changes with Language Modeling","G. Demirci; V. Murali; I. Ahmad; R. Rao; G. A. Aye","Meta Platforms, Inc, Menlo Park, CA, USA; Meta Platforms, Inc, Menlo Park, CA, USA; Meta Platforms, Inc, Menlo Park, CA, USA; Meta Platforms, Inc, Menlo Park, CA, USA; Meta Platforms, Inc, Menlo Park, CA, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","761","762","At Meta, we work to incorporate privacy-by-design into all of our products and keep user information secure. We have created an ML model that detects code changes (“diffs”) that have privacy-sensitive implications. At our scale of tens of thousands of engineers creating hundreds of thousands of diffs each month, we use automated tools for detecting such diffs. Inspired by recent studies on detecting defects [2], [3], [5] and security vulnerabilities [4], [6], [7], we use techniques from natural language processing to build a deep learning system for detecting privacy-sensitive code.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796200","privacy;software;repository;change;detection;machine learning;privacy sensitive;neural networks","Deep learning;Codes;Databases;Static analysis;Manuals;Feature extraction;Software","","","","7","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Who's This? Developer Identification Using IDE Event Data","J. Wilkie; Z. Al Halabi; A. Karaoglu; J. Liao; G. Ndungu; C. Ragkhitwetsagul; M. Paixao; J. Krinke","CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK; CREST, University College London, London, UK","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","90","93","This paper presents a technique to identify a developer based on their IDE event data. We exploited the KaVE data set which recorded IDE activities from 85 developers with 11M events. We found that using an SVM with a linear kernel on raw event count outperformed k-NN in identifying developers with an accuracy of 0.52. Moreover, after setting the optimal number of events and sessions to train the classifier, we achieved a higher accuracy of 0.69 and 0.71 respectively. The findings shows that we can identify developers based on their IDE event data. The technique can be expanded further to group similar developers for IDE feature recommendations.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595187","Machine Learning;Integrated Development Environment;Recommendation Systems","Support vector machines;Classification algorithms;Training;Testing;Data mining;Kernel","","","","10","","30 Dec 2018","","","IEEE","IEEE Conferences"
"On the Costs and Profit of Software Defect Prediction","S. Herbold","Institute of Computer Science, University of Goettingen, Goettingen, Germany","IEEE Transactions on Software Engineering","11 Nov 2021","2021","47","11","2617","2631","Defect prediction can be a powerful tool to guide the use of quality assurance resources. However, while lots of research covered methods for defect prediction as well as methodological aspects of defect prediction research, the actual cost saving potential of defect prediction is still unclear. Within this article, we close this research gap and formulate a cost model for software defect prediction. We derive mathematically provable boundary conditions that must be fulfilled by defect prediction models such that there is a positive profit when the defect prediction model is used. Our cost model includes aspects like the costs for quality assurance, the costs of post-release defects, the possibility that quality assurance fails to reveal predicted defects, and the relationship between software artifacts and defects. We initialize the cost model using different assumptions, perform experiments to show trends of the behavior of costs on real projects. Our results show that the unrealistic assumption that defects only affect a single software artifact, which is a standard practice in the defect prediction literature, leads to inaccurate cost estimations. Moreover, the results indicate that thresholds for machine learning metrics are also not suited to define success criteria for software defect prediction.","1939-3520","","10.1109/TSE.2019.2957794","DFG(grant numbers:402774445); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924628","Defect prediction;costs;return on investment","Predictive models;Software;Quality assurance;Measurement;Mathematical model;Machine learning;Computational modeling","","20","","47","IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Identifying Design and Requirement Self-Admitted Technical Debt Using N-gram IDF","S. Wattanakriengkrai; R. Maipradit; H. Hata; M. Choetkiertikul; T. Sunetnanta; K. Matsumoto","Mahidol University, Thailand; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Mahidol University, Thailand; Mahidol University, Thailand; Nara Institute of Science and Technology, Japan","2018 9th International Workshop on Empirical Software Engineering in Practice (IWESEP)","7 Mar 2019","2018","","","7","12","In software projects, technical debt takes place when a developer adopting a trivial solution containing quick and easy shortcuts to implement over a suitable solution that can take a longer time to solve a problem. This can cause major additional costs leading to negative impacts for software maintenance since those shortcuts might need to be reworked in the future. Detecting technical debt early can help a team cope with those risks. In this paper, we focus on Self-Admitted Technical Debt (SATD) that is a debt intentionally produced by developers. We propose an automated model to identify two most common types of self-admitted technical debt, requirement and design debt, from source code comments. We combine N-gram IDF and auto-sklearn machine learning to build the model. With the empirical evaluation on ten projects, our approach outperform the baseline method by improving the performance over 20% when identifying requirement self-admitted technical debt and achieving an average F1-score of 64% when identifying design self-admitted technical debt.","2573-2021","978-1-7281-0439-3","10.1109/IWESEP.2018.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661216","Self-admitted-technical-debt;N-gram-IDF;Automated-machine-learning;Comment-classification","Machine learning;Dictionaries;Sparse matrices;Software;Manuals;Inspection;Training","","17","","18","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"What is the Vocabulary of Flaky Tests?","G. Pinto; B. Miranda; S. Dissanayake; M. D'Amorim; C. Treude; A. Bertolino","Federal University of Pará, Belém, Brazil; Federal University of Pernambuco, Recife, Brazil; University of Adelaide, Adelaide, Australia; Federal University of Pernambuco, Recife, Brazil; University of Adelaide, Adelaide, Australia; ISTI — CNR, Pisa, Italy","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","492","502","Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites. We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148770","Test flakiness;Regression testing;Text classification","Support vector machines;Vocabulary;Java;Machine learning algorithms;Codes;Source coding;Programming","","31","","32","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Whistleblowing and Tech on Twitter","L. Duits; I. Kashyap; J. Bekkink; K. Aslam; E. Guzmán",Vrije Universiteit Amsterdam; Vrije Universiteit Amsterdam; Vrije Universiteit Amsterdam; Vrije Universiteit Amsterdam; Vrije Universiteit Amsterdam,"2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","296","308","From airports to banks, healthcare, space crafts, and even amazon services, technology impacts almost every aspect of today’s life. If wrongdoings occur within or in relation to technology, they can have big implications on individuals, groups of people, or society as a whole. Whistleblowers are insiders who expose such wrongdoings— eventually stopping misconducts, such as fraud, endangerment to public health and safety, or damage to the environment. Twitter is a microblogging service that allows millions of users to share their views with people distributed all over the world on a daily basis. Tweets have the potential to contain useful information about whistleblowing in tech, from the general public and whistleblowers. However, until now this point has not been researched.To fill this gap, we conducted an exploratory study on technology-related whistleblowing tweets by manually analysing tweets, utilising descriptive statistics, and machine learning techniques. We mined 7,400 tweets from whistleblowers themselves, as well as news and opinions about certain whistleblowers and whistleblowing cases. Although our results show that only 30% of the tweets in our sample dataset (obtained through specific search terms) contained relevant information about whistleblowing in technology, our analysis shows that tweets provide valuable information for both researchers and companies to understand the public opinion regarding whistleblowing cases. Furthermore, we found that machine learning techniques are promising means for extracting information about whistleblowing in tech from the vast stream of tweets.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174094","whistleblowing;twitter;ethics in tech","Social networking (online);Filtering;Blogs;Machine learning;Companies;Software;Safety","","1","","76","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Empirical Validation of Automated Vulnerability Curation and Characterization","A. Okutan; P. Mell; M. Mirakhorli; I. Khokhlov; J. C. S. Santos; D. Gonzalez; S. Simmons","Leidos, Reston, VA, USA; National Institute of Standards and Technology, Gaithersburg, MD, USA; Department of Software Engineering, Rochester Institute of Technology, Rochester, NY, USA; Sacred Heart University, Fairfield, CT, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Software Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Software Engineering, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Software Engineering","15 May 2023","2023","49","5","3241","3260","Prior research has shown that public vulnerability systems such as US National Vulnerability Database (NVD) rely on a manual, time-consuming, and error-prone process which has led to inconsistencies and delays in releasing final vulnerability results. This work provides an approach to curate vulnerability reports in real-time and map textual vulnerability reports to machine readable structured vulnerability attribute data. Designed to support the time consuming human analysis done by vulnerability databases, the system leverages the Common Vulnerabilities and Exposures (CVE) list of vulnerabilities and the vulnerability attributes described by the National Institute of Standards and Technology (NIST) Vulnerability Description Ontology (VDO) framework. Our work uses Natural Language Processing (NLP), Machine Learning (ML) and novel Information Theoretical (IT) methods to provide automated techniques for near real-time publishing, and characterization of vulnerabilities using 28 attributes in 5 domains. Experiment results indicate that vulnerabilities can be evaluated up to 95 hours earlier than using manual methods, they can be characterized with F-Measure values over 0.9, and the proposed automated approach could save up to 47% of the time spent for CVE characterization.","1939-3520","","10.1109/TSE.2023.3250479","U.S. Department of Homeland Security(grant numbers:70RSAT19CB0000020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056768","CVE;NIST vulnerability description ontology;software vulnerability;vulnerability characterization","Security;NIST;Databases;Virtual machine monitors;Software;Feature extraction;Codes","","1","","71","IEEE","28 Feb 2023","","","IEEE","IEEE Journals"
"Does Configuration Encoding Matter in Learning Software Performance? An Empirical Study on Encoding Schemes","J. Gong; T. Chen","Loughborough University, Loughborough, UK; Loughborough University, Loughborough, UK","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","482","494","Learning and predicting the performance of a configurable software system helps to provide better quality assurance. One important engineering decision therein is how to encode the configuration into the model built. Despite the presence of different encoding schemes, there is still little understanding of which is better and under what circumstances, as the community often relies on some general beliefs that inform the decision in an ad-hoc manner. To bridge this gap, in this paper, we empirically compared the widely used encoding schemes for software performance learning, namely label, scaled label, and one-hot encoding. The study covers five systems, seven models, and three encoding schemes, leading to 105 cases of investigation. Our key findings reveal that: (1) conducting trial-and-error to find the best encoding scheme in a case by case manner can be rather expensive, requiring up to 400+ hours on some models and systems; (2) the one-hot encoding often leads to the most accurate results while the scaled label encoding is generally weak on accuracy over different models; (3) conversely, the scaled label encoding tends to result in the fastest training time across the models/systems while the one-hot encoding is the slowest; (4) for all models studied, label and scaled label encoding often lead to relatively less biased outcomes between accuracy and training time, but the paired model varies according to the system. We discuss the actionable suggestions derived from our findings, hoping to provide a better understanding of this topic for the community. To promote open science, the data and code of this work can be publicly accessed at https://github.com/ideas-Iabo/MSR2022-encoding-study.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796199","Encoding Scheme;Machine Learning;Software Engineering;Per-formance Prediction;Performance Learning;Configurable Software","Training;Bridges;Systematics;Quality assurance;Neural networks;Software performance;Predictive models","","1","","56","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Predicting Vulnerable Software Components via Text Mining","R. Scandariato; J. Walden; A. Hovsepyan; W. Joosen","IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; Department of Computer Science, Northern Kentucky University, Highland Heights, KY; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium","IEEE Transactions on Software Engineering","9 Oct 2014","2014","40","10","993","1006","This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.","1939-3520","","10.1109/TSE.2014.2340398","EU FP7(grant numbers:NESSoS); Research Fund KU Leuven; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6860243","Vulnerabilities;prediction model;machine learning","Software;Predictive models;Measurement;Security;Androids;Humanoid robots;Text mining","","224","","40","IEEE","18 Jul 2014","","","IEEE","IEEE Journals"
"Transfer Learning Across Variants and Versions: The Case of Linux Kernel Size","H. Martin; M. Acher; J. A. Pereira; L. Lesoil; J. -M. Jézéquel; D. E. Khelladi","Inria, CNRS, IRISA, University Rennes, Rennes, France; Inria, CNRS, IRISA, University Rennes, Rennes, France; PUC-Rio, Rio de Janeiro, RJ, Brazil; Inria, CNRS, IRISA, University Rennes, Rennes, France; Inria, CNRS, IRISA, University Rennes, Rennes, France; Inria, CNRS, IRISA, University Rennes, Rennes, France","IEEE Transactions on Software Engineering","11 Nov 2022","2022","48","11","4274","4290","With large scale and complex configurable systems, it is hard for users to choose the right combination of options (i.e., configurations) in order to obtain the wanted trade-off between functionality and performance goals such as speed or size. Machine learning can help in relating these goals to the configurable system options, and thus, predict the effect of options on the outcome, typically after a costly training step. However, many configurable systems evolve at such a rapid pace that it is impractical to retrain a new model from scratch for each new version. In this paper, we propose a new method to enable transfer learning of binary size predictions among versions of the same configurable system. Taking the extreme case of the Linux kernel with its $\approx 14,500$≈14,500 configuration options, we first investigate how binary size predictions of kernel size degrade over successive versions. We show that the direct reuse of an accurate prediction model from 2017 quickly becomes inaccurate when Linux evolves, up to a 32% mean error by August 2020. We thus propose a new approach for transfer evolution-aware model shifting (tEAMS). It leverages the structure of a configurable system to transfer an initial predictive model towards its future versions with a minimal amount of extra processing for each version. We show that tEAMS vastly outperforms state of the art approaches over the 3 years history of Linux kernels, from 4.13 to 5.8.","1939-3520","","10.1109/TSE.2021.3116768","Agence Nationale de la Recherche(grant numbers:ANR-17-CE25-0010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555247","Software product line;software evolution;machine learning;transfer learning;performance prediction","Kernel;Linux;Transfer learning;Predictive models;Codes;Training;Software systems","","7","","104","IEEE","30 Sep 2021","","","IEEE","IEEE Journals"
"A Search-Based Testing Approach for Deep Reinforcement Learning Agents","A. Zolfagharian; M. Abdellatif; L. C. Briand; M. Bagherzadeh; R. S","School of Electrical Engineering and Computer Science (EECS), University of Ottawa, Ottawa, ON, Canada; Software and Information Technology Engineering Department, École de Technologie Supérieure, Montreal, QC, Canada; School of Electrical Engineering and Computer Science (EECS), University of Ottawa, Ottawa, ON, Canada; Cisco, Ottawa, ON, Canada; Department of Research and Development, General Motors, Warren, MI, USA","IEEE Transactions on Software Engineering","17 Jul 2023","2023","49","7","3715","3735","Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agents’ policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.","1939-3520","","10.1109/TSE.2023.3269804","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107813","Genetic algorithm;machine learning;reinforcement learning;state abstraction;testing","Testing;Reinforcement learning;Safety;Deep learning;Closed box;Training;Genetic algorithms","","5","","85","CCBY","25 Apr 2023","","","IEEE","IEEE Journals"
"Lightweight Assessment of Test-Case Effectiveness Using Source-Code-Quality Indicators","G. Grano; F. Palomba; H. C. Gall","University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland","IEEE Transactions on Software Engineering","16 Apr 2021","2021","47","4","758","774","Test cases are crucial to help developers preventing the introduction of software faults. Unfortunately, not all the tests are properly designed or can effectively capture faults in production code. Some measures have been defined to assess test-case effectiveness: the most relevant one is the mutation score, which highlights the quality of a test by generating the so-called mutants, i.e., variations of the production code that make it faulty and that the test is supposed to identify. However, previous studies revealed that mutation analysis is extremely costly and hard to use in practice. The approaches proposed by researchers so far have not been able to provide practical gains in terms of mutation testing efficiency. This leaves the problem of efficiently assessing test-case effectiveness as still open. In this paper, we investigate a novel, orthogonal, and lightweight methodology to assess test-case effectiveness: in particular, we study the feasibility to exploit production and test-code-quality indicators to estimate the mutation score of a test case. We first select a set of 67 factors and study their relation with test-case effectiveness. Then, we devise a mutation score estimation model exploiting such factors and investigate its performance as well as its most relevant features. The key results of the study reveal that our estimation model only based on static features has 86 percent of both F-Measure and AUC-ROC. This means that we can estimate the test-case effectiveness, using source-code-quality indicators, with high accuracy and without executing the tests. As a consequence, we can provide a practical approach that is beyond the typical limitations of current mutation testing techniques.","1939-3520","","10.1109/TSE.2019.2903057","Swiss National Science Foundation (SNSF)(grant numbers:200021_166275,PP00P2_170529); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658120","Automated software testing;mutation testing;software quality","Testing;Production;Estimation;Measurement;Predictive models;Machine learning;Computational modeling","","25","","108","IEEE","4 Mar 2019","","","IEEE","IEEE Journals"
"Exploring Word Embedding Techniques to Improve Sentiment Analysis of Software Engineering Texts","E. Biswas; K. Vijay-Shanker; L. Pollock","Computer and Information Sciences, University of Delaware, Newark, United States; Computer and Information Sciences, University of Delaware, Newark, United States; Computer and Information Sciences, University of Delaware, Newark, United States","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","68","78","Sentiment analysis (SA) of text-based software artifacts is increasingly used to extract information for various tasks including providing code suggestions, improving development team productivity, giving recommendations of software packages and libraries, and recommending comments on defects in source code, code quality, possibilities for improvement of applications. Studies of state-of-the-art sentiment analysis tools applied to software-related texts have shown varying results based on the techniques and training approaches. In this paper, we investigate the impact of two potential opportunities to improve the training for sentiment analysis of SE artifacts in the context of the use of neural networks customized using the Stack Overflow data developed by Lin et al. We customize the process of sentiment analysis to the software domain, using software domain-specific word embeddings learned from Stack Overflow (SO) posts, and study the impact of software domain-specific word embeddings on the performance of the sentiment analysis tool, as compared to generic word embeddings learned from Google News. We find that the word embeddings learned from the Google News data performs mostly similar and in some cases better than the word embeddings learned from SO posts. We also study the impact of two machine learning techniques, oversampling and undersampling of data, on the training of a sentiment classifier for handling small SE datasets with a skewed distribution. We find that oversampling alone, as well as the combination of oversampling and undersampling together, helps in improving the performance of a sentiment classifier.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816816","Sentiment Analysis;Software Engineering;Word Embeddings","Sentiment analysis;Tools;Software;Training;Neural networks;Software engineering;Machine learning","","14","","37","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"The Why, When, What, and How About Predictive Continuous Integration: A Simulation-Based Investigation","B. Liu; H. Zhang; W. Ma; G. Li; S. Li; H. Shen","State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Jiangsu, China; Peter Faber Business School, Australian Catholic University, Sydney, NSW, Australia","IEEE Transactions on Software Engineering","12 Dec 2023","2023","49","12","5223","5249","Continuous Integration (CI) enables developers to detect defects early and thus reduce lead time. However, the high frequency and long duration of executing CI have a detrimental effect on this practice. Existing studies have focused on using CI outcome predictors to reduce frequency. Since there is no reported project using predictive CI, it is difficult to evaluate its economic impact. This research aims to investigate predictive CI from a process perspective, including why and when to adopt predictors, what predictors to be used, and how to practice predictive CI in real projects. We innovatively employ Software Process Simulation to simulate a predictive CI process with a Discrete-Event Simulation (DES) model and conduct simulation-based experiments. We develop the Rollback-based Identification of Defective Commits (RIDEC) method to account for the negative effects of false predictions in simulations. Experimental results show that: 1) using predictive CI generally improves the effectiveness of CI, reducing time costs by up to 36.8% and the average waiting time before executing CI by 90.5%; 2) the time-saving varies across projects, with higher commit frequency projects benefiting more; and 3) predictor performance does not strongly correlate with time savings, but the precision of both failed and passed predictions should be paid more attention. Simulation-based evaluation helps identify overlooked aspects in existing research. Predictive CI saves time and resources, but improved prediction performance has limited cost-saving benefits. The primary value of predictive CI lies in providing accurate and quick feedback to developers, aligning with the goal of CI.","1939-3520","","10.1109/TSE.2023.3330510","National Natural Science Foundation of China(grant numbers:62072227,62202219,62302210,72372070); Jiangsu Provincial Key Research and Development Program(grant numbers:BE2021002-2); National Key Research and Development Program of China(grant numbers:2019YFE0105500); Innovation Project and Overseas Open Projects of State Key Laboratory for Novel Software Technology (Nanjing University)(grant numbers:ZZKT2022A25,KFKT2022A09,KFKT2023A09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10315109","Continuous integration;machine learning;software process simulation;discrete-event simulation","Software;Costs;Servers;Testing;Machine learning;Codes;Surveys","","","","64","IEEE","10 Nov 2023","","","IEEE","IEEE Journals"
"On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models","T. H. M. Le; M. A. Babar","CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide, Adelaide, Australia","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","621","633","Many studies have developed Machine Learning (ML) approaches to detect Software Vulnerabilities (SVs) in functions and fine-grained code statements that cause such SVs. However, there is little work on leveraging such detection outputs for data-driven SV assessment to give information about exploitability, impact, and severity of SVs. The information is important to understand SVs and prioritize their fixing. Using large-scale data from 1,782 functions of 429 SVs in 200 real-world projects, we investigate ML models for automating function-level SV assessment tasks, i.e., predicting seven Common Vulnerability Scoring System (CVSS) metrics. We particularly study the value and use of vulnerable statements as inputs for developing the assessment models because SVs in functions are originated in these statements. We show that vulnerable statements are 5.8 times smaller in size, yet exhibit 7.5-114.5% stronger assessment performance (Matthews Correlation Coefficient (MCC)) than non-vulnerable statements. Incorporating context of vulnerable statements further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score). Overall, we provide the initial yet promising ML-based baselines for function-level SV assessment, paving the way for further research in this direction.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796209","Security Vulnerability;Vulnerability Assessment;Machine Learning;Mining Software Repositories","Measurement;Correlation coefficient;Codes;Machine learning;Predictive models;Software;Data models","","5","","93","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Using K-core Decomposition on Class Dependency Networks to Improve Bug Prediction Model's Practical Performance","Y. Qu; Q. Zheng; J. Chi; Y. Jin; A. He; D. Cui; H. Zhang; T. Liu","Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; School of Computer Science, Xi'an University of Posts and Telecommunications, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE Transactions on Software Engineering","11 Feb 2021","2021","47","2","348","366","In recent years, Complex Network theory and graph algorithms have been proved to be effective in predicting software bugs. On the other hand, as a widely-used algorithm in Complex Network theory, k-core decomposition has been used in software engineering domain to identify key classes. Intuitively, key classes are more likely to be buggy since they participate in more functions or have more interactions and dependencies. However, there is no existing research uses k-core decomposition to analyze software bugs. To fill this gap, we first use k-core decomposition on Class Dependency Networks to analyze software bug distribution from a new perspective. An interesting and widely existed tendency is observed: for classes in k-cores with larger k values, there is a stronger possibility for them to be buggy. Based on this observation, we then propose a simple but effective equation named as top-core which improves the order of classes in the suspicious class list produced by effort-aware bug prediction models. Based on an empirical study on 18 open-source Java systems, we show that the bug prediction models' performances are significantly improved in 85.2 percent experiments in the cross-validation scenario and in 80.95 percent experiments in the forward-release scenario, after using top-core. The models' average performances are improved by 11.5 and 12.6 percent, respectively. It is concluded that the proposed top-core equation can help the testers or code reviewers locate the real bugs more quickly and easily in software bug prediction practices.","1939-3520","","10.1109/TSE.2019.2892959","National Key Research and Development Program of China(grant numbers:2016YFB0800202); National Natural Science Foundation of China(grant numbers:61602369,61632015,61772408,U1766215,61721002,61833015); Ministry of Education Innovation Research Team(grant numbers:IRT_17R86); Shaanxi Province postdoctoral research project funding(grant numbers:2016BSHEDZZ108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611396","Bug prediction;software defects;complex network;class dependency network;effort-aware bug prediction","Computer bugs;Software;Mathematical model;Predictive models;Complex networks;Prediction algorithms;Software algorithms","","30","","66","IEEE","13 Jan 2019","","","IEEE","IEEE Journals"
"Challenging Machine Learning-Based Clone Detectors via Semantic-Preserving Code Transformations","W. Zhang; S. Guo; H. Zhang; Y. Sui; Y. Xue; Y. Xu","University of Science and Technology of China, Hefei, Anhui, China; Baidu Security, Beijing, China; University of Newcastle, Callaghan, NSW, Australia; University of New South Wales, Sydney, NSW, Australia; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Software Engineering","15 May 2023","2023","49","5","3052","3070","Software clone detection identifies similar or identical code snippets. It has been an active research topic that attracts extensive attention over the last two decades. In recent years, machine learning (ML) based detectors, especially deep learning-based ones, have demonstrated impressive capability on clone detection. It seems that this longstanding problem has already been tamed owing to the advances in ML techniques. In this work, we would like to challenge the robustness of the recent ML-based clone detectors through code semantic-preserving transformations. We first utilize fifteen simple code transformation operators combined with commonly-used heuristics (i.e., Random Search, Genetic Algorithm, and Markov Chain Monte Carlo) to perform equivalent program transformation. Furthermore, we propose a deep reinforcement learning-based sequence generation (DRLSG) strategy to effectively guide the search process of generating clones that could escape from the detection. We then evaluate the ML-based detectors with the pairs of original and generated clones. We realize our method in a framework named CloneGen (stands for Clone Generator). CloneGen In evaluation, we challenge the three state-of-the-art ML-based detectors and four traditional detectors with the code clones after semantic-preserving transformations via the aid of CloneGen. Surprisingly, our experiments show that, despite the notable successes achieved by existing clone detectors, the ML models inside these detectors still cannot distinguish numerous clones produced by the code transformations in CloneGen. In addition, adversarial training of ML-based clone detectors using clones generated by CloneGen can improve their robustness and accuracy. Meanwhile, compared with the commonly-used heuristics, the DRLSG strategy has shown the best effectiveness in generating code clones to decrease the detection accuracy of the ML-based detectors. Our investigation reveals an explicable but always ignored robustness issue of the latest ML-based detectors. Therefore, we call for more attention to the robustness of these new ML-based detectors.","1939-3520","","10.1109/TSE.2023.3240118","National Natural Science Foundation of China(grant numbers:61972373); Basic Research Program of Jiangsu Province(grant numbers:BK20201192); CAS Pioneer Hundred Talents Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10028657","Clone detection;code transformaiton;semantic clone;machinae learning","Cloning;Codes;Detectors;Semantics;Source coding;Robustness;Training","","3","","88","IEEE","27 Jan 2023","","","IEEE","IEEE Journals"
"FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes","K. Peng; J. Chakraborty; T. Menzies","Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2426","2439","Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. Method: Here we propose ${{\sf FairMask}}$FairMask, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. In our ${{\sf FairMask}}$FairMask approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, ${{\sf FairMask}}$FairMask can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed ${{\sf FairMask}}$FairMask is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. Reproduction Package: In order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.","1939-3520","","10.1109/TSE.2022.3220713","Meta Inc; Laboratory for Analytical Sciences; North Carolina State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951398","Software fairness;explanation;bias mitigation","Measurement;Software;Data models;Predictive models;Social groups;Extrapolation;Software algorithms","","8","","64","IEEE","15 Nov 2022","","","IEEE","IEEE Journals"
"A Data Transfer and Relevant Metrics Matching Based Approach for Heterogeneous Defect Prediction","P. R. Bal; S. Kumar","Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India; Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India","IEEE Transactions on Software Engineering","14 Mar 2023","2023","49","3","1232","1245","Heterogeneous defect prediction (HDP) is a promising research area in the software defect prediction domain to handle the unavailability of the past homogeneous data. In HDP, the prediction is performed using source dataset in which the independent features (metrics) are entirely different than the independent features of target dataset. One important assumption in machine learning is that independent features of the source and target datasets should be relevant to each other for better prediction accuracy. However, these assumptions do not generally hold in HDP. Further in HDP, the selected source dataset for a given target dataset may be of small size causing insufficient training. To resolve these issues, we have proposed a novel heterogeneous data preprocessing method, namely, Transfer of Data from Target dataset to Source dataset selected using Relevance score (TDTSR), for heterogeneous defect prediction. In the proposed approach, we have used chi-square test to select the relevant metrics between source and target datasets and have performed experiments using proposed approach with various machine learning algorithms. Our proposed method shows an improvement of at least 14% in terms of AUC score in the HDP scenario compared to the existing state of the art models.","1939-3520","","10.1109/TSE.2022.3173678","Ministry of Human Resource Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772411","Heterogeneous defect prediction;heterogeneous metrics;chi square test;random forest;relevant metrics","Measurement;Software;Data models;Predictive models;Kernel;Correlation;Transfer learning","","2","","36","IEEE","10 May 2022","","","IEEE","IEEE Journals"
"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair","Z. Chen; S. Kommrusch; M. Tufano; L. -N. Pouchet; D. Poshyvanyk; M. Monperrus","KTH Royal Institute of Technology, Stockholm, Sweden; Colorado State University, Fort Collins, CO, USA; The College of William and Mary, Williamsburg, VA, USA; Colorado State University, Fort Collins, CO, USA; The College of William and Mary, Williamsburg, VA, USA; KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Software Engineering","16 Sep 2021","2021","47","9","1943","1959","This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SequenceR on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SequenceR captures a wide range of repair operators without any domain-specific top-down design.","1939-3520","","10.1109/TSE.2019.2940179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827954","Program repair;machine learning","Maintenance engineering;Computer bugs;Vocabulary;Training;Natural languages;Benchmark testing","","107","","51","IEEE","10 Sep 2019","","","IEEE","IEEE Journals"
"RefactorScore: Evaluating Refactor Prone Code","K. Jesse; C. Kuhmuench; A. Sawant","Department of Computer Science, University of California Davis, Davis, CA, USA; Siemens Corporation, Princeton, NJ, USA; Endor Labs, Palo Alto, CA, USA","IEEE Transactions on Software Engineering","16 Nov 2023","2023","49","11","5008","5026","We propose RefactorScore, an automatic evaluation metric for code. RefactorScore computes the number of refactor prone locations on each token in a candidate file and maps the occurrences into a quantile to produce a score. RefactorScore is evaluated across 61,735 commits and uses a model called RefactorBERT trained to predict refactors on 1,111,246 commits. Finally, we validate RefactorScore on a set of industry leading projects providing each with a RefactorScore. We calibrate RefactorScore's detection of low quality code with human developers through a human subject study. RefactorBERT, the model driving the scoring mechanism, is capable of predicting defects and refactors predicted by RefDiff 2.0. To our knowledge, our approach, coupled with the use of large scale data for training and validated with human developers, is the first code quality scoring metric of its kind.","1939-3520","","10.1109/TSE.2023.3324613","NSF CCF (SHF)(grant numbers:2107592); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286315","Refactor;automatic evaluation;machine learning;software repositories","Codes;Java;Measurement;Unified modeling language;Predictive models;C++ languages;Computational modeling","","","","76","IEEE","16 Oct 2023","","","IEEE","IEEE Journals"
"Prevent: An Unsupervised Approach to Predict Software Failures in Production","G. Denaro; R. Heydarov; A. Mohebbi; M. Pezzè","Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milano, Italy; Faculty of Informatics, USI Università della Svizzera Italiana (USI), Lugano, Switzerland; Faculty of Informatics, USI Università della Svizzera Italiana (USI), Lugano, Switzerland; Faculty of Informatics, USI Università della Svizzera Italiana, Lugano, Switzerland","IEEE Transactions on Software Engineering","12 Dec 2023","2023","49","12","5139","5153","This paper presents Prevent, a fully unsupervised approach to predict and localize failures in distributed enterprise applications. Software failures in production are unavoidable. Predicting failures and locating failing components online are the first steps to proactively manage faults in production. Many techniques predict failures from anomalous combinations of system metrics with supervised, weakly supervised, and semi-supervised learning models. Supervised approaches require large sets of labelled data not commonly available in large enterprise applications, and address failure types that can be either captured with predefined rules or observed while training supervised models. Prevent integrates the core ingredients of unsupervised approaches into a novel fully unsupervised approach to predict failures and localize failing resources. The results of experimenting with Prevent on a commercially-compliant distributed cloud system indicate that Prevent provides more stable, reliable and timely predictions than supervised learning approaches, without requiring the often impractical training with labeled data.","1939-3520","","10.1109/TSE.2023.3327583","Swiss SNF project ASTERIx: Automatic System TEsting of InteRactive software applications(grant numbers:SNF 200021_178742); Italian PRIN project SISMA(grant numbers:PRIN 201752ENYB); Italian PRIN project BigSistah(grant numbers:PRIN 2022EYX28N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305549","Failure prediction;distributed applications;machine learning","Training;Production;Predictive models;Monitoring;Key performance indicator;Training data;Time measurement","","","","67","CCBY","2 Nov 2023","","","IEEE","IEEE Journals"
"Embedding Java Classes with code2vec: Improvements from Variable Obfuscation","R. Compton; E. Frank; P. Patros; A. Koay","University of Waikato, Hamilton, New Zealand; University of Waikato, Hamilton, New Zealand; University of Waikato, Hamilton, New Zealand; University of Waikato, Hamilton, New Zealand","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","243","253","Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared11https://github.com/basedrhys/obfuscated-code2vec for further ML research on source code.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148715","machine learning;neural networks;code2vec;source code;code obfuscation","Training;Java;Codes;Source coding;Semantics;Software;Security","","17","","22","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Predicting Likelihood of Requirement Implementation within the Planned Iteration: An Empirical Study at IBM","A. Dehghan; A. Neal; K. Blincoe; J. Linaker; D. Damian","Computer Science Department, University of Victoria, BC, Canada; Persistent Systems, Kanata, ON, Canada; Department of Electrical and Computer Engineering, University of Auckland, New Zealand; Computer Science Department, Lund University, Sweden; Computer Science Department, University of Victoria, BC, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","124","134","There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners.","","978-1-5386-1544-7","10.1109/MSR.2017.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962362","mining software repositories;machine learning;completion time prediction;release planning","Software;Predictive models;Computer bugs;Interviews;Planning;History;Computer science","","7","","43","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"A Study of Gender Discussions in Mobile Apps","M. Shahin; M. Zahedi; H. Khalajzadeh; A. Rezaei Nasab","School of Computing Technologies, RMIT University, Melbourne, Australia; School of Computing and Information Systems, University of Melbourne, Melbourne, Australia; School of Information Technology, Deakin University, Melbourne, Australia; School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","598","610","Mobile software apps (""apps"") are one of the prevailing digital technologies that our modern life heavily depends on. A key issue in the development of apps is how to design gender-inclusive apps. Apps that do not consider gender inclusion, diversity, and equality in their design can create barriers (e.g., excluding some of the users because of their gender) for their diverse users. While there have been some efforts to develop gender-inclusive apps, a lack of deep understanding regarding user perspectives on gender may prevent app developers and owners from identifying issues related to gender and proposing solutions for improvement. Users express many different opinions about apps in their reviews, from sharing their experiences, and reporting bugs, to requesting new features. In this study, we aim at unpacking gender discussions about apps from the user perspective by analysing app reviews. We first develop and evaluate several Machine Learning (ML) and Deep Learning (DL) classifiers that automatically detect gender reviews (i.e., reviews that contain discussions about gender). We apply our ML and DL classifiers on a manually constructed dataset of 1,440 app reviews from the Google App Store, composing 620 gender reviews and 820 non-gender reviews. Our best classifier achieves an F1-score of 90.77%. Second, our qualitative analysis of a randomly selected 388 out of 620 gender reviews shows that gender discussions in app reviews revolve around six topics: App Features, Appearance, Content, Company Policy and Censorship, Advertisement, and Community. Finally, we provide some practical implications and recommendations for developing gender-inclusive apps.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174058","Gender;Mobile App;App Review;Machine Learning;Deep Learning","Deep learning;Computer bugs;Companies;Software;Censorship;Mobile applications;Internet","","","","87","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study","J. Castaño; S. Martínez-Fernández; X. Franch; J. Bogner","Universitat Politècnica de Catalunya, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain; University of Stuttgart, Stuttgart, Germany","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","12","Background: The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. Aims: This paper analyzes the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face. Hugging Face is the most popular repository for pretrained ML models. We aim to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. Method: We conduct the first repository mining study on the Hugging Face Hub API on carbon emissions and answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? Results: Key findings from the study include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain reporting emissions. The study also uncovers correlations between carbon emissions and various attributes, such as model size, dataset size, ML application domains and performance metrics. Conclusions: The results emphasize the need for software measurements to improve energy reporting practices and the promotion of carbon-efficient model development within the Hugging Face community. To address this issue, we propose two classifications: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. With these classification proposals, we aim to encourage transparency and sustainable model development within the ML community.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304801","European Union(grant numbers:TED2021-130923B-I00,MCIN/AEI/10.13039/501100011033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304801","repository mining;software measurement;sustainable software;green AI;carbon-aware ML;carbon-efficient ML","Training;Correlation;Green products;Carbon dioxide;Energy efficiency;Software measurement;Data mining","","4","","43","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests","S. Fatima; T. A. Ghaleb; L. Briand","School of EECS, University of Ottawa, Ottawa, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1912","1927","Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79% and 73% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98% and 89% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25% and 64%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases.","1939-3520","","10.1109/TSE.2022.3201209","Huawei Technologies Canada; Mitacs; Canada Research Chairs; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866550","Flaky tests;software testing;black-box testing;natural language processing;CodeBERT","Codes;Predictive models;Production;Computational modeling;Software testing;Software;Feature extraction","","12","","69","IEEE","24 Aug 2022","","","IEEE","IEEE Journals"
"VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning","Z. Li; S. Lu; R. Zhang; Z. Zhao; R. Liang; R. Xue; W. Li; F. Zhang; S. Gao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Information Engineering University, Zhengzhou, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Information Engineering University, Zhengzhou, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information, Central University of Finance and Economics, Beijing, China","IEEE Transactions on Software Engineering","16 Nov 2023","2023","49","11","4886","4916","With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. (i) Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; (ii) reduce the reliance on manual pre-defined rules/patterns and expert involvement; (iii) assist contract developers in completing the contract lifecycle more safely, e.g., vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04% accuracy and 85.60% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, e.g., vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.","1939-3520","","10.1109/TSE.2023.3317209","National Key R&D Program of China(grant numbers:2021YFB2700603); National Natural Science Foundation of China(grant numbers:62172405,62072487,62227805,62072398); Major Public Welfare Projects Foundation of Henan(grant numbers:201300210200); Beijing Natural Science Foundation(grant numbers:M21036); Zhejiang Key R&D Plan(grant numbers:2021C01116); Leading Innovative and Entrepreneur Team Introduction Program of Zhejiang(grant numbers:2018R01005); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LD22F020002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261219","Blockchain;smart contract;security analysis;multiple instance learning;symbolic execution","Source coding;Smart contracts;Codes;Pattern matching;Testing;Monitoring;Maintenance engineering","","","","76","CCBY","22 Sep 2023","","","IEEE","IEEE Journals"
"Rationale in Development Chat Messages: An Exploratory Study","R. Alkadhi; T. Lata; E. Guzmany; B. Bruegge","Faculty of Informatics, Technische Universität München, Garching, Germany; Faculty of Informatics, Technische Universität München, Garching, Germany; Universitat Zurich, Zurich, ZH, CH; Faculty of Informatics, Technische Universität München, Garching, Germany","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","436","446","Chat messages of development teams play an increasinglysignificant role in software development, having replacedemails in some cases. Chat messages contain informationabout discussed issues, considered alternatives and argumentationleading to the decisions made during software development. These elements, defined as rationale, are invaluable duringsoftware evolution for documenting and reusing developmentknowledge. Rationale is also essential for coping with changesand for effective maintenance of the software system. However, exploiting the rationale hidden in the chat messages is challengingdue to the high volume of unstructured messages covering a widerange of topics. This work presents the results of an exploratorystudy examining the frequency of rationale in chat messages, the completeness of the available rationale and the potential ofautomatic techniques for rationale extraction. For this purpose, we apply content analysis and machine learning techniques onmore than 8,700 chat messages from three software developmentprojects. Our results show that chat messages are a rich source ofrationale and that machine learning is a promising technique fordetecting rationale and identifying different rationale elements.","","978-1-5386-1544-7","10.1109/MSR.2017.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962392","Chat messages;Rationale;Empirical Software Engineering","Encoding;Manuals;Data mining;Software systems;Logic gates;Informatics","","42","","45","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"BotHunter: An Approach to Detect Software Bots in GitHub","A. Abdellatif; M. Wessel; I. Steinmacher; M. A. Gerosa; E. Shihab","Concordia University, Montreal, Canada; Delft University of Technology, Delft, Netherlands; Universidade Tecnológica Federal do Paraná, Campo Mourão, Brazil; Northern Arizona University, Flagstaff, USA; Concordia University, Montreal, Canada","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","6","17","Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3527959","National Science Foundation(grant numbers:1815503,1900903); CNPq(grant numbers:313067/2020-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796262","Software Bots;Empirical Software Engineering","Bot (Internet);Feature extraction;Transformers;Software;Encoding;Data mining;Time factors","","4","","53","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Predicting Design Impactful Changes in Modern Code Review: A Large-Scale Empirical Study","A. Uchôa; C. Barbosa; D. Coutinho; W. Oizumi; W. K. G. Assunção; S. R. Vergilio; J. A. Pereira; A. Oliveira; A. Garcia","Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Computer Science Department, Federal University of Paraná (UFPR), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Brazil","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","471","482","Companies have adopted modern code review as a key technique for continuously monitoring and improving the quality of software changes. One of the main motivations for this is the early detection of design impactful changes, to prevent that design-degrading ones prevail after each code review. Even though design degradation symptoms often lead to changes' rejections, practices of modern code review alone are actually not sufficient to avoid or mitigate design decay. Software design degrades whenever one or more symptoms of poor structural decisions, usually represented by smells, end up being introduced by a change. Design degradation may be related to both technical and social aspects in collaborative code reviews. Unfortunately, there is no study that investigates if code review stakeholders, e.g, reviewers, could benefit from approaches to distinguish and predict design impactful changes with technical and/or social aspects. By analyzing 57,498 reviewed code changes from seven open-source systems, we report an investigation on prediction of design impactful changes in modern code review. We evaluated the use of six ML algorithms to predict design impactful changes. We also extracted and assessed 41 different features based on both social and technical aspects. Our results show that Random Forest and Gradient Boosting are the best algorithms. We also observed that the use of technical features results in more precise predictions. However, the use of social features alone, which are available even before the code review starts (e.g., for team managers or change assigners), also leads to highly-accurate prediction. Therefore social and/or technical prediction models can be used to support further design inspection of suspicious changes early in a code review process. Finally, we provide an enriched dataset that allows researchers to investigate the context behind design impactful changes during the code review process.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463137","design changes;code review;machine learning","Degradation;Software design;Predictive models;Inspection;Prediction algorithms;Feature extraction;Stakeholders","","11","","76","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Learning Off-By-One Mistakes: An Empirical Study","H. Sellik; O. van Paridon; G. Gousios; M. Aniche","Delft University of Technology, Delft, The Netherlands; Adyen N.V., Amsterdam, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","58","67","Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., `<;' or `>' instead of `<;=' or `>='. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers. While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463090","machine learning for software engineering;deep learning for software engineering;software testing;boundary testing","Deep learning;Java;Analytical models;Adaptation models;Computer bugs;Static analysis;Tools","","2","","36","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Keynote abstract","M. Allamanis","The University of Edinburgh, Edinburgh, Edinburgh, GB","2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP)","5 May 2016","2016","","","xi","xi","Provides an abstract of the keynote presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","","978-1-5090-1851-2","10.1109/IWESEP.2016.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464543","","Codes;Machine learning;Software engineering;Software;Probabilistic logic;Informatics;Computer languages","","","","","IEEE","5 May 2016","","","IEEE","IEEE Conferences"
"Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps","K. Moran; C. Bernal-Cárdenas; M. Curcio; R. Bonett; D. Poshyvanyk","Department of Computer Science, College of William & Mary, Williamsburg, USA; Department of Computer Science, College of William & Mary, Williamsburg, USA; Department of Computer Science, College of William & Mary, Williamsburg, USA; Department of Computer Science, College of William & Mary, Williamsburg, USA; Department of Computer Science, College of William & Mary, Williamsburg, USA","IEEE Transactions on Software Engineering","12 Feb 2020","2020","46","2","196","221","It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.","1939-3520","","10.1109/TSE.2018.2844788","National Science Foundation(grant numbers:CCF-1525902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374985","GUI;CNN;mobile;prototyping;machine-learning;mining software repositories","Graphical user interfaces;Software;Task analysis;Prototypes;Metadata;Androids;Humanoid robots","","96","","109","IEEE","7 Jun 2018","","","IEEE","IEEE Journals"
"Exploiting Natural Language Structures in Software Informal Documentation","A. Di Sorbo; S. Panichella; C. A. Visaggio; M. D. Penta; G. Canfora; H. C. Gall","Department of Engineering, University of Sannio, Benevento, Italy; Zurich University of Applied Science (ZHAW), Zurich, Switzerland; Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Informatics, University of Zurich, Zurich, Switzerland","IEEE Transactions on Software Engineering","12 Aug 2021","2021","47","8","1587","1604","Communication means, such as issue trackers, mailing lists, Q&A forums, and app reviews, are premier means of collaboration among developers, and between developers and end-users. Analyzing such sources of information is crucial to build recommenders for developers, for example suggesting experts, re-documenting source code, or transforming user feedback in maintenance and evolution strategies for developers. To ease this analysis, in previous work we proposed Development Emails Content Analyzer (DECA), a tool based on Natural Language Parsing that classifies with high precision development emails' fragments according to their purpose. However, DECA has to be trained through a manual tagging of relevant patterns, which is often effort-intensive, error-prone and requires specific expertise in natural language parsing. In this paper, we first show, with an empirical study, the extent to which producing rules for identifying such patterns requires effort, depending on the nature and complexity of patterns. Then, we propose an approach, named Nlp-based softwarE dOcumentation aNalyzer (NEON), that automatically mines such rules, minimizing the manual effort. We assess the performances of NEON in the analysis and classification of mobile app reviews, developers discussions, and issues. NEON simplifies the patterns identification and rules definition processes, allowing a savings of more than 70 percent of the time otherwise spent on performing such activities manually. Results also show that NEON-generated rules are close to the manually identified ones, achieving comparable recall.","1939-3520","","10.1109/TSE.2019.2930519","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:200021-166275); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769918","Mining unstructured data;natural language processing;empirical study","Neon;Software;Linguistics;Pattern recognition;Documentation;Manuals","","13","","47","IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Simpler Hyperparameter Optimization for Software Analytics: Why, How, When?","A. Agrawal; X. Yang; R. Agrawal; R. Yedida; X. Shen; T. Menzies","Wayfair, Boston, MA, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","16 Aug 2022","2022","48","8","2939","2954","How can we make software analytics simpler and faster? One method is to match the complexity of analysis to the intrinsic complexity of the data being explored. For example, hyperparameter optimizers find the control settings for data miners that improve the predictions generated via software analytics. Sometimes, very fast hyperparameter optimization can be achieved by “DODGE-ing”; i.e., simply steering way from settings that lead to similar conclusions. But when is it wise to use that simple approach and when must we use more complex (and much slower) optimizers? To answer this, we applied hyperparameter optimization to 120 SE data sets that explored bad smell detection, predicting Github issue close time, bug report analysis, defect prediction, and dozens of other non-SE problems. We find that the simple DODGE works best for data sets with low “intrinsic dimensionality” ($\mu _D\approx 3$μD≈3) and very poorly for higher-dimensional data ($\mu _D > 8$μD>8). Nearly all the SE data seen here was intrinsically low-dimensional, indicating that DODGE is applicable for many SE analytics tasks.","1939-3520","","10.1109/TSE.2021.3073242","National Science Foundation(grant numbers:CCF-1703487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9405415","Software analytics;hyperparameter optimization;defect prediction;bad smell detection;issue close time;bug reports","Software;Optimization;Clustering algorithms;Text mining;Measurement;Computer bugs;Task analysis","","12","","101","IEEE","15 Apr 2021","","","IEEE","IEEE Journals"
"Self-Admitted Technical Debt in the Embedded Systems Industry: An Exploratory Case Study","Y. Li; M. Soliman; P. Avgeriou; L. Somers","Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, CP, The Netherlands; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, CP, The Netherlands; Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence, University of Groningen, Groningen, CP, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2545","2565","Technical debt denotes shortcuts taken during software development, mostly for the sake of expedience. When such shortcuts are admitted explicitly by developers (e.g., writing a TODO/Fixme comment), they are termed as Self-Admitted Technical Debt or SATD. There has been a fair amount of work studying SATD management in Open Source projects, but SATD in industry is relatively unexplored. At the same time, there is no work focusing on developers’ perspectives towards SATD and its management. To address this, we conducted an exploratory case study in cooperation with an industrial partner to study how they think of SATD and how they manage it. Specifically, we collected data by identifying and characterizing SATD in different sources (issues, source code comments, and commits) and carried out a series of interviews with 12 software practitioners. The results show: 1) the core characteristics of SATD in industrial projects; 2) developers’ attitudes towards identified SATD and statistics; 3) triggers for practitioners to introduce and repay SATD; 4) relations between SATD in different sources; 5) practices used to manage SATD; 6) challenges and tooling ideas for SATD management.","1939-3520","","10.1109/TSE.2022.3224378","ITEA3; RVO(grant numbers:17038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961946","Technical debt;self-admitted technical debt;mining software repositories;source code comment;issue tracking system;commit;empirical study","Source coding;Software;Codes;Industries;Interviews;Embedded systems;Documentation","","2","","70","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Facilitating Coordination between Software Developers: A Study and Techniques for Timely and Efficient Recommendations","K. Blincoe; G. Valetto; D. Damian","Software Engineering Global Interaction Lab, University of Victoria, Victoria, BC, Canada; Fondazione Bruno Kessler, Trento, Italy; Software Engineering Global Interaction Lab, University of Victoria, Victoria, BC, Canada","IEEE Transactions on Software Engineering","13 Oct 2015","2015","41","10","969","985","When software developers fail to coordinate, build failures, duplication of work, schedule slips and software defects can result. However, developers are often unaware of when they need to coordinate, and existing methods and tools that help make developers aware of their coordination needs do not provide timely or efficient recommendations. We describe our techniques to identify timely and efficient coordination recommendations, which we developed and evaluated in a study of coordination needs in the Mylyn software project. We describe how data obtained from tools that capture developer actions within their Integrated Development Environment (IDE) as they occur can be used to timely identify coordination needs; we also describe how properties of tasks coupled with machine learning can focus coordination recommendations to those that are more critical to the developers to reduce information overload and provide more efficient recommendations. We motivate our techniques through developer interviews and report on our quantitative analysis of coordination needs in the Mylyn project. Our results suggest that by leveraging IDE logging facilities, properties of tasks and machine learning techniques awareness tools could make developers aware of critical coordination needs in a timely way. We conclude by discussing implications for software engineering research and tool design.","1939-3520","","10.1109/TSE.2015.2431680","US National Science Foundation (NSF)(grant numbers:OCI-1221254); NECSIS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105409","Human Factors in Software Design;Management;Metrics/Measurement;Productivity;Programming Teams;Computer-supported cooperative work;human factors in software design;management;metrics/measurement;productivity;programming teams","Software;Encoding;Interviews;Statistical analysis;Manuals;Accuracy;Correlation","","13","","70","IEEE","11 May 2015","","","IEEE","IEEE Journals"
"A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software","S. E. Ponta; H. Plate; A. Sabetta; M. Bezzi; C. Dangremont","SAP Security Research, Mougins, France; SAP Security Research, Mougins, France; SAP Security Research, Mougins, France; SAP Security Research, Mougins, France; SAP Security Research, Mougins, France","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","383","387","Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure. While operating a vulnerability assessment tool, which we developed, and that is currently used by hundreds of development units at SAP, we manually collected and curated a dataset of vulnerabilities of open-source software, and the commits fixing them. The data were obtained both from the National Vulnerability Database (NVD), and from project-specific web resources, which we monitor on a continuous basis. From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct opensource Java projects, used in SAP products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a CVE (Common Vulnerability and Exposure) identifier at all, and 46, which do have such identifier assigned by a numbering authority, are not available in the NVD yet. The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories, and to augment the attributes available for each instance. Moreover, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications). Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; it also represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816802","vulnerabilities;open source software;nvd;dataset","Open source software;Tools;Databases;Security;Monitoring;Java","","57","","9","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction","H. Keshavarz; M. Nagappan","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","191","195","In this paper, we present ApacheJIT, a large dataset for Just-In-Time (JIT) defect prediction. ApacheJIT consists of clean and bug-inducing software changes in 14 popular Apache projects. ApacheJIT has a total of 106,674 commits (28,239 bug-inducing and 78,435 clean commits). Having a large number of commits makes ApacheJIT a suitable dataset for machine learning JIT models, especially deep learning models that require large training sets to effectively generalize the patterns present in the historical data to future data.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3527996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796180","Defect Prediction;Software Engineering;Dataset","Training;Deep learning;Computer bugs;Predictive models;Data models;Software;Data mining","","4","","26","","21 Jun 2022","","","IEEE","IEEE Conferences"
"iTree: Efficiently Discovering High-Coverage Configurations Using Interaction Trees","C. Song; A. Porter; J. S. Foster","Fraunhofer USA Center for Experimental Software Engineering, College Park, MD; Department of Computer Science, University of Maryland, College Park, MD; Department of Computer Science, University of Maryland, College Park, MD","IEEE Transactions on Software Engineering","31 Mar 2014","2014","40","3","251","265","Modern software systems are increasingly configurable. While this has many benefits, it also makes some software engineering tasks,such as software testing, much harder. This is because, in theory,unique errors could be hiding in any configuration, and, therefore,every configuration may need to undergo expensive testing. As this is generally infeasible, developers need cost-effective technique for selecting which specific configurations they will test. One popular selection approach is combinatorial interaction testing (CIT), where the developer selects a strength t and then computes a covering array (a set of configurations) in which all t-way combinations of configuration option settings appear at least once. In prior work, we demonstrated several limitations of the CIT approach. In particular, we found that a given system's effective configuration space - the minimal set of configurations needed to achieve a specific goal - could comprise only a tiny subset of the system's full configuration space. We also found that effective configuration space may not be well approximated by t-way covering arrays. Based on these insights we have developed an algorithm called interaction tree discovery (iTree). iTree is an iterative learning algorithm that efficiently searches for a small set of configurations that closely approximates a system's effective configuration space. On each iteration iTree tests the system on a small sample of carefully chosen configurations, monitors the system's behaviors, and then applies machine learning techniques to discover which combinations of option settings are potentially responsible for any newly observed behaviors. This information is used in the next iteration to pick a new sample of configurations that are likely to reveal further new behaviors. In prior work, we presented an initial version of iTree and performed an initial evaluation with promising results. This paper presents an improved iTree algorithm in greater detail. The key improvements are based on our use of composite proto-interactions - a construct that improves iTree's ability to correctly learn key configuration option combinations, which in turn significantly improves iTree's running time, without sacrificing effectiveness. Finally, the paper presents a detailed evaluation of the improved iTree algorithm by comparing the coverage it achieves versus that of covering arrays and randomly generated configuration sets, including a significantly expanded scalability evaluation with the ~ 1M-LOC MySQL. Our results strongly suggest that the improved iTree algorithm is highly scalable and can identify a high-coverage test set of configurations more effectively than existing methods.","1939-3520","","10.1109/TSE.2013.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671585","Empirical software engineering;software configurations;software testing and analysis","Testing;Arrays;Software algorithms;Software engineering;Machine learning algorithms;Software systems;Algorithm design and analysis","","22","","39","IEEE","20 Nov 2013","","","IEEE","IEEE Journals"
"The Perspective of Software Professionals on Algorithmic Racism","R. De Souza Santos; L. F. De Lima; C. Magalhães","Cape Breton University & CESAR School, Sydney, NS, Canada; CESAR, Recife, Brazil; CESAR School, Recife, Brazil","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","10","Context. Algorithmic racism is the term used to describe the behavior of technological solutions that constrains users based on their ethnicity. Lately, various data-driven software systems have been reported to discriminate against Black people, either for the use of biased data sets or due to the prejudice propagated by software professionals in their code. As a result, Black people are experiencing disadvantages in accessing technology-based services, such as housing, banking, and law enforcement. Goal. This study aims to explore algorithmic racism from the perspective of software professionals. Method. A survey questionnaire was applied to explore the understanding of software practitioners on algorithmic racism, and data analysis was conducted using descriptive statistics and coding techniques. Results. We obtained answers from a sample of 73 software professionals discussing their understanding and perspectives on algorithmic racism in software development. Our results demonstrate that the effects of algorithmic racism are well-known among practitioners. However, there is no consensus on how the problem can be effectively addressed in software engineering. In this paper, some solutions to the problem are proposed based on the professionals' narratives. Conclusion. Combining technical and social strategies, including training on structural racism for software professionals, is the most promising way to address the algorithmic racism problem and its effects on the software solutions delivered to our society.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304856","EDI;racism;software development","Surveys;Training;Industries;Machine learning algorithms;Software algorithms;Diversity reception;Machine learning","","","","47","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Technical Debt in the Peer-Review Documentation of R Packages: a rOpenSci Case Study","Z. Codabux; M. Vidoni; F. H. Fard",University of Saskatchewan; RMIT University; University of British Columbia,"2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","195","206","Context: Technical Debt (TD) is a metaphor used to describe code that is ""not quite right."" Although TD studies have gained momentum, TD has yet to be studied as thoroughly in non-Object-Oriented (OO) or scientific software such as R. R is a multi-paradigm programming language, whose popularity in data science and statistical applications has amplified in recent years. Due to R’s inherent ability to expand through user-contributed packages, several community-led organizations were created to organize and peer-review packages in a concerted effort to increase their quality. Nonetheless, it is well-known that most R users do not have a technical programming background, being from multiple disciplines. Objective: The goal of this study is to investigate TD in the documentation of the peer-review of R packages led by rOpenSci. Method: We collected over 5,000 comments from 157 packages that had been reviewed and approved to be published at rOpenSci. We manually analyzed a sample dataset of these comments posted by package authors, editors of rOpenSci, and reviewers during the review process to investigate the types of TD present in these reviews. Results: The findings of our study include (i) a taxonomy of TD derived from our analysis of the peer-reviews (ii) documentation debt as being the most prevalent type of debt (iii) different user roles are concerned with different types of TD. For instance, reviewers tend to report some types of TD more than other roles, and the types of TD they report are different from those reported by the authors of a package. Conclusion: TD analysis in scientific software or peer-review is almost non-existent. Our study is a pioneer but within the context of R packages. However, our findings can serve as a starting point for replication studies, given our public datasets, to perform similar analyses in other scientific software or to investigate the rationale behind our findings.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00032","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463083","Technical Debt;R Programming;rOpensci;Technical Debt Taxonomy;Mining Software Repositories","Computer languages;Taxonomy;Documentation;Organizations;Programming;Data science;Software","","3","","66","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"An Empirical Study on the Performance of Individual Issue Label Prediction","J. Heo; S. Lee","Department of AI Convergence Engineering, Gyeongsang National University, Jinju, Republic of Korea; Department of AI Convergence Engineering Department of Aerospace and Software Engineering, Gyeongsang National University, Jinju, Republic of Korea","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","228","233","In GitHub, open-source software (OSS) developers label issue reports. As issue labeling is a labor-intensive manual task, automatic approaches have developed to label issue reports. However, those approaches have shown limited performance. Therefore, it is necessary to analyze the performance of predicting labels for an issue report. Understanding labels with high performance and those with low performance can help improve the performance of automatic issue labeling tasks. In this paper, we investigate the performance of individual label prediction. Our investigation uncovers labels with high performance and those with low performance. Our results can help researchers to understand the different characteristics of labels and help developers to develop a unified approach that combines several effective approaches for different kinds of issues.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174030","Issue Classification;Empirical Study;Github;Issue Report;Label Prediction;Performance Analysis;Labeling","Manuals;Labeling;Data mining;Task analysis;Open source software;Software development management","","","","28","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"ConPredictor: Concurrency Defect Prediction in Real-World Applications","T. Yu; W. Wen; X. Han; J. H. Hayes","Department of Computer Science, University of Kentucky, Lexington, KY; Department of Computer Science, University of Kentucky, Lexington, KY; Department of Computer Science, University of Kentucky, Lexington, KY; Department of Computer Science, University of Kentucky, Lexington, KY","IEEE Transactions on Software Engineering","12 Jun 2019","2019","45","6","558","575","Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features.","1939-3520","","10.1109/TSE.2018.2791521","National Science Foundation(grant numbers:CCF-1464032,CCF-1652149,CCF-1511117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252721","Concurrency;defect prediction;software quality;software metrics","Concurrent computing;Predictive models;Software;Programming;Testing;Synchronization","","22","","100","IEEE","9 Jan 2018","","","IEEE","IEEE Journals"
"Neural Transfer Learning for Repairing Security Vulnerabilities in C Code","Z. Chen; S. Kommrusch; M. Monperrus","KTH Royal Institute of Technology, Stockholm, Sweden; Colorado State University, Fort Collins, CO, USA; KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Software Engineering","6 Jan 2023","2023","49","1","147","165","In this paper, we address the problem of automatic repair of software vulnerabilities with deep learning. The major problem with data-driven vulnerability repair is that the few existing datasets of known confirmed vulnerabilities consist of only a few thousand examples. However, training a deep learning model often requires hundreds of thousands of examples. In this work, we leverage the intuition that the bug fixing task and the vulnerability fixing task are related and that the knowledge learned from bug fixes can be transferred to fixing vulnerabilities. In the machine learning community, this technique is called transfer learning. In this paper, we propose an approach for repairing security vulnerabilities named VRepair which is based on transfer learning. VRepair is first trained on a large bug fix corpus and is then tuned on a vulnerability fix dataset, which is an order of magnitude smaller. In our experiments, we show that a model trained only on a bug fix corpus can already fix some vulnerabilities. Then, we demonstrate that transfer learning improves the ability to repair vulnerable C functions. We also show that the transfer learning model performs better than a model trained with a denoising task and fine-tuned on the vulnerability fixing task. To sum up, this paper shows that transfer learning works well for repairing security vulnerabilities in C compared to learning on a small dataset.","1939-3520","","10.1109/TSE.2022.3147265","Wallenberg Artificial Intelligence, Autonomous Systems and Software Program; Knut och Alice Wallenbergs Stiftelse; Swedish Foundation for Strategic Research; National Science Foundation(grant numbers:CCF-1750399); Vetenskapsrådet(grant numbers:2018-05973); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699412","Vulnerability fixing;transfer learning;seq2seq learning","Transfer learning;Task analysis;Computer bugs;Transformers;Codes;Training;Software","","21","","81","CCBY","1 Feb 2022","","","IEEE","IEEE Journals"
"Revisiting Binary Code Similarity Analysis Using Interpretable Feature Engineering and Lessons Learned","D. Kim; E. Kim; S. K. Cha; S. Son; Y. Kim","KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1661","1682","Binary code similarity analysis (BCSA) is widely used for diverse security applications, including plagiarism detection, software license violation detection, and vulnerability discovery. Despite the surging research interest in BCSA, it is significantly challenging to perform new research in this field for several reasons. First, most existing approaches focus only on the end results, namely, increasing the success rate of BCSA, by adopting uninterpretable machine learning. Moreover, they utilize their own benchmark, sharing neither the source code nor the entire dataset. Finally, researchers often use different terminologies or even use the same technique without citing the previous literature properly, which makes it difficult to reproduce or extend previous work. To address these problems, we take a step back from the mainstream and contemplate fundamental research questions for BCSA. Why does a certain technique or a certain feature show better results than the others? Specifically, we conduct the first systematic study on the basic features used in BCSA by leveraging interpretable feature engineering on a large-scale benchmark. Our study reveals various useful insights on BCSA. For example, we show that a simple interpretable model with a few basic features can achieve a comparable result to that of recent deep learning-based approaches. Furthermore, we show that the way we compile binaries or the correctness of underlying binary analysis tools can significantly affect the performance of BCSA. Lastly, we make all our source code and benchmark public and suggest future directions in this field to help further research.","1939-3520","","10.1109/TSE.2022.3187689","Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2021-0-01332); Developing Next-Generation Binary Decompiler; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813408","Binary code similarity analysis;similarity measures;feature evaluation and selection;benchmark","Benchmark testing;Computer architecture;Binary codes;Syntactics;Semantics;Licenses;Market research","","13","","166","CCBY","1 Jul 2022","","","IEEE","IEEE Journals"
"A Survey on the Use of Computer Vision to Improve Software Engineering Tasks","M. Bajammal; A. Stocco; D. Mazinanian; A. Mesbah","University of British Columbia, Vancouver, BC, Canada; Università della Svizzera Italiana, Lugano, Switzerland; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","16 May 2022","2022","48","5","1722","1742","Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.","1939-3520","","10.1109/TSE.2020.3032986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237151","Computer vision;software engineering;survey","Testing;Visualization;Software engineering;Computer vision;Software;Task analysis;Graphical user interfaces","","6","","111","IEEE","22 Oct 2020","","","IEEE","IEEE Journals"
"Managing Technical Debt Using Intelligent Techniques - A Systematic Mapping Study","D. Albuquerque; E. Guimarães; G. Tonin; P. Rodríguezs; M. Perkusich; H. Almeida; A. Perkusich; F. Chagas","Intelligent Software Engineering Group (ISE/VIRTUS), Federal University of Campina Grande, Campina Grande, PB, Brazil; Pennsylvania State University, Malvern, PA, USA; Federal University of Fronteira Sul, Chapecó, SC, Brazil; Polytechnic University of Madrid, Madrid, Spain; Intelligent Software Engineering Group (ISE/VIRTUS), Federal University of Campina Grande, Campina Grande, PB, Brazil; Intelligent Software Engineering Group (ISE/VIRTUS), Federal University of Campina Grande, Campina Grande, PB, Brazil; Intelligent Software Engineering Group (ISE/VIRTUS), Federal University of Campina Grande, Campina Grande, PB, Brazil; Intelligent Software Engineering Group (ISE/VIRTUS), Federal University of Campina Grande, Campina Grande, PB, Brazil","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2202","2220","Technical Debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefits but might hurt the long-term health of a software system. With the increasing amount of data generated when performing software development activities, an emergent research field has gained attention: applying Intelligent Techniques to solve Software Engineering problems. Intelligent Techniques were used to explore data for knowledge discovery, reasoning, learning, planning, perception, or supporting decision-making. Although these techniques can be promising, there is no structured understanding related to their application to support Technical Debt Management (TDM) activities. Within this context, this study aims to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. To this end, we performed a Systematic Mapping Study (SMS) to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. In total, 150 primary studies were identified and analyzed, dated from 2012 to 2021. The results indicated a growing interest in applying Intelligent Techniques to support TDM activities, the most used: Machine Learning and Reasoning under uncertainty. Intelligent Techniques aimed to assist mainly TDM activities related to identification, measurement, and monitoring. Design TD, Code TD, and Architectural TD are the TD types in the spotlight. Most studies were categorized at automation levels 1 and 2, meaning that existing approaches still require substantial human intervention. Symbolists and Analogizers are levels of explanation presented by most Intelligent Techniques, implying that these solutions conclude a general truth after considering a sufficient number of particular cases. Moreover, we also cataloged the empirical research types, contributions, and validation strategies described in primary studies. Based on our findings, we argue that there is still room to improve the use of Intelligent Techniques to support TDM activities. The open issues that emerged from this study can represent future opportunities for practitioners and researchers.","1939-3520","","10.1109/TSE.2022.3214764","IFPB employee qualification incentive program(grant numbers:21/2021/PRPIPG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919389","Technical debt;intelligent techniques;technical debt management activities;systematic mapping study","Time division multiplexing;Cognition;Codes;Costs;Systematics;Software systems;Knowledge discovery","","1","","58","IEEE","14 Oct 2022","","","IEEE","IEEE Journals"
"Learning to Predict User-Defined Types","K. Jesse; P. T. Devanbu; A. Sawant","Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1508","1522","TypeScript is a widely adopted gradual typed language where developers can optionally type variables, functions, parameters and more. Probabilistic type inference approaches with ML (machine learning) work well especially for commonly occurring types such as boolean, number, and string. TypeScript permits a wide range of types including developer defined class names and type interfaces. These developer defined types, termed user-defined types, can be written within the realm of language naming conventions. The set of user-defined types is boundless and existing bounded type guessing approaches are an imperfect solution. Existing works either under perform in user-defined types or ignore user-defined types altogether. This work leverages a BERT-style pre-trained model, with multi-task learning objectives, to learn how to type user-defined classes and interfaces. Thus we present DiverseTyper, a solution that explores the diverse set of user-defined types by uniquely aligning classes and interfaces declarations to the places in which they are used. DiverseTyper surpasses all existing works including those that model user-defined types.","1939-3520","","10.1109/TSE.2022.3178945","National Science Foundation(grant numbers:1414172,2107592); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785755","Multi-Task learning;representation learning;transfer learning;type inference.","Annotations;Codes;Vocabulary;Encoding;Task analysis;Software development management;Training","","1","","64","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Towards Automated Classification of Code Review Feedback to Support Analytics","A. K. Turzo; F. Faysal; O. Poddar; J. Sarker; A. Iqbal; A. Bosu","Wayne State University, Detroit, Michigan, USA; Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Wayne State University, Detroit, Michigan, USA; Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Wayne State University, Detroit, Michigan, USA","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","12","Background: As improving code review (CR) effectiveness is a priority for many software development organizations, projects have deployed CR analytics platforms to identify potential improvement areas. The number of issues identified, which is a crucial metric to measure CR effectiveness, can be misleading if all issues are placed in the same bin. Therefore, a finer-grained classification of issues identified during CRs can provide actionable insights to improve CR effectiveness. Although a recent work by Fregnan et al. proposed automated models to classify CR-induced changes, we have noticed two potential improvement areas – i) classifying comments that do not induce changes and ii) using deep neural networks (DNN) in conjunction with code context to improve performances. Aims: This study aims to develop an automated CR comment classifier that leverages DNN models to achieve a more reliable performance than Fregnan et al. Method: Using a manually labeled dataset of 1,828 CR comments, we trained and evaluated supervised learning-based DNN models leveraging code context, comment text, and a set of code metrics to classify CR comments into one of the five high-level categories proposed by Turzo and Bosu. Results: Based on our 10-fold cross-validation-based evaluations of multiple combinations of tokenization approaches, we found a model using CodeBERT achieving the best accuracy of 59.3%. Our approach outperforms Fregnan et al.'s approach by achieving 18.7% higher accuracy. Conclusion: In addition to facilitating improved CR analytics, our proposed model can be useful for developers in prioritizing code review feedback and selecting reviewers.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304851","US National Science Foundation(grant numbers:1850475); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304851","code review;review comment;classification;open source software;analytics;OSS","Codes;Artificial neural networks;Machine learning;Tokenization;Software;Software measurement;Reliability","","","","58","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Self-Adaptive and Online QoS Modeling for Cloud-Based Software Services","T. Chen; R. Bahsoon","CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom; CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom","IEEE Transactions on Software Engineering","12 May 2017","2017","43","5","453","475","In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-the-art modelings.","1939-3520","","10.1109/TSE.2016.2608826","EPSRC Grant(grant numbers:EP/J017515/1); DAASE: Dynamic Adaptive Automated Software Engineering; The PhD scholarship from the School of Computer Science; University of Birmingham; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572219","Software quality;search-based software engineering;self-adaptive systems;machine learning;cloud computing;performance modeling","Quality of service;Cloud computing;Interference;Adaptation models;Sensitivity;Uncertainty","","63","","54","CCBY","20 Sep 2016","","","IEEE","IEEE Journals"
"Metric-Based Fault Prediction for Spreadsheets","P. Koch; K. Schekotihin; D. Jannach; B. Hofer; F. Wotawa","AAU Klagenfurt, Klagenfurt, Austria; AAU Klagenfurt, Klagenfurt, Austria; AAU Klagenfurt, Klagenfurt, Austria; TU Graz, Graz, Austria; TU Graz, Graz, Austria","IEEE Transactions on Software Engineering","14 Oct 2021","2021","47","10","2195","2207","Electronic spreadsheets are widely used in organizations for various data analytics and decision-making tasks. Even though faults within such spreadsheets are common and can have significant negative consequences, today's tools for creating and handling spreadsheets provide limited support for fault detection, localization, and repair. Being able to predict whether a certain part of a spreadsheet is faulty or not is often central for the implementation of such supporting functionality. In this work, we propose a novel approach to fault prediction in spreadsheet formulas, which combines an extensive catalog of spreadsheet metrics with modern machine learning algorithms. An analysis of the individual metrics from our catalog reveals that they are generally suited to discover a wide range of faults. Their predictive power is, however, limited when considered in isolation. Therefore, in our approach we apply supervised learning algorithms to obtain fault predictors that utilize all data provided by multiple spreadsheet metrics from our catalog. Experiments on different datasets containing faulty spreadsheets show that particularly Random Forests classifiers are often effective. As a result, the proposed method is in many cases able to make highly accurate predictions whether a given formula of a spreadsheet is faulty.11.Results of a preliminary study were published in [1] .","1939-3520","","10.1109/TSE.2019.2944604","Austrian Science Fund(grant numbers:I2144); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859280","Spreadsheets;fault prediction;machine learning","Measurement;Software;Prediction algorithms;Predictive models;Tools;Radio frequency;Task analysis","","5","","50","CCBY","4 Oct 2019","","","IEEE","IEEE Journals"
"Astraea: Grammar-Based Fairness Testing","E. Soremekun; S. Udeshi; S. Chattopadhyay","Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Esch-sur-Alzette, Luxembourg; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore","IEEE Transactions on Software Engineering","9 Dec 2022","2022","48","12","5188","5211","Software often produces biased outputs. In particular, machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs. Such unfair program behavior can be caused by societal bias. In the last few years, Amazon, Microsoft and Google have provided software services that produce unfair outputs, mostly due to societal bias (e.g., gender or race). In such events, developers are saddled with the task of conducting fairness testing. Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases. We propose a grammar-based fairness testing approach (called Astraea) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems. Using probabilistic grammars, Astraea also provides fault diagnosis by isolating the cause of observed software bias. Astraea’s diagnoses facilitate the improvement of ML fairness. Astraea was evaluated on 18 software systems that provide three major natural language processing (NLP) services. In our evaluation, Astraea generated fairness violations at a rate of about 18%. Astraea generated over 573K discriminatory test cases and found over 102K fairness violations. Furthermore, Astraea improves software fairness by about 76% via model-retraining, on average.","1939-3520","","10.1109/TSE.2022.3141758","University of Luxembourg, Ezekiel Soremekun; University of Luxembourg(grant numbers:AUDACITY-2019-Laiwyers); OneConnect Financial(grant numbers:RGOCFT2001); Singapore Ministry of Education; MOE(grant numbers:MOE2018-T2-1-098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678017","software fairness;machine learning;natural language processing;software testing;program debugging","Testing;Grammar;Task analysis;Sentiment analysis;Test pattern generators;Software testing;Software systems","","4","","88","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Supporting Developers in Addressing Human-Centric Issues in Mobile Apps","H. Khalajzadeh; M. Shahin; H. O. Obie; P. Agrawal; J. Grundy","School of Information Technology, Faculty of Science Engineering and Built Environment, Deakin University, Melbourne, VIC, Australia; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of IT, Monash University, Melbourne, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of IT, Monash University, Melbourne, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of IT, Monash University, Melbourne, VIC, Australia","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2149","2168","Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development.","1939-3520","","10.1109/TSE.2022.3212329","ARC Laureate Program(grant numbers:FL190100035); ARC Discovery(grant numbers:DP200100020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925643","Human-centric issues;GitHub repositories;Google Play Store;human aspects;machine learning;deep learning","Mobile applications;Software development management;Internet;Taxonomy;Software systems;Australia;Security","","3","","83","IEEE","20 Oct 2022","","","IEEE","IEEE Journals"
"Task-Oriented ML/DL Library Recommendation Based on a Knowledge Graph","M. Liu; C. Zhao; X. Peng; S. Yu; H. Wang; C. Sha","School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; Tongji University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China","IEEE Transactions on Software Engineering","14 Aug 2023","2023","49","8","4081","4096","AI applications often use ML/DL (Machine Learning/Deep Learning) models to implement specific AI tasks. As application developers usually are not AI experts, they often choose to integrate existing implementations of ML/DL models as libraries for their AI tasks. As an active research area, AI attracts many researchers and produces a lot of papers every year. Many of the papers propose ML/DL models for specific tasks and provide their implementations. However, it is not easy for developers to find ML/DL libraries that are suitable for their tasks. The challenges lie in not only the fast development of AI application domains and techniques, but also the lack of detailed information of the libraries such as environmental dependencies and supporting resources. In this paper, we conduct an empirical study on ML/DL library seeking questions on Stack Overflow to understand the developers’ requirements for ML/DL libraries. Based on the findings of the study, we propose a task-oriented ML/DL library recommendation approach, called MLTaskKG. It constructs a knowledge graph that captures AI tasks, ML/DL models, model implementations, repositories, and their relationships by extracting knowledge from different sources such as ML/DL resource websites, papers, ML/DL frameworks, and repositories. Based on the knowledge graph, MLTaskKG recommends ML/DL libraries for developers by matching their requirements on tasks, model characteristics, and implementation information. Our evaluation shows that 92.8% of the tuples sampled from the resulting knowledge graph are correct, demonstrating the high quality of the knowledge graph. A further experiment shows that MLTaskKG can help developers find suitable ML/DL libraries using 47.6% shorter time and with 68.4% higher satisfaction.","1939-3520","","10.1109/TSE.2023.3285280","National Natural Science Foundation of China(grant numbers:61972098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149441","Deep learning;knowledge graph;library recommendation;machine learning","Task analysis;Artificial intelligence;Libraries;Predictive models;Knowledge graphs;Codes;Operating systems","","1","","79","IEEE","13 Jun 2023","","","IEEE","IEEE Journals"
"A Comprehensive Investigation of the Impact of Class Overlap on Software Defect Prediction","L. Gong; H. Zhang; J. Zhang; M. Wei; Z. Huang","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2440","2458","Software Defect Prediction (SDP) is one of the most vital and cost-efficient operations to ensure the software quality. However, there exists the phenomenon of class overlap in the SDP datasets (i.e., defective and non-defective modules are similar in terms of values of metrics), which hinders the performance as well as the use of SDP models. Even though efforts have been made to investigate the impact of removing overlapping technique on the performance of SDP, many open issues are still challenging yet unknown. Therefore, we conduct an empirical study to comprehensively investigate the impact of class overlap on SDP. Specifically, we first propose an overlapping instances identification approach by analyzing the class distribution in the local neighborhood of a given instance. We then investigate the impact of class overlap and two common overlapping instance handling techniques on the performance and the interpretation of seven representative SDP models. Through an extensive case study on 230 diversity datasets, we observe that: i) 70.0% of SDP datasets contain overlapping instances; ii) different levels of class overlap have different impacts on the performance of SDP models; iii) class overlap affects the rank of the important feature list of SDP models, particularly the feature lists at the top 2 and top 3 ranks; IV) Class overlap handling techniques could statistically significantly improve the performance of SDP models trained on datasets with over 12.5% overlap ratios. We suggest that future work should apply our KNN method to identify the overlap ratios of datasets before building SDP models.","1939-3520","","10.1109/TSE.2022.3220740","National Natural Science Foundation of China(grant numbers:62202223); Natural Science Foundation of Jiangsu Province(grant numbers:BK20220881); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944157","Class overlap;data quality;k-nearest neighbourhood;local analysis;software defect prediction;software metrics","Software;Measurement;Predictive models;Classification tree analysis;Stability analysis;NASA;Machine learning algorithms","","5","","84","IEEE","9 Nov 2022","","","IEEE","IEEE Journals"
"A Dataset for GitHub Repository Deduplication","D. Spinellis; Z. Kotti; A. Mockus",Athens University of Economics and Business; Athens University of Economics and Business; University of Tennessee,"2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","523","527","GitHub projects can be easily replicated through the site's fork process or through a Git clone-push sequence. This is a problem for empirical software engineering, because it can lead to skewed results or mistrained machine learning models. We provide a dataset of 10.6 million GitHub projects that are copies of others, and link each record with the project's ultimate parent. The ultimate parents were derived from a ranking along six metrics. The related projects were calculated as the connected components of an 18.2 million node and 12 million edge denoised graph created by directing edges to ultimate parents. The graph was created by filtering out more than 30 hand-picked and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted clumping were identified by repeatedly visualizing shortest path distances between unrelated important projects. Our dataset identified 30 thousand duplicate projects in an existing popular reference dataset of 1.8 million projects. An evaluation of our dataset against another created independently with different methods found a significant overlap, but also differences attributed to the operational definition of what projects are considered as related.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148680","Deduplication;fork;project clone;GitHub;dataset","Measurement;Visualization;Filtering;Machine learning;Software;Data mining;Software development management","","9","","39","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Do Subjectivity and Objectivity Always Agreeƒ A Case Study with Stack Overflow Questions","S. Mondal; M. M. Rahman; C. K. Roy","University of Saskatchewan, Canada; Dalhousie University, Canada; University of Saskatchewan, Canada","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","389","401","In Stack Overflow (SO), the quality of posts (i.e., questions and answers) is subjectively evaluated by users through a voting mechanism. The net votes (upvotes − downvotes) obtained by a post are often considered an approximation of its quality. However, about half of the questions that received working solutions got more downvotes than upvotes. Furthermore, about 18% of the accepted answers (i.e., verified solutions) also do not score the maximum votes. All these counter-intuitive findings cast doubts on the reliability of the evaluation mechanism employed at SO. Moreover, many users raise concerns against the evaluation, especially downvotes to their posts. Therefore, rigorous verification of the subjective evaluation is highly warranted to ensure a non-biased and reliable quality assessment mechanism. In this paper, we compare the subjective assessment of questions with their objective assessment using 2.5 million questions and ten text analysis metrics. According to our investigation, four objective metrics agree with the subjective evaluation, two do not agree, one either agrees or disagrees, and the remaining three neither agree nor disagree with the subjective evaluation. We then develop machine learning models to classify the promoted and discouraged questions. Our models outperform the state-of-the-art models with a maximum of about 76%–87% accuracy.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00060","Natural Sciences and Engineering Research Council of Canada; Canada First Research Excellence Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174053","Stack Overflow;question quality;objective evaluation;subjective evaluation;quality metrics;classification model","Measurement;Codes;Text analysis;Correlation;Machine learning;Entropy;User experience","","1","","60","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"SoftMon: A Tool to Compare Similar Open-source Software from a Performance Perspective","S. S. Singh; S. R. Sarangi","Computer Science and Engineering, IIT Delhi; Computer Science, IIT Delhi","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","397","408","Over the past two decades, a rich ecosystem of open-source software has evolved. For every type of application, there are a wide variety of alternatives. We observed that even if different applications that perform similar tasks and compiled with the same versions of the compiler and the libraries, they perform very differently while running on the same system. Sadly prior work in this area that compares two code bases for similarities does not help us in finding the reasons for the differences in performance. In this paper, we develop a tool, SoftMon, that can compare the codebases of two separate applications and pinpoint the exact set of functions that are disproportionately responsible for differences in performance. Our tool uses machine learning and NLP techniques to analyze why a given open-source application has a lower performance as compared to its peers, design bespoke applications that can incorporate specific innovations (identified by SoftMon) in competing applications, and diagnose performance bugs. In this paper, we compare a wide variety of large open-source programs such as image editors, audio players, text editors, PDF readers, mail clients and even full-fledged operating systems (OSs). In all cases, our tool was able to pinpoint a set of at the most 10–15 functions that are responsible for the differences within 200 seconds. A subsequent manual analysis assisted by our graph visualization engine helps us find the reasons. We were able to validate most of the reasons by correlating them with subsequent observations made by developers or from existing technical literature. The manual phase of our analysis is limited to 30 minutes (tested with human subjects).","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148752","Software comparison;Performance debugging;NLP based matching","Visualization;Technological innovation;Codes;Operating systems;Manuals;Machine learning;Libraries","","","","83","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Detecting Developers’ Task Switches and Types","A. N. Meyer; C. Satterfield; M. Züger; K. Kevic; G. C. Murphy; T. Zimmermann; T. Fritz","Department of Informatics, University of Zurich, Zürich, Switzerland; University of British Columbia, Vancouver, BC, Canada; Department of Informatics, University of Zurich, Zürich, Switzerland; Microsoft Research, Redmond, Washington, USA; University of British Columbia, Vancouver, BC, Canada; Microsoft Research, Redmond, Washington, USA; Department of Informatics, University of Zurich, Zürich, Switzerland","IEEE Transactions on Software Engineering","10 Jan 2022","2022","48","1","225","240","Developers work on a broad variety of tasks during their workdays and constantly switch between them. While these task switches can be beneficial, they can also incur a high cognitive burden on developers, since they have to continuously remember and rebuild the task context–the artifacts and applications relevant to the task. Researchers have therefore proposed to capture task context more explicitly and use it to provide better task support, such as task switch reduction or task resumption support. Yet, these approaches generally require the developer to manually identify task switches. Automatic approaches for predicting task switches have so far been limited in their accuracy, scope, evaluation, and the time discrepancy between predicted and actual task switches. In our work, we examine the use of automatically collected computer interaction data for detecting developers’ task switches as well as task types. In two field studies–a 4h observational study and a multi-day study with experience sampling–we collected data from a total of 25 professional developers. Our study results show that we are able to use temporal and semantic features from developers’ computer interaction data to detect task switches and types in the field with high accuracy of 84 percent and 61 percent respectively, and within a short time window of less than 1.6 minutes on average from the actual task switch. We discuss our findings and their practical value for a wide range of applications in real work settings.","1939-3520","","10.1109/TSE.2020.2984086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069309","Task detection;task switching;multi-tasking;work fragmentation;activity recognition;machine learning","Task analysis;Feature extraction;Semantics;Microsoft Windows;Switches;Machine learning","","4","","85","CCBY","16 Apr 2020","","","IEEE","IEEE Journals"
"PUMiner: Mining Security Posts from Developer Question and Answer Websites with PU Learning","T. H. Minh Le; D. Hin; R. Croft; M. A. Babar","School of Computer Science, The University of Adelaide, Adelaide, Australia; School of Computer Science, The University of Adelaide, Adelaide, Australia; School of Computer Science, The University of Adelaide, Adelaide, Australia; School of Computer Science, The University of Adelaide, Adelaide, Australia","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","350","361","Security is an increasing concern in software development. Developer Question and Answer (Q&A) websites provide a large amount of security discussion. Existing studies have used human-defined rules to mine security discussions, but these works still miss many posts, which may lead to an incomplete analysis of the security practices reported on Q&A websites. Traditional supervised Machine Learning methods can automate the mining process; however, the required negative (non-security) class is too expensive to obtain. We propose a novel learning framework, PUMiner, to automatically mine security posts from Q&A websites. PUMiner builds a context-aware embedding model to extract features of the posts, and then develops a two-stage PU model to identify security content using the labelled Positive and Unlabelled posts. We evaluate PUMiner on more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange. We show that PUMiner is effective with the validation performance of at least 0.85 across all model configurations. Moreover, Matthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner also performs well with an MCC of 0.745 for scenarios where string matching totally fails. Even when the ratio of the labelled positive posts to the unlabelled ones is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security content on Q&A websites for practitioners and researchers.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148721","Mining Software Repositories;Positive Unlabelled Learning;Machine Learning;Natural Language Processing;Software Security","Support vector machines;Correlation coefficient;Filtering;Machine learning;Feature extraction;Software;Security","","6","","80","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Tell Me Who Are You Talking to and I Will Tell You What Issues Need Your Skills","F. Santos; J. Penney; J. F. Pimentel; I. Wiese; I. Steinmacher; M. A. Gerosa","Northern Arizona University, Flagstaff, AZ, USA; Northern Arizona University, Flagstaff, AZ, USA; Northern Arizona University, Flagstaff, AZ, USA; Universidade Tecnológica Federal Do Paraná, Campo Mourão, PR, Brazil; Northern Arizona University, Flagstaff, AZ, USA; Northern Arizona University, Flagstaff, AZ, USA","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","611","623","Selecting an appropriate task is challenging for newcomers to Open Source Software (OSS) projects. To facilitate task selection, researchers and OSS projects have leveraged machine learning techniques, historical information, and textual analysis to label tasks (a.k.a. issues) with information such as the issue type and domain. These approaches are still far from mainstream adoption, possibly because of a lack of good predictors. Inspired by previous research, we advocate that label prediction might benefit from leveraging metrics derived from communication data and social network analysis (SNA) for issues in which social interaction occurs. Thus, we study how these ""social metrics"" can improve the automatic labeling of open issues with API domains—categories of APIs used in the source code that solves the issue—which the literature shows that newcomers to the project consider relevant for task selection. We mined data from OSS projects’ repositories and organized it in periods to reflect the seasonality of the contributors’ project participation. We replicated metrics from previous work and added social metrics to the corpus to predict API-domain labels. Social metrics improved the performance of the classifiers compared to using only the issue description text in terms of precision, recall, and F-measure. Precision (0.922) increased by 15.82% and F-measure (0.942) by 15.89% for a project with high social activity. These results indicate that social metrics can help capture the patterns of social interactions in a software project and improve the labeling of issues in an issue tracker.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00087","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173992","Labels;Tags;Skills;Human Factors;Mining Software Repositories;Social Network Analysis;Open Source Software;Machine Learning","Measurement;Social networking (online);Source coding;Oral communication;Machine learning;Predictive models;Labeling","","1","","70","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"An Empirical Comparison of Model Validation Techniques for Defect Prediction Models","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Department of Electrical and Computer Engineering, Montreal, QC, Canada; School of Computing, Queen’s University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan","IEEE Transactions on Software Engineering","9 Jan 2017","2017","43","1","1","18","Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as $k$ -fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results– - selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.","1939-3520","","10.1109/TSE.2016.2584050","JSPS; Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers; Interdisciplinary Global Networks for Accelerating Theory and Practice in Software Ecosystem; JSPS Fellows(grant numbers:16J03360); Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497471","Defect prediction models;model validation techniques;bootstrap validation;cross validation;holdout validation","Predictive models;Data models;Analytical models;Context;Context modeling;Software;Logistics","","344","","5","IEEE","22 Jun 2016","","","IEEE","IEEE Journals"
"Data Scientists in Software Teams: State of the Art and Challenges","M. Kim; T. Zimmermann; R. DeLine; A. Begel","University of California, Los Angeles, CA; Microsoft Research, One Microsoft Way, Redmond, WA; Microsoft Research, One Microsoft Way, Redmond, WA; Microsoft Research, One Microsoft Way, Redmond, WA","IEEE Transactions on Software Engineering","11 Nov 2018","2018","44","11","1024","1038","The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.","1939-3520","","10.1109/TSE.2017.2754374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8046093","Data science;development roles;software engineering;industry","Data science;Tools;Sociology;Statistics;Software;Best practices;Interviews","","109","","33","IEEE","19 Sep 2017","","","IEEE","IEEE Journals"
"An Empirical Study of Obsolete Answers on Stack Overflow","H. Zhang; S. Wang; T. -H. Chen; Y. Zou; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), Queen’s University, Kingston, ON, Canada; Software Analysis and Intelligence Lab (SAIL), Queen’s University, Kingston, ON, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada; Software Analysis and Intelligence Lab (SAIL), Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","16 Apr 2021","2021","47","4","850","862","Stack Overflow accumulates an enormous amount of software engineering knowledge. However, as time passes, certain knowledge in answers may become obsolete. Such obsolete answers, if not identified or documented clearly, may mislead answer seekers and cause unexpected problems (e.g., using an out-dated security protocol). In this paper, we investigate how the knowledge in answers becomes obsolete and identify the characteristics of such obsolete answers. We find that: 1) More than half of the obsolete answers (58.4 percent) were probably already obsolete when they were first posted. 2) When an obsolete answer is observed, only a small proportion (20.5 percent) of such answers are ever updated. 3) Answers to questions in certain tags (e.g., node.js, ajax, android, and objective-c) are more likely to become obsolete. Our findings suggest that Stack Overflow should develop mechanisms to encourage the whole community to maintain answers (to avoid obsolete answers) and answer seekers are encouraged to carefully go through all information (e.g., comments) in answer threads.","1939-3520","","10.1109/TSE.2019.2906315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669958","Q&A website;stack overflow;obsolete knowledge;knowledge sharing","Aging;Knowledge engineering;Message systems;Google;Computer languages;Software engineering;Security","","33","","39","IEEE","19 Mar 2019","","","IEEE","IEEE Journals"
"A Systematic Review of Interaction in Search-Based Software Engineering","A. Ramírez; J. R. Romero; C. L. Simons","Department of Computer Science and Numerical Analysis, University of Córdoba, Córdoba, Spain; Department of Computer Science and Numerical Analysis, University of Córdoba, Córdoba, Spain; Department of Computer Science and Creative Technologies, University of the West of England, Bristol, United Kingdom","IEEE Transactions on Software Engineering","26 Aug 2019","2019","45","8","760","781","Search-Based Software Engineering (SBSE) has been successfully applied to automate a wide range of software development activities. Nevertheless, in those software engineering problems where human evaluation and preference are crucial, such insights have proved difficult to characterize in search, and solutions might not look natural when that is the expectation. In an attempt to address this, an increasing number of researchers have reported the incorporation of the 'human-in-the-loop' during search and interactive SBSE has attracted significant attention recently. However, reported results are fragmented over different development phases, and a great variety of novel interactive approaches and algorithmic techniques have emerged. To better integrate these results, we have performed a systematic literature review of interactive SBSE. From a total of 669 papers, 26 primary studies were identified. To enable their analysis, we formulated a classification scheme focused on four crucial aspects of interactive search, i.e., the problem formulation, search technique, interactive approach, and the empirical framework. Our intention is that the classification scheme affords a methodological approach for interactive SBSE. Lastly, as well as providing a detailed cross analysis, we identify and discuss some open issues and potential future trends for the research community.","1939-3520","","10.1109/TSE.2018.2803055","Spanish Ministry of Economy and Competitiveness(grant numbers:TIN2017-83445-P,TIN2015-71841-REDT); Spanish Ministry of Education(grant numbers:FPU13/01466); FEDER; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283568","Search-based software engineering;interaction;systematic literature review;optimization","Software;Software engineering;Search problems;Optimization;Systematics;Market research;Software metrics","","33","","100","IEEE","6 Feb 2018","","","IEEE","IEEE Journals"
"Towards Security Threats of Deep Learning Systems: A Survey","Y. He; G. Meng; K. Chen; X. Hu; J. He","Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China","IEEE Transactions on Software Engineering","16 May 2022","2022","48","5","1743","1770","Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning’s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches’ merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.","1939-3520","","10.1109/TSE.2020.3034721","National Key Research and Development Program of China(grant numbers:2020AAA0140001); Beijing Natural Science Foundation(grant numbers:JQ18011); National Natural Science Foundation of China(grant numbers:U1836211,61902395); National Top-notch Youth Talents Program of China; Youth Innovation Promotion Association of the Chinese Academy of Sciences; Beijing Nova Program; National Frontier Science and Technology Innovation(grant numbers:YJKYYQ20170070); Beijing Academy of Artificial Intelligence; CCF-Tencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252914","Deep learning;poisoning attack;adversarial attack;model extraction attack;model inversion attack","Deep learning;Security;Data models;Privacy;Predictive models;Training data","","29","","277","IEEE","9 Nov 2020","","","IEEE","IEEE Journals"
"Reinforcement Learning for Test Case Prioritization","M. Bagherzadeh; N. Kahani; L. Briand","School of EECS, University of Ottawa, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; School of EECS, University of Ottawa, Ottawa, ON, Canada","IEEE Transactions on Software Engineering","16 Aug 2022","2022","48","8","2836","2856","Continuous Integration (CI) significantly reduces integration problems, speeds up development time, and shortens release time. However, it also introduces new challenges for quality assurance activities, including regression testing, which is the focus of this work. Though various approaches for test case prioritization have shown to be very promising in the context of regression testing, specific techniques must be designed to deal with the dynamic nature and timing constraints of CI. Recently, Reinforcement Learning (RL) has shown great potential in various challenging scenarios that require continuous adaptation, such as game playing, real-time ads bidding, and recommender systems. Inspired by this line of work and building on initial efforts in supporting test case prioritization with RL techniques, we perform here a comprehensive investigation of RL-based test case prioritization in a CI context. To this end, taking test case prioritization as a ranking problem, we model the sequential interactions between the CI environment and a test case prioritization agent as an RL problem, using three alternative ranking models. We then rely on carefully selected and tailored state-of-the-art RL techniques to automatically and continuously learn a test case prioritization strategy, whose objective is to be as close as possible to the optimal one. Our extensive experimental analysis shows that the best RL solutions provide a significant accuracy improvement over previous RL-based work, with prioritization strategies getting close to being optimal, thus paving the way for using RL to prioritize test cases in a CI context.","1939-3520","","10.1109/TSE.2021.3070549","Huawei Technologies Canada; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394799","Continuous integration;CI;reinforcement learning;test prioritization","Testing;History;Training;Reinforcement learning;Software systems;Adaptation models;Software algorithms","","27","","78","IEEE","2 Apr 2021","","","IEEE","IEEE Journals"
"PatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the Linux Kernel","T. Hoang; J. Lawall; Y. Tian; R. J. Oentaryo; D. Lo","Singapore Management University, Singapore; Inria, LIP6, Sorbonne University, Paris, France; Queen's University, Kingston, ON, Canada; McLaren Applied Technologies, Singapore; Singapore Management University, Singapore","IEEE Transactions on Software Engineering","11 Nov 2021","2021","47","11","2471","2486","Linux kernel stable versions serve the needs of users who value stability of the kernel over new features. The quality of such stable versions depends on the initiative of kernel developers and maintainers to propagate bug fixing patches to the stable versions. Thus, it is desirable to consider to what extent this process can be automated. A previous approach relies on words from commit messages and a small set of manually constructed code features. This approach, however, shows only moderate accuracy. In this paper, we investigate whether deep learning can provide a more accurate solution. We propose PatchNet, a hierarchical deep learning-based approach capable of automatically extracting features from commit messages and commit code and using them to identify stable patches. PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of commit code, making it distinctive from the existing deep learning models on source code. Experiments on 82,403 recent Linux patches confirm the superiority of PatchNet against various state-of-the-art baselines, including the one recently-adopted by Linux kernel maintainers.","1939-3520","","10.1109/TSE.2019.2952614","National Research Foundation Singapore(grant numbers:NRF2016-NRF-ANR003); ANR ITrans; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896061","Linux kernel;patch classification;deep learning","Kernel;Linux;Computer bugs;Feature extraction;Deep learning;Indexes;Manuals","","12","","72","IEEE","11 Nov 2019","","","IEEE","IEEE Journals"
"Revisiting, Benchmarking and Exploring API Recommendation: How Far Are We?","Y. Peng; S. Li; W. Gu; Y. Li; W. Wang; C. Gao; M. R. Lyu","Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies, Harbin Institute of Technology, Shenzhen, China; Chinese University of Hong Kong, Hong Kong, China","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1876","1897","Application Programming Interfaces (APIs), which encapsulate the implementation of specific functions as interfaces, greatly improve the efficiency of modern software development. As the number of APIs grows up fast nowadays, developers can hardly be familiar with all the APIs and usually need to search for appropriate APIs for usage. So lots of efforts have been devoted to improving the API recommendation task. However, it has been increasingly difficult to gauge the performance of new models due to the lack of a uniform definition of the task and a standardized benchmark. For example, some studies regard the task as a code completion problem, while others recommend relative APIs given natural language queries. To reduce the challenges and better facilitate future research, in this paper, we revisit the API recommendation task and aim at benchmarking the approaches. Specifically, the paper groups the approaches into two categories according to the task definition, i.e., query-based API recommendation and code-based API recommendation. We study 11 recently-proposed approaches along with 4 widely-used IDEs. One benchmark named APIBench is then built for the two respective categories of approaches. Based on APIBench, we distill some actionable insights and challenges for API recommendation. We also achieve some implications and directions for improving the performance of recommending APIs, including appropriate query reformulation, data source selection, low resource setting, user-defined APIs, and query-based API recommendation with usage patterns.","1939-3520","","10.1109/TSE.2022.3197063","Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:CUHK 14210920); General Research Fund; National Natural Science Foundation of China(grant numbers:62002084); Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:2022B1212010005); Stable support plan for colleges and universities in Shenzhen(grant numbers:GXWD2020 1230155427003-20200730101839009); Major Key Project of PCL(grant numbers:PCL2022A03,PCL2021A02,PCL2021A09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851934","API recommendation;benchmark;empirical study","Task analysis;Codes;Benchmark testing;Knowledge based systems;Soft sensors;Java;Tutorials","","7","","71","IEEE","8 Aug 2022","","","IEEE","IEEE Journals"
"Detecting Continuous Integration Skip Commits Using Multi-Objective Evolutionary Search","I. Saidani; A. Ouni; M. W. Mkaouer","Department of Software Engineering and IT, ETS Montreal, University of Quebec, Montreal, QC, Canada; Department of Software Engineering and IT, ETS Montreal, University of Quebec, Montreal, QC, Canada; Department of Software Engineering, Rochester Institue of Technology, Rochester, NY, USA","IEEE Transactions on Software Engineering","12 Dec 2022","2022","48","12","4873","4891","Continuous Integration (CI) consists of integrating the changes introduced by different developers more frequently through the automation of build process. Nevertheless, the CI build process is seen as a major barrier that causes delays in the product release dates. One of the main reasons for such delays is that some simple changes (i.e., can be skipped) trigger the build, which represents an unnecessary overhead and particularly painful for large projects. In order to cut off the expenses of CI build time, we propose in this paper, SkipCI, a novel search-based approach to automatically detect CI Skip commits based on the adaptation of Strength-Pareto Evolutionary Algorithm (SPEA-2). Our approach aims to provide the optimal trade-off between two conflicting objectives to deal with both skipped and non-skipped commits. We evaluate our approach and investigate the performance of both within and cross-project validations on a benchmark of 14,294 CI commits from 15 projects that use Travis CI system. The statistical tests revealed that our approach shows a clear advantage over the baseline approaches with average scores of 92% and 84% in terms of AUC for cross-validation and cross-project validations respectively. Furthermore, the features analysis reveals that documentation changes, terms appearing in the commit message and the committer experience are the most prominent features in CI skip detection. When it comes to the cross-project scenario, the results reveal that besides the documentation changes, there is a strong link between current and previous commits results. Moreover, we deployed and evaluated the usefulness of SkipCI with our industrial partner. Qualitative results demonstrate the effectiveness of SkipCI in providing relevant CI skip commit recommendations to developers for two large software projects from practitioner’s point of view.","1939-3520","","10.1109/TSE.2021.3129165","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2018-05960); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622158","Continuous integration;travis CI;software build;search-based software engineering;genetic programming","Feature extraction;Tools;Search problems;Software engineering;Optimization;Documentation;Codes","","4","","73","IEEE","19 Nov 2021","","","IEEE","IEEE Journals"
"Building Maintainable Software Using Abstraction Layering","J. Spray; R. Sinha; A. Sen; X. Cheng","Datamars, Auckland, New Zealand; Department of Computer Science & Software Engineering, Auckland University of Technology, Auckland, New Zealand; Department of Computer Science & Software Engineering, Auckland University of Technology, Auckland, New Zealand; Datamars, Auckland, New Zealand","IEEE Transactions on Software Engineering","11 Nov 2022","2022","48","11","4397","4410","Increased software maintainability can help improve a company's profitability by directly reducing ongoing software development costs. Abstraction Layered Architecture (ALA) is a reference architecture for building maintainable applications, but its effectiveness in commercial projects has remained unexplored. This research, carried out as a 16-month joint industry-academic project, explores developing commercial code bases using ALA and the extent to which ALA improves maintainability. An existing application from Datamars, New Zealand, was re-developed by using ALA and compared with the original application. In order to carry out these comparisons, we developed suitable measures by adapting maintainability characteristics from the ISO 25010 family of standards. Specifically, we determined metrics to capture the five sub-characteristics of maintainability: modularity, reusability, analysability, modifiability, and testability; and used them to test our hypothesis that the use of ALA improved maintainability of the application. During the evaluation, we found that the modularity, reusability, analysability, and testability of the re-developed ALA application were higher than for the original application. The modifiability of the ALA-based application was lower in the short-term, but shown to trend upwards in the longer term. Our findings led to proposing a generalised ALA-based development method that promises a significant reduction in maintenance costs.","1939-3520","","10.1109/TSE.2021.3119012","Callaghan Innovation(grant numbers:DAMAE1802/PROP-62523-FELLOW-DAMAE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566775","Maintainability;ALA;modularity;reusability;analysability;modifiability;testability","Codes;Software;Programming;Couplings;Measurement;Maintenance engineering;ISO Standards","","3","","43","IEEE","11 Oct 2021","","","IEEE","IEEE Journals"
"Use and Misuse of the Term “Experiment” in Mining Software Repositories Research","C. Ayala; B. Turhan; X. Franch; N. Juristo","Universitat Politècnica de Catalunya, BarcelonaTECH, Barcelona, Spain; University of Oulu, Oulu, Finland; Universitat Politècnica de Catalunya, BarcelonaTECH, Barcelona, Spain; Universidad Politécnica de Madrid, Boadilla del Monte, Spain","IEEE Transactions on Software Engineering","11 Nov 2022","2022","48","11","4229","4248","The significant momentum and importance of Mining Software Repositories (MSR) in Software Engineering (SE) has fostered new opportunities and challenges for extensive empirical research. However, MSR researchers seem to struggle to characterize the empirical methods they use into the existing empirical SE body of knowledge. This is especially the case of MSR experiments. To provide evidence on the special characteristics of MSR experiments and their differences with experiments traditionally acknowledged in SE so far, we elicited the hallmarks that differentiate an experiment from other types of empirical studies and characterized the hallmarks and types of experiments in MSR. We analyzed MSR literature obtained from a small-scale systematic mapping study to assess the use of the term experiment in MSR. We found that 19% of the papers claiming to be an experiment are indeed not an experiment at all but also observational studies, so they use the term in a misleading way. From the remaining 81% of the papers, only one of them refers to a genuine controlled experiment while the others stand for experiments with limited control. MSR researchers tend to overlook such limitations, compromising the interpretation of the results of their studies. We provide recommendations and insights to support the improvement of MSR experiments.","1939-3520","","10.1109/TSE.2021.3113558","Spanish project(grant numbers:MCI PID2020-117191RB-I00,AEI/10.13039/501100011033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547824","Empirical software engineering;controlled experiment;mining software repositories;research methodology","Software;Software engineering;Data mining;Systematics;Resource management;Tools;Terminology","","3","","74","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"Legion: Massively Composing Rankers for Improved Bug Localization at Adobe","D. Jarman; J. Berry; R. Smith; F. Thung; D. Lo","Adobe, Lehi, UT, USA; Adobe, Lehi, UT, USA; Adobe, Lehi, UT, USA; School of Information Systems, Singapore Management University, Singapore, Singapore; School of Information Systems, Singapore Management University, Singapore, Singapore","IEEE Transactions on Software Engineering","16 Aug 2022","2022","48","8","3010","3024","Studies have estimated that, in industrial settings, developers spend between 30 and 90 percent of their time fixing bugs. As such, tools that assist in identifying the location of bugs provide value by reducing debugging costs. One such tool is BugLocator. This study initially aimed to determine if developers working on the Adobe Analytics product could use BugLocator. The initial results show that BugLocator achieves a similar accuracy on five of seven Adobe Analytics repositories and on open-source projects. However, these results do not meet the minimum applicability requirement deemed necessary by Adobe Analytics developers prior to possible adoption. Thus, we consequently examine how BugLocator can achieve the targeted accuracy with two extensions: (1) adding more data corpora, and (2) massively composing individual rankers consisting of augmented BugLocator instances trained on various combinations of corpora and parameter configurations with a Random Forest model. We refer to our final extension as Legion. On average, applying Legion to Adobe Analytics repositories results in at least one buggy file ranked in the top-ten recommendations 76.8 percent of the time for customer-reported bugs across all 7 repositories. This represents a substantial improvement over BugLocator of 36.4 percent, and satisfies the minimum applicability requirement. Additionally, our extensions boost Mean Average Precision by 107.7 percent, Mean Reciprocal Rank by 86.1 percent, Top 1 by 143.4 percent and Top 5 by 58.1 percent.","1939-3520","","10.1109/TSE.2021.3075215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415126","Bug localization;information retrieval;bug reports;data augmentation;ranker composition;industrial study","Computer bugs;Location awareness;Tools;Debugging;Random forests;Programming;Information retrieval","","3","","65","IEEE","23 Apr 2021","","","IEEE","IEEE Journals"
"Hyperparameter Optimization for AST Differencing","M. Martinez; J. -R. Falleri; M. Monperrus","Universitat Politècnica de Catalunya, CP, Barcelona, Spain; CNRS, Bordeaux INP, LaBRI, Univ. Bordeaux, Talence, France; KTH Royal Institute of Technology, Sweden","IEEE Transactions on Software Engineering","16 Oct 2023","2023","49","10","4814","4828","Computing the differences between two versions of the same program is an essential task for software development and software evolution research. AST differencing is the most advanced way of doing so, and an active research area. Yet, AST differencing algorithms rely on configuration parameters that may have a strong impact on their effectiveness. In this paper, we present a novel approach named DAT (D iff Auto Tuning) for hyperparameter optimization of AST differencing. We thoroughly state the problem of hyper-configuration for AST differencing. We evaluate our data-driven approach DAT to optimize the edit-scripts generated by the state-of-the-art AST differencing algorithm named GumTree in different scenarios. DAT is able to find a new configuration for GumTree that improves the edit-scripts in 21.8% of the evaluated cases.","1939-3520","","10.1109/TSE.2023.3315935","Ministerio de Ciencia e Innovación(grant numbers:RYC2021-031523-I); GAISSA Spanish research(grant numbers:TED2021-130923B-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286467","Software evolution;Tree differencing;Abstract Syntax Trees (AST);hyperparameter optimization, edit-script","Training;Software algorithms;Computer bugs;Syntactics;Maintenance engineering;Hyperparameter optimization;Software","","","","55","IEEE","16 Oct 2023","","","IEEE","IEEE Journals"
"Towards Extracting Web API Specifications from Documentation","J. Yang; E. Wittern; A. T. T. Ying; J. Dolby; L. Tan","University of Waterloo, Waterloo, Ontario, Canada; IBM T.J. Watson Research Center, Yorktown Heights, NY, USA; Equity Sim, Vancouver, BC, Canada; IBM T.J. Watson Research Center, Yorktown Heights, NY, USA; University of Waterloo, Waterloo, Ontario, Canada","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","454","464","Web API specifications are machine-readable descriptions of APIs. These specifications, in combination with related tooling, simplify and support the consumption of APIs. However, despite the increased distribution of web APIs, specifications are rare and their creation and maintenance heavily rely on manual efforts by third parties. In this paper, we propose an automatic approach and an associated tool called D2Spec for extracting significant parts of such specifications from web API documentation pages. Given a seed online documentation page of an API, D2Spec first crawls all documentation pages on the API, and then uses a set of machine-learning techniques to extract the base URL, path templates, and HTTP methods - collectively describing the endpoints of the API. We evaluate whether D2Spec can accurately extract endpoints from documentation on 116 web APIs. The results show that D2Spec achieves a precision of 87.1% in identifying base URLs, a precision of 80.3% and a recall of 80.9% in generating path templates, and a precision of 83.8% and a recall of 77.2% in extracting HTTP methods. In addition, in an evaluation on 64 APIs with pre-existing API specifications, D2Spec revealed many inconsistencies between web API documentation and their corresponding publicly available specifications. API consumers would benefit from D2Spec pointing them to, and allowing them thus to fix, such inconsistencies.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595229","Web API;Web API Specification;Information Extraction","Uniform resource locators;Documentation;Clustering algorithms;Data mining;Software;Tools;Semantics","","","","32","","30 Dec 2018","","","IEEE","IEEE Conferences"
"The Vocabulary of Flaky Tests in the Context of SAP HANA","A. Berndt; Z. Nochta; T. Bach","Karlsruhe University of Applied Sciences, Karlsruhe, Germany; Karlsruhe University of Applied Sciences, Karlsruhe, Germany; SAP, Walldorf, Germany","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","9","Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304860","test flakiness;software testing;regression testing;machine learning","Training;Vocabulary;Codes;Source coding;Soft sensors;Feature extraction;Data models","","","","58","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Learning the Relation Between Code Features and Code Transforms With Structured Prediction","Z. Yu; M. Martinez; Z. Chen; T. F. Bissyandé; M. Monperrus","Shandong University, Jinan, China; Universitat Politècnica de Catalunya, Barcelona, Spain; KTH Royal Institute of Technology, Stockholm, Sweden; University of Luxembourg, Esch-sur-Alzette, Luxembourg; KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Software Engineering","17 Jul 2023","2023","49","7","3872","3900","To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this article the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises transform constraints that are specific to code. We conduct a large-scale experimental evaluation based on a dataset of bug fixing commits from real-world Java projects. The results show that when the popular evaluation metric top-3 is used, our approach predicts the code transforms with an accuracy varying from 41% to 53% depending on the transforms. Our model outperforms two baselines based on history probability and neural machine translation (NMT), suggesting the importance of considering code structure in achieving good prediction accuracy. In addition, a proof-of-concept synthesizer is implemented to concretize some repair transforms to get the final patches. The evaluation of the synthesizer on the Defects4j benchmark confirms the usefulness of the predicted AST-level repair transforms in producing high-quality patches.","1939-3520","","10.1109/TSE.2023.3275380","National Natural Science Foundation of China(grant numbers:62102233); Shandong Province Overseas Outstanding Youth Fund(grant numbers:2022HWYQ-043); Qilu Young Scholar Program of Shandong University; Wallenberg Artificial Intelligence; Wallenberg Autonomous Systems and Software Program; Knut och Alice Wallenbergs Stiftelse; Swedish National Infrastructure for Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130317","Code transform;big code;machine learning;program repair","Transforms;Codes;Maintenance engineering;Predictive models;Synthesizers;Computer bugs;Feature extraction","","1","","113","IEEE","22 May 2023","","","IEEE","IEEE Journals"
"Automated Software Vulnerability Assessment with Concept Drift","T. H. M. Le; B. Sabir; M. A. Babar","School of Computer Science, The University of Adelaide, Adelaide, Australia; School of Computer Science, The University of Adelaide, Adelaide, Australia; School of Computer Science, The University of Adelaide, Adelaide, Australia","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","371","382","Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816739","software vulnerability;machine learning;multi-class classification;natural language processing;mining software repositories","Training;Software;Natural language processing;Buildings;Feature extraction;Predictive models;Classification algorithms","","19","","58","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Toward Deep Learning Software Repositories","M. White; C. Vendome; M. Linares-Vasquez; D. Poshyvanyk","Department of Computer Science, College of William and Mary, Williamsburg, Virginia; Department of Computer Science, College of William and Mary, Williamsburg, Virginia; Department of Computer Science, College of William and Mary, Williamsburg, Virginia; Department of Computer Science, College of William and Mary, Williamsburg, Virginia","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","334","345","Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180092","Software repositories;machine learning;deep learning;software language models;n-grams;neural networks","Software;Machine learning;Computational modeling;Context;Training;Context modeling;Computer architecture","","156","","76","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Assessing the Use of AutoML for Data-Driven Software Engineering","F. Calefato; L. Quaranta; F. Lanubile; M. Kalinowski","University of Bari, Bari, Italy; University of Bari, Bari, Italy; University of Bari, Bari, Italy; PUC-Rio, Rio de Janeiro, Brazil","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","12","Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304796","DARE(grant numbers:PNC0000002,CUP: B53C22006420001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304796","AutoAI;benchmark;mixed-method study","Surveys;Analytical models;Automation;Buildings;Pipelines;Software;Software measurement","","1","","37","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Fast and Memory-Efficient Neural Code Completion","A. Svyatkovskiy; S. Lee; A. Hadjitofi; M. Riechert; J. V. Franco; M. Allamanis",Microsoft; University of Oxford; University of Edinburgh; Microsoft; Microsoft; Microsoft,"2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","329","340","Code completion is one of the most widely used features of modern integrated development environments (IDEs). While deep learning has made significant progress in the statistical prediction of source code, state-of-the-art neural network models consume hundreds of megabytes of memory, bloating the development environment. We address this in two steps: first we present a modular neural framework for code completion. This allows us to explore the design space and evaluate different techniques. Second, within this framework we design a novel reranking neural completion model that combines static analysis with granular token encodings. The best neural reranking model consumes just 6 MB of RAM, - 19x less than previous models - computes a single completion in 8 ms, and achieves 90% accuracy in its top five suggestions.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463109","code completion;deep learning;API completion","Deep learning;Computational modeling;Neural networks;Random access memory;Static analysis;Predictive models;Software","","29","","44","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Replication Study on the Usability of Code Vocabulary in Predicting Flaky Tests","G. Haben; S. Habchi; M. Papadakis; M. Cordy; Y. Le Traon",University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg; University of Luxembourg,"2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","219","229","Industrial reports indicate that flaky tests are one of the primary concerns of software testing mainly due to the false signals they provide. To deal with this issue, researchers have developed tools and techniques aiming at (automatically) identifying flaky tests with encouraging results. However, to reach industrial adoption and practice, these techniques need to be replicated and evaluated extensively on multiple datasets, occasions and settings. In view of this, we perform a replication study of a recently proposed method that predicts flaky tests based on their vocabulary. We thus replicate the original study on three different dimensions. First, we replicate the approach on the same subjects as in the original study but using a different evaluation methodology, i.e., we adopt a time-sensitive selection of training and test sets to better reflect the envisioned use case. Second, we consolidate the findings of the initial study by building a new dataset of 837 flaky tests from 9 projects in a different programming language, i.e., Python while the original study was in Java, which comforts the generalisability of the results. Third, we propose an extension to the original approach by experimenting with different features extracted from the Code Under Test. We find that a more robust validation consistently decreases performance on the reported results of the original study, but, fortunately, the model remains capable to decently predict flaky tests. We find re-assuring results that the vocabulary-based models can also be used to predict test flakiness in Python. Finally, we find that the information lying in the Code Under Test has a limited impact on the performance of the vocabulary-based models.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463145","Software testing;regression testing;flakiness","Training;Software testing;Vocabulary;Java;Predictive models;Tools;Feature extraction","","18","","39","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"The General Index of Software Engineering Papers","Z. A. Khalil; S. Zacchiroli","Inria, Paris, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, France","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","98","102","We introduce the General Index of Software Engineering Papers, a dataset of fulltext-indexed papers from the most prominent scientific venues in the field of Software Engineering. The dataset includes both complete bibliographic information and indexed n-grams (sequence of contiguous words after removal of stopwords and non-words, for a total of 577 276 382 unique n-grams in this release) with length 1 to 5 for 44 581 papers retrieved from 34 venues over the 1971–2020 period. The dataset serves use cases in the field of meta-research, allowing to introspect the output of software engineering research even when access to papers or scholarly search engines is not possible (e.g., due to contractual reasons). The dataset also contributes to making such analyses reproducible and independently verifiable, as opposed to what happens when they are conducted using 3rd-party and non-open scholarly indexing services. The dataset is available as a portable Postgres database dump and released as open data.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796170","dataset;academic publishing;software engineering;ngrams;meta-analysis;natural language processing;fulltext index","Codes;Databases;Semantics;Search engines;Software;Data mining;Open data","","","","24","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Toward Predicting Architectural Significance of Implementation Issues","A. Shahbazian; D. Nam; N. Medvidovic","University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","215","219","In a software system's development lifecycle, engineers make numerous design decisions that subsequently cause architectural change in the system. Previous studies have shown that, more often than not, these architectural changes are unintentional by-products of continual software maintenance tasks. The result of inadvertent architectural changes is accumulation of technical debt and deterioration of software quality. Despite their important implications, there is a relative shortage of techniques, tools, and empirical studies pertaining to architectural design decisions. In this paper, we take a step toward addressing that scarcity by using the information in the issue and code repositories of open-source software systems to investigate the cause and frequency of such architectural design decisions. Furthermore, building on these results, we develop a predictive model that is able to identify the architectural significance of newly submitted issues, thereby helping engineers to prevent the adverse effects of architectural decay. The results of this study are based on the analysis of 21,062 issues affecting 301 versions of 5 large open-source systems for which the code changes and issues were publicly accessible.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595204","Architectural Design Decisions;Implementation Issues;Architectural Change Prediction","Computer architecture;Software systems;Data mining;Open source software;Manganese;Task analysis","","","","42","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Researcher Bias: The Use of Machine Learning in Software Defect Prediction","M. Shepperd; D. Bowes; T. Hall","Brunel University, Uxbridge, Middlesex, United Kingdom; Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom; Brunel University, Uxbridge, Middlesex, United Kingdom","IEEE Transactions on Software Engineering","16 Jun 2014","2014","40","6","603","616","Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.","1939-3520","","10.1109/TSE.2014.2322358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824804","Software defect prediction;meta-analysis;researcher bias","Software;Predictive models;Correlation;Data models;Buildings;Software engineering;Measurement","","245","","53","IEEE","3 Jun 2014","","","IEEE","IEEE Journals"
"An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code","M. Hort; A. Grishina; L. Moonen","Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, University of Oslo, Oslo, Norway; Simula Research Laboratory, BI Norwegian Business School, Oslo, Norway","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","12","Context: Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair. Large amounts of data for training such models benefit the models' performance. However, the size of the data and models results in long training times and high energy consumption. While publishing source code allows for replicability, users need to repeat the expensive training process if models are not shared. Goals: The main goal of the study is to investigate if publications that trained language models for software engineering (SE) tasks share source code and trained artifacts. The second goal is to analyze the transparency on training energy usage. Methods: We perform a snowballing-based literature search to find publications on language models for source code, and analyze their reusability from a sustainability standpoint. Results: From a total of 494 unique publications, we identified 293 relevant publications that use language models to address code-related tasks. Among them, 27% (79 out of 293) make artifacts available for reuse. This can be in the form of tools or IDE plugins designed for specific tasks or task-agnostic models that can be fine-tuned for a variety of downstream tasks. Moreover, we collect insights on the hardware used for model training, as well as training time, which together determine the energy consumption of the development process. Conclusion: We find that there are deficiencies in the sharing of information and artifacts for current studies on source code models for software engineering tasks, with 40% of the surveyed papers not sharing source code or trained artifacts. We recommend the sharing of source code as well as trained artifacts, to enable sustainable reproducibility. Moreover, comprehensive information on training times and hardware configurations should be shared for transparency on a model's carbon footprint.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304803","Research Council of Norway(grant numbers:288787); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304803","sustainability;reuse;replication;energy;DL4SE","Training;Energy consumption;Analytical models;Source coding;Hardware;Data models;Task analysis","","1","","127","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Within-Project Defect Prediction of Infrastructure-as-Code Using Product and Process Metrics","S. Dalla Palma; D. Di Nucci; F. Palomba; D. A. Tamburri","Jheronimous Academy of Data Science, Tilburg University, Tilburg, The Netherlands; Jheronimous Academy of Data Science, Tilburg University, Tilburg, The Netherlands; Software Engineering (SeSa) Lab, University of Salerno, Fisciano, Italy; Jheronimous Academy of Data Science, Eindhoven University of Technology, Eindhoven, The Netherlands","IEEE Transactions on Software Engineering","14 Jun 2022","2022","48","6","2086","2104","Infrastructure-as-code (IaC) is the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files, hereinafter referred to as IaC scripts. Similarly to other source code artefacts, these files may contain defects that can preclude their correct functioning. In this paper, we aim at assessing the role of product and process metrics when predicting defective IaC scripts. We propose a fully integrated machine-learning framework for IaC Defect Prediction, that allows for repository crawling, metrics collection, model building, and evaluation. To evaluate it, we analyzed 104 projects and employed five machine-learning classifiers to compare their performance in flagging suspicious defective IaC scripts. The key results of the study report Random Forest as the best-performing model, with a median AUC-PR of 0.93 and MCC of 0.80. Furthermore, at least for the collected projects, product metrics identify defective IaC scripts more accurately than process metrics. Our findings put a baseline for investigating IaC Defect Prediction and the relationship between the product and process metrics, and IaC scripts’ quality.","1939-3520","","10.1109/TSE.2021.3051492","European Commission(grant numbers:825040 (RADON H2020),825480 (SODALITE H2020)); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:PZ00P2 186090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321740","Infrastructure-as-code;defect prediction;empirical software engineering","Measurement;Software;Predictive models;Machine learning;Radon;Cloud computing;Task analysis","","24","","52","CCBY","13 Jan 2021","","","IEEE","IEEE Journals"
"Will This Bug-Fixing Change Break Regression Testing?","X. Tang; S. Wang; K. Mao","State Key Laboratory of Computer Science, Chinese Academy of Sciences; Electrical and Computer Engineering, University of Waterloo, Canada; University College London, CREST Centre, UK","2015 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","9 Nov 2015","2015","","","1","10","Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3%, recall up to 92.3%, and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall.","1949-3789","978-1-4673-7899-4","10.1109/ESEM.2015.7321218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321218","","Testing;Measurement;Computer bugs;Software;Semantics;Predictive models;History","","7","1","42","IEEE","9 Nov 2015","","","IEEE","IEEE Conferences"
"How to “DODGE” Complex Software Analytics","A. Agrawal; W. Fu; D. Chen; X. Shen; T. Menzies","Wayfair, Boston, MA, USA; Landing.AI, Palo Alto, CA, USA; Facebook, Menlo Park, California, USA; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","14 Oct 2021","2021","47","10","2182","2194","Machine learning techniques applied to software engineering tasks can be improved by hyperparameter optimization, i.e., automatic tools that find good settings for a learner's control parameters. We show that such hyperparameter optimization can be unnecessarily slow, particularly when the optimizers waste time exploring “redundant tunings”, i.e., pairs of tunings which lead to indistinguishable results. By ignoring redundant tunings, DODGE($\mathcal {E})$E), a tuning tool, runs orders of magnitude faster, while also generating learners with more accurate predictions than seen in prior state-of-the-art approaches.","1939-3520","","10.1109/TSE.2019.2945020","National Science Foundation(grant numbers:#1703487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854183","Software analytics;hyperparameter optimization;defect prediction;text mining","Tuning;Text mining;Software;Task analysis;Optimization;Software engineering;Tools","","26","","70","IEEE","1 Oct 2019","","","IEEE","IEEE Journals"
"Code Review Knowledge Perception: Fusing Multi-Features for Salient-Class Location","Y. Huang; N. Jia; X. Chen; K. Hong; Z. Zheng","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Management Science and Engineering, Hebei GEO University, Shijiazhuang, China; Guangdong Key Laboratory for Big Data Analysis and Simulation of Public Opinion, School of Communication and Design, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Software Engineering","16 May 2022","2022","48","5","1463","1479","Code review is a common software engineering practice of practical importance to reduce software defects. Review today is often with the help of specialized tools, such as Gerrit. However, even in a tool-supported code review involves a significant amount of human effort to understand the code change, because the information required to inspect code changes may distribute across multiple files that reviewers are not familiar with. Code changes are often organized as commits for review. In this paper, we found that most of the commits contain a salient class(es), which is saliently modified and causes the modification of the rest classes in a commit. Our user studies confirmed that identifying the salient class in a commit can facilitate reviewers in understanding code change. Inspired by the effectiveness of machine learning techniques in the classification field, we model the salient class identification as a binary classification problem and a number of discriminative features is extracted for a commit and used to characterize the salience of a class. The experiments results show that our approach achieves an accuracy of 88 percent. A user study with industrial developers shows that our approach can really improve the efficiency of reviewers understanding code changes in a reviewing scenario without using comment.","1939-3520","","10.1109/TSE.2020.3021902","National Key R&D Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:61902441,61722214,61672545); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164002); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515010973); China Postdoctoral Science Foundation(grant numbers:2018M640855); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186620","Code review;code comprehension;code change;code discriminative features;code commit","Feature extraction;Semantics;Tools;Couplings;Open source software;Knowledge engineering","","7","","56","IEEE","4 Sep 2020","","","IEEE","IEEE Journals"
"Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling","P. Zhang; J. Wang; J. Sun; X. Wang; G. Dong; X. Wang; T. Dai; J. S. Dong","Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China; Singapore Management University, Singapore, Singapore; Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China; Huawei International Pte. Ltd., Shenzhen, China; National University of Singapore, Singapore, Singapore","IEEE Transactions on Software Engineering","16 Sep 2022","2022","48","9","3593","3612","Although deep learning has demonstrated astonishing performance in many applications, there are still concerns about its dependability. One desirable property of deep learning applications with societal impact is fairness (i.e., non-discrimination). Unfortunately, discrimination might be intrinsically embedded into the models due to the discrimination in the training data. As a countermeasure, fairness testing systemically identifies discriminatory samples, which can be used to retrain the model and improve the model’s fairness. Existing fairness testing approaches however have two major limitations. First, they only work well on traditional machine learning models and have poor performance (e.g., effectiveness and efficiency) on deep learning models. Second, they only work on simple structured (e.g., tabular) data and are not applicable for domains such as text. In this work, we bridge the gap by proposing a scalable and effective approach for systematically searching for discriminatory samples while extending existing fairness testing approaches to address a more challenging domain, i.e., text classification. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which is significantly more scalable and effective. Experimental results show that on average, our approach explores the search space much more effectively (9.62 and 2.38 times more than the state-of-the-art methods respectively on tabular and text datasets) and generates much more discriminatory samples (24.95 and 2.68 times) within a same reasonable time. Moreover, the retrained models reduce discrimination by 57.2 and 60.2 percent respectively on average.","1939-3520","","10.1109/TSE.2021.3101478","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101100005); Key Research and Development Program of Zhejiang Province(grant numbers:2021C01014); Zhejiang University; Guangdong Science and Technology Department(grant numbers:2018B010107004); Ministry of Education - Singapore(grant numbers:MOET32020-0004,T2EP20120-0019,T1-251RES1901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506918","Deep learning;fairness testing;individual discrimination;gradient","Data models;Deep learning;Systematics;Recurrent neural networks;Neurons;Training data;Task analysis","","7","","60","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Increasing the Confidence of Deep Neural Networks by Coverage Analysis","G. Rossolini; A. Biondi; G. Buttazzo","Department of Excellence in Robotics & AI, Scuola Superiore Sant'Anna, Pisa, Italy; Department of Excellence in Robotics & AI, Scuola Superiore Sant'Anna, Pisa, Italy; Department of Excellence in Robotics & AI, Scuola Superiore Sant'Anna, Pisa, Italy","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","802","815","The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logic. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.","1939-3520","","10.1109/TSE.2022.3163682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745342","Neural networks coverage;DNNs robustness;adversarial examples detection","Robustness;Testing;Predictive models;Computer architecture;Monitoring;Deep learning;Task analysis","","1","","55","IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction","Y. Yang; X. Hu; Z. Gao; J. Chen; C. Ni; X. Xia; D. Lo","Department of Computer Science and Technology, Zhejiang University, Ningbo, China; School of Software Technology, Zhejiang University, Ningbo, China; Shanghai Institute for Advanced Study, Zhejiang University, Hangzhou, China; School of Computer Science, Wuhan University, Wuhan, China; School of Software Technology, Zhejiang University, Ningbo, China; Software Engineering Application Technology Lab, Huawei, Hangzhou, China; School of Computing and Information Systems, Singapore Management University, Singapore","IEEE Transactions on Software Engineering","13 Feb 2024","2024","50","2","296","321","In various research domains, artificial intelligence (AI) has gained significant prominence, leading to the development of numerous learning-based models in research laboratories, which are evaluated using benchmark datasets. While the models proposed in previous studies may demonstrate satisfactory performance on benchmark datasets, translating academic findings into practical applications for industry practitioners presents challenges. This can entail either the direct adoption of trained academic models into industrial applications, leading to a performance decrease, or retraining models with industrial data, a task often hindered by insufficient data instances or skewed data distributions. Real-world industrial data is typically significantly more intricate than benchmark datasets, frequently exhibiting data-skewing issues, such as label distribution skews and quantity skews. Furthermore, accessing industrial data, particularly source code, can prove challenging for Software Engineering (SE) researchers due to privacy policies. This limitation hinders SE researchers’ ability to gain insights into industry developers’ concerns and subsequently enhance their proposed models. To bridge the divide between academic models and industrial applications, we introduce a federated learning (FL)-based framework called Almity. Our aim is to simplify the process of implementing research findings into practical use for both SE researchers and industry developers. Almity enhances model performance on sensitive skewed data distributions while ensuring data privacy and security. It introduces an innovative aggregation strategy that takes into account three key attributes: data scale, data balance, and minority class learnability. This strategy is employed to refine model parameters, thereby enhancing model performance on sensitive skewed datasets. In our evaluation, we employ two well-established SE tasks, i.e., code clone detection and defect prediction, as evaluation tasks. We compare the performance of Almity on both machine learning (ML) and deep learning (DL) models against two mainstream training methods, specifically the Centralized Training Method (CTM) and Vanilla Federated Learning (VFL), to validate the effectiveness and generalizability of Almity. Our experimental results demonstrate that our framework is not only feasible but also practical in real-world scenarios. Almity consistently enhances the performance of learning-based models, outperforming baseline training methods across all types of data distributions.","1939-3520","","10.1109/TSE.2023.3347898","National Natural Science Foundation of China(grant numbers:62141222); National Research Foundation(grant numbers:NRF-NRFI08-2022-0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10379838","Federated learning;parameter aggregation strategy;skewed data distribution;code clone detection;defect prediction","Data models;Training;Codes;Cloning;Task analysis;Benchmark testing;Industries","","","","85","IEEE","3 Jan 2024","","","IEEE","IEEE Journals"
"FeatRacer: Locating Features Through Assisted Traceability","M. Mukelabai; K. Hermann; T. Berger; J. -P. Steghöfer","University of Zambia, Lusaka, Zambia; Ruhr University Bochum, Bochum, Germany; Ruhr University Bochum, Bochum, Germany; Xitaso, Augsburg, Germany","IEEE Transactions on Software Engineering","12 Dec 2023","2023","49","12","5060","5083","Locating features is one of the most common software development activities. It is typically done during maintenance and evolution, when developers need to identify the exact places in a codebase where specific features are implemented. Unfortunately, locating features is laborious and error-prone, since feature knowledge fades, projects are developed by different developers, and features are often scattered across the codebase. Recognizing the need, many automated feature location techniques have been proposed, which try to retroactively recover features, i.e., very domain-specific information from the codebase. Unfortunately, such techniques require large training datasets, only recover coarse-grained locations and produce too many false positives to be useful in practice. An alternative is recording features during development, when they are still fresh in a developer's mind. However, recording is easily forgotten and also costly, especially when the software evolves and such recordings need to be updated. We address the infamous feature location problem (a.k.a., concern location or concept assignment problem) differently. We present FeatRacer, which combines feature recording and automated feature location in a way that allows developers to proactively and continuously record features and their locations during development, while addressing the shortcomings of both strategies. Specifically, FeatRacer relies on embedded code annotations and a machine-learning-based recommender system. When a developer forgets to annotate, FeatRacer reminds the developer about potentially missing features, which it learned from the feature recording practices in the project at hand. FeatRacer also facilitates fine-grained locations as decided by the developer. Our evaluation shows that FeatRacer outperforms traditional automated feature location based on Latent Semantic Indexing (LSI) and Linear Discriminant Analysis (LDA)—two of the most common methods to realize such techniques—when predicting features for 4,650 commit changesets from the histories of 16 open-source projects spanning an average of three years between 1985 and 2015. Compared to the traditional techniques, FeatRacer showed a 3x higher precision and a 4.5x higher recall, with an average precision and recall of 89.6% among all 16 projects. It can accurately predict feature locations within the first five commits of our evaluation projects, being effective already for small datasets. FeatRacer takes on average 1.9ms to learn from past code fragments of a project, and 0.002ms to predict forgotten feature annotations in new code.","1939-3520","","10.1109/TSE.2023.3324719","Swedish Research Council Vetenskapsrådet; Royal Swedish Academy of Sciences and of the Wallenberg Foundation; Deutsche Forschungsgemeinschaft(grant numbers:Germany’s Excellence Strategy—EXC 2092 CASA—390781972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299566","Feature location;traceability;recommender","Codes;Annotations;Recording;Software;Large scale integration;Manuals;Maintenance engineering","","","","97","IEEE","27 Oct 2023","","","IEEE","IEEE Journals"
"Investigating the Validity of Ground Truth in Code Reviewer Recommendation Studies","E. Doğan; E. Tüzün; K. A. Tecimer; H. A. Güvenir","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","17 Oct 2019","2019","","","1","6","Background: Selecting the ideal code reviewer in modern code review is a crucial first step to perform effective code reviews. There are several algorithms proposed in the literature for recommending the ideal code reviewer for a given pull request. The success of these code reviewer recommendation algorithms is measured by comparing the recommended reviewers with the ground truth that is the assigned reviewers selected in real life. However, in practice, the assigned reviewer may not be the ideal reviewer for a given pull request.Aims: In this study, we investigate the validity of ground truth data in code reviewer recommendation studies.Method: By conducting an informal literature review, we compared the reviewer selection heuristics in real life and the algorithms used in recommendation models. We further support our claims by using empirical data from code reviewer recommendation studies.Results: By literature review, and accompanying empirical data, we show that ground truth data used in code reviewer recommendation studies is potentially problematic. This reduces the validity of the code reviewer datasets and the reviewer recommendation studies. Conclusion: We demonstrated the cases where the ground truth in code reviewer recommendation studies are invalid and discussed the potential solutions to address this issue.","1949-3789","978-1-7281-2968-6","10.1109/ESEM.2019.8870190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870190","reviewer recommendation;ground truth;cognitive bias;attribute substitution;systematic noise;threats to validity","Task analysis;Software;Systematics;Support vector machines;Inspection;Machine learning;Measurement","","3","","37","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"Syntactic Versus Semantic Similarity of Artificial and Real Faults in Mutation Testing Studies","M. Ojdanic; A. Garg; A. Khanfir; R. Degiovanni; M. Papadakis; Y. Le Traon","University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","17 Jul 2023","2023","49","7","3922","3938","Fault seeding is typically used in empirical studies to evaluate and compare test techniques. Central to these techniques lies the hypothesis that artificially seeded faults involve some form of realistic properties and thus provide realistic experimental results. In an attempt to strengthen realism, a recent line of research uses machine learning techniques, such as deep learning and Natural Language Processing, to seed faults that look like (syntactically) real ones, implying that fault realism is related to syntactic similarity. This raises the question of whether seeding syntactically similar faults indeed results in semantically similar faults and, more generally whether syntactically dissimilar faults are far away (semantically) from the real ones. We answer this question by employing 4 state-of-the-art fault-seeding techniques (PiTest - a popular mutation testing tool, IBIR - a tool with manually crafted fault patterns, DeepMutation - a learning-based fault seeded framework and $\mu$μBERT - a mutation testing tool based on the pre-trained language model CodeBERT) that operate in a fundamentally different way, and demonstrate that syntactic similarity does not reflect semantic similarity. We also show that 65.11%, 76.44%, 61.39% and 9.76% of the real faults of Defects4J V2 are semantically resembled by PiTest, IBIR, $\mu$μBERT and DeepMutation faults, respectively.","1939-3520","","10.1109/TSE.2023.3277564","Luxembourg National Research Fund(grant numbers:C19/IS/13646587/RASoRS,PayPal); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136793","Fault injection;fault seeding;machine learning;mutation testing;semantic model;syntactic distance","Syntactics;Semantics;Testing;Measurement;Codes;Bit error rate;Pattern matching","","1","","57","CCBY","26 May 2023","","","IEEE","IEEE Journals"
"Human-in-the-Loop Automatic Program Repair","C. Geethal; M. Böhme; V. -T. Pham","Monash University, Clayton, VIC, Australia; Monash University, Clayton, VIC, Australia; The University of Melbourne, Carlton, VIC, Australia","IEEE Transactions on Software Engineering","17 Oct 2023","2023","49","10","4526","4549","learn2fix is a human-in-the-loop interactive program repair technique, which can be applied when no bug oracle—except the user who is reporting the bug—is available. This approach incrementally learns the condition under which the bug is observed by systematic negotiation with the user. In this process, learn2fix generates alternative test inputs and sends some of those to the user for obtaining their labels. A limited query budget is assigned to the user for this task. A query is a Yes/No question: “When executing this alternative test input, the program under test produces the following output; is the bug observed?”. Using the labelled test inputs, learn2fix incrementally learns an automatic bug oracle to predict the user's response. A classification algorithm in machine learning is used for this task. Our key challenge is to maximise the oracle's accuracy in predicting the tests that expose the bug given a practical, small budget of queries. After learning the automatic oracle, an existing program repair tool attempts to repair the bug using the alternative tests that the user has labelled. Our experiments demonstrate that learn2fix trains a sufficiently accurate automatic oracle with a reasonably low labelling effort (lt. 20 queries), and the oracles represented by interpolation-based classifiers produce more accurate predictions than those represented by approximation-based classifiers. Given the user-labelled test inputs, generated using the interpolation-based approach, the GenProg and Angelix automatic program repair tools produce patches that pass a much larger proportion of validation tests than the manually constructed test suites provided by the repair benchmark.","1939-3520","","10.1109/TSE.2023.3305052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225252","Automated test oracles;semi-automatic program repair;classification algorithms;active machine learning","Maintenance engineering;Computer bugs;Labeling;Classification algorithms;Human in the loop;Fuzzing;Training","","","","70","CCBYNCND","21 Aug 2023","","","IEEE","IEEE Journals"
"UnGoML: Automated Classification of unsafe Usages in Go","A. -K. Wickert; C. Damke; L. Baumgärtner; E. Hüllermeier; M. Mezini","Software Technology Group, Technische Universität Darmstadt, Darmstadt, Germany; Institute of Informatics University of Munich, Munich, Germany; Software Technology Group, Technische Universität Darmstadt, Darmstadt, Germany; Munich Center for Machine Learning Institute of Informatics, University of Munich, Munich, Germany; Hessian Center for Artificial Intelligence (hessian.AI) National Research Center for Applied Cybersecurity ATHENE Software Technology Group, Technische Universität Darmstadt, Darmstadt, Germany","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","309","321","The Go programming language offers strong protection from memory corruption. As an escape hatch of these protections, it provides the unsafe package. Previous studies identified that this unsafe package is frequently used in real-world code for several purposes, e.g., serialization or casting types. Due to the variety of these reasons, it may be possible to refactor specific usages to avoid potential vulnerabilities. However, the classification of unsafe usages is challenging and requires the context of the call and the program’s structure. In this paper, we present the first automated classifier for unsafe usages in Go, UnGoML, to identify what is done with the unsafe package and why it is used. For UnGoML, we built four custom deep learning classifiers trained on a manually labeled data set. We represent Go code as enriched control-flow graphs (CFGs) and solve the label prediction task with one single-vertex and three context-aware classifiers. All three context-aware classifiers achieve a top-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore, in a set-valued conformal prediction setting, we achieve accuracies of more than 93% with mean label set sizes of 2 for both dimensions. Thus, UnGoML can be used to efficiently filter unsafe usages for use cases such as refactoring or a security audit. UnGoML: https://github.com/stg-tud/UnGoML Artifact: https://dx.doi.org/10.6084/m9.figshare.22293052","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174087","graph neural networks;Go;unsafe package;classification;API-misuse","Deep learning;Computer languages;Codes;Static analysis;Computer architecture;Software;Safety","","","","47","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention","W. Wang; Y. Zhang; Y. Sui; Y. Wan; Z. Zhao; J. Wu; P. S. Yu; G. Xu","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, P.R. China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, P.R. China; School of Computer Science, University of Technology, Sydney, NSW, Austrialia; Zhejiang University, Hangzhou, Zhejiang, P.R. China; Zhejiang University, Hangzhou, Zhejiang, P.R. China; Zhejiang University, Hangzhou, Zhejiang, P.R. China; University of Illinois at Chicago, Chicago, IL, USA; School of Computer Science, University of Technology, Sydney, NSW, Austrialia","IEEE Transactions on Software Engineering","10 Jan 2022","2022","48","1","102","119","Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays “attention”) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.","1939-3520","","10.1109/TSE.2020.2979701","National Natural Science Foundation of China(grant numbers:61902169); Shenzhen Science and Technology Program(grant numbers:KQTD2016112514355531); Science and Technology Innovation Committee Foundation of Shenzhen(grant numbers:JCYJ20170817110848086); Australian Research Council(grant numbers:DP200101374,LP170100891,DE170101081,DP200101328); National Key Research and Development Program of China(grant numbers:2018AAA0102100); Zhejiang public welfare technology(grant numbers:LGF20F020013); National Key Research and Development Program of China(grant numbers:2019YFC0118802); National Institutes for Food and Drug Control(grant numbers:2019YFB1404802); National Science Foundation(grant numbers:III-1526499,III-1763325,III-1909323,CNS-1930941); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031440","Code summarization;hierarchical attention;reinforcement learning","Software;Recurrent neural networks;Training;Machine learning;Decoding;Syntactics;Maintenance engineering","","42","","99","IEEE","10 Mar 2020","","","IEEE","IEEE Journals"
"Bayesian Data Analysis in Empirical Software Engineering Research","C. A. Furia; R. Feldt; R. Torkar","Faculty of Informatics, Software Institute, Università della Svizzera italiana (USI), Lugano, Switzerland; Department of Computer Science and Engineering, Softwre Engineering Division, Chalmers University of Technology and the University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Softwre Engineering Division, Chalmers University of Technology and the University of Gothenburg, Gothenburg, Sweden","IEEE Transactions on Software Engineering","16 Sep 2021","2021","47","9","1786","1810","Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings-such as lack of flexibility and results that are unintuitive and hard to interpret-that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice. In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that provide tangible benefits-as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, we present the reanalysis of two empirical studies on the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analyses with our new Bayesian analyses, we demonstrate the concrete advantages of the latter. To conclude we advocate a more prominent role for Bayesian statistical techniques in empirical software engineering research and practice.","1939-3520","","10.1109/TSE.2019.2935974","Marianne and Marcus Wallenberg Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807222","Bayesian data analysis;statistical analysis;statistical hypothesis testing;empirical software engineering","Bayes methods;Software engineering;Testing;Data analysis;Machine learning;Statistical analysis;Software","","32","","103","IEEE","20 Aug 2019","","","IEEE","IEEE Journals"
"How Well Do Change Sequences Predict Defects? Sequence Learning from Software Changes","M. Wen; R. Wu; S. -C. Cheung","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China","IEEE Transactions on Software Engineering","11 Nov 2020","2020","46","11","1155","1175","Software defect prediction, which aims to identify defective modules, can assist developers in finding bugs and prioritizing limited quality assurance resources. Various features to build defect prediction models have been proposed and evaluated. Among them, process metrics are one important category. Yet, existing process metrics are mainly encoded manually from change histories and ignore the sequential information arising from the changes during software evolution. Are the change sequences derived from such information useful to characterize buggy program modules? How can we leverage such sequences to build good defect prediction models? Unlike traditional process metrics used for existing defect prediction models, change sequences are mostly vectors of variable length. This makes it difficult to apply such sequences directly in prediction models that are driven by conventional classifiers. To resolve this challenge, we utilize Recurrent Neural Network (RNN), which is a deep learning technique, to encode features from sequence data automatically. In this paper, we propose a novel approach called Fences, which extracts six types of change sequences covering different aspects of software changes via fine-grained change analysis. It approaches defects prediction by mapping it to a sequence labeling problem solvable by RNN. Our evaluations on 10 open source projects show that Fences can predict defects with high performance. In particular, our approach achieves an average F-measure of 0.657, which improves the prediction models built on traditional metrics significantly. The improvements vary from 31.6 to 46.8 percent on average. In terms of AUC, Fences achieves an average value of 0.892, and the improvements over baselines vary from 4.2 to 16.1 percent. Fences also outperforms the state-of-the-art technique which learns semantic features automatically from static code via deep learning.","1939-3520","","10.1109/TSE.2018.2876256","Hong Kong RGC/GRF(grant numbers:16202917); 2018 MSRA collaborative research fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493303","Defect prediction;process metrics;sequence learning","Measurement;Software;Predictive models;Semantics;History;Machine learning;Feature extraction","","22","","74","IEEE","16 Oct 2018","","","IEEE","IEEE Journals"
"Mining Semantic Loop Idioms","M. Allamanis; E. T. Barr; C. Bird; P. Devanbu; M. Marron; C. Sutton","Microsoft Research, Cambridge, UK; University College London, London, United Kingdom; Microsoft Research, Redmond, WA; University of California, Davis, CA; Microsoft Research, Redmond, WA; University of Edinburgh, Edinburgh, United Kingdom","IEEE Transactions on Software Engineering","16 Jul 2018","2018","44","7","651","668","To write code, developers stitch together patterns, like API protocols or data structure traversals. Discovering these patterns can identify inconsistencies in code or opportunities to replace these patterns with an API or a language construct. We present coiling, a technique for automatically mining code for semantic idioms: surprisingly probable, semantic patterns. We specialize coiling for loop idioms, semantic idioms of loops. First, we show that automatically identifiable patterns exist, in great numbers, with a largescale empirical study of loops over 25MLOC. We find that most loops in this corpus are simple and predictable: 90 percent have fewer than 15LOC and 90 percent have no nesting and very simple control. Encouraged by this result, we then mine loop idioms over a second, buildable corpus. Over this corpus, we show that only 50 loop idioms cover 50 percent of the concrete loops. Our framework opens the door to data-driven tool and language design, discovering opportunities to introduce new API calls and language constructs. Loop idioms show that LINQ would benefit from an Enumerate operator. This can be confirmed by the exitence of a StackOverflow question with 542k views that requests precisely this feature.","1939-3520","","10.1109/TSE.2018.2832048","Microsoft Research; Engineering and Physical Sciences Research Council(grant numbers:EP/K024043/1); US National Science Foundation(grant numbers:1414172); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355713","Data-driven tool design;idiom mining;code patterns","Semantics;Tools;Syntactics;Data mining;C# languages;Machine learning;Testing","","14","","68","IEEE","7 May 2018","","","IEEE","IEEE Journals"
"Tracking Buggy Files: New Efficient Adaptive Bug Localization Algorithm","M. Fejzer; J. Narębski; P. Przymus; K. Stencel","Nicolaus Copernicus University in Toruń, Torun, Poland; Nicolaus Copernicus University in Toruń, Torun, Poland; Nicolaus Copernicus University in Toruń, Torun, Poland; University of Warsaw, Warszawa, Poland","IEEE Transactions on Software Engineering","15 Jul 2022","2022","48","7","2557","2569","Upon receiving a new bug report, developers need to find its cause in the source code. Bug localization can be helped by a tool that ranks all source files according to how likely they include the bug. This problem was thoroughly examined by numerous scientists. We introduce a novel adaptive bug localization algorithm. The concept behind it is based on new feature weighting approaches and an adaptive selection algorithm utilizing pointwise learn–to–rank method. The algorithm is evaluated on publicly available datasets, and is competitive in terms of accuracy and required computational resources compared to state–of–the–art. Additionally, to improve reproducibility we provide extended datasets that include computed features and partial steps, and we also provide the source code.","1939-3520","","10.1109/TSE.2021.3064447","Narodowa Agencja Wymiany Akademickiej; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372820","Bug reports;software maintenance;learning to rank","Computer bugs;Location awareness;Software;History;Software algorithms;Machine learning algorithms;Training","","5","","47","CCBY","8 Mar 2021","","","IEEE","IEEE Journals"
"Stakeholder Preference Extraction From Scenarios","Y. Shen; T. Breaux","Software and Societal Systems Department, Carnegie Mellon University, Pittsburgh, PA, USA; Software and Societal Systems Department, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","8 Jan 2024","2024","50","1","69","84","Companies use personalization to tailor user experiences. Personalization appears in search engines and online stores, which include salutations and statistically learned correlations over search-, browsing- and purchase-histories. However, users have a wider variety of substantive, domain-specific preferences that affect their choices when they use directory services, and these have largely been overlooked or ignored. The contributions of this paper include: (1) a grounded theory describing how stakeholder preferences are expressed in text scenarios; (2) an app feature survey to assess whether elicited preferences represent missing requirements in existing systems; (3) an evaluation of three classifiers to label preference words in scenarios; and (4) a linker to build preference phrases by linking labeled preference words to each other based on word position. In this study, the authors analyzed 217 elicited directory service scenarios across 12 domain categories to yield a total of 7,661 stakeholder preferences labels. The app survey yielded 43 stakeholder preferences that were missed on average 49.7% by 15 directory service websites studied. The BERT-based transformer showed the best average overall 81.1% precision, 84.4% recall and 82.6% F1-score when tested on unseen domains. Finally, the preference linker correctly links preference phrases with 90.1% accuracy. Given these results, we believe directory service developers can use this approach to automatically identify user preferences to improve service designs.","1939-3520","","10.1109/TSE.2023.3333265","NSF(grant numbers:2007298,1453139); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323231","Requirements engineering;elicitation;stakeholder preferences;natural language processing","Stakeholders;Software;Surveys;Feature extraction;Transformers;Syntactics;Machine learning","","","","73","IEEE","20 Nov 2023","","","IEEE","IEEE Journals"
"Challenges in Chatbot Development: A Study of Stack Overflow Posts","A. Abdellatif; D. Costa; K. Badran; R. Abdalkareem; E. Shihab","Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, Canada; Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, Canada; Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, Canada; Software Analysis and Intelligence Lab (SAIL), Queen's University, Kingston, Canada; Data-driven Analysis of Software (DAS) Lab, Concordia University, Montreal, Canada","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","174","185","Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development. Therefore, in this paper, we examine the Q&A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148686","Software Chatbots Development;Empirical Study;Chatbot Issues;Stack Overflow","Training;Pressing;Oral communication;Machine learning;Chatbots;Software systems;Data mining","","23","","76","","20 Jun 2023","","","IEEE","IEEE Conferences"
"GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses","W. Ma; M. Zhao; E. Soremekun; Q. Hu; J. M. Zhang; M. Papadakis; M. Cordy; X. Xie; Y. Le Traon","University of Luxembourg, Luxembourg; LMU, Munich, Germany; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University College London, United Kingdom; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; Singapore Management University, Singapore; University of Luxembourg, Luxembourg","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","524","536","Code embedding is a keystone in the application of machine learning on several Software Engineering (SE) tasks. To effectively support a plethora of SE tasks, the embedding needs to capture program syntax and semantics in a way that is generic. To this end, we propose the first self-supervised pre-training approach (called Graphcode2vec) which produces task-agnostic embedding of lexical and program dependence features. Graphcode2vec achieves this via a synergistic combination of code analysis and Graph Neural Networks. Graphcode2vec is generic, it allows pre-training, and it is applicable to several SE downstream tasks. We evaluate the effectiveness of Graphcode2vec on four (4) tasks (method name prediction, solution classification, mutation testing and overfitted patch classification), and compare it with four (4) similarly generic code embedding baselines (Code2Seq, Code2Vec, CodeBERT, Graph-CodeBERT) and seven (7) task-specific, learning-based methods. In particular, Graphcode2vec is more effective than both generic and task-specific learning-based baselines. It is also complementary and comparable to GraphCodeBERT (a larger and more complex model). We also demonstrate through a probing and ablation study that Graphcode2vec learns lexical and program dependence features and that self-supervised pre-training improves effectiveness.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796339","code embedding;code representation;code analysis","Learning systems;Codes;Semantics;Machine learning;Syntactics;Maintenance engineering;Software","","10","","78","CCBY","21 Jun 2022","","","IEEE","IEEE Conferences"
"Software-related Slack Chats with Disentangled Conversations","P. Chatterjee; K. Damevski; N. A. Kraft; L. Pollock","University of Delaware, Newark, DE, USA; Virginia Commonwealth University, Richmond, VA, USA; Uservoice, Raleigh, NC, USA; University of Delaware, Newark, DE, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","588","592","More than ever, developers are participating in public chat communities to ask and answer software development questions. With over ten million daily active users, Slack is one of the most popular chat platforms, hosting many active channels focused on software development technologies, e.g., python, react. Prior studies have shown that public Slack chat transcripts contain valuable information, which could provide support for improving automatic software maintenance tools or help researchers understand developer struggles or concerns. In this paper, we present a dataset of software-related Q&A chat conversations, curated for two years from three open Slack communities (python, clojure, elm). Our dataset consists of 38,955 conversations, 437,893 utterances, contributed by 12,171 users. We also share the code for a customized machine-learning based algorithm that automatically extracts (or disentangles) conversations from the downloaded chat transcripts.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387493","National Science Foundation(grant numbers:1812968,1813253); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148757","online software developer chats;chat disentanglement","Software maintenance;Codes;Software algorithms;Oral communication;Machine learning;Data mining;Python","","9","","41","","20 Jun 2023","","","IEEE","IEEE Conferences"
"A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts","K. Grotov; S. Titov; V. Sotnikov; Y. Golubev; T. Bryksin","JetBrains Research, ITMO University; JetBrains Research; JetBrains Research; JetBrains Research; JetBrains Research","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","353","364","In recent years, Jupyter notebooks have grown in popularity in several domains of software engineering, such as data science, machine learning, and computer science education. Their popularity has to do with their rich features for presenting and visualizing data, however, recent studies show that notebooks also share a lot of drawbacks: high number of code clones, low reproducibility, etc. In this work, we carry out a comparison between Python code written in Jupyter Notebooks and in traditional Python scripts. We compare the code from two perspectives: structural and stylistic. In the first part of the analysis, we report the difference in the number of lines, the usage of functions, as well as various complexity metrics. In the second part, we show the difference in the number of stylistic issues and provide an extensive overview of the 15 most frequent stylistic issues in the studied mediums. Overall, we demonstrate that notebooks are characterized by the lower code complexity, however, their code could be perceived as more entangled than in the scripts. As for the style, notebooks tend to have 1.4 times more stylistic issues, but at the same time, some of them are caused by specific coding practices in notebooks and should be considered as false positives. With this research, we want to pave the way to studying specific problems of notebooks that should be addressed by the development of notebook-specific tools, and provide various insights that can be useful in this regard.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796343","jupyter;jupyter notebooks;python;mining software repositories;static analysis;code style","Codes;Data visualization;Machine learning;Data science;Software;Reproducibility of results;Encoding","","7","","43","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Categorizing and Visualizing Issue Tickets to Better Understand the Features Implemented in Existing Software Systems","R. Ishizuka; H. Washizaki; Y. Fukazawa; S. Saito; S. Ouji","Dept. of Computer Science and Engineering, Waseda University, Tokyo, Japan; Dept. of Computer Science and Engineering, Waseda University, Tokyo, Japan; Dept. of Computer Science and Engineering, Waseda University, Tokyo, Japan; Software Innovation Center NTT Corporation, Tokyo, Japan; Software Innovation Center NTT Corporation, Tokyo, Japan","2019 10th International Workshop on Empirical Software Engineering in Practice (IWESEP)","30 Dec 2019","2019","","","25","255","Acquiring knowledge and context of the ongoing project is one of the most challenging issues for new project members. New project members need to comprehend the contents of features already implemented in software systems. However, most software documents (e.g., flow chart and data-model) were initially created and not updated. Herein we focus on tickets issued during the project period because they are created based on stakeholders' requirements as a project evolves. We propose a novel approach to categorize and visualize tickets to better understand the features implemented in the existing software systems. Specifically, tickets are grouped by a clustering method and the number of clusters (i.e., ticket categories) is automatically estimated. Then the ticket categories are visualized by (i) creating a heatmap of the tickets lifetimes, (ii) extracting keywords of the ticket categories, and (iii) analyzing the relationships between ticket categories. We applied our approach to an NTT software development project. Additionally, we interviewed the project members and external experts to evaluate the effectiveness of our approach. Our approach helps comprehend the features of the software system.","2573-2021","978-1-7281-5590-6","10.1109/IWESEP49350.2019.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945077","reverse engineering, software comprehension, issue ticket, text mining, machine learning, interviewing","Data visualization;Software;Computer science;Text mining;Machine learning;Technological innovation;Data collection","","1","","20","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Automatic Unsupervised Bug Report Categorization","N. Limsettho; H. Hata; A. Monden; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","2014 6th International Workshop on Empirical Software Engineering in Practice","6 Dec 2014","2014","","","7","12","Background: Information in bug reports is implicit and therefore difficult to comprehend. To extract its meaning, some processes are required. Categorizing bug reports is a technique that can help in this regard. It can be used to help in the bug reports management or to understand the underlying structure of the desired project. However, most researches in this area are focusing on a supervised learning approach that still requires a lot of human afford to prepare a training data. Aims: Our aim is to develop an automated framework than can categorize bug reports, according to their hidden characteristics and structures, without the needed for training data. Method: We solve this problem using clustering, unsupervised learning approach. It can automatically group bug reports together based on their textual similarity. We also propose a novel method to label each group with meaningful and representative names. Results: Experiment results show that our framework can achieve performance comparable to the supervised learning approaches. We also show that our labeling process can label each cluster with representative names according to its characteristic. Conclusion: Our framework could be used as an automated categorization system that can be applied without prior knowledge or as an automated labeling suggestion system.","","978-1-4799-6666-0","10.1109/IWESEP.2014.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976015","automated bug report categorization;topic modeling;clustering;cluster labeling","Labeling;Vectors;Logistics;Supervised learning;Equations;Accuracy;Mathematical model","","20","","13","IEEE","6 Dec 2014","","","IEEE","IEEE Conferences"
"An Ontology-Based Approach to Automate Tagging of Software Artifacts","S. S. Alqahtani; J. Rilling","Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","169","174","Context: Software engineering repositories contain a wealth of textual information such as source code comments, developers' discussions, commit messages and bug reports. These free form text descriptions can contain both direct and implicit references to security concerns. Goal: Derive an approach to extract security concerns from textual information that can yield several benefits, such as bug management (e.g., prioritization), bug triage or capturing zero-day attack. Method: Propose a fully automated classification and tagging approach that can extract security tags from these texts without the need for manual training data. Results: We introduce an ontology based Software Security Tagger Framework that can automatically identify and classify cybersecurity-related entities, and concepts in text of software artifacts. Conclusion: Our preliminary results indicate that the framework can successfully extract and classify cybersecurity knowledge captured in unstructured text found in software artifacts.","","978-1-5090-4039-1","10.1109/ESEM.2017.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170098","automated security concern classification;topic modeling;bug reports;tagging","Computer bugs;Software;Ontologies;Tagging;Software engineering;Computer security","","7","","20","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"Metamorphic Relations for Enhancing System Understanding and Use","Z. Q. Zhou; L. Sun; T. Y. Chen; D. Towey","Institute of Cybersecurity and Cryptology, School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Computer Science, University of Nottingham Ningbo China, Ningbo, Zhejiang, China","IEEE Transactions on Software Engineering","14 Oct 2020","2020","46","10","1120","1154","Modern information technology paradigms, such as online services and off-the-shelf products, often involve a wide variety of users with different or even conflicting objectives. Every software output may satisfy some users, but may also fail to satisfy others. Furthermore, users often do not know the internal working mechanisms of the systems. This situation is quite different from bespoke software, where developers and users typically know each other. This paper proposes an approach to help users to better understand the software that they use, and thereby more easily achieve their objectives—even when they do not fully understand how the system is implemented. Our approach borrows the concept of metamorphic relations from the field of metamorphic testing (MT), using it in an innovative way that extends beyond MT. We also propose a “symmetry” metamorphic relation pattern and a “change direction” metamorphic relation input pattern that can be used to derive multiple concrete metamorphic relations. Empirical studies reveal previously unknown failures in some of the most popular applications in the world, and show how our approach can help users to better understand and better use the systems. The empirical results provide strong evidence of the simplicity, applicability, and effectiveness of our methodology.","1939-3520","","10.1109/TSE.2018.2876433","Australian Research Council(grant numbers:LP160101691); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493260","Metamorphic exploration;symmetry;metamorphic testing;metamorphic relation;metamorphic relation pattern;metamorphic relation input pattern;change direction;oracle problem;user experience;user countermeasure;software validation","Software testing;Information technology;Electronic mail;Software systems;Software maintenance","","65","","95","IEEE","16 Oct 2018","","","IEEE","IEEE Journals"
"Automatically Assessing Code Understandability","S. Scalabrino; G. Bavota; C. Vendome; M. Linares-Vásquez; D. Poshyvanyk; R. Oliveto","University of Molise, Campobasso, CB, Italy; Università della Svizzera italiana(USI), Lugano, Switzerland; Miami University, Oxford, OH, USA; Universidad de los Andes, Bogota, Colombia; College of William & Mary, Williamsburg, VA, USA; University of Molise, Campobasso, CB, Italy","IEEE Transactions on Software Engineering","16 Mar 2021","2021","47","3","595","613","Understanding software is an inherent requirement for many maintenance and evolution tasks. Without a thorough understanding of the code, developers would not be able to fix bugs or add new features timely. Measuring code understandability might be useful to guide developers in writing better code, and could also help in estimating the effort required to modify code components. Unfortunately, there are no metrics designed to assess the understandability of code snippets. In this work, we perform an extensive evaluation of 121 existing as well as new code-related, documentation-related, and developer-related metrics. We try to (i) correlate each metric with understandability and (ii) build models combining metrics to assess understandability. To do this, we use 444 human evaluations from 63 developers and we obtained a bold negative result: none of the 121 experimented metrics is able to capture code understandability, not even the ones assumed to assess quality attributes apparently related, such as code readability and complexity. While we observed some improvements while combining metrics in models, their effectiveness is still far from making them suitable for practical applications. Finally, we conducted interviews with five professional developers to understand the factors that influence their ability to understand code snippets, aiming at identifying possible new metrics.","1939-3520","","10.1109/TSE.2019.2901468","SNF project JITRA(grant numbers:172479); National Science Foundation(grant numbers:CCF-1525902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651396","Software metrics;code understandability;empirical study;negative result","Complexity theory;Software;Computer bugs;Readability metrics;Software measurement;Indexes","","40","","55","IEEE","24 Feb 2019","","","IEEE","IEEE Journals"
"Text Filtering and Ranking for Security Bug Report Prediction","F. Peters; T. T. Tun; Y. Yu; B. Nuseibeh","Lero - The Irish Software Research Centre, University of Limerick, Limerick, Ireland; Department of Computing and Communications, The Open University, Milton Keynes, United Kingdom; Department of Computing and Communications, The Open University, Milton Keynes, United Kingdom; Lero - The Irish Software Research Centre, University of Limerick, Limerick, Ireland","IEEE Transactions on Software Engineering","12 Jun 2019","2019","45","6","615","631","Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent.","1939-3520","","10.1109/TSE.2017.2787653","Science Foundation Ireland(grant numbers:13/RC/2094); H2020 European Research Council(grant numbers:291652 - ASAP); EPSRC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240740","Security cross words;security related keywords;security bug reports;text filtering;ranking;prediction models;transfer learning","Security;Computer bugs;Predictive models;Software;Data models;Measurement;Buildings","","39","","50","IEEE","27 Dec 2017","","","IEEE","IEEE Journals"
"Mining Likely Analogical APIs Across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding","C. Chen; Z. Xing; Y. Liu; K. O. L. Xiong","Faculty of Information Technology, Monash University, Clayton, VIC, Australia; College of Engineering & Computer Science, Australian National University, Canberra, ACT, Australia; SCSE, Nanayng Technological University, Singapore; SCSE, Nanayng Technological University, Singapore","IEEE Transactions on Software Engineering","15 Mar 2021","2021","47","3","432","447","Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on deep learning models trained using tens of millions of API call sequences, method names and comments of 2.8 millions of methods from 135,127 GitHub projects, our approach significantly outperforms other deep learning or traditional information retrieval (IR) methods for inferring likely analogical APIs. We implement a proof-of-concept website (https://similarapi.appspot.com) which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. This scale of third-party analogical-API database has never been achieved before.","1939-3520","","10.1109/TSE.2019.2896123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630054","Analogical API;word embedding;skip thoughts","Libraries;Semantics;Databases;Task analysis;Recurrent neural networks;Deep learning;Java","","31","","84","IEEE","30 Jan 2019","","","IEEE","IEEE Journals"
"On the Use of Hidden Markov Model to Predict the Time to Fix Bugs","M. Habayeb; S. S. Murtaza; A. Miranskyy; A. B. Bener","Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada; Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada; Department of Computer Science, Ryerson University, Toronto, Ontario, Canada; Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada","IEEE Transactions on Software Engineering","9 Dec 2018","2018","44","12","1224","1244","A significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov Models and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project. Our proposed model correctly identifies bug reports with expected bug fix times. We also compared our proposed approach with the state of the art technique in the literature in the context of our case study. Our approach results in approximately 33 percent higher F-measure than the contemporary technique based on the Firefox project data.","1939-3520","","10.1109/TSE.2017.2757480","NSERC(grant numbers:402003-2012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8052546","Bug repositories;temporal activities;time to fix a bug;hidden markov model","Computer bugs;Hidden Markov models;Predictive models;Software quality;Data science;Stochastic processes","","31","","42","IEEE","28 Sep 2017","","","IEEE","IEEE Journals"
"SQAPlanner: Generating Data-Informed Software Quality Improvement Plans","D. Rajapaksha; C. Tantithamthavorn; J. Jiarpakdee; C. Bergmeir; J. Grundy; W. Buntine","Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia","IEEE Transactions on Software Engineering","16 Aug 2022","2022","48","8","2814","2835","Software Quality Assurance (SQA) planning aims to define proactive plans, such as defining maximum file size, to prevent the occurrence of software defects in future releases. To aid this, defect prediction models have been proposed to generate insights as the most important factors that are associated with software quality. Such insights that are derived from traditional defect models are far from actionable—i.e., practitioners still do not know what they should do or avoid to decrease the risk of having defects, and what is the risk threshold for each metric. A lack of actionable guidance and risk threshold can lead to inefficient and ineffective SQA planning processes. In this paper, we investigate the practitioners’ perceptions of current SQA planning activities, current challenges of such SQA planning activities, and propose four types of guidance to support SQA planning. We then propose and evaluate our AI-Driven SQAPlanner approach, a novel approach for generating four types of guidance and their associated risk thresholds in the form of rule-based explanations for the predictions of defect prediction models. Finally, we develop and evaluate a visualization for our SQAPlanner approach. Through the use of qualitative survey and empirical evaluation, our results lead us to conclude that SQAPlanner is needed, effective, stable, and practically applicable. We also find that 80 percent of our survey respondents perceived that our visualization is more actionable. Thus, our SQAPlanner paves a way for novel research in actionable software analytics—i.e., generating actionable guidance on what should practitioners do and not do to decrease the risk of having defects to support SQA planning.","1939-3520","","10.1109/TSE.2021.3070559","Australian Research Council(grant numbers:DE200100941); Australian Research Council(grant numbers:DE190100045); Australian Research Council(grant numbers:FL190100035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394771","Software quality assurance;SQA planning;actionable software analytics;explainable AI","Planning;Software;Predictive models;Visualization;Tools;Artificial intelligence;Software quality","","20","","59","IEEE","2 Apr 2021","","","IEEE","IEEE Journals"
"Characterizing Crowds to Better Optimize Worker Recommendation in Crowdsourced Testing","J. Wang; S. Wang; J. Chen; T. Menzies; Q. Cui; M. Xie; Q. Wang","Laboratory for Internet Software Technologies, State Key Laboratory of Computer Sciences, Institute of Software Chinese Academy of Sciences, Beijing, China; York University, Toronto, ON, Canada; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Bytedance Inc., Beijing, China; DAMO Academy of Alibaba Group, Beijing, China; Laboratory for Internet Software Technologies, State Key Laboratory of Computer Sciences, Institute of Software Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Software Engineering","11 Jun 2021","2021","47","6","1259","1276","Crowdsourced testing is an emerging trend, in which test tasks are entrusted to the online crowd workers. Typically, a crowdsourced test task aims to detect as many bugs as possible within a limited budget. However not all crowd workers are equally skilled at finding bugs; Inappropriate workers may miss bugs, or report duplicate bugs, while hiring them requires nontrivial budget. Therefore, it is of great value to recommend a set of appropriate crowd workers for a test task so that more software bugs can be detected with fewer workers. This paper first presents a new characterization of crowd workers and characterizes them with testing context, capability, and domain knowledge. Based on the characterization, we then propose Multi-Objective Crowd wOrker recoMmendation approach (MOCOM), which aims at recommending a minimum number of crowd workers who could detect the maximum number of bugs for a crowdsourced testing task. Specifically, MOCOM recommends crowd workers by maximizing the bug detection probability of workers, the relevance with the test task, the diversity of workers, and minimizing the test cost. We experimentally evaluate MOCOM on 532 test tasks, and results show that MOCOM significantly outperforms five commonly-used and state-of-the-art baselines. Furthermore, MOCOM can reduce duplicate reports and recommend workers with high relevance and larger bug detection probability; because of this it can find more bugs with fewer workers.","1939-3520","","10.1109/TSE.2019.2918520","National Key Research and Development Program of China(grant numbers:2018YFB1403400); National Natural Science Foundation of China(grant numbers:61602450,61432001); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721154","Crowdsourced testing;crowd worker recommendation;multi-objective optimization","Task analysis;Computer bugs;Testing;Software;Videos;Software engineering;Optimization","","20","","66","IEEE","23 May 2019","","","IEEE","IEEE Journals"
"Improving Vulnerability Inspection Efficiency Using Active Learning","Z. Yu; C. Theisen; L. Williams; T. Menzies","Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Microsoft, Seattle, WA, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","11 Nov 2021","2021","47","11","2401","2420","Software engineers can find vulnerabilities with less effort if they are directed towards code that might contain more vulnerabilities. HARMLESS is an incremental support vector machine tool that builds a vulnerability prediction model from the source code inspected to date, then suggests what source code files should be inspected next. In this way, HARMLESS can reduce the time and effort required to achieve some desired level of recall for finding vulnerabilities. The tool also provides feedback on when to stop (at that desired level of recall) while at the same time, correcting human errors by double-checking suspicious files. This paper evaluates HARMLESS on Mozilla Firefox vulnerability data. HARMLESS found 80, 90, 95, 99 percent of the vulnerabilities by inspecting 10, 16, 20, 34 percent of the source code files. When targeting 90, 95, 99 percent recall, HARMLESS could stop after inspecting 23, 30, 47 percent of the source code files. Even when human reviewers fail to identify half of the vulnerabilities (50 percent false negative rate), HARMLESS could detect 96 percent of the missing vulnerabilities by double-checking half of the inspected files. Our results serve to highlight the very steep cost of protecting software from vulnerabilities (in our case study that cost is, for example, the human effort of inspecting 28,750 × 20% = 5,750 source code files to identify 95 percent of the vulnerabilities). While this result could benefit the mission-critical projects where human resources are available for inspecting thousands of source code files, the research challenge for future work is how to further reduce that cost. The conclusion of this paper discusses various ways that goal might be achieved.","1939-3520","","10.1109/TSE.2019.2949275","National Science Foundation(grant numbers:#1506586,#1909516); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883076","Active learning;security;vulnerabilities;software engineering;error correction","Inspection;Software;Tools;Security;Predictive models;Error correction;NIST","","15","","50","IEEE","25 Oct 2019","","","IEEE","IEEE Journals"
"GPT2SP: A Transformer-Based Agile Story Point Estimation Approach","M. Fu; C. Tantithamthavorn","Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","611","625","Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34%-57% more accurate than existing baseline approaches for within-project estimations; (2) 39%-49% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6%-47%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach.","1939-3520","","10.1109/TSE.2022.3158252","Australian Research Council(grant numbers:DE200100941); Monash University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732669","Agile story point estimation;AI for SE;explainable AI","Estimation;Transformers;Computer architecture;Planning;Task analysis;Training;Artificial intelligence","","15","","56","IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"Theoretical and Empirical Analyses of the Effectiveness of Metamorphic Relation Composition","K. Qiu; Z. Zheng; T. Y. Chen; P. -L. Poon","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Engineering and Technology, Central Queensland University, Melbourne, VIC, Australia","IEEE Transactions on Software Engineering","15 Mar 2022","2022","48","3","1001","1017","Metamorphic Relations (MRs) play a key role in determining the fault detection capability of Metamorphic Testing (MT). As human judgement is required for MR identification, systematic MR generation has long been an important research area in MT. Additionally, due to the extra program executions required for follow-up test cases, some concerns have been raised about MT cost-effectiveness. Consequently, the reduction in testing costs associated with MT has become another important issue to be addressed. MR composition can address both of these problems. This technique can automatically generate new MRs by composing existing ones, thereby reducing the number of follow-up test cases. Despite this advantage, previous studies on MR composition have empirically shown that some composite MRs have lower fault detection capability than their corresponding component MRs. To investigate this issue, we performed theoretical and empirical analyses to identify what characteristics component MRs should possess so that their corresponding composite MR has at least the same fault detection capability as the component MRs do. We have also derived a convenient, but effective guideline so that the fault detection capability of MT will most likely not be reduced after composition.","1939-3520","","10.1109/TSE.2020.3009698","National Natural Science Foundation of China(grant numbers:61772055,61872169); Technical Foundation Project of Ministry of Industry and Information Technology of China(grant numbers:JSZL2016601B003); Equipment Preliminary R&D Project of China(grant numbers:41402020102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144441","Metamorphic testing;metamorphic relation;metamorphic relation composition;test oracle;fault detection capability","Testing;Fault detection;Software;Guidelines;Systematics;Australia;Companies","","11","","49","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
"Empirical Evaluation of Fault Localisation Using Code and Change Metrics","J. Sohn; S. Yoo","School of Computing, Korea Advanced Institute of Science and Technology, Daejon, Republic of Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejon, Republic of Korea","IEEE Transactions on Software Engineering","12 Aug 2021","2021","47","8","1605","1625","Fault localisation aims to reduce the debugging efforts of human developers by highlighting the program elements that are suspected to be the root cause of the observed failure. Spectrum Based Fault Localisation (SBFL), a coverage based approach, has been widely studied in many researches as a promising localisation technique. Recently, however, it has been proven that SBFL techniques have reached the limit of further improvement. To overcome the limitation, we extend SBFL with code and change metrics that have been mainly studied in defect prediction, such as size, age, and churn. FLUCCS, our fault learn-to-rank localisation technique, employs both existing SBFL formulæ and these metrics as input. We investigate the effect of employing code and change metrics for fault localisation using four different learn-to-rank techniques: Genetic Programming, Gaussian Process Modelling, Support Vector Machine, and Random Forest. We evaluate the performance of FLUCCS with 386 real world faults collected from Defects4J repository. The results show that FLUCCS with code and change metrics places 144 faults at the top and 304 faults within the top ten. This is a significant improvement over the state-of-art SBFL formulæ, which can locate 65 and 212 faults at the top and within the top ten, respectively. We also investigate the feasibility of cross-project transfer learning of fault localisation. The results show that, while there exist project-specific properties that can be exploited for better localisation per project, ranking models learnt from one project can be applied to others without significant loss of effectiveness.","1939-3520","","10.1109/TSE.2019.2930977","National Research Foundation of Korea(grant numbers:NRF-2016R1C1B1011042); Next-Generation Information Computing Development Program; National Research Foundation of Korea(grant numbers:2017M3C4A7068179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772166","Fault localisation;SBSE;genetic programming","Measurement;Debugging;Genetic programming;Feature extraction;Support vector machines;Training","","10","","51","IEEE","25 Jul 2019","","","IEEE","IEEE Journals"
"Fine-Grained Dynamic Resource Allocation for Big-Data Applications","L. Baresi; A. Leva; G. Quattrocchi","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, MI, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, MI, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, MI, Italy","IEEE Transactions on Software Engineering","12 Aug 2021","2021","47","8","1668","1682","Many big-data applications are batch applications that exploit dedicated frameworks to perform massively parallel computations across clusters of machines. The time needed to process the entirety of the inputs represents the application's response time, which can be subject to deadlines. Spark, probably the most famous incarnation of these frameworks today, allocates resources to applications statically at the beginning of the execution and deviations are not managed: to meet the applications' deadlines, resources must be allocated carefully. This paper proposes an extension to Spark, called dynaSpark, that is able to allocate and redistribute resources to applications dynamically to meet deadlines and cope with the execution of unanticipated applications. This work is based on two key enablers: containers, to isolate Spark's parallel executors and allow for the dynamic and fast allocation of resources, and control-theory to govern resource allocation at runtime and obtain required precision and speed. Our evaluation shows that dynaSpark can (i) allocate resources efficiently to execute single applications with respect to set deadlines and (ii) reduce deadline violations (w.r.t. Spark) when executing multiple concurrent applications.","1939-3520","","10.1109/TSE.2019.2931537","Italian Technology Cluster For Smart Communities(grant numbers:CTN01_00034_594053); GAUSS national research project(grant numbers:2015KWREMX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778680","Distributed architectures;control theory;quality assurance;batch processing systems","Sparks;Resource management;Dynamic scheduling;Containers;Task analysis;Runtime;Control theory","","10","","61","IEEE","29 Jul 2019","","","IEEE","IEEE Journals"
"Comments on “Using $k$k-Core Decomposition on Class Dependency Networks to Improve Bug Prediction Model's Practical Performance”","W. Pan; H. Ming; Z. Yang; T. Wang","School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, Xi'an Jiaotong University and GuardStrike Inc., Shaanxi, China; School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China","IEEE Transactions on Software Engineering","9 Dec 2022","2022","48","12","5176","5187","In a very recent paper by (Qu et al., 2021), the authors propose an effective equation, top-core, to improve the performance of effort-aware bug prediction models. A distinctive feature of top-core is that it takes into account the coreness of a class in a Class Dependency Network (CDN) when calculating the relative risk of a class to be buggy. In this comment, we show that Qu et al.'s paper contains three shortcomings that may influence the performance of top-core or even have the potential to lead to erroneous results. First, we show that the CDN that they use to calculate the coreness of classes is not very accurate, neglecting many important types of dependency relations between classes such as method call relation, access relation, and instantiates relation. Second, they trained a Logistic Regression model using the scikit-learn framework to predict the probability of a specific class to be buggy. It is actually an L2 regularized Logistic Regression model, which is dependent on the scale of the features. But they neglected to normalize the features, making the obtained results erroneous. Finally, the number of execution times (viz. 10 times in the paper of Qu et al.) they used to reduce the bias caused by the randomness (viz. random split of instances and the process to handle class-imbalance problem) in the experiments is too small to ensure that the obtained results converge to stable values; but they failed to signify the precision level of their results for comparison. In this comment, we provide solutions to the problems by using i) an improved CDN (ICDN) to represent the structure of software systems, ii) the z-score method to normalize the features, and iii) an adaptive mechanism to determine the number of execution times. In the experiments, we find that Qu et al.'s approach based on the Logistic Regression model does not perform significantly better than the state-of-the-art approach Ree, which is inconsistent with the conclusion in Qu et al.'s work. We also observe that replacing CDN with ICDN does improve the performance of Qu et al.'s approach.","1939-3520","","10.1109/TSE.2022.3140599","Natural Science Foundation of Zhejiang Province(grant numbers:LY22F020007); National Natural Science Foundation of China(grant numbers:62032010,61832014); Key R&D Program of Zhejiang Province(grant numbers:2019C01004,2019C03123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733807","Effort-aware bug prediction;coreness;  $k$   k     -core;class dependency network;complex network","Computer bugs;Logistics;Predictive models;Mathematical models;Testing;Software systems;Codes","","10","","17","IEEE","14 Mar 2022","","","IEEE","IEEE Journals"
"CBUA: A Probabilistic, Predictive, and Practical Approach for Evaluating Test Suite Effectiveness","P. Zhang; Y. Li; W. Ma; Y. Yang; L. Chen; H. Lu; Y. Zhou; B. Xu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Software Engineering","15 Mar 2022","2022","48","3","1067","1096","Knowing the effectiveness of a test suite is essential for many activities such as assessing the test adequacy of code and guiding the generation of new test cases. Mutation testing is a commonly used defect injection technique for evaluating the effectiveness of a test suite. However, it is usually computationally expensive, as a large number of mutants (buggy versions) are needed to be generated from a production code under test and executed against the test suite. In order to reduce the expensive testing cost, recent studies proposed to use supervised models to predict the effectiveness of a test suite without executing the test suite against the mutants. Nonetheless, the training of such a supervised model requires labeled data, which still depends on the costly mutant execution. Furthermore, existing models are based on traditional supervised learning techniques, which assume that the training and testing data come from the same distribution. But, in practice, software systems are subject to considerable concept drifts, i.e., the same distribution assumption usually does not hold. This can lead to inaccurate predictions of a learned supervised model on the target code as time progresses. To tackle these problems, in this paper, we propose a Coverage-Based Unsupervised Approach (CBUA) for evaluating the effectiveness of a test suite. Given a production code under test, the corresponding mutants, and a test suite, CBUA first collects the coverage information of the mutated statements in the target production code under the execution of the test suite. Then, CBUA employs coverage to estimate the probability of each mutant being alive. As such, a mutation score is computed to evaluate the test suite effectiveness and the predicted labels (i.e., killed or alive) are obtained. The whole process only requires a one-time execution of the test suite against the target production code, without involving any mutant execution and any training data. CBUA can ensure the score monotonicity property (i.e., adding test cases to a test suite does not decrease its mutation score), which may be violated by a supervised approach. The experimental results show that CBUA is very competitive with the state-of-the-art supervised approaches in prediction accuracy. In particular, CBUA is shown to be more effective in finding mutants that are covered but not killed by a test suite, which is helpful in identifying the weaknesses in the current test suite and generating new test cases accordingly. Since CBUA is an easy-to-implement approach with a low cost, we suggest that it should be used as a baseline approach for comparison when any novel prediction approach is proposed in future studies.","1939-3520","","10.1109/TSE.2020.3010361","National Key Research and Development Program of China(grant numbers:2018YFB1003901); National Natural Science Foundation of China(grant numbers:61772259,61872177,61832009,61772263); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144443","Effectiveness;test suites;coverage;unsupervised model;mutation testing","Testing;Predictive models;Production;Data models;Computational modeling;Training;Training data","","8","","54","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
"Sequential Model Optimization for Software Effort Estimation","T. Xia; R. Shu; X. Shen; T. Menzies","Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Software Engineering","14 Jun 2022","2022","48","6","1994","2009","Many methods have been proposed to estimate how much effort is required to build and maintain software. Much of that research tries to recommend a single method – an approach that makes the dubious assumption that one method can handle the diversity of software project data. To address this drawback, we apply a configuration technique called “ROME” (Rapid Optimizing Methods for Estimation), which uses sequential model-based optimization (SMO) to find what configuration settings of effort estimation techniques work best for a particular data set. We test this method using data from 1161 traditional waterfall projects and 120 contemporary projects (from GitHub). In terms of magnitude of relative error and standardized accuracy, we find that ROME achieves better performance than the state-of-the-art methods for both traditional waterfall and contemporary projects. In addition, we conclude that we should not recommend one method for estimation. Rather, it is better to search through a wide range of different methods to find what works best for the local data. To the best of our knowledge, this is the largest effort estimation experiment yet attempted and the only one to test its methods on traditional waterfall and contemporary projects.","1939-3520","","10.1109/TSE.2020.3047072","National Science Foundation(grant numbers:#1703487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307246","Effort estimation;COCOMO;hyperparameter tuning;regression trees;sequential model optimization","Estimation;Software;Tools;Optimization;Data models;Task analysis;Mathematical model","","8","","82","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"Pride: Prioritizing Documentation Effort Based on a PageRank-Like Algorithm and Simple Filtering Rules","W. Pan; H. Ming; D. -K. Kim; Z. Yang","School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, Zhejiang, China; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, Xi’an Jiaotong University and GuardStrike Inc., Xi’an, Shaanxi, China","IEEE Transactions on Software Engineering","14 Mar 2023","2023","49","3","1118","1151","Code documentation can be helpful in many software quality assurance tasks. However, due to resource constraints (e.g., time, human resources, and budget), programmers often cannot document their work completely and timely. In the literature, two approaches (one is supervised and the other is unsupervised) have been proposed to prioritize documentation effort to ensure the most important classes to be documented first. However, both of them contain several limitations. The supervised approach overly relies on a difficult-to-obtain labeled data set and has high computation cost. The unsupervised one depends on a graph representation of the software structure, which is inaccurate since it neglects many important couplings between classes. In this paper, we propose an improved approach, named Pride, to prioritize documentation effort. First, Pride uses a weighted directed class coupling network to precisely describe classes and their couplings. Second, we propose a PageRank-like algorithm to quantify the importance of classes in the whole class coupling network. Third, we use a set of software metrics to quantify source code complexity and further propose a simple but easy-to-operate filtering rule. Fourth, we sort all the classes according to their importance in descending order and use the filtering rule to filter out unimportant classes. Finally, a threshold $k$k is utilized, and the top-$k$k% ranked classes are the identified important classes to be documented first. Empirical results on a set of nine software systems show that, according to the average ranking of the Friedman test, Pride is superior to the existing approaches in the whole data set.","1939-3520","","10.1109/TSE.2022.3171469","Natural Science Foundation of Zhejiang Province(grant numbers:LY22F020007); National Natural Science Foundation of China(grant numbers:62032010,61832014); Key R&D Program of Zhejiang Province(grant numbers:2019C01004,2019C03123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765699","Code documentation;program comprehension;software maintenance;PageRank;software metrics","Codes;Documentation;Couplings;Software;Software metrics;Java;Task analysis","","8","","70","IEEE","29 Apr 2022","","","IEEE","IEEE Journals"
"Active Learning of Discriminative Subgraph Patterns for API Misuse Detection","H. J. Kang; D. Lo","School of Information Systems, Singapore Management University, Singapore, Singapore; School of Information Systems, Singapore Management University, Singapore, Singapore","IEEE Transactions on Software Engineering","16 Aug 2022","2022","48","8","2761","2783","A common cause of bugs and vulnerabilities are the violations of usage constraints associated with Application Programming Interfaces (APIs). API misuses are common in software projects, and while there have been techniques proposed to detect such misuses, studies have shown that they fail to reliably detect misuses while reporting many false positives. One limitation of prior work is the inability to reliably identify correct patterns of usage. Many approaches confuse a usage pattern's frequency for correctness. Due to the variety of alternative usage patterns that may be uncommon but correct, anomaly detection-based techniques have limited success in identifying misuses. We address these challenges and propose ALP (Actively Learned Patterns), reformulating API misuse detection as a classification problem. After representing programs as graphs, ALP mines discriminative subgraphs. While still incorporating frequency information, through limited human supervision, we reduce the reliance on the assumption relating frequency and correctness. The principles of active learning are incorporated to shift human attention away from the most frequent patterns. Instead, ALP samples informative and representative examples while minimizing labeling effort. In our empirical evaluation, ALP substantially outperforms prior approaches on both MUBench, an API Misuse benchmark, and a new dataset that we constructed from real-world software projects.","1939-3520","","10.1109/TSE.2021.3069978","National Research Foundation Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392340","API-Misuse detection;discriminative subgraph mining;graph classification;active learning","Detectors;Software development management;Java;Tools;Software;Computer bugs;Ciphers","","7","","103","IEEE","31 Mar 2021","","","IEEE","IEEE Journals"
"Mithra: Anomaly Detection as an Oracle for Cyberphysical Systems","A. Afzal; C. Le Goues; C. S. Timperley","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","11 Nov 2022","2022","48","11","4535","4552","Testing plays an essential role in ensuring the safety and quality of cyberphysical systems (CPSs). One of the main challenges in automated and software-in-the-loop simulation testing of CPSs is defining effective oracles that can check that a given system conforms to expectations of desired behavior. Manually specifying such oracles can be tedious, complex, and error-prone, and so techniques for automatically learning oracles are attractive. Characteristics of CPSs, such as limited or no access to source code, behavior that is non-deterministic and sensitive to noise, and that the system may respond differently to input based on its context introduce considerable challenges for automated oracle learning. We present Mithra, a novel, unsupervised oracle learning technique for CPSs that operates on existing telemetry data. It uses a three-step multivariate time series clustering to discover the set of unique, correct behaviors for a CPS, which it uses to construct robust oracles. We instantiate our proposed technique for ArduPilot, a popular, open-source autopilot software. On a set of 24 bugs, we show that Mithra effectively identifies buggy executions with few false positives and outperforms AR-SI, a state-of-the-art CPS oracle learning technique. We demonstrate Mithra’s wider applicability by applying it to an autonomous racer built for the Robot Operating System.","1939-3520","","10.1109/TSE.2021.3120680","Air Force Research Laboratory(grant numbers:#FA8750- 15-2-0075); Defense Advanced Research Projects Agency(grant numbers:#FA8750-16-2-0042,NSF-1563797); Air Force Research Laboratory(grant numbers:19-PAF00747); National Science Foundation(grant numbers:#CCF-1750116); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576615","Robotics and autonomous systems;cyberphysical systems testing;anomaly detection;oracle learning;clustering;Mithra","Computer bugs;Codes;Testing;Telemetry;Splines (mathematics);Sensors;Sensor systems","","6","","120","IEEE","15 Oct 2021","","","IEEE","IEEE Journals"
"Learning Approximate Execution Semantics From Traces for Binary Function Similarity","K. Pei; Z. Xuan; J. Yang; S. Jana; B. Ray","Columbia University, New York, NY, USA; Purdue University, West Lafayette, IN, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2776","2790","Detecting semantically similar binary functions – a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics – requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions’ execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions’ traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8× faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex – on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.","1939-3520","","10.1109/TSE.2022.3231621","National Science Foundation(grant numbers:CCF-18-45893,CCF-18-22965,CCF-16-19123,CNS-18-42456,CNS-18-01426,CNS-16-18771,CNS-16-17670,CNS-15-64055,CNS-15-63843); ONR(grant numbers:N00014-17-1-2010,N00014-16-1-2263,N00014-17-1-2788); NSF CAREER; ARO Young Investigator; Google Faculty Fellowship; JP Morgan Faculty Research Award; DiDi Faculty Research Award; Google Cloud; Capital One Research; Amazon Web Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002189","Binary analysis;large language models;software security","Semantics;Task analysis;Computer architecture;Optimization;Codes;Behavioral sciences;Computational modeling","","6","","100","IEEE","28 Dec 2022","","","IEEE","IEEE Journals"
"Enhancing Dynamic Symbolic Execution by Automatically Learning Search Heuristics","S. Cha; S. Hong; J. Bak; J. Kim; J. Lee; H. Oh","Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Software Engineering","16 Sep 2022","2022","48","9","3640","3663","We present a technique to automatically generate search heuristics for dynamic symbolic execution. A key challenge in dynamic symbolic execution is how to effectively explore the program's execution paths to achieve high code coverage in a limited time budget. Dynamic symbolic execution employs a search heuristic to address this challenge, which favors exploring particular types of paths that are most likely to maximize the final coverage. However, manually designing a good search heuristic is nontrivial and typically ends up with suboptimal and unstable outcomes. The goal of this paper is to overcome this shortcoming of dynamic symbolic execution by automatically learning search heuristics. We define a class of search heuristics, namely a parametric search heuristic, and present an algorithm that efficiently finds an optimal heuristic for each subject program. Experimental results with industrial-strength symbolic execution tools (e.g., KLEE) show that our technique can successfully generate search heuristics that significantly outperform existing manually-crafted heuristics in terms of branch coverage and bug-finding.","1939-3520","","10.1109/TSE.2021.3101870","Samsung Research Funding & Incubation Center of Samsung Electronics(grant numbers:SRFC-IT1701-51); Institute of Information & Communications Technology Planning & Evaluation; Ministry of Science and ICT, South Korea(grant numbers:2020-0-01337); ICT Creative Consilience program(grant numbers:IITP-2021-2020-0-01819); Institute for Information & communications Technology Planning & Evaluation; National Research Foundation of Korea(grant numbers:NRF-2021R1C1C2006410); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507083","Dynamic symbolic execution;concolic testing;execution-generated testing;search heuristics;software testing","Testing;Heuristic algorithms;Tools;Software testing;Search problems;Open source software;Software algorithms","","5","","78","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Automatically ‘Verifying’ Discrete-Time Complex Systems through Learning, Abstraction and Refinement","J. Wang; J. Sun; S. Qin; C. Jegourel","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Singapore University of Technology and Design, Singapore; School of Computing, Media and the Arts, Teesside University, Middlesbrough, United Kingdom; Singapore University of Technology and Design, Singapore","IEEE Transactions on Software Engineering","8 Jan 2021","2021","47","1","189","203","Precisely modeling complex systems like cyber-physical systems is challenging, which often renders model-based system verification techniques like model checking infeasible. To overcome this challenge, we propose a method called LAR to automatically `verify' such complex systems through a combination of learning, abstraction and refinement from a set of system log traces. We assume that log traces and sampling frequency are adequate to capture `enough' behaviour of the system. Given a safety property and the concrete system log traces as input, LAR automatically learns and refines system models, and produces two kinds of outputs. One is a counterexample with a bounded probability of being spurious. The other is a probabilistic model based on which the given property is `verified'. The model can be viewed as a proof obligation, i.e., the property is verified if the model is correct. It can also be used for subsequent system analysis activities like runtime monitoring or model-based testing. Our method has been implemented as a self-contained software toolkit. The evaluation on multiple benchmark systems as well as a real-world water treatment system shows promising results.","1939-3520","","10.1109/TSE.2018.2886898","National Natural Science Foundation of China(grant numbers:61772347); Science and Technology Foundation of Shenzhen City(grant numbers:JCYJ20170302153712968); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576657","Verification;model learning;abstraction refinement;cyber-physical system","Probabilistic logic;Model checking;Analytical models;Safety;Complex systems;System analysis and design","","3","","59","IEEE","14 Dec 2018","","","IEEE","IEEE Journals"
"Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning","T. Le-Cong; D. -M. Luong; X. B. D. Le; D. Lo; N. -H. Tran; B. Quang-Huy; Q. -T. Huynh","School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; School of Information and Communication Technology, Hanoi University of Science and Technology, Ha Noi, Vietnam; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; School of Computing and Information Systems, Singapore Management University, Singapore; School of Information and Communication Technology, Hanoi University of Science and Technology, Ha Noi, Vietnam; School of Information and Communication Technology, Hanoi University of Science and Technology, Ha Noi, Vietnam; School of Information and Communication Technology, Hanoi University of Science and Technology, Ha Noi, Vietnam","IEEE Transactions on Software Engineering","13 Jun 2023","2023","49","6","3411","3429","Automated program repair (APR) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, Invalidator, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. Invalidator leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, Invalidator infers likely invariants on both programs. Then, Invalidator determines that an APR-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on invariants, Invalidator utilizes a trained model from labeled patches to assess patch correctness based on program syntax. The benefit of Invalidator is threefold. First, Invalidator leverages both semantic and syntactic reasoning to enhance its discriminative capability. Second, Invalidator does not require new test cases to be generated, but instead only relies on the current test suite and uses invariant inference to generalize program behaviors. Third, Invalidator is fully automated. Experimental results demonstrate that Invalidator outperforms existing methods in terms of Accuracy and F-measure, correctly identifying 79% of overfitting patches and detecting 23% more overfitting patches than the best baseline.","1939-3520","","10.1109/TSE.2023.3255177","Australian Research Council(grant numbers:DE220101057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066209","Automated patch correctness assessment;automated program repair;code representations;overfitting problem;program invariants","Syntactics;Semantics;Maintenance engineering;Cognition;Manuals;Codes;Source coding","","2","","79","IEEE","10 Mar 2023","","","IEEE","IEEE Journals"
"Improving Cross-Language Code Clone Detection via Code Representation Learning and Graph Neural Networks","N. Mehrotra; A. Sharma; A. Jindal; R. Purandare","Department of Computer Science Engineering, IIIT Delhi, Delhi, India; Department of Computer Science Engineering, IIIT Delhi, Delhi, India; Department of Computer Science Engineering, IIIT Delhi, Delhi, India; University of Nebraska–Lincoln, Lincoln, NE, USA","IEEE Transactions on Software Engineering","16 Nov 2023","2023","49","11","4846","4868","Code clone detection is an important aspect of software development and maintenance. The extensive research in this domain has helped reduce the complexity and increase the robustness of source code, thereby assisting bug detection tools. However, the majority of the clone detection literature is confined to a single language. With the increasing prevalence of cross-platform applications, functionality replication across multiple languages is common, resulting in code fragments having similar functionality but belonging to different languages. Since such clones are syntactically unrelated, single language clone detection tools are not applicable in their case. In this article, we propose a semi-supervised deep learning-based tool Rubhus, capable of detecting clones across different programming languages. Rubhus uses the control and data flow enriched abstract syntax trees (ASTs) of code fragments to leverage their syntactic and structural information and then applies graph neural networks (GNNs) to extract this information for the task of clone detection. We demonstrate the effectiveness of our proposed system through experiments conducted over datasets consisting of Java, C, and Python programs and evaluate its performance in terms of precision, recall, and F1 score. Our results indicate that Rubhus outperforms the state-of-the-art cross-language clone detection tools.","1939-3520","","10.1109/TSE.2023.3311796","Department of Science and Technology (DST) (India); Science and Engineering Research Board (SERB); Confederation of Indian Industry (CII); Infosys Center for Artificial Intelligence at IIIT-Delhi; Nucleus Software Exports Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242168","Program representation learning;cross-language code clone detection;graph-based neural networks;abstract syntax trees","Codes;Cloning;Syntactics;Semantics;Java;Task analysis;Source coding","","1","","76","IEEE","6 Sep 2023","","","IEEE","IEEE Journals"
"Accelerating Finite State Machine-Based Testing Using Reinforcement Learning","U. C. Türker; R. M. Hierons; K. El-Fakih; M. R. Mousavi; I. Y. Tyukin","School of Computing and Communications, Lancaster University, Lancaster, U.K.; Department of Computer Science, The University of Sheffield, Sheffield, U.K.; Department of Computer Science and Engineering, American University of Sharjah, University City, UAE; Department of Informatics, King’s College London, London, U.K.; Department of Mathematics, King’s College London, London, U.K.","IEEE Transactions on Software Engineering","18 Mar 2024","2024","50","3","574","597","Testing is a crucial phase in the development of complex systems, and this has led to interest in automated test generation techniques based on state-based models. Many approaches use models that are types of finite state machine (FSM). Corresponding test generation algorithms typically require that certain test components, such as reset sequences (RSs) and preset distinguishing sequences (PDSs), have been produced for the FSM specification. Unfortunately, the generation of RSs and PDSs is computationally expensive, and this affects the scalability of such FSM-based test generation algorithms. This paper addresses this scalability problem by introducing a reinforcement learning framework: the $\mathcal{Q}$Q-Graph framework for MBT. We show how this framework can be used in the generation of RSs and PDSs and consider both (potentially partial) timed and untimed models. The proposed approach was evaluated using three types of FSMs: randomly generated FSMs, FSMs from a benchmark, and an FSM of an Engine Status Manager for a printer. In experiments, the proposed approach was much faster and used much less memory than the state-of-the-art methods in computing PDSs and RSs.","1939-3520","","10.1109/TSE.2024.3358416","UKRI Trustworthy Autonomous Systems Node in Verifiability(grant numbers:EP/V026801/2); EPSRC: RoboTest: Systematic Model-Based Testing and Simulation of Mobile Autonomous Robots(grant numbers:EP/R025134/1); Security Lancaster(grant numbers:IRL1032 Poison Attack Mitigation); AUS(grant numbers:FRG23-R-E39); EPSRC project on Verified Simulation for Large Quantum Systems (VSL-Q)(grant numbers:EP/Y005244/1); EPSRC project on Robust and Reliable Quantum Computing (RoaRQ), Investigation 009 Model-based monitoring and calibration of quantum computations (ModeMCQ)(grant numbers:EP/W032635/1); King’s College London(grant numbers:King’s Quantum); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10414288","Finite state machines;reset sequences;state identification sequences;reinforcement learning;Q-value function;software engineering/software/program verification;software engineering/test design;software engineering/testing and debugging","Test pattern generators;Scalability;Graphics processing units;Automata;Software systems;Engines;Real-time systems","","","","86","IEEE","25 Jan 2024","","","IEEE","IEEE Journals"
"Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints","E. A. Alhazami; A. M. Sheneamer","Faculty of Computer Science, Jazan University, Jazan, Saudi Arabia; Faculty of Computer Science, Jazan University, Jazan, Saudi Arabia","IEEE Transactions on Software Engineering","14 Aug 2023","2023","49","8","3972","3988","The code clone detection issue has been researched using a number of explicit factors based on the tokens and contents and found effective results. However, exposing code contents may be an impractical option because of privacy and security factors. Moreover, the lack of scalability of past methods is an important challenge. The code flow states can be inferred by code structure and implicitly represented using empirical graphs. The assumption is that modelling of the code clone detection problem can be achieved without the content of the codes being revealed. Here, a Graph-of-Code concept for the code clone detection problem is introduced, which represents codes into graphs. While Graph-of-Code provides structural properties and quantification of its characteristics, it can exclude code contents or tokens to identify the clone type. The aim is to evaluate the impact of graph-of-code structural properties on the performance of code clone detection. This work employs a feature extraction-based approach for unlabelled graphs. The approach generates a “Graph Fingerprint” which represents different topological feature levels. The results of code clone detection indicate that code structure has a significant role in detecting clone types. We found different GoC-models outperform others. The models achieve between 96% to 99% in detecting code clones based on recall, precision, and F1-Score. The GoC approach is capable in detecting code clones with scalable dataset and with preserving codes privacy.","1939-3520","","10.1109/TSE.2023.3276780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125077","Code clones;software clones;semantic clones;graph-of-code;graph properties","Codes;Cloning;Semantics;Software systems;Fingerprint recognition;Feature extraction;Source coding","","","","86","IEEE","16 May 2023","","","IEEE","IEEE Journals"
"Better Pay Attention Whilst Fuzzing","S. Zhu; J. Wang; J. Sun; J. Yang; X. Lin; T. Wang; L. Zhang; P. Cheng","College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; School of Computing and Information Systems, Singapore Management University, Singapore; School of Medicine, Zhejiang University, Zhejiang, China; Ant Group, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China","IEEE Transactions on Software Engineering","12 Feb 2024","2024","50","2","190","208","Fuzzing is one of the prevailing methods for vulnerability detection. However, even state-of-the-art fuzzing methods become ineffective after some period of time, i.e., the coverage hardly improves as existing methods are ineffective to focus the attention of fuzzing on covering the hard-to-trigger program paths. In other words, they cannot generate inputs that can break the bottleneck due to the fundamental difficulty in capturing the complex relations between the test inputs and program coverage. In particular, existing fuzzers suffer from the following main limitations: 1) lacking an overall analysis of the program to identify the most “rewarding” seeds, and 2) lacking an effective mutation strategy which could continuously select and mutates the more relevant “bytes” of the seeds. In this work, we propose an approach called ATTuzz to address these two issues systematically. First, we propose a lightweight dynamic analysis technique that estimates the “reward” of covering each basic block and selects the most rewarding seeds accordingly. Second, we mutate the selected seeds according to a neural network model which predicts whether a certain “rewarding” block will be covered given certain mutations on certain bytes of a seed. The model is a deep learning model equipped with an attention mechanism which is learned and updated periodically whilst fuzzing. Our evaluation shows that ATTuzz significantly outperforms 5 state-of-the-art grey-box fuzzers on 6 popular real-world programs and MAGMA data sets at achieving higher edge coverage and finding new bugs. In particular, ATTuzz achieved 1.2X edge coverage and 1.8X bugs detected than AFL++ over 24-hour runs. In addition, ATTuzz also finds 4 new bugs in the latest version of some popular software including p7zip and openUSD.","1939-3520","","10.1109/TSE.2023.3338129","National Natural Science Foundation of China(grant numbers:61833015,62293511,62102359); Academic Research Fund Tier 3, Ministry of Education, Singapore(grant numbers:MOET32020-0004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339688","Fuzzing;deep learning;program analysis;attention model","Fuzzing;Deep learning;Computer bugs;Codes;Image edge detection;Electronic mail;Recurrent neural networks","","","","72","IEEE","4 Dec 2023","","","IEEE","IEEE Journals"
"Mitigating False Positive Static Analysis Warnings: Progress, Challenges, and Opportunities","Z. Guo; T. Tan; S. Liu; X. Liu; W. Lai; Y. Yang; Y. Li; L. Chen; W. Dong; Y. Zhou","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Beijing Bytedance Network Technology Company Ltd., Beijing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; College of Computer Science, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Software Engineering","12 Dec 2023","2023","49","12","5154","5188","Static analysis (SA) tools can generate useful static warnings to reveal the problematic code snippets in a software system without dynamically executing the corresponding source code. In the literature, static warnings are of paramount importance because they can easily indicate specific types of software defects in the early stage of a software development process, which accordingly reduces the maintenance costs by a substantial margin. Unfortunately, due to the conservative approximations of such SA tools, a large number of false positive (FP for short) warnings (i.e., they do not indicate real bugs) are generated, making these tools less effective. During the past two decades, therefore, many false positive mitigation (FPM for short) approaches have been proposed so that more accurate and critical warnings can be delivered to developers. This paper offers a detailed survey of research achievements on the topic of FPM. Given the collected 130 surveyed papers, we conduct a comprehensive investigation from five different perspectives. First, we reveal the research trends of this field. Second, we classify the existing FPM approaches into five different types and then present the concrete research progress. Third, we analyze the evaluation system applied to examine the performance of the proposed approaches in terms of studied SA tools, evaluation scenarios, performance indicators, and collected datasets, respectively. Fourth, we summarize the four types of empirical studies relating to SA warnings to exploit the insightful findings that are helpful to reduce FP warnings. Finally, we sum up 10 challenges unresolved in the literature from the aspects of systematicness, effectiveness, completeness, and practicability and outline possible research opportunities based on three emerging techniques in the future.","1939-3520","","10.1109/TSE.2023.3329667","Natural Science Foundation of China(grant numbers:62172205,62072194,62172202,62272221,62032019); National Key Research and Development Program of China(grant numbers:2022YFB4501903); Natural Science Foundation of Jiangsu Province(grant numbers:SBK2023022696); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305541","Static warnings;false positives;defects;static analysis tools;software quality assurance","Surveys;Software;Static analysis;Software quality;Codes;Computer bugs;Market research","","","","186","IEEE","2 Nov 2023","","","IEEE","IEEE Journals"
"Classifying Code Comments in Java Open-Source Software Systems","L. Pascarella; A. Bacchelli","Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","227","237","Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments, subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning, initial results are promising and suggest that an accurate classification is within reach.","","978-1-5386-1544-7","10.1109/MSR.2017.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962372","source code comments;comment taxonomy;software quality","Taxonomy;Java;Maintenance engineering;Measurement;Google;Open source software","","63","","35","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"The Impact of Using Regression Models to Build Defect Classifiers","G. K. Rajbahadur; S. Wang; Y. Kamei; A. E. Hassan","Queen's University, Canada; Queen's University, Canada; Kyushu University, Japan; Queen's University, Canada","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","135","145","It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers). In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches, ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance. Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.","","978-1-5386-1544-7","10.1109/MSR.2017.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962363","Classification via regression;Random forest;Bug prediction;Discretization;Non-Discretization;Model Interpretation","Buildings;Correlation;Software;Predictive models;Redundancy;Computational modeling;Data collection","","41","","44","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Mining Workflows for Anomalous Data Transfers","H. Tu; G. Papadimitriou; M. Kiran; C. Wang; A. Mandal; E. Deelman; T. Menzies","Department of Computer Science, North Carolina State University, Raleigh, USA; University of Southern California, Information Sciences Institute, Marina del Rey, CA, USA; Energy Sciences Network (ESnet), Lawrence Berkeley National Labs, CA, USA; RENCI, University of North Carolina, Chapel Hill, NC, USA; RENCI, University of North Carolina, Chapel Hill, NC, USA; University of Southern California, Information Sciences Institute, Marina del Rey, CA, USA; Department of Computer Science, North Carolina State University, Raleigh, USA","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","1","12","Modern scientific workflows are data-driven and are often executed on distributed, heterogeneous, high-performance computing infrastructures. Anomalies and failures in the work-flow execution cause loss of scientific productivity and inefficient use of the infrastructure. Hence, detecting, diagnosing, and mitigating these anomalies are immensely important for reliable and performant scientific workflows. Since these workflows rely heavily on high-performance network transfers that require strict QoS constraints, accurately detecting anomalous network performance is crucial to ensure reliable and efficient workflow execution. To address this challenge, we have developed X-FLASH, a network anomaly detection tool for faulty TCP workflow transfers. X-FLASH incorporates novel hyperparameter tuning and data mining approaches for improving the performance of the machine learning algorithms to accurately classify the anomalous TCP packets. X-FLASH leverages XGBoost as an ensemble model and couples XGBoost with a sequential optimizer, FLASH, borrowed from search-based Software Engineering to learn the optimal model parameters. X-FLASH found configurations that outperformed the existing approach up to 28%, 29%, and 40% relatively for F-measure, G-score, and recall in less than 30 evaluations. From (1) large improvement and (2) simple tuning, we recommend future research to have additional tuning study as a new standard, at least in the area of scientific workflow anomaly detection.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463120","Scientific Workflow;TCP Signatures;Anomaly Detection;Hyper-Parameter Tuning;Sequential Optimization","Radio frequency;Tools;Data transfer;Data models;Software;Software reliability;Tuning","","7","","69","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Using Developer-Interaction Trails to Triage Change Requests","M. B. Zanjani; H. Kagdi; C. Bird","Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas, USA; Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas, USA; Microsoft Research, Redmond, WA, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","88","98","The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180070","","History;Computer bugs;Mathematical model;Software;Data mining;XML;Context","","6","1","38","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"MANDO-HGT: Heterogeneous Graph Transformers for Smart Contract Vulnerability Detection","H. H. Nguyen; N. -M. Nguyen; C. Xie; Z. Ahmadi; D. Kudendo; T. -N. Doan; L. Jiang","L3S Research Center, Leibniz Universität Hannover, Hannover, Germany; Singapore Management University, Singapore; L3S Research Center, Leibniz Universität Hannover, Hannover, Germany; L3S Research Center, Leibniz Universität Hannover, Hannover, Germany; L3S Research Center, Leibniz Universität Hannover, Hannover, Germany; Independent Researcher, Atlanta, Georgia, USA; Singapore Management University, Singapore","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","334","346","Smart contracts in blockchains have been increasingly used for high-value business applications. It is essential to check smart contracts' reliability before and after deployment. Although various program analysis and deep learning techniques have been proposed to detect vulnerabilities in either Ethereum smart contract source code or bytecode, their detection accuracy and scalability are still limited. This paper presents a novel framework named MANDO-HGT for detecting smart contract vulnerabilities. Given Ethereum smart contracts, either in source code or bytecode form, and vulnerable or clean, MANDO-HGT custom-builds heterogeneous contract graphs (HCGs) to represent control-flow and/or function-call information of the code. It then adapts heterogeneous graph transformers (HGTs) with customized meta relations for graph nodes and edges to learn their embeddings and train classifiers for detecting various vulnerability types in the nodes and graphs of the contracts more accurately. We have collected more than 55K Ethereum smart contracts from various data sources and verified the labels for 423 buggy and 2,742 clean contracts to evaluate MANDO-HGT. Our empirical results show that MANDO-HGT can significantly improve the detection accuracy of other state-of-the-art vulnerability detection techniques that are based on either machine learning or conventional analysis techniques. The accuracy improvements in terms of F1-score range from 0.7% to more than 76% at either the coarse-grained contract level or the fine-grained line level for various vulnerability types in either source code or bytecode. Our method is general and can be retrained easily for different vulnerability types without the need for manually defined vulnerability patterns.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174104","vulnerability detection;smart contracts;source code;bytecode;heterogeneous graph learning;graph transformer","Deep learning;Codes;Source coding;Soft sensors;Scalability;Image edge detection;Smart contracts","","1","","79","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Chaff from the Wheat: Characterizing and Determining Valid Bug Reports","Y. Fan; X. Xia; D. Lo; A. E. Hassan","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Faculty of Information Technology, Monash University, Melbourne, Australia; School of Information Systems, Singapore Management University, Singapore; School of Computing, Queen's University, Kingston, Canada","IEEE Transactions on Software Engineering","14 May 2020","2020","46","5","495","525","Developers use bug reports to triage and fix bugs. When triaging a bug report, developers must decide whether the bug report is valid (i.e., a real bug). A large amount of bug reports are submitted every day, with many of them end up being invalid reports. Manually determining valid bug report is a difficult and tedious task. Thus, an approach that can automatically analyze the validity of a bug report and determine whether a report is valid can help developers prioritize their triaging tasks and avoid wasting time and effort on invalid bug reports. In this study, motivated by the above needs, we propose an approach which can determine whether a newly submitted bug report is valid. Our approach first extracts 33 features from bug reports. The extracted features are grouped along 5 dimensions, i.e., reporter experience, collaboration network, completeness, readability and text. Based on these features, we use a random forest classifier to identify valid bug reports. To evaluate the effectiveness of our approach, we experiment on large-scale datasets containing a total of 560,697 bug reports from five open source projects (i.e., Eclipse, Netbeans, Mozilla, Firefox and Thunderbird). On average, across the five datasets, our approach achieves an F1-score for valid bug reports and F1-score for invalid ones of 0.74 and 0.67, respectively. Moreover, our approach achieves an average AUC of 0.81. In terms of AUC and F1-scores for valid and invalid bug reports, our approach statistically significantly outperforms two baselines using features that are proposed by Zanetti et al. [104] . We also study the most important features that distinguish valid bug reports from invalid ones. We find that the textual features of a bug report and reporter's experience are the most important factors to distinguish valid bug reports from invalid ones.","1939-3520","","10.1109/TSE.2018.2864217","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003904); National Natural Science Foundation of China(grant numbers:61602403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428477","Bug report;feature generation;machine learning","Computer bugs;Feature extraction;Collaboration;Forestry;Support vector machines;Task analysis;Software","","54","","109","IEEE","7 Aug 2018","","","IEEE","IEEE Journals"
"Coverage Prediction for Accelerating Compiler Testing","J. Chen; G. Wang; D. Hao; Y. Xiong; H. Zhang; L. Zhang; B. Xie","Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China; University of Newcastle, Newcastle, NSW, Australia; Institute of Software, EECS, Peking University, Beijing, China; Institute of Software, EECS, Peking University, Beijing, China","IEEE Transactions on Software Engineering","11 Feb 2021","2021","47","2","261","278","Compilers are one of the most fundamental software systems. Compiler testing is important for assuring the quality of compilers. Due to the crucial role of compilers, they have to be well tested. Therefore, automated compiler testing techniques (those based on randomly generated programs) tend to run a large number of test programs (which are test inputs of compilers). The cost for compilation and execution for these test programs is significant. These techniques can take a long period of testing time to detect a relatively small number of compiler bugs. That may cause many practical problems, e.g., bringing a lot of costs including time costs and financial costs, and delaying the development/release cycle. Recently, some approaches have been proposed to accelerate compiler testing by executing test programs that are more likely to trigger compiler bugs earlier according to some criteria. However, these approaches ignore an important aspect in compiler testing: different test programs may have similar test capabilities (i.e., testing similar functionalities of a compiler, even detecting the same compiler bug), which may largely discount their acceleration effectiveness if the test programs with similar test capabilities are executed all the time. Test coverage is a proper approximation to help distinguish them, but collecting coverage dynamically is infeasible in compiler testing since most test programs are generated on the fly by automatic test-generation tools like Csmith. In this paper, we propose the first method to predict test coverage statically for compilers, and then propose to prioritize test programs by clustering them according to the predicted coverage information. The novel approach to accelerating compiler testing through coverage prediction is called COP (short for COverage Prediction). Our evaluation on GCC and LLVM demonstrates that COP significantly accelerates compiler testing, achieving an average of 51.01 percent speedup in test execution time on an existing dataset including three old release versions of the compilers and achieving an average of 68.74 percent speedup on a new dataset including 12 latest release versions. Moreover, COP outperforms the state-of-the-art acceleration approach significantly by improving $17.16\%\sim 82.51\%$17.16%∼82.51% speedups in different settings on average.","1939-3520","","10.1109/TSE.2018.2889771","National Key Research and Development Program of China(grant numbers:2017YFB1001803); National Natural Science Foundation of China(grant numbers:61672047,61529201,61872008,61828201,61672045,61861130363); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588375","Compiler testing;test prioritization;machine learning","Testing;Program processors;Computer bugs;Life estimation;Acceleration;Optimization;Electromagnetic interference","","20","","86","IEEE","25 Dec 2018","","","IEEE","IEEE Journals"
"SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning","J. Chi; Y. Qu; T. Liu; Q. Zheng; H. Yin","Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Computer Science and Technology, Xian Jiaotong University, Xian, China; Department of Computer Science and Engineering, UC Riverside, Riverside, CA, USA; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Computer Science and Technology, Xian Jiaotong University, Xian, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Computer Science and Technology, Xian Jiaotong University, Xian, China; Department of Computer Science and Engineering, UC Riverside, Riverside, CA, USA","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","564","585","Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmers’ manual efforts. Developers need to deeply understand the vulnerability and affect the system’s functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization).","1939-3520","","10.1109/TSE.2022.3156637","National Key Research and Development Program of China(grant numbers:2018YFB1004500); National Natural Science Foundation of China(grant numbers:62002280,61632015,61772408,U1766215,61833015,61902306); National Natural Science Foundation of China(grant numbers:61721002); Innovation Research Team of Ministry of Education(grant numbers:IRT_17R86); China Knowledge Centre for Engineering Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729554","Machine learning;neural machine translation;software engineering;vulnerability fix","Maintenance engineering;Codes;Computer bugs;Predictive models;Transformers;Decoding;Training","","9","","104","IEEE","7 Mar 2022","","","IEEE","IEEE Journals"
"Combining Genetic Programming and Model Checking to Generate Environment Assumptions","K. Gaaloul; C. Menghi; S. Nejati; L. C. Briand; Y. I. Parache","University of Luxembourg, Esch-sur-Alzette, Luxembourg; University of Luxembourg, Esch-sur-Alzette, Luxembourg; University of Ottawa, Ottawa, ON, Canada; University of Ottawa, Ottawa, ON, Canada; LuxSpace, Betzdorf, Luxembourg","IEEE Transactions on Software Engineering","16 Sep 2022","2022","48","9","3664","3685","Software verification may yield spurious failures when environment assumptions are not accounted for. Environment assumptions are the expectations that a system or a component makes about its operational environment and are often specified in terms of conditions over the inputs of that system or component. In this article, we propose an approach to automatically infer environment assumptions for Cyber-Physical Systems (CPS). Our approach improves the state-of-the-art in three different ways: First, we learn assumptions for complex CPS models involving signal and numeric variables; second, the learned assumptions include arithmetic expressions defined over multiple variables; third, we identify the trade-off between soundness and coverage of environment assumptions and demonstrate the flexibility of our approach in prioritizing either of these criteria. We evaluate our approach using a public domain benchmark of CPS models from Lockheed Martin and a component of a satellite control system from LuxSpace, a satellite system provider. The results show that our approach outperforms state-of-the-art techniques on learning assumptions for CPS models, and further, when applied to our industrial CPS model, our approach is able to learn assumptions that are sufficiently close to the assumptions manually developed by engineers to be of practical value.","1939-3520","","10.1109/TSE.2021.3101818","Luxembourg National Research Fund(grant numbers:BRIDGES18/IS/12632261); European Research Council; European Union's Horizon 2020 Research and Innovation Programme(grant numbers:694277); NSERC of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507379","Environment assumptions;model checking;machine learning;decision trees;genetic programming;search-based software testing","Software packages;Satellites;Mathematical model;Computational modeling;Model checking;Numerical models;Attitude control","","3","","109","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"A Software Requirements Ecosystem: Linking Forum, Issue Tracker, and FAQs for Requirements Management","J. Tizard; P. Devine; H. Wang; K. Blincoe","Human Aspects of Software Engineering Lab, University of Auckland, Auckland, New Zealand; Human Aspects of Software Engineering Lab, University of Auckland, Auckland, New Zealand; Human Aspects of Software Engineering Lab, University of Auckland, Auckland, New Zealand; Human Aspects of Software Engineering Lab, University of Auckland, Auckland, New Zealand","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2381","2393","User feedback is an important resource in modern software development, often containing requirements that help address user concerns and desires for a software product. The feedback in online channels is a recent focus for software engineering researchers, with multiple studies proposing automatic analysis tools. In this work, we investigate the product forums of two large open source software projects. Through a quantitative analysis, we show that forum feedback is often manually linked to related issue tracker entries and product documentation. By linking feedback to their existing documentation, development teams enhance their understanding of known issues, and direct their users to known solutions. We discuss how the links between forum, issue tracker, and product documentation form a requirements ecosystem that has not been identified in the previous literature. We apply state-of-the-art deep-learning to automatically match forum posts with related issue tracker entries. Our approach identifies requirement matches with a mean average precision of 58.9% and hit ratio of 82.2%. Additionally, we apply deep-learning using an innovative clustering technique, achieving promising performance when matching forum posts to related product documentation. We discuss the possible applications of these automated techniques to support the flow of requirements between forum, issue tracker, and product documentation.","1939-3520","","10.1109/TSE.2022.3219458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940985","Requirements engineering;machine learning;natural language processing;deep learning;open source software;user feedback;software engineering","Software;Documentation;Computer bugs;Open source software;Ecosystems;Browsers;Software engineering","","2","","44","IEEE","7 Nov 2022","","","IEEE","IEEE Journals"
"Multi-Granularity Detector for Vulnerability Fixes","T. G. Nguyen; T. Le-Cong; H. J. Kang; R. Widyasari; C. Yang; Z. Zhao; B. Xu; J. Zhou; X. Xia; A. E. Hassan; X. -B. D. Le; D. Lo","School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; Software Engineering Application Technology Lab, Shenzhen, Guangdong, China; Software Engineering Application Technology Lab, Shenzhen, Guangdong, China; School of Computing, Queen's University, Kingston, ON, Canada; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; School of Computing and Information Systems, Singapore Management University, Singapore","IEEE Transactions on Software Engineering","14 Aug 2023","2023","49","8","4035","4057","With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately. To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, MiDas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organization and then use an ensemble model combining all base models to output the final prediction. This design allows MiDas to better cope with the noisy and highly-imbalanced nature of vulnerability-fixing commit data. In addition, to reduce the human effort required to inspect code changes, we have designed an effort-aware adjustment for MiDas's outputs based on commit length. The evaluation result demonstrates that MiDas outperforms the current state-of-the-art baseline on both Java and Python-based datasets in terms of AUC by 4.9% and 13.7%, respectively. Furthermore, in terms of two effort-aware metrics, i.e., EffortCost@L and Popt@L, MiDas also performs better than the state-of-the-art baseline up to 28.2% and 15.9% on Java, 60% and 51.4% on Python, respectively.","1939-3520","","10.1109/TSE.2023.3281275","National Research Foundation, Singapore; National University of Singapore; National Satellite of Excellence in Trustworthy Software Systems (NSOE-TSS) office; Trustworthy Computing for Secure Smart Nation Grant(grant numbers:NSOE-TSS2020-02); Australian Research Council's Discovery Early Career Researcher Award(grant numbers:DE220101057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138621","Vulnerability-fixing commit classification;machine learning;deep learning;software security","Codes;Task analysis;Security;Java;Libraries;Testing;Predictive models","","2","","106","IEEE","30 May 2023","","","IEEE","IEEE Journals"
"CombTransformers: Statement-Wise Transformers for Statement-Wise Representations","F. Bertolotti; W. Cazzola","Department of Computer Science, Università degli Studi di Milano, Milan, Italy; Department of Computer Science, Università degli Studi di Milano, Milan, Italy","IEEE Transactions on Software Engineering","17 Oct 2023","2023","49","10","4677","4690","This study presents a novel category of Transformer architectures known as comb transformers, which effectively reduce the space complexity of the self-attention layer from a quadratic to a subquadratic level. This is achieved by processing sequence segments independently and incorporating $\mathcal{X}$X-word embeddings to merge cross-segment information. The reduction in attention memory requirements enables the deployment of deeper architectures, potentially leading to more competitive outcomes. Furthermore, we design an abstract syntax tree (AST)-based code representation to effectively exploit comb transformer properties. To explore the potential of our approach, we develop nine specific instances based on three popular architectural concepts: funnel, hourglass, and encoder-decoder. These architectures are subsequently trained on three code-related tasks: method name generation, code search, and code summarization. These tasks encompass a range of capabilities: short/long sequence generation and classification. In addition to the proposed comb transformers, we also evaluate several baseline architectures for comparative analysis. Our findings demonstrate that the comb transformers match the performance of the baselines and frequently perform better.","1939-3520","","10.1109/TSE.2023.3310793","MUR project “T-LADIES”(grant numbers:PRIN 2020TL3X8X); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10242162","Programming languages;machine learning;learning representations;code search and summarization;method name Gen","Codes;Transformers;Task analysis;Computer architecture;Artificial neural networks;Documentation;Training","","1","","71","CCBYNCND","6 Sep 2023","","","IEEE","IEEE Journals"
"INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers","A. Karmakar; R. Robbes","Free University of Bozen-Bolzano, Bozen-Bolzano, Italy; CNRS, University of Bordeaux, Bordeaux INP, LaBRI, Talence, France","IEEE Transactions on Software Engineering","12 Feb 2024","2024","50","2","220","238","Pre-trained models of source code have recently been successfully applied to a wide variety of Software Engineering tasks; they have also seen some practical adoption in practice, e.g. for code completion. Yet, we still know very little about what these pre-trained models learn about source code. In this article, we use probing—simple diagnostic tasks that do not further train the models—to discover to what extent pre-trained models learn about specific aspects of source code. We use an extensible framework to define 15 probing tasks that exercise surface, syntactic, structural and semantic characteristics of source code. We probe 8 pre-trained source code models, as well as a natural language model (BERT) as our baseline. We find that models that incorporate some structural information (such as GraphCodeBERT) have a better representation of source code characteristics. Surprisingly, we find that for some probing tasks, BERT is competitive with the source code models, indicating that there are ample opportunities to improve source-code specific pre-training on the respective code characteristics. We encourage other researchers to evaluate their models with our probing task suite, so that they may peer into the hidden layers of the models and identify what intrinsic code characteristics are encoded.","1939-3520","","10.1109/TSE.2023.3341624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354028","Machine learning for source code;probing;benchmarking;transformers;pre-trained models","Task analysis;Source coding;Probes;Codes;Training;Natural languages;Data models","","","","73","IEEE","12 Dec 2023","","","IEEE","IEEE Journals"
"Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules","S. Kommrusch; M. Monperrus; L. -N. Pouchet","Colorado State University, Fort Collins, CO, USA; KTH Royal Institute of Technology, Stockholm, Sweden; Colorado State University, Fort Collins, CO, USA","IEEE Transactions on Software Engineering","17 Jul 2023","2023","49","7","3771","3792","We target the problem of automatically synthesizing proofs of semantic equivalence between two programs made of sequences of statements. We represent programs using abstract syntax trees (AST), where a given set of semantics-preserving rewrite rules can be applied on a specific AST pattern to generate a transformed and semantically equivalent program. In our system, two programs are equivalent if there exists a sequence of application of these rewrite rules that leads to rewriting one program into the other. We propose a neural network architecture based on a transformer model to generate proofs of equivalence between program pairs. The system outputs a sequence of rewrites, and the validity of the sequence is simply checked by verifying it can be applied. If no valid sequence is produced by the neural network, the system reports the programs as non-equivalent, ensuring by design no programs may be incorrectly reported as equivalent. Our system is fully implemented for one single grammar which can represent straight-line programs with function calls and multiple types. To efficiently train the system to generate such sequences, we develop an original incremental training technique, named self-supervised sample selection. We extensively study the effectiveness of this novel training approach on proofs of increasing complexity and length. Our system, $\mathsf {S4Eq}$S4Eq, achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent programs.","1939-3520","","10.1109/TSE.2023.3271065","National Science Foundation(grant numbers:CCF-1750399); Wallenberg Artificial Intelligence, Autonomous Systems and Software Program; Knut och Alice Wallenbergs Stiftelse; Swedish Foundation for Strategic Research; Swedish National Infrastructure for Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10109816","Machine learning;program equivalence;self-supervised learning;symbolic reasoning","Symbols;Codes;Training;Software development management;Computational modeling;Syntactics;Source coding","","","","98","IEEE","27 Apr 2023","","","IEEE","IEEE Journals"
"Constructing Cyber-Physical System Testing Suites Using Active Sensor Fuzzing","F. Zhang; Q. Wu; B. Xuan; Y. Chen; W. Lin; C. M. Poskitt; J. Sun; B. Chen","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Software Engineering","16 Nov 2023","2023","49","11","4829","4845","Cyber-physical systems (CPSs) automating critical public infrastructure face a pervasive threat of attack, motivating research into different types of countermeasures. Assessing the effectiveness of these countermeasures is challenging, however, as benchmarks are difficult to construct manually, existing automated testing solutions often make unrealistic assumptions, and blindly fuzzing is ineffective at finding attacks due to the enormous search spaces and resource requirements. In this work, we propose active sensor fuzzing, a fully automated approach for building test suites without requiring any a prior knowledge about a CPS. Our approach employs active learning techniques. Applied to a real-world water treatment system, our approach manages to find attacks that drive the system into 15 different unsafe states involving water flow, pressure, and tank levels, including nine that were not covered by an established attack benchmark. Furthermore, we successfully generate targeted multi-point attacks which have been long suspected to be possible. We reveal that active sensor fuzzing successfully extends the attack benchmarks generated by our previous work, an ML-guided fuzzing tool, with two more kinds of attacks. Finally, we investigate the impact of active learning on models and the reason that the model trained with active learning is able to discover more attacks.","1939-3520","","10.1109/TSE.2023.3309330","National Natural Science Foundation of China(grant numbers:62227805,62072398); National Key R&D Program of China(grant numbers:2020AAA0107700); SUTD-ZJU IDEA Grant for Visiting Professors(grant numbers:SUTD-ZJUVP201901); Alibaba-Zhejiang University Joint Institute of Frontier Technologies; National Key Laboratory of Science and Technology on Information System Security(grant numbers:6142111210301); State Key Laboratory of Mathematical Engineering and Advanced Computing; Key Laboratory of Cyberspace Situation Awareness of Henan Province(grant numbers:HNTS2022001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292692","Cyber-physical systems;fuzzing;testing;machine learning;metaheuristic optimisation","Fuzzing;Benchmark testing;Actuators;Predictive models;Space exploration;Data models;Process control","","","","76","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Cross-Language Clone Detection by Learning Over Abstract Syntax Trees","D. Perez; S. Chiba","Imperial College London, London, United Kingdom; Imperial College London, London, United Kingdom","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","518","528","Clone detection across programs written in the same programming language has been studied extensively in the literature. On the contrary, the task of detecting clones across multiple programming languages has not been studied as much, and approaches based on comparison cannot be directly applied. In this paper, we present a clone detection method based on semi-supervised machine learning designed to detect clones across programming languages with similar syntax. Our method uses an unsupervised learning approach to learn token-level vector representations and an LSTM-based neural network to predict whether two code fragments are clones. To train our network, we present a cross-language code clone dataset - which is to the best of our knowledge the first of its kind - containing around 45,000 code fragments written in Java and Python. We evaluate our approach on the dataset we created and show that our method gives promising results when detecting similarities between code fragments written in Java and Python.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816761","clone detection, machine learning, source code representation","Cloning;Vocabulary;Java;Python;Syntactics;Task analysis","","26","","40","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Senatus - A Fast and Accurate Code-to-Code Recommendation Engine","F. Silavong; S. Moran; A. Georgiadis; R. Saphal; R. Otter","CTO, JPMorgan Chase, London, United Kingdom; CTO, JPMorgan Chase, London, United Kingdom; CTO, JPMorgan Chase, London, United Kingdom; CTO, JPMorgan Chase, London, United Kingdom; CTO, JPMorgan Chase, London, United Kingdom","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","511","523","Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with Senatus, a new code-to-code recommendation engine. At the core of Senatus is De-Skew LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example on the CodeSearchNet dataset Senatus improves performance by 31.21% F1 and 147.9x faster query time compared to Facebook Aroma. Senatus also outperforms standard MinHash LSH by 29.2% F1 and 51.02x faster query time.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3527947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796316","Locality sensitive hashing;MinHash LSH;machine learning on source code;Code-to-code recommendation","Codes;Social networking (online);Switches;Syntactics;Search engines;Software;Data mining","","1","","59","","21 Jun 2022","","","IEEE","IEEE Conferences"
"How Often Do Single-Statement Bugs Occur? The ManySStuBs4J Dataset","R. -M. Karampatsis; C. Sutton","University of Edinburgh, Edinburgh, United Kingdom; Google Research, University of Edinburgh and The Alan Turing Institute, Mountain View, CA, United States","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","573","577","Program repair is an important but difficult software engineering problem. One way to achieve acceptable performance is to focus on classes of simple bugs, such as bugs with single statement fixes, or that match a small set of bug templates. However, it is very difficult to estimate the recall of repair techniques for simple bugs, as there are no datasets about how often the associated bugs occur in code. To fill this gap, we provide a dataset of 153,652 single statement bug-fix changes mined from 1,000 popular open-source Java projects, annotated by whether they match any of a set of 16 bug templates, inspired by state-of-the-art program repair techniques. In an initial analysis, we find that about 33% of the simple bug fixes match the templates, indicating that a remarkable number of single-statement bugs can be repaired with a relatively small set of templates. Further, we find that template fitting bugs appear with a frequency of about one bug per 1,600-2,500 lines of code (as measured by the size of the project's latest version). We hope that the dataset will prove a resource for both future work in program repair and studies in empirical software engineering.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148782","Program Repair;Mining Software Repositories;Datasets","Java;Codes;Computer bugs;Machine learning;Maintenance engineering;Size measurement;Software","","21","","27","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Escaping the Time Pit: Pitfalls and Guidelines for Using Time-Based Git Data","S. W. Flint; J. Chauhan; R. Dyer","University of Nebraska–Lincoln, Lincoln, NE, USA; University of Nebraska–Lincoln, Lincoln, NE, USA; University of Nebraska–Lincoln, Lincoln, NE, USA","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","85","96","Many software engineering research papers rely on time-based data (e.g., commit timestamps, issue report creation/update/close dates, release dates). Like most real-world data however, time-based data is often dirty. To date, there are no studies that quantify how frequently such data is used by the software engineering research community, or investigate sources of and quantify how often such data is dirty. Depending on the research task and method used, including such dirty data could affect the research results. This paper presents the first survey of papers that utilize time-based data, published in the Mining Software Repositories (MSR) conference series. Out of the 690 technical track and data papers published in MSR 2004–2020, we saw at least 35% of papers utilized time-based data. We then used the Boa and Software Heritage infrastructures to help identify and quantify several sources of dirty commit timestamp data. Finally we provide guidelines/best practices for researchers utilizing time-based data from Git repositories.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463113","time-based;survey","Training;Out of order;Filtering;Machine learning;Tools;Software;Data mining","","8","","39","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Are Bullies More Productive? Empirical Study of Affectiveness vs. Issue Fixing Time","M. Ortu; B. Adams; G. Destefanis; P. Tourani; M. Marchesi; R. Tonelli","DIEE, University of Cagliari, Italy; École Polytechnique de Montréal, Canada; CRIM, Computer Research Institute, Montreal, Canada; École Polytechnique de Montréal, Canada; DIEE, University of Cagliari, Italy; DIEE, University of Cagliari, Italy","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","303","313","Human Affectiveness, i.e., The emotional state of a person, plays a crucial role in many domains where it can make or break a team's ability to produce successful products. Software development is a collaborative activity as well, yet there is little information on how affectiveness impacts software productivity. As a first measure of this impact, this paper analyzes the relation between sentiment, emotions and politeness of developers in more than 560K Jira comments with the time to fix a Jira issue. We found that the happier developers are (expressing emotions such as JOY and LOVE in their comments), the shorter the issue fixing time is likely to be. In contrast, negative emotions such as SADNESS, are linked with longer issue fixing time. Politeness plays a more complex role and we empirically analyze its impact on developers' productivity.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180089","","Measurement;Software;Logistics;Training;Software engineering;Information services;Electronic publishing","","104","1","34","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Euphony: Harmonious Unification of Cacophonous Anti-Virus Vendor Labels for Android Malware","M. Hurier; G. Suarez-Tangil; S. K. Dash; T. F. Bissyandé; Y. Le Traon; J. Klein; L. Cavallaro","University of Luxembourg, Luxembourg; Royal Holloway, University of London, United Kingdom; University College London, United Kingdom; University of Luxembourg, Luxembourg; Royal Holloway, University of London, United Kingdom; University of Luxembourg, Luxembourg; Royal Holloway, University of London, United Kingdom","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","425","435","Android malware is now pervasive and evolving rapidly. Thousands of malware samples are discovered every day with new models of attacks. The growth of these threats has come hand in hand with the proliferation of collective repositories sharing the latest specimens. Having access to a large number of samples opens new research directions aiming at efficiently vetting apps. However, automatically inferring a reference ground-truth from those repositories is not straightforward and can inadvertently lead to unforeseen misconceptions. On the one hand, samples are often mis-labeled as different parties use distinct naming schemes for the same sample. On the other hand, samples are frequently mis-classified due to conceptual errors made during labeling processes. In this paper, we analyze the associations between all labels given by different vendors and we propose a system called EUPHONY to systematically unify common samples into family groups. The key novelty of our approach is that no a-priori knowledge on malware families is needed. We evaluate our approach using reference datasets and more than 0.4 million additional samples outside of these datasets. Results show that EUPHONY provides competitive performance against the state-of-the-art.","","978-1-5386-1544-7","10.1109/MSR.2017.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962391","malware;android;ground-truth;datasets;labeling","Androids;Humanoid robots;Labeling;Engines;Trojan horses;Electronic mail","","49","","48","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience","M. M. Rahman; C. K. Roy; R. G. Kula","University of Saskatchewan, Canada; University of Saskatchewan, Canada; Osaka University, Japan","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","215","226","Although peer code review is widely adopted in both commercial and open source development, existing studies suggest that such code reviews often contain a significant amount of non-useful review comments. Unfortunately, to date, no tools or techniques exist that can provide automatic support in improving those non-useful comments. In this paper, we first report a comparative study between useful and non-useful review comments where we contrast between them using their textual characteristics, and reviewers' experience. Then, based on the findings from the study, we develop RevHelper, a prediction model that can help the developers improve their code review comments through automatic prediction of their usefulnessduring review submission. Comparative study using 1,116 review comments suggested that useful comments share more vocabulary with the changed code, contain salient items like relevant code elements, and their reviewers are generally more experienced. Experiments using 1,482 review comments report that our model can predict comment usefulness with 66% prediction accuracy which is promising. Comparison with three variants of a baseline model using a case study validates our empirical findings and demonstrates the potential of our model.","","978-1-5386-1544-7","10.1109/MSR.2017.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962371","Code review quality;review comment usefulness;change triggering capability;code element;reviewing experience","Predictive models;Companies;Software;Tools;Libraries;Vocabulary;Inspection","","38","","41","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Practitioners’ Perceptions of the Goals and Visual Explanations of Defect Prediction Models","J. Jiarpakdee; C. K. Tantithamthavorn; J. Grundy","Monash University, Melbourne, Australia; Monash University, Melbourne, Australia; Monash University, Melbourne, Australia","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","432","443","Software defect prediction models are classifiers that are constructed from historical software data. Such software defect prediction models have been proposed to help developers optimize the limited Software Quality Assurance (SQA) resources and help managers develop SQA plans. Prior studies have different goals for their defect prediction models and use different techniques for generating visual explanations of their models. Yet, it is unclear what are the practitioners' perceptions of (1) these defect prediction model goals, and (2) the model-agnostic techniques used to visualize these models. We conducted a qualitative survey to investigate practitioners' perceptions of the goals of defect prediction models and the model-agnostic techniques used to generate visual explanations of defect prediction models. We found that (1) 82%-84% of the respondents perceived that the three goals of defect prediction models are useful; (2) LIME is the most preferred technique for understanding the most important characteristics that contributed to a prediction of a file, while ANOVA/VarImp is the second most preferred technique for understanding the characteristics that are associated with software defects in the past. Our findings highlight the significance of investigating how to improve the understanding of defect prediction models and their predictions. Hence, model-agnostic techniques from explainable AI domain may help practitioners to understand defect prediction models and their predictions.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463073","Software Quality Assurance;Defect Prediction;Explainable AI;Software Analytics","Visualization;Analytical models;Privacy;Software quality;Predictive models;Data models;Data mining","","36","","54","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Detecting and Characterizing Bots that Commit Code","T. Dey; S. Mousavi; E. Ponce; T. Fry; B. Vasilescu; A. Filippova; A. Mockus","The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Github, San Francisco, CA, USA; The University of Tennessee, Knoxville, TN, USA","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","209","219","Background: Some developer activity traditionally performed manually, such as making code commits, opening, managing, or closing issues is increasingly subject to automation in many OSS projects. Specifically, such activity is often performed by tools that react to events or run at specific times. We refer to such automation tools as bots and, in many software mining scenarios related to developer productivity or code quality, it is desirable to identify bots in order to separate their actions from actions of individuals. Aim: Find an automated way of identifying bots and code committed by these bots, and to characterize the types of bots based on their activity patterns. Method and Result: We propose BIMAN, a systematic approach to detect bots using author names, commit messages, files modified by the commit, and projects associated with the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized these bots based on the time patterns of their code commits and the types of files modified, and found that they primarily work with documentation files and web pages, and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled a shareable dataset containing detailed information about 461 bots we found (all of which have more than 1000 commits) and 13,762,430 commits they created.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387478","NSF(grant numbers:CNS-1925615,IIS-1633437,IIS-1901102,1717415,1901311); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148716","bots;automated commits;random forest;ensemble model;social coding platforms;software engineering","Productivity;Codes;Automation;Systematics;Social networking (online);Web pages;Chatbots","","23","","49","","20 Jun 2023","","","IEEE","IEEE Conferences"
"Can I Solve It? Identifying APIs Required to Complete OSS Tasks","F. Santos; I. Wiese; B. Trinkenreich; I. Steinmacher; A. Sarma; M. A. Gerosa","Northern Arizona University, USA; Universidade Tecnológica Federal do Paraná, Brazil; Northern Arizona University, USA; Northern Arizona University, USA; Oregon State University, USA; Northern Arizona University, USA","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","346","257","Open Source Software projects add labels to open issues to help contributors choose tasks. However, manually labeling issues is time-consuming and error-prone. Current automatic approaches for creating labels are mostly limited to classifying issues as a bug/non-bug. In this paper, we investigate the feasibility and relevance of labeling issues with the domain of the APIs required to complete the tasks. We leverage the issues’ description and the project history to build prediction models, which resulted in precision up to 82% and recall up to 97.8%. We also ran a user study (n=74) to assess these labels’ relevancy to potential contributors. The results show that the labels were useful to participants in choosing tasks, and the API-domain labels were selected more often than the existing architecture-based labels. Our results can inspire the creation of tools to automatically label issues, helping developers to find tasks that better match their skills.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00047","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463078","API identification;Labelling;Tagging;Skills;Multi-Label Classification;Mining Software Repositories;Case Study","Industries;Tools;Predictive models;Prediction algorithms;Data models;Labeling;History","","8","","63","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"500+ Times Faster than Deep Learning: (A Case Study Exploring Faster Methods for Text Mining StackOverflow)","T. Menzies; S. Majumder; N. Balaji; K. Brey; W. Fu","Computer Science, NC State, USA; Computer Science, NC State, USA; Computer Science, NC State, USA; Computer Science, NC State, USA; Computer Science, NC State, USA","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","554","563","Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train.This paper extends that recent result by clustering the dataset, then tuning every learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources. More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives(e.g applying simpler learners to build local models).","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595239","Deep learning;parameter tuning;DE;KNN;local versus global;K-Means;SVM;CNN","Deep learning;Computational modeling;Support vector machines;Data models;Training;Tuning;Data mining","","5","","55","","30 Dec 2018","","","IEEE","IEEE Conferences"
"On the Violation of Honesty in Mobile Apps: Automated Detection and Categories","H. O. Obie; I. Ilekura; H. Du; M. Shahin; J. Grundy; L. Li; J. Whittle; B. Turhan","HumaniSE Lab, Monash University, Melbourne, Australia; Data Science Nigeria, Lagos, Nigeria; Applied Artificial Intelligence Inst., Deakin University, Melbourne, Australia; School of Computing Technologies, RMIT University, Melbourne, Australia; HumaniSE Lab, Monash University, Melbourne, Australia; Faculty of IT, Monash University, Melbourne, Australia; CSIRO's Data61, Melbourne, Australia; University of Oulu, Oulu, Finland","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","321","332","Human values such as integrity, privacy, curiosity, security, and honesty are guiding principles for what people consider important in life. Such human values may be violated by mobile software applications (apps), and the negative effects of such human value violations can be seen in various ways in society. In this work, we focus on the human value of honesty. We present a model to support the automatic identification of violations of the value of honesty from app reviews from an end-user perspective. Beyond the automatic detection of honesty violations by apps, we also aim to better understand different categories of honesty violations expressed by users in their app reviews. The result of our manual analysis of our honesty violations dataset shows that honesty violations can be characterised into ten categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. Based on these results, we argue for a conscious effort in developing more honest software artefacts including mobile apps, and the promotion of honesty as a key value in software development practices. Furthermore, we discuss the role of app distribution platforms as enforcers of ethical systems supporting human values, and highlight some proposed next steps for human values in software engineering (SE) research.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3527937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796360","human values;mobile apps;app reviews;Android;Automatic detection;taxonomy;honesty","Support vector machines;Privacy;Ethics;Manuals;Software;Mobile applications;Classification algorithms","","4","","74","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study","T. C. Vélez; R. Khatchadourian; M. Bagherzadeh; A. Raja","City University of New York (CUNY) Graduate Center, New York, NY, USA; City University of New York (CUNY) Hunter College, New York, NY, USA; Oakland University, Rochester, MI, USA; City University of New York (CUNY) Hunter College, New York, NY, USA","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","469","481","Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the “best of both worlds,” the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges-and resultant bugs-involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation-the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528455","City University of New York; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796227","empirical studies;deep learning;imperative programs;hybrid programming paradigms;graph-based execution;software evolution","Deep learning;Codes;Neural networks;Computer bugs;Writing;Reliability;Data mining","","2","","103","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Beyond Duplicates: Towards Understanding and Predicting Link Types in Issue Tracking Systems","C. M. Lüders; A. Bouraffa; W. Maalej","University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","48","60","Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, or Subtask. While previous research has mostly focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. For this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal/Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal/ Causal links. Motivated by the differences between the link types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on the JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528457","European Union(grant numbers:732463); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796252","Issue Management;Issue Tracking System;Duplicate Detection;Link Type Detection;Dependency Management","Training;Analytical models;Uncertainty;Semantics;Training data;Organizations;Predictive models","","2","","42","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Pre-trained Model Based Feature Envy Detection","W. Ma; Y. Yu; X. Ruan; B. Cai","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","430","440","Code smells slow down software system development and makes them harder to maintain. Existing research aims to develop automatic detection algorithms to reduce the labor and time costs within the detection process. Deep learning techniques have recently been demonstrated to enhance the performance of recognizing code smells even more than metric-based heuristic detection algorithms. As large-scale pre-trained models for Programming Languages (PL), such as CodeT5, have lately achieved the top results in a variety of downstream tasks, some researchers begin to explore the use of pre-trained models to extract the contextual semantics of code to detect code smells. However, little research has employed contextual code semantics relationship between code snippets obtained by pre-trained models to identify code smells. In this paper, we investigate the use of the pre-trained model CodeT5 to extract semantic relationships between code snippets to detect feature envy, which is one of the most common code smells. In addition, to investigate the performance of these semantic relationships extracted by pre-trained models of different architectures on detecting feature envy, we compare CodeT5 with two other pre-trained models CodeBERT and CodeGPT. We have performed our experimental evaluation on ten open-source projects, our approach improves F-measure by 29.32% on feature envy detection and 16.57% on moving destination recommendation. Using semantic relations extracted by several pre-trained models to detect feature envy outperforms the state-of-the-art. This shows that using this semantic relation to detect feature envy is promising. To enable future research on feature envy detection, we have made all the code and datasets utilized in this article open source.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174241","Feature Envy;Deep Learning;Software Refactoring;Pre-trained Model;Code Smell","Codes;Source coding;Semantics;Computer architecture;Predictive models;Feature extraction;Software systems","","1","","42","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Data-Driven Search-Based Software Engineering","V. Nair; A. Agrawal; J. Chen; W. Fu; G. Mathew; T. Menzies; L. Minku; M. Wagner; Z. Yu","North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; University of Leicester, UK; The University of Adelaide, Australia; North Carolina State University, USA","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","341","352","This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulates Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a)~which require learning from a large data source or (b)~when optimizers need to know the lay of the land to find better solutions, faster. This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource. This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595218","Software Analytics;SBSE","Software engineering;Data mining;Software algorithms;Optimization;Software;Prediction algorithms;Predictive models","","","","111","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Cold-Start Software Analytics","J. Guo; M. Rahimi; J. Cleland-Huang; A. Rasin; J. H. Hayes; M. Vierhauser","School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; Computer Science Department, University of Kentucky, USA; CDL MEVSS, Johannes Kepler University, Linz, Austria","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","26 Jan 2017","2016","","","142","153","Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as ""who is the expert?,'' ""which classes are fault prone?,'' or even ""who are the domain experts for these fault-prone classes?'' Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.","","978-1-4503-4186-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832895","Cold-start;Software Analytics;Configuration","Software;Measurement;Predictive models;Analytical models;Context;Training;Software engineering","","","","51","","26 Jan 2017","","","IEEE","IEEE Conferences"
"Import2vec: Learning Embeddings for Software Libraries","B. Theeten; F. Vandeputte; T. Van Cutsem","Nokia Bell Labs, Antwerp, Belgium; Nokia Bell Labs, Antwerp, Belgium; Nokia Bell Labs, Antwerp, Belgium","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","18","28","We consider the problem of developing suitable learning representations (embeddings) for library packages that capture semantic similarity among libraries. Such representations are known to improve the performance of downstream learning tasks (e.g. classification) or applications such as contextual search and analogical reasoning. We apply word embedding techniques from natural language processing (NLP) to train embeddings for library packages (""library vectors""). Library vectors represent libraries by similar context of use as determined by import statements present in source code. Experimental results obtained from training such embeddings on three large open source software corpora reveals that library vectors capture semantically meaningful relationships among software libraries, such as the relationship between frameworks and their plug-ins and libraries commonly used together within ecosystems such as big data infrastructure projects (in Java), front-end and back-end web development frameworks (in JavaScript) and data science toolkits (in Python).","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816754","machine learning;software engineering;information retrieval","Libraries;Semantics;Ecosystems;Natural language processing;Java;Python","","20","","25","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Can We Use SE-specific Sentiment Analysis Tools in a Cross-Platform Setting?","N. Novielli; F. Calefato; D. Dongiovanni; D. Girardi; F. Lanubile","University of Bari, Italy; University of Bari, Italy; University of Bari, Italy; University of Bari, Italy; University of Bari, Italy","2020 IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)","20 Jun 2023","2020","","","158","168","In this paper, we address the problem of using sentiment analysis tools ‘off-the-shelf’, that is when a gold standard is not available for retraining. We evaluate the performance of four SE-specific tools in a cross-platform setting, i.e., on a test set collected from data sources different from the one used for training. We find that (i) the lexicon-based tools outperform the supervised approaches retrained in a cross-platform setting and (ii) retraining can be beneficial in within-platform settings in the presence of robust gold standard datasets, even using a minimal training set. Based on our empirical findings, we derive guidelines for reliable use of sentiment analysis tools in software engineering.","2574-3864","978-1-4503-7517-7","10.1145/3379597.3387446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148751","Sentiment analysis;empirical software engineering;human fac-tors;NLP;machine learning","Training;Deep learning;Sentiment analysis;Soft sensors;Reliability engineering;Software reliability;Data mining","","13","","50","","20 Jun 2023","","","IEEE","IEEE Conferences"
"On Improving Deep Learning Trace Analysis with System Call Arguments","Q. Fournier; D. Aloise; S. V. Azhari; F. Tetreault","Polytechnique Montréal, Quebec; Polytechnique Montréal, Quebec; Ciena, Ottawa; Ciena, Ottawa","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","120","130","Kernel traces are sequences of low-level events comprising a name and multiple arguments, including a timestamp, a process id, and a return value, depending on the event. Their analysis helps uncover intrusions, identify bugs, and find latency causes. However, their effectiveness is hindered by omitting the event arguments. To remedy this limitation, we introduce a general approach to learning a representation of the event names along with their arguments using both embedding and encoding. The proposed method is readily applicable to most neural networks and is task-agnostic. The benefit is quantified by conducting an ablation study on three groups of arguments: call-related, process-related, and time-related. Experiments were conducted on a novel web request dataset and validated on a second dataset collected on pre-production servers by Ciena, our partnering company. By leveraging additional information, we were able to increase the performance of two widely-used neural networks, an LSTM and a Transformer, by up to 11.3% on two unsupervised language modelling tasks. Such tasks may be used to detect anomalies, pre-train neural networks to improve their performance, and extract a contextual representation of the events.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463093","Tracing;Machine Learning;Deep Learning","Deep learning;Neural networks;Computer bugs;Companies;Software;Encoding;Servers","","5","","31","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Noisy Label Learning for Security Defects","R. Croft; M. A. Babar; H. Chen","CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","435","447","Data-driven software engineering processes, such as vulnerability prediction heavily rely on the quality of the data used. In this paper, we observe that it is infeasible to obtain a noise-free security defect dataset in practice. Despite the vulnerable class, the non-vulnerable modules are difficult to be verified and determined as truly exploit free given the limited manual efforts available. It results in uncertainty, introduces labeling noise in the datasets and affects conclusion validity. To address this issue, we propose novel learning methods that are robust to label impurities and can leverage the most from limited label data; noisy label learning. We investigate various noisy label learning methods applied to soft-ware vulnerability prediction. Specifically, we propose a two-stage learning method based on noise cleaning to identify and remediate the noisy samples, which improves AUC and recall of baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several hurdles in terms of achieving a performance upper bound with semi-omniscient knowledge of the label noise. Overall, the experimental results show that learning from noisy labels can be effective for data-driven software and security analytics.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796240","machine learning;noisy label learning;software vulnerabilities","Learning systems;Upper bound;Uncertainty;Manuals;Predictive models;Software;Noise measurement","","4","","76","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Extracting Facts from Performance Tuning History of Scientific Applications for Predicting Effective Optimization Patterns","M. Hashimoto; M. Terai; T. Maeda; K. Minami","RIKEN Advanced Institute for Computational Science, Kobe, Hyogo, Japan; RIKEN Advanced Institute for Computational Science, Kobe, Hyogo, Japan; RIKEN Advanced Institute for Computational Science, Kobe, Hyogo, Japan; RIKEN Advanced Institute for Computational Science, Kobe, Hyogo, Japan","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","6 Aug 2015","2015","","","13","23","To improve performance of large-scale scientific applications, scientists or tuning experts make various empirical attempts to change compiler options, program parameters or even the syntactic structure of programs. Those attempts followed by performance evaluation are repeated until satisfactory results are obtained. The task of performance tuning requires a great deal of time and effort. On account of combinatorial explosion of possible attempts, scientists/tuning experts have a tendency to make decisions on what to be explored just based on their intuition or good sense of tuning. We advocate evidence-based performance tuning (EBT) that facilitates the use of database of facts extracted from tuning histories of applications to guide the exploration of the search space. However, in general, performance tuning is conducted as transient tasks without version control systems. Tuning histories may lack explicit facts about what kind of program transformation contributed to the better performance or even about the chronological order of the source code snapshots. For reconstructing the missing information, we employ a state-of-the-art fine-grained change pattern identification tool for inferring applied transformation patterns only from an unordered set of source code snapshots. The extracted facts are intended to be stored and queried for further data mining. This paper reports on experiments of tuning pattern identification followed by predictive model construction conducted for a few scientific applications tuned for the K supercomputer.","2160-1860","978-0-7695-5594-2","10.1109/MSR.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180063","large-scale scientific computing;application performance tuning;abstract syntax tree differencing;semantic web;machine learning","Tuning;Arrays;Kernel;Ontologies;Phylogeny;History;Data mining","","4","","29","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Analyzing Requirements and Traceability Information to Improve Bug Localization","M. Rath; D. Lo; P. Mäder","Technische Universitat Ilmenau, Ilmenau, Germany; Singapore Management University, Singapore; Technische Universitat Ilmenau, Ilmenau, Germany","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","30 Dec 2018","2018","","","442","453","Locating bugs in industry-size software systems is time consuming and challenging. An automated approach for assisting the process of tracing from bug descriptions to relevant source code benefits developers. A large body of previous work aims to address this problem and demonstrates considerable achievements. Most existing approaches focus on the key challenge of improving techniques based on textual similarity to identify relevant files. However, there exists a lexical gap between the natural language used to formulate bug reports and the formal source code and its comments. To bridge this gap, state-of-the-art approaches contain a component for analyzing bug history information to increase retrieval performance. In this paper, we propose a novel approach TraceScore that also utilizes projects' requirements information and explicit dependency trace links to further close the gap in order to relate a new bug report to defective source code files. Our evaluation on more than 13,000 bug reports shows, that TraceScore significantly outperforms two state-of-the-art methods. Further, by integrating TraceScore into an existing bug localization algorithm, we found that TraceScore significantly improves retrieval performance by 49% in terms of mean average precision (MAP).","2574-3864","978-1-4503-5716-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595228","Requirements Traceability;Bug Localization;Software Maintenance;Traceability Recovery;Version History;Machine Learning","Computer bugs;History;Software systems;Data mining;Natural languages;Bridges","","3","","61","","30 Dec 2018","","","IEEE","IEEE Conferences"
"Leveraging Models to Reduce Test Cases in Software Repositories","G. Gharachorlu; N. Sumner","Simon Fraser University, Canada; Simon Fraser University, Canada","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","230","241","Given a failing test case, test case reduction yields a smaller test case that reproduces the failure. This process can be time consuming due to repeated trial and error with smaller test cases. Current techniques speed up reduction by only exploring syntactically valid candidates, but they still spend significant effort on semantically invalid candidates. In this paper, we propose a model-guided approach to speed up test case reduction. The approach trains a model of semantic properties driven by syntactic test case properties. By using this model, we can skip testing even syntactically valid test case candidates that are unlikely to succeed. We evaluate this model-guided reduction on a suite of 14 large fuzzer-generated C test cases from the bug repositories of two well-known C compilers, GCC and Clang. Our results show that with an average precision of 77%, we can decrease the number of removal trials by 14% to 61%. We observe a 30% geomean improvement in reduction time over the state of the art technique while preserving similar reduction power.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463100","test case reduction;semantic validity;machine learning;compilation errors","Computer bugs;Semantics;Syntactics;Predictive models;Feature extraction;Software;Data mining","","2","","29","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Comparative Study of Feature Reduction Techniques in Software Change Prediction","R. Malhotra; R. Kapoor; D. Aggarwal; P. Garg","Department of Software Engineering, Delhi Technological University, Delhi, India; Department of Software Engineering, Delhi Technological University, Delhi, India; Department of Software Engineering, Delhi Technological University, Delhi, India; Department of Software Engineering, Delhi Technological University, Delhi, India","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)","28 Jun 2021","2021","","","18","28","Software change prediction (SCP) is the process of identifying change-prone software classes using various structural and quality metrics by developing predictive techniques. The previous studies done in this field strongly confer the correlation between the quality of metrics and the performance of such SCP models. Past SCP studies have also applied different feature reduction (FR) techniques to address issues of high dimensionality, feature irrelevance, and feature repetition. Due to the vast variety of metric suites and FR techniques applied in SCP, there is a need to analyze and compare them. It will help in identifying the most crucial features and the most effective FR techniques. So, in this research, we conduct experiments to compare and contrast 60 Object-Oriented plus 26 Graph-based metrics and 11 state-of-the-art FR techniques previously employed for SCP over a range of 6 Java projects and 3 diverse classifiers. The AUC-ROC measures and statistical tests over experimental SCP models indicate that FR techniques are effective in SCP. Also, there exist significant differences in the performance of the different FR techniques. Furthermore, from this extensive experimentation, we were able to identify a set of the most effective FR techniques and the most crucial metrics which can be used to build effective SCP models.","2574-3864","978-1-7281-8710-5","10.1109/MSR52588.2021.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463086","Software change prediction;Feature reduction;Object-Oriented metrics;Graph-based metrics;Machine Learning","Measurement;Java;Correlation;Object oriented modeling;Software;Data mining","","2","","42","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Evaluating the effectiveness of local explanation methods on source code-based defect prediction models","Y. Gao; Y. Zhu; Q. Yu","School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China; School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China; School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","640","645","Interpretation has been considered as one of key factors for applying defect prediction in practice. As one way for interpretation, local explanation methods has been widely used for certain predictions on datasets of traditional features. There are also attempts to use local explanation methods on source code-based defect prediction models, but unfortunately, it will get poor results. Since it is unclear how effective those local explanation methods are, we evaluate such methods with automatic metrics which focus on local faithfulness and explanation precision. Based on the results of experiments, we find that the effectiveness of local explanation methods depends on the adopted defect prediction models. They are effective on token frequency-based models, while they may not be effective enough to explain all predictions of deep learning-based models. Besides, we also find that the hyperparameter of local explanation methods should be carefully optimized to get more precise and meaningful explanation.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3528472","National Natural Science Foundation of China(grant numbers:62077029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796250","Software Defect Prediction;Local Explanation;Explainable Machine Learning;LIME","Measurement;Codes;Predictive models;Software;Data mining","","1","","31","","21 Jun 2022","","","IEEE","IEEE Conferences"
"SynShine: Improved Fixing of Syntax Errors","T. Ahmed; N. R. Ledesma; P. Devanbu","Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, University of California, Davis, CA, USA","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","2169","2181","Novice programmers struggle with the complex syntax of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Novices could be helped, and instructors’ time saved, by automated repair suggestions when dealing with syntax errors. Large samples of novice errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help novices fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SynShine, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SynShine's architecture in detail, and provide a detailed evaluation. We have built SynShine into a free, open-source version of Visual Studio Code (VSCode); we make all our source code and models freely available.","1939-3520","","10.1109/TSE.2022.3212635","National Science Foundation(grant numbers:1414172,2107592); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913705","Deep learning;program repair;naturalness","Syntactics;Maintenance engineering;Program processors;Codes;Java;Transformers;Data models","","3","","51","IEEE","10 Oct 2022","","","IEEE","IEEE Journals"
"Detecting, Tracing, and Monitoring Architectural Tactics in Code","M. Mirakhorli; J. Cleland-Huang","Department of Software Engineering, Rochester Institute of Technology, Rochester, NY; School of Computing, DePaul University, Chicago, IL","IEEE Transactions on Software Engineering","11 Mar 2016","2016","42","3","205","220","Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.","1939-3520","","10.1109/TSE.2015.2479217","US National Science Foundation(grant numbers:CCF-0810924); Research Experience for Undergraduates(grant numbers:CCF 1341072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270338","Architecture;traceability;tactics;traceability information models;Architecture;traceability;tactics;traceability information models","Heart beat;Monitoring;Detectors;Reliability;Biomedical monitoring;Authentication","","58","","73","IEEE","16 Sep 2015","","","IEEE","IEEE Journals"
"Predicting Consistency-Maintenance Requirement of Code Clonesat Copy-and-Paste Time","X. Wang; Y. Dang; L. Zhang; D. Zhang; E. Lan; H. Mei","Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Corporation, One Microsoft Way, Redmond, WA; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio","IEEE Transactions on Software Engineering","8 Aug 2014","2014","40","8","773","794","Code clones have always been a double edged sword in software development. On one hand, it is a very convenient way to reuse existing code, and to save coding effort. On the other hand, since developers may need to ensure consistency among cloned code segments, code clones can lead to extra maintenance effort and even bugs. Recently studies on the evolution of code clones show that only some of the code clones experience consistent changes during their evolution history. Therefore, if we can accurately predict whether a code clone will experience consistent changes, we will be able to provide useful recommendations to developers onleveraging the convenience of some code cloning operations, while avoiding other code cloning operations to reduce future consistency maintenance effort. In this paper, we define a code cloning operation as consistency-maintenance-required if its generated code clones experience consistent changes in the software evolution history, and we propose a novel approach that automatically predicts whether a code cloning operation requires consistency maintenance at the time point of performing copy-and-paste operations. Our insight is that whether a code cloning operation requires consistency maintenance may relate to the characteristics of the code to be cloned and the characteristics of its context. Based on a number of attributes extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict whether an intended code cloning operation requires consistency maintenance. We evaluated our approach on four subjects-two large-scale Microsoft software projects, and two popular open-source software projects-under two usage scenarios: 1) recommend developers to perform only the cloning operations predicted to be very likely to be consistency-maintenance-free, and 2) recommend developers to perform all cloning operations unless they are predicted very likely to be consistency-maintenance-required. In the first scenario, our approach is able to recommend developers to perform more than 50 percent cloning operations with a precision of at least 94 percent in the four subjects. In the second scenario, our approach is able to avoid 37 to 72 percent consistency-maintenance-required code clones by warning developers on only 13 to 40 percent code clones, in the four subjects.","1939-3520","","10.1109/TSE.2014.2323972","National 863 Program(grant numbers:2013AA01A605); National 973 Program(grant numbers:2011CB302604); Science Fund for Creative Research Groups(grant numbers:61121063); Natural Science Foundation(grant numbers:91118004,61228203,61225007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815760","Code cloning;consistency maintenance;programming aid","Cloning;Software;Maintenance engineering;Bayes methods;History;Training;Educational institutions","","21","","46","IEEE","14 May 2014","","","IEEE","IEEE Journals"
"AI-Enabled Automation for Completeness Checking of Privacy Policies","O. Amaral; S. Abualhaija; D. Torre; M. Sabetzadeh; L. C. Briand","SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Esch-sur-Alzette, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Esch-sur-Alzette, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Esch-sur-Alzette, Luxembourg, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Esch-sur-Alzette, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Esch-sur-Alzette, Luxembourg","IEEE Transactions on Software Engineering","11 Nov 2022","2022","48","11","4647","4674","Technological advances in information sharing have raised concerns about data protection. Privacy policies contain privacy-related requirements about how the personal data of individuals will be handled by an organization or a software system (e.g., a web service or an app). In Europe, privacy policies are subject to compliance with the General Data Protection Regulation (GDPR). A prerequisite for GDPR compliance checking is to verify whether the content of a privacy policy is complete according to the provisions of GDPR. Incomplete privacy policies might result in large fines on violating organization as well as incomplete privacy-related software specifications. Manual completeness checking is both time-consuming and error-prone. In this paper, we propose AI-based automation for the completeness checking of privacy policies. Through systematic qualitative methods, we first build two artifacts to characterize the privacy-related provisions of GDPR, namely a conceptual model and a set of completeness criteria. Then, we develop an automated solution on top of these artifacts by leveraging a combination of natural language processing and supervised machine learning. Specifically, we identify the GDPR-relevant information content in privacy policies and subsequently check them against the completeness criteria. To evaluate our approach, we collected 234 real privacy policies from the fund industry. Over a set of 48 unseen privacy policies, our approach detected 300 of the total of 334 violations of some completeness criteria correctly, while producing 23 false positives. The approach thus has a precision of 92.9% and recall of 89.8%. Compared to a baseline that applies keyword search only, our approach results in an improvement of 24.5% in precision and 38% in recall.","1939-3520","","10.1109/TSE.2021.3124332","Linklaters, Luxembourg's National Research Fund(grant numbers:BRIDGES/19/IS/13759068/ARTAGO); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599471","Requirements engineering;legal compliance;privacy policies;the general data protection regulation (GDPR);artificial intelligence (AI);conceptual modeling;qualitative research","Privacy;Metadata;Law;General Data Protection Regulation;Software;Organizations;Europe","","9","","75","IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"Enhancement of Mutation Testing via Fuzzy Clustering and Multi-Population Genetic Algorithm","X. Dang; D. Gong; X. Yao; T. Tian; H. Liu","School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China; School of Mathematics, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, Shandong Jianzhu University, Jinan, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia","IEEE Transactions on Software Engineering","14 Jun 2022","2022","48","6","2141","2156","Mutation testing, a fundamental software testing technique, which is a typical way to evaluate the adequacy of a test suite. In mutation testing, a set of mutants are generated by seeding the different classes of faults into a program under test. Test data shall be generated in the way that as many mutants can be killed as possible. Thanks to numerous tools to implement mutation testing for different languages, a huge amount of mutants are normally generated even for small-sized programs. However, a large number of mutants not only leads to a high cost of mutation testing, but also make the corresponding test data generation a non-trivial task. In this paper, we make use of intelligent technologies to improve the effectiveness and efficiency of mutation testing from two perspectives. A machine learning technique, namely fuzzy clustering, is applied to categorize mutants into different clusters. Then, a multi-population genetic algorithm via individual sharing is employed to generate test data for killing the mutants in different clusters in parallel when the problem of test data generation as an optimization one. A comprehensive framework, termed as $\mathbf {FUZGENMUT}$FUZGENMUT, is thus developed to implement the proposed techniques. The experiments based on nine programs of various sizes show that fuzzy clustering can help to reduce the cost of mutation testing effectively, and that the multi-population genetic algorithm improves the efficiency of test data generation while delivering the high mutant-killing capability. The results clearly indicate that the huge potential of using intelligent technologies to enhance the efficacy and thus the practicality of mutation testing.","1939-3520","","10.1109/TSE.2021.3052987","National Key Research and Development Program of China(grant numbers:2018YFB1003802-01); National Natural Science Foundation of China(grant numbers:61773384,61573362,61503220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328619","Mutation testing;fuzzy clustering;mutation clustering;test data generation;multi-population genetic algorithm (MGA)","Testing;Genetic algorithms;Sorting;Clustering algorithms;Syntactics;Statistics;Sociology","","9","","60","IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"An Ensemble Approach for Annotating Source Code Identifiers With Part-of-Speech Tags","C. D. Newman; M. J. Decker; R. S. Alsuhaibani; A. Peruma; M. W. Mkaouer; S. Mohapatra; T. Vishnoi; M. Zampieri; T. J. Sheldon; E. Hill","Software Engineering Deptartment, Rochester Institute of Techonology, Rochester, NY, USA; Software Engineering Department, Bowling Green State University, Bowling Green, OH, USA; Computer Science Department, Prince Sultan University, Riyadh, Saudi Arabia; Software Engineering Deptartment, Rochester Institute of Techonology, Rochester, NY, USA; Software Engineering Deptartment, Rochester Institute of Techonology, Rochester, NY, USA; Software Engineering Deptartment, Rochester Institute of Techonology, Rochester, NY, USA; Software Engineering Deptartment, Rochester Institute of Techonology, Rochester, NY, USA; Language Technology Group, Rochester Institute of Techonology, Rochester, NY, USA; Financial Services Sector, Risk and Compliance, BNY Mellon, Pittburgh, PA, USA; Department of Math and Computer Science, Drew University, Madison, NJ, USA","IEEE Transactions on Software Engineering","16 Sep 2022","2022","48","9","3506","3522","This paper presents an ensemble part-of-speech tagging approach for source code identifiers. Ensemble tagging is a technique that uses machine-learning and the output from multiple part-of-speech taggers to annotate natural language text at a higher quality than the part-of-speech taggers are able to obtain independently. Our ensemble uses three state-of-the-art part-of-speech taggers: SWUM, POSSE, and Stanford. We study the quality of the ensemble’s annotations on five different types of identifier names: function, class, attribute, parameter, and declaration statement at the level of both individual words and full identifier names. We also study and discuss the weaknesses of our tagger to promote the future amelioration of these problems through further research. Our results show that the ensemble achieves 75 percent accuracy at the identifier level and 84-86 percent accuracy at the word level. This is an increase of +17% points at the identifier level from the closest independent part-of-speech tagger.","1939-3520","","10.1109/TSE.2021.3098242","National Science Foundation(grant numbers:1850412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491989","Program comprehension;software maintenance;natural language processing;part-of-speech tagging","Grammar;Tagging;Natural languages;Tools;Annotations;Software engineering;Semantics","","6","","57","IEEE","20 Jul 2021","","","IEEE","IEEE Journals"
"DeepMerge: Learning to Merge Programs","E. Dinella; T. Mytkowicz; A. Svyatkovskiy; C. Bird; M. Naik; S. Lahiri","CIS, University of Pennsylvania, Philadelphia, PA, USA; Research in Software Engineering, Microsoft Research, Redmond, WA, USA; DevDiv, Microsoft Corp, Redmond, WA, USA; Microsoft Research, Microsoft Corp, Redmond, WA, USA; CIS, University of Pennsylvania, Philadelphia, PA, USA; Research in Software Engineering, Microsoft Research, Redmond, WA, USA","IEEE Transactions on Software Engineering","18 Apr 2023","2023","49","4","1599","1614","In collaborative software development, program merging is the mechanism to integrate changes from multiple programmers. Merge algorithms in modern version control systems report a conflict when changes interfere textually. Merge conflicts require manual intervention and frequently stall modern continuous integration pipelines. Prior work found that, although costly, a large majority of resolutions involve re-arranging text without writing any new code. Inspired by this observation we propose the first data-driven approach to resolve merge conflicts with a machine learning model. We realize our approach in a tool DeepMerge that uses a novel combination of (i) an edit-aware embedding of merge inputs and (ii) a variation of pointer networks, to construct resolutions from input segments. We also propose an algorithm to localize manual resolutions in a resolved file and employ it to curate a ground-truth dataset comprising 8,719 non-trivial resolutions in JavaScript programs. Our evaluation shows that, on a held out test set, DeepMerge can predict correct resolutions for 37% of non-trivial merges, compared to only 4% by a state-of-the-art semistructured merge technique. Furthermore, on the subset of merges with upto 3 lines (comprising 24% of the total dataset), DeepMerge can predict correct resolutions with 78% accuracy.","1939-3520","","10.1109/TSE.2022.3183955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9814963","Merge conflicts;conflict resolution;software maintenance;software tools","Reactive power;Codes;Merging;Control systems;Task analysis;Software development management;Java","","6","","48","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"A Procedure to Continuously Evaluate Predictive Performance of Just-In-Time Software Defect Prediction Models During Software Development","L. Song; L. L. Minku","Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen, Guangdong, China; School of Computer Science, University of Birmingham, Birmingham, U.K.","IEEE Transactions on Software Engineering","14 Feb 2023","2023","49","2","646","666","Just-In-Time Software Defect Prediction (JIT-SDP) uses machine learning to predict whether software changes are defect-inducing or clean. When adopting JIT-SDP, changes in the underlying defect generating process may significantly affect the predictive performance of JIT-SDP models over time. Therefore, being able to continuously track the predictive performance of JIT-SDP models during the software development process is of utmost importance for software companies to decide whether or not to trust the predictions provided by such models over time. However, there has been little discussion on how to continuously evaluate predictive performance in practice, and such evaluation is not straightforward. In particular, labeled software changes that can be used for evaluation arrive over time with a delay, which in part corresponds to the time we have to wait to label software changes as ‘clean’ (waiting time). A clean label assigned based on a given waiting time may not correspond to the true label of the software changes. This can potentially hinder the validity of any continuous predictive performance evaluation procedure for JIT-SDP models. This paper provides the first discussion of how to continuously evaluate predictive performance of JIT-SDP models over time during the software development process, and the first investigation of whether and to what extent waiting time affects the validity of such continuous performance evaluation procedure in JIT-SDP. Based on 13 GitHub projects, we found that waiting time had a significant impact on the validity. Though typically small, the differences in estimated predicted performance were sometimes large, and thus inappropriate choices of waiting time can lead to misleading estimations of predictive performance over time. Such impact did not normally change the ranking between JIT-SDP models, and thus conclusions in terms of which JIT-SDP model performs better are likely reliable independent of the choice of waiting time, especially when considered across projects.","1939-3520","","10.1109/TSE.2022.3158831","National Natural Science Foundation of China(grant numbers:62002148); Engineering and Physical Sciences Research Council(grant numbers:EP/R006660/2); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X386); Shenzhen Science and Technology Program(grant numbers:KQTD2016112514355531); Research Institute of Trustworthy Autonomous Systems; Southern University of Science and Technology(grant numbers:518055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735354","Just-in-time software defect prediction;performance evaluation procedure;concept drift;data stream learning;online learning;verification latency;and label noise","Software;Performance evaluation;Predictive models;Training;Estimation;Software reliability;Delays","","3","","39","IEEE","15 Mar 2022","","","IEEE","IEEE Journals"
"DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-Based Systems","L. Traini; V. Cortellessa","Department of Information Engineering, Computer Science and Mathematics, University of L'Aquila, L'Aquila, Italy; Department of Information Engineering, Computer Science and Mathematics, University of L'Aquila, L'Aquila, Italy","IEEE Transactions on Software Engineering","13 Jun 2023","2023","49","6","3554","3580","Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms Latency Degradation Patterns. DeLag simultaneously searches for multiple latency degradation patterns while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provides better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. DeLag is more effective than all baseline techniques in at least one case study (with $p\leq 0.05$p≤0.05 and non-negligible effect size). Moreover, DeLag outperforms in terms of efficiency the second and the third most effective baseline techniques on the largest datasets used in our evaluation (up to 22%).","1939-3520","","10.1109/TSE.2023.3266041","Fondo Territori Lavoro e Conoscenza CGIL, CSIL and UIL; European Union; Piano Nazionale di Ripresa e Resilienza(grant numbers:3264 del 28/12/2021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098585","AIOps;anomaly correlation;automated diagnosis;microservices;Performance issues","Degradation;Software;Microservice architectures;Optimization;Testing;Search problems;Genetic algorithms","","1","","77","CCBY","10 Apr 2023","","","IEEE","IEEE Journals"
"DexBERT: Effective, Task-Agnostic and Fine-Grained Representation Learning of Android Bytecode","T. Sun; K. Allix; K. Kim; X. Zhou; D. Kim; D. Lo; T. F. Bissyandé; J. Klein","University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; Singapore Management University, Singapore; Singapore Management University, Singapore; Kyungpook National University, Daegu, Republic of Korea; Singapore Management University, Singapore; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg","IEEE Transactions on Software Engineering","18 Oct 2023","2023","49","10","4691","4706","The automation of an increasingly large number of software engineering tasks is becoming possible thanks to Machine Learning (ML). One foundational building block in the application of ML to software artifacts is the representation of these artifacts (e.g., source code or executable code) into a form that is suitable for learning. Traditionally, researchers and practitioners have relied on manually selected features, based on expert knowledge, for the task at hand. Such knowledge is sometimes imprecise and generally incomplete. To overcome this limitation, many studies have leveraged representation learning, delegating to ML itself the job of automatically devising suitable representations and selections of the most relevant features. Yet, in the context of Android problems, existing models are either limited to coarse-grained whole-app level (e.g., apk2vec) or conducted for one specific downstream task (e.g., smali2vec). Thus, the produced representation may turn out to be unsuitable for fine-grained tasks or cannot generalize beyond the task that they have been trained on. Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (e.g., at the class-level). We are inspired by the field of Natural Language Processing, where the problem of universal representation was addressed by building Universal Language Models, such as BERT, whose goal is to capture abstract semantic information about sentences, in a way that is reusable for a variety of tasks. We propose DexBERT, a BERT-like Language Model dedicated to representing chunks of DEX bytecode, the main binary format used in Android applications. We empirically assess whether DexBERT is able to model the DEX language and evaluate the suitability of our model in three distinct class-level software engineering tasks: Malicious Code Localization, Defect Prediction, and Component Type Classification. We also experiment with strategies to deal with the problem of catering to apps having vastly different sizes, and we demonstrate one example of using our technique to investigate what information is relevant to a given task.","1939-3520","","10.1109/TSE.2023.3310874","Fonds National de la Recherche (FNR), Luxembourg(grant numbers:REPROCESS C21/IS/16344458); National Research Foundation of Korea (NRF); Korea government (MSIT)(grant numbers:2021R1A5A1021944,2021R1I1A3048013); National Cybersecurity Research and Development Programme(grant numbers:NCRP25-P03-NCR-TAU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10237047","Representation learning;Android app analysis;code representation;malicious code localization;defect prediction","Task analysis;Malware;Predictive models;Codes;Location awareness;Operating systems;Software engineering","","1","","93","CCBY","1 Sep 2023","","","IEEE","IEEE Journals"
"Range Specification Bug Detection in Flight Control System Through Fuzzing","R. Han; S. Ma; J. Li; S. Nepal; D. Lo; Z. Ma; J. Ma","School of Cyber Engineering, Xidian University, Xi’an, China; School of Engineering and Information System, University of New South Wales, Sydney, NSW, Australia; Zhiyuan College, Shanghai Jiao Tong University, Shanghai, China; Commonwealth Scientific and Industrial Research, Sydney, NSW, Australia; School of Computing and Information Systems, Singapore Management University, Singapore; School of Cyber Engineering, Xidian University, Xi’an, China; School of Cyber Engineering, Xidian University, Xi’an, China","IEEE Transactions on Software Engineering","18 Mar 2024","2024","50","3","461","473","Developers and manufacturers provide configurable control parameters for flight control programs to support various environments and missions, along with suggested ranges for these parameters to ensure flight safety. However, this flexible mechanism can also introduce a vulnerability known as range specification bugs. The vulnerability originates from the evidence that certain combinations of parameter values may affect the drone's physical stability even though its parameters are within the suggested range. The paper introduces a novel system called icsearcher, designed to identify incorrect configurations or unreasonable combinations of parameters and suggest more reasonable ranges for these parameters. icsearcher applies a metaheuristic search algorithm to find configurations with a high probability of driving the drone into unstable states. In particular, icsearcher adopts a machine learning-based predictor to assist the searcher in evaluating the fitness of configuration. Finally, leveraging searched incorrect configurations, icsearcher can summarize the feasible ranges through multi-objective optimization. icsearcher applies a predictor to guide the search, which eliminates the need for realistic/simulation executions when evaluating configurations and further promotes search efficiency. We have carried out experimental evaluations of icsearcher in different control programs. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over $94\%$94% leads to unstable states.","1939-3520","","10.1109/TSE.2024.3354739","National Natural Science Foundation of China (Key Program)(grant numbers:62232013); Major Research Plan of the National Natural Science Foundation of China(grant numbers:92267204,92167203); National Natural Science Foundation of China(grant numbers:62302363); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10401946","Drone security;configuration test;range specification bug;deep learning approximation","Drones;Aerospace control;Trajectory;Computer bugs;Fuzzing;Actuators;Codes","","","","37","IEEE","17 Jan 2024","","","IEEE","IEEE Journals"
"Identifying the Hazard Boundary of ML-Enabled Autonomous Systems Using Cooperative Coevolutionary Search","S. Sharifi; D. Shin; L. C. Briand; N. Aschbacher","Department of Electrical and Computer Engineering, University of Ottawa, Ottawa, ON, Canada; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Department of Electrical and Computer Engineering, University of Ottawa, Ottawa, ON, Canada; Auxon Corporation, Portland, OR, USA","IEEE Transactions on Software Engineering","12 Dec 2023","2023","49","12","5120","5138","In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential to identify the hazard boundary of ML Components (MLCs) in the MLAS under analysis. Given that such boundary captures the conditions in terms of MLC behavior and system context that can lead to hazards, it can then be used to, for example, build a safety monitor that can take any predefined fallback mechanisms at runtime when reaching the hazard boundary. However, determining such hazard boundary for an ML component is challenging. This is due to the problem space combining system contexts (i.e., scenarios) and MLC behaviors (i.e., inputs and outputs) being far too large for exhaustive exploration and even to handle using conventional metaheuristics, such as genetic algorithms. Additionally, the high computational cost of simulations required to determine any MLAS safety violations makes the problem even more challenging. Furthermore, it is unrealistic to consider a region in the problem space deterministically safe or unsafe due to the uncontrollable parameters in simulations and the non-linear behaviors of ML models (e.g., deep neural networks) in the MLAS under analysis. To address the challenges, we propose MLCSHE (ML Component Safety Hazard Envelope), a novel method based on a Cooperative Co-Evolutionary Algorithm (CCEA), which aims to tackle a high-dimensional problem by decomposing it into two lower-dimensional search subproblems. Moreover, we take a probabilistic view of safe and unsafe regions and define a novel fitness function to measure the distance from the probabilistic hazard boundary and thus drive the search effectively. We evaluate the effectiveness and efficiency of MLCSHE on a complex Autonomous Vehicle (AV) case study. Our evaluation results show that MLCSHE is significantly more effective and efficient compared to a standard genetic algorithm and random search.","1939-3520","","10.1109/TSE.2023.3327575","Natural Sciences and Research Council of Canada (NSERC); Canada Research Chairs (CRC); Mitacs Accelerate; Ontario Graduate Scholarship (OGS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10311084","ML-enabled autonomous system;hazard boundary;system safety monitoring;cooperative co-evolutionary search","Statistics;Sociology;Hazards;Behavioral sciences;Genetic algorithms;Monitoring;Probabilistic logic","","","","52","IEEE","7 Nov 2023","","","IEEE","IEEE Journals"
"FA-Fuzz: A Novel Scheduling Scheme Using Firefly Algorithm for Mutation-Based Fuzzing","Z. Gao; H. Xiong; W. Dong; R. Chang; R. Yang; Y. Zhou; L. Jiang","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; Zhejiang University, Hangzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; Zhejiang University, Hangzhou, China; Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","IEEE Transactions on Software Engineering","8 Jan 2024","2024","50","1","1","15","Mutation-based fuzzing has been widely used in both academia and industry. Recently, researchers observe that the mutation scheduling scheme affects the efficiency of fuzzing. Accordingly, they propose PSO algorithm or machine learning-based technique to optimize the scheduling process. However, these methods fail to consider the fact that the optimal operator distribution of different seeds is different, even for the same program. In this paper, we propose a novel general scheduling scheme, named FA-fuzz, to find the optimal selecting probability distribution of mutation operators, which is based on the observations that the effective mutation operators are different for different seeds. Specifically, our method is based on the firefly algorithm. The positions of fireflies are mapped to the selection probability distribution of different mutation operators. The brightness of fireflies is expressed as the efficiency of discovering unique testcases. We implement prototype systems on multiple state-of-art fuzzers, and perform evaluations on two datasets. Our proposed method improves both the number of unique paths and unique bugs on real-world datasets. In addition, we discover 30 zero-day vulnerabilities in eight real-world programs, which demonstrate the effectiveness of FA-fuzz.","1939-3520","","10.1109/TSE.2023.3326144","Key R&D Program of Zhejiang Province(grant numbers:2022C01165); Key R&D Special Program of Henan Province(grant numbers:221111210300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305545","Mutation-based fuzzing;firefly algorithm","Fuzzing;Probability distribution;Optimization;Software algorithms;Job shop scheduling;Computer bugs;Research and development","","","","59","IEEE","2 Nov 2023","","","IEEE","IEEE Journals"
"LineVD: Statement-level Vulnerability Detection using Graph Neural Networks","D. Hin; A. Kan; H. Chen; M. A. Babar","CREST - The Centre for Research on Engineering Software Technologies, University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; AWS AI Labs*, Adelaide, SA, Australia; CREST - The Centre for Research on Engineering Software Technologies, University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia","2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)","21 Jun 2022","2022","","","596","607","Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art.","2574-3864","978-1-4503-9303-4","10.1145/3524842.3527949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796269","Software Vulnerability Detection;Program Representation;Deep Learning","Deep learning;Training;Codes;Supervised learning;Predictive models;Transformers;Feature extraction","","20","","62","","21 Jun 2022","","","IEEE","IEEE Conferences"
"Revisiting and Improving SZZ Implementations","E. C. Neto; D. A. d. Costa; U. Kulesza","Federal Institute of Education, Science and Technology of Rio Grande do Norte, Natal, Brazil; Department of Information Science, University of Otago, Dunedin, New Zealand; Dept. of Informatics and Applied Maths, Federal University of Rio Grande do Norte, Natal, Brazil","2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","17 Oct 2019","2019","","","1","12","Background: The SZZ algorithm was proposed to identify bug-introducing changes, i.e., changes that are likely to induce bugs. Previous studies improved its implementation and evaluated its results.Aims: To address existing limitations of SZZ to improve the maturity of the algorithm. We also aim to verify if the improvements that have been proposed to the SZZ algorithm also hold in different datasets.Method: We re-evaluate two recent SZZ implementations using an adaptation of the Defects4J dataset, which works as a preprocessed dataset that can be used by SZZ. Furthermore, we revisit the limitations of RA-SZZ (refactoring aware SZZ) to improve the precision and recall of the algorithm.Results: We observe that a median of 44% of the lines that are flagged by the improved SZZ are very likely to introduce a bug. We manually analyze the SZZ-generated data and observe that there exist refactoring operations (31.17%) and equivalent changes (13.64%) that are still misidentified by the improved SZZ.Conclusion: By preprocessing the dataset that is used as input by SZZ, the accuracy of SZZ may be considerably improved. For example, we observe that SZZ implementations are approximately 40% more accurate if only valid bug-fix lines are used as the input for SZZ.","1949-3789","978-1-7281-2968-6","10.1109/ESEM.2019.8870178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870178","SZZ algorithm;refactoring change;bug-introducing change;bug-fix change","Computer bugs;Tools;History;Bars;Machine learning algorithms;Prediction algorithms;Software algorithms","","20","","49","IEEE","17 Oct 2019","","","IEEE","IEEE Conferences"
"REACT: An Approach for Capturing Rationale in Chat Messages","R. Alkadhi; J. O. Johanssen; E. Guzman; B. Bruegge","Department of Informatics, Technical University of Munich, Germany; Technische Universitat Munchen, Munchen, Bayern, DE; Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Informatics, Technical University of Munich, Germany","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","175","180","Developers' chat messages are a rich source of rationale behind development decisions. Rationale comprises valuable knowledge during software evolution for understanding and maintaining the software system. However, developers resist explicit methods for rationale capturing in practice, due to their intrusiveness and cognitive overhead. Aim: Our primary goal is to help developers capture rationale in chat messages with low effort. Further, we seek to encourage the collaborative capturing of rationale in development teams. Method: In this paper, we present REACT, a lightweight approach for annotating chat messages that contain rationale. To evaluate the feasibility of REACT, we conducted two studies. In the first study, we evaluated the approach with eleven development teams during a short-term design task. In the second study, we evaluated the approach with one development team over a duration of two months. In addition, we distributed a questionnaire to both studies' participants. Results: Our results show that REACT is easily learned and used by developers. Furthermore, it encourages the collaborative capturing of rationale. Remarkably, the majority of participants do not perceive privacy as a barrier when capturing rationale from their informal communication. Conclusions: REACT is a first step towards enhancing rationale capturing in developers' chat messages.","","978-1-5090-4039-1","10.1109/ESEM.2017.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170099","Rationale;Knowledge management;Developers Chat messages","Collaboration;Privacy;Software systems;Manuals;Software engineering","","20","","19","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"An Empirical Study on Low- and High-Level Explanations of Deep Learning Misbehaviours","T. Zohdinasab; V. Riccio; P. Tonella","Università della Svizzera italiana, Lugano, Switzerland; Università della Svizzera italiana, Lugano, Switzerland; Università della Svizzera italiana, Lugano, Switzerland","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","11","Background: Most quality assessment approaches for Deep Learning (DL) focus on finding misbehaviour-inducing inputs. However, it is difficult to clearly understand the causes of misbehaviours, due to the DL software opaqueness. Recent research proposed different techniques to explain DL misbehaviours, producing input explanations either at a “low level” (raw input elements) or at a “high level” (input features). Aims: We aim to compare the similarity between different explanations and assess to what extent they are understandable. Method: We have conducted an empirical study involving 3 state-of-the-art techniques for DL explanation in 13 configurations, applied to 2 different DL tasks. We have also collected answers from 48 questionnaires submitted to SE experts. Results: Low- and high-level techniques provide dissimilar explanations for the same inputs. However, experts deemed none of the explanations as useful in 28% of the cases. Conclusion: Despite the complementarity of existing explanations, further research is needed to produce better explanations.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304866","deep learning;software testing;explainable artificial intelligence","Deep learning;Software;Quality assessment;Software measurement;Task analysis;Artificial intelligence;Software engineering","","","","46","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"An Exploratory Analysis of a Hybrid OSS Company's Forum in Search of Sales Leads","M. Munezero; T. Kojo; T. Männistö","Department of Computer Science, University of Helsinki, Helsinki, Finland; The Qt Company, Espoo, Finland; Department of Computer Science, University of Helsinki, Helsinki, Finland","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","442","447","Background: Online forums are instruments through which information or problems are shared and discussed, including expressions of interests and intentions. Objective: In this paper, we present ongoing work aimed at analyzing the content of forum posts of a hybrid open source company that offers both free and commercial licenses, in order to help its community manager gain improved understanding of the forum discussions and sentiments and automatically discover new opportunities such as sales leads, i.e., people who are interested in buying a license. These leads can then be forwarded to the sales team for follow-up and can result in them potentially making a sale, thus increasing company revenue. Method: For the analysis of the forums, an untapped channel for sales leads by the company, text analysis techniques are utilized to identify potential sales leads and the discussion topics and sentiments in those leads. Results: Results of our preliminary work make a positive contribution in lessening the community manager's work in understanding the sentiment and discussion topics in the hybrid open source forum community, as well as make it easier and faster to identify potential future customers. Conclusion: We believe that the results will positively contribute to improving the sales of licenses for the hybrid open source company.","","978-1-5090-4039-1","10.1109/ESEM.2017.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170132","Sales lead identification;text analysis;online forums;sentiment analysis;topic modeling;hybrid OSS company","Companies;Licenses;Lead;Software;Text analysis;Sentiment analysis","","","","24","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"Where Is the Road for Issue Reports Classification Based on Text Mining?","Q. Fan; Y. Yu; G. Yin; T. Wang; H. Wang","College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","121","130","Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.","","978-1-5090-4039-1","10.1109/ESEM.2017.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170092","issue tracking system;machine learning technique;mining software repositories","Computer bugs;Software;Data mining;Feature extraction;Semantics;Measurement","","31","","34","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"An Empirical Examination of the Relationship between Code Smells and Merge Conflicts","I. Ahmed; C. Brindescu; U. A. Mannan; C. Jensen; A. Sarma","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","58","67","Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both ""smelly"" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.","","978-1-5090-4039-1","10.1109/ESEM.2017.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170085","Code Smell;Merge Conflict;Empirical Analysis;Machine Learning","Software;Merging;Computer bugs;Tools;Software measurement;Semantics","","26","","68","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"What if I Had No Smells?","D. Falessi; B. Russo; K. Mullen","Dept. of Computer Science and Software Engineering, California Polytechnic State University, San Luis Obispo, CA, USA; Faculty of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy, USA; Keymind A Division of Axiom Resource Management, Inc. Falls Church, VA, USA","2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","11 Dec 2017","2017","","","78","84","What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.","","978-1-5090-4039-1","10.1109/ESEM.2017.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170087","code smells;machine learning;software estimation;technical debt","Tools;Maintenance engineering;Software;Logic gates;Estimation;History;Business","","12","","36","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"TransDPR: Design Pattern Recognition Using Programming Language Models","S. K. Pandey; M. Staron; J. Horkoff; M. Ochodek; N. Mucci; D. Durisic","Chalmers | University of Gothenburg, Sweden; Chalmers | University of Gothenburg, Sweden; Chalmers | University of Gothenburg, Sweden; Poznan University of Technology, Poland; Volvo Cars, Gothenburg, Sweden; Volvo Cars, Gothenburg, Sweden","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","7","Current Design Pattern Recognition (DPR) methods have limitations, such as the reliance on semantic information, limited recognition of novel or modified pattern versions, and other factors. We present an introductory DPR technique by using a Programming Language Model (PLM) called TransDPR, which utilizes a Facebook pre-trained model (TransCoder), which is a Cross-lingual programming Language Model (XLM) based on a transformer architecture. We leverage an n-dimensional vector representation of programs and apply logistic regression to learn design patterns (DPs). Our approach utilizes the GitHub repository to collect singleton and prototype DP programs written in $C$++ source code. Our results indicate that TransDPR achieves 90% accuracy and an F1-score of 0.88 on open-source projects. We evaluate the proposed model on two developed modules from Volvo Cars and invite the original developers to validate the prediction results.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304862","Vinnova; University of Gothenburg; National Science Centre, Poland(grant numbers:2021/41/B/ST6/02510); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304862","Design patterns recognition;Programming language models;NLP;Machine learning;Deep learning","Computer languages;Social networking (online);Source coding;Semantics;Prototypes;Transformers;Pattern recognition","","","","28","EU","8 Nov 2023","","","IEEE","IEEE Conferences"
"DeepJIT: An End-to-End Deep Learning Framework for Just-in-Time Defect Prediction","T. Hoang; H. Khanh Dam; Y. Kamei; D. Lo; N. Ubayashi","Singapore Management University, Singapore; University of Wollongong, Australia; Kyushu University, Japan; Singapore Management University, Singapore; Kyushu University, Japan","2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","34","45","Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36-11.02% for the project QT and 9.51-13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC).","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816772","deep learning;just-in-time defect prediction;convolutional neural network","Feature extraction;Software;Predictive models;Convolutional codes;Deep learning;Natural language processing;Testing","","115","","78","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Semantic Source Code Models Using Identifier Embeddings","V. Efstathiou; D. Spinellis",Athens University of Economics and Business; Athens University of Economics and Business,"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)","29 Aug 2019","2019","","","29","33","The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.","2574-3864","978-1-7281-3412-3","10.1109/MSR.2019.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816775","fastText;Code Semantics;Vector Space Models;Semantic Similarity","Semantics;Computer languages;Natural languages;Biological system modeling;Task analysis;Software;Data models","","14","","45","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Extracting Code Segments and Their Descriptions from Research Articles","P. Chatterjee; B. Gause; H. Hedinger; L. Pollock","Computer and Information Sciences, University of Delaware, Newark, DE, USA; Computer and Information Sciences, University of Delaware, Newark, DE, USA; Computer and Information Sciences, University of Delaware, Newark, DE, USA; Computer and Information Sciences, University of Delaware, Newark, DE, USA","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","3 Jul 2017","2017","","","91","101","The availability of large corpora of online software-related documents today presents an opportunity to use machine learning to improve integrated development environments by first automatically collecting code examples along with associated descriptions. Digital libraries of computer science research and education conference and journal articles can be a rich source for code examples that are used to motivate or explain particular concepts or issues. Because they are used as examples in an article, these code examples are accompanied by descriptions of their functionality, properties, or other associated information expressed in natural language text. Identifying code segments in these documents is relatively straightforward, thus this paper tackles the problem of extracting the natural language text that is associated with each code segment in an article. We present and evaluate a set of heuristics that address the challenges of the text often not being colocated with the code segment as in developer communications such as online forums.","","978-1-5386-1544-7","10.1109/MSR.2017.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962359","mining software repositories;information extraction;code snippet description;text analysis","Electronic mail;Libraries;Natural languages;Redundancy;Data mining;Computer science;Documentation","","4","1","28","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Automating Arduino Programming: From Hardware Setups to Sample Source Code Generation","I. N. Bani Yusuf; D. Binte Abdul Jamal; L. Jiang","School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","453","464","An embedded system is a system consisting of software code, controller hardware, and I/O (Input/Output) hardware that performs a specific task. Developing an embedded system presents several challenges. First, the development often involves configuring hardware that requires domain-specific knowledge. Second, the library for the hardware may have API usage patterns that must be followed. To overcome such challenges, we propose a framework called ArduinoProg towards the automatic generation of Arduino applications. ArduinoProg takes a natural language query as input and outputs the configuration and API usage pattern for the hardware described in the query. Motivated by our findings on the characteristics of real-world queries posted in the official Arduino forum, we formulate ArduinoProg as three components, i.e., Library Retriever, Configuration Classifier, and Pattern Generator. First, Library Retriever preprocesses the input query and retrieves a set of relevant libraries using either lexical matching or vector-based similarity. Second, given Library Retriever’s output, Configuration Classifier infers the hardware configuration by classifying the method definitions found in the library’s implementation files into a hardware configuration class. Third, Pattern Generator also takes Library Retriever’s output as input and leverages a sequence-to-sequence model to generate the API usage pattern. Having instantiated each component of ArduinoProg with various machine learning models, we have evaluated ArduinoProg on real-world queries. Library Retriever achieves a Precision@K range of 44.0%-97.1%; Configuration Classifier achieves an Area under the Receiver Operating Characteristics curve (AUC) of 0.79-0.95; Pattern Generator yields a Normalized Discounted Cumulative Gain (NDCG)@K of 0.45-0.73. Such results indicate that ArduinoProg can generate practical and useful hardware configurations and API usage patterns to guide developers in writing Arduino code.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00069","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174037","arduino;embedded system;deep learning;information retrieval;code generation;api recommendation","Adaptation models;Embedded systems;Codes;Natural languages;Writing;Programming;Hardware","","1","","44","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Cross-Domain Evaluation of a Deep Learning-Based Type Inference System","B. Gruner; T. Sonnekalb; T. S. Heinze; C. -A. Brust","Institute of Data Science, German Aerospace Center, Jena, Germany; Institute of Data Science, German Aerospace Center, Jena, Germany; Gera-Eisenach, Cooperative University, Gera, Germany; Institute of Data Science, German Aerospace Center, Jena, Germany","2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)","12 Jul 2023","2023","","","158","169","Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain.In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes.To perform such experiments, we use the datasets Many-Types4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference systems in different domains of software projects and has over 1,000,000 type annotations mined on the platforms GitHub and Libraries. It consists of data from the two domains web development and scientific calculation.Through our experiments, we detect that the shifts in the dataset and the long-tailed distribution with many rare and unknown data types decrease the performance of the deep learning-based type inference system drastically. In this context, we test unsupervised domain adaptation methods and fine-tuning to overcome these issues. Moreover, we investigate the impact of out-of-vocabulary words.","2574-3864","979-8-3503-1184-6","10.1109/MSR59073.2023.00034","Ministry of Economy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174049","type inference;dataset;cross-domain;python;long-tailed;out-of-vocabulary;repository mining;deep learning","Training;Runtime;Annotations;Feature extraction;Software;Libraries;Dynamic programming","","1","","53","IEEE","12 Jul 2023","","","IEEE","IEEE Conferences"
"Exploring the Advances in Identifying Useful Code Review Comments","S. Ahmed; N. U. Eisty","Computer Science Department, Boise State University, Boise, ID, USA; Computer Science Department, Boise State University, Boise, ID, USA","2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","7","Effective peer code review in collaborative software development necessitates useful reviewer comments and supportive automated tools. Code review comments are a central component of the Modern Code Review process in the industry and open-source development. Therefore, it is important to ensure these comments serve their purposes. This paper reflects the evolution of research on the usefulness of code review comments. It examines papers that define the usefulness of code review comments, mine and annotate datasets, study developers' perceptions, analyze factors from different aspects, and use machine learning classifiers to automatically predict the usefulness of code review comments. Finally, it discusses the open problems and challenges in recognizing useful code review comments for future research.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304792","Modern Code Review;Useful Comments;Software Quality;Software Engineering","Codes;Taxonomy;Static analysis;Predictive models;Feature extraction;Trajectory;Data mining","","","","13","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"Leveraging Evidence Theory to Improve Fault Localization: An Exploratory Study","Y. Zhang; K. Leach; Y. Huang",Vanderbilt University; Vanderbilt University; Vanderbilt University,"2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)","8 Nov 2023","2023","","","1","12","Background: Fault localization in software maintenance and debugging can be a costly process. Spectrum-Based Fault Localization (SBFL) is a widely-used method for fault localization. It assigns suspicion scores to code elements based on tests, indicating the likelihood of defects in specific code lines. However, the effectiveness of SBFL approaches varies depending on the subject code. Aims: In this paper, our aim is to present an approach that combines multiple SBFL formulae using evidence theory. Method: We first introduce a taxonomy of SBFL techniques. Then, we describe how we fuse suspiciousness scores obtained from a set of SBFL formulae. We also introduce a concept of fuzzy windows, and describe how they can enhance localization accuracy and how they can be tuned to further refine results. Results: We present an empirical evaluation of our approach using the Defects4J dataset. Our results demonstrate improvements in fault localization accuracy over existing statement-level SBFL techniques. Specifically, by fusing three SBFL methods, our approach reduces code inspection effort by up to 34.5 % with a size-4 window and increases the hit rate for the top 10% most suspicious lines by 27.9 % using a size-7 window. Moreover, in multi-line bug scenarios, our approach reduces code inspection effort by up to 35.6% and achieves a maximum increase of 43.2% in the hit rate of the top 10% most suspicious lines. Additionally, our approach outperforms state-of-the-art machine learning-based method-level fusion approaches in terms of top rank fault localization accuracy. Conclusions: Our study highlights the applicability of evidence theory in addressing fault localization as an uncertain and ambiguous information fusion problem involving multiple SBFL techniques. The combination of SBFL formulae using evidence theory, along with the use of fuzzy windows, shows promise in enhancing fault localization accuracy.","","978-1-6654-5223-6","10.1109/ESEM56168.2023.10304791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304791","evidence theory;information fusion;uncertainty;fault localization","Location awareness;Software maintenance;Codes;Fuses;Evidence theory;Taxonomy;Debugging","","","","59","IEEE","8 Nov 2023","","","IEEE","IEEE Conferences"
"[Title page i]","",,"2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP)","5 May 2016","2016","","","i","i","The following topics are dealt with: software engineering; machine learning; and software repositories mining.","","978-1-5090-1851-2","10.1109/IWESEP.2016.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464535","","","","","","","IEEE","5 May 2016","","","IEEE","IEEE Conferences"
