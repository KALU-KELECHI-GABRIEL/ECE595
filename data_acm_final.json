[
  {
    "Key": "V68QAANU",
    "Item Type": "conferencePaper",
    "Publication Year": 2022,
    "Author": "Jacobs, Arthur S.; Beltiukov, Roman; Willinger, Walter; Ferreira, Ronaldo A.; Gupta, Arpit; Granville, Lisandro Z.",
    "Title": "AI/ML for Network Security: The Emperor has no Clothes",
    "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
    "ISBN": "978-1-4503-9450-5",
    "ISSN": "",
    "DOI": "10.1145/3548606.3560609",
    "Url": "https://doi.org/10.1145/3548606.3560609",
    "Abstract": "Several recent research efforts have proposed Machine Learning (ML)-based solutions that can detect complex patterns in network traffic for a wide range of network security problems. However, without understanding how these black-box models are making their decisions, network operators are reluctant to trust and deploy them in their production settings. One key reason for this reluctance is that these models are prone to the problem of underspecification, defined here as the failure to specify a model in adequate detail. Not unique to the network security domain, this problem manifests itself in ML models that exhibit unexpectedly poor behavior when deployed in real-world settings and has prompted growing interest in developing interpretable ML solutions (e.g., decision trees) for \"explaining\u201d to humans how a given black-box model makes its decisions. However, synthesizing such explainable models that capture a given black-box model's decisions with high fidelity while also being practical (i.e., small enough in size for humans to comprehend) is challenging.In this paper, we focus on synthesizing high-fidelity and low-complexity decision trees to help network operators determine if their ML models suffer from the problem of underspecification. To this end, we present Trustee, a framework that takes an existing ML model and training dataset as input and generates a high-fidelity, easy-to-interpret decision tree and associated trust report as output. Using published ML models that are fully reproducible, we show how practitioners can use Trustee to identify three common instances of model underspecification; i.e., evidence of shortcut learning, presence of spurious correlations, and vulnerability to out-of-distribution samples.",
    "Date": "2022",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "1537\u20131551",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "CCS '22",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: Los Angeles, CA, USA",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "artificial intelligence; explainability; interpretability; machine learning; network security; trust",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "ZCC6XACX",
    "Item Type": "conferencePaper",
    "Publication Year": 2023,
    "Author": "Arazzi, Marco; Conti, Mauro; Nocera, Antonino; Picek, Stjepan",
    "Title": "Turning Privacy-preserving Mechanisms against Federated Learning",
    "Publication Title": "Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security",
    "ISBN": "9.80E+12",
    "ISSN": "",
    "DOI": "10.1145/3576915.3623114",
    "Url": "https://doi.org/10.1145/3576915.3623114",
    "Abstract": "Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, researchers proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data.In this paper, we identify a crucial security flaw in such a configuration and design an attack capable of deceiving state-of-the-art defenses for federated learning. The proposed attack includes two operating modes, the first one focusing on convergence inhibition (Adversarial Mode), and the second one aiming at building a deceptive rating injection on the global federated model (Backdoor Mode). The experimental results show the effectiveness of our attack in both its modes, returning on average 60% performance detriment in all the tests on Adversarial Mode and fully effective backdoors in 93% of cases for the tests performed on Backdoor Mode.",
    "Date": "2023",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "1482\u20131495",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "CCS '23",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "federated learning; graph neural network; model poisoning; privacy; recommender systems",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "CGVE9NWW",
    "Item Type": "conferencePaper",
    "Publication Year": 2021,
    "Author": "He, Chaoxiang; Zhu, Bin Benjamin; Ma, Xiaojing; Jin, Hai; Hu, Shengshan",
    "Title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
    "Publication Title": "Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
    "ISBN": "978-1-4503-8454-4",
    "ISSN": "",
    "DOI": "10.1145/3460120.3485378",
    "Url": "https://doi.org/10.1145/3460120.3485378",
    "Abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
    "Date": "2021",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "3159\u20133176",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "CCS '21",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: Virtual Event, Republic of Korea",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "adversarial attacks; adversarial examples; feature-indistinguishable attack; neural networks; trapdoor enabled defense",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "CI2F564T",
    "Item Type": "conferencePaper",
    "Publication Year": 2020,
    "Author": "Pang, Ren; Shen, Hua; Zhang, Xinyang; Ji, Shouling; Vorobeychik, Yevgeniy; Luo, Xiapu; Liu, Alex; Wang, Ting",
    "Title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
    "Publication Title": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security",
    "ISBN": "978-1-4503-7089-9",
    "ISSN": "",
    "DOI": "10.1145/3372297.3417253",
    "Url": "https://doi.org/10.1145/3372297.3417253",
    "Abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs \u2013 maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models \u2013 adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors \u2013 leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
    "Date": "2020",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "85\u201399",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "CCS '20",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: Virtual Event, USA",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "adversarial attack; backdoor attack; trojaning attack",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "3JZ3KK5L",
    "Item Type": "conferencePaper",
    "Publication Year": 2022,
    "Author": "Li, Zheng; Liu, Yiyong; He, Xinlei; Yu, Ning; Backes, Michael; Zhang, Yang",
    "Title": "Auditing Membership Leakages of Multi-Exit Networks",
    "Publication Title": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
    "ISBN": "978-1-4503-9450-5",
    "ISSN": "",
    "DOI": "10.1145/3548606.3559359",
    "Url": "https://doi.org/10.1145/3548606.3559359",
    "Abstract": "Relying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
    "Date": "2022",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "1917\u20131931",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "CCS '22",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: Los Angeles, CA, USA",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "machine learning; membership leakages; multi-exit networks",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "3P7NCBYB",
    "Item Type": "conferencePaper",
    "Publication Year": 2023,
    "Author": "Greshake, Kai; Abdelnabi, Sahar; Mishra, Shailesh; Endres, Christoph; Holz, Thorsten; Fritz, Mario",
    "Title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
    "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
    "ISBN": "9.80E+12",
    "ISSN": "",
    "DOI": "10.1145/3605764.3623985",
    "Url": "https://doi.org/10.1145/3605764.3623985",
    "Abstract": "Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",
    "Date": "2023",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "79\u201390",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "AISec '23",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "indirect prompt injection; large language models",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "SWCK2LLK",
    "Item Type": "conferencePaper",
    "Publication Year": 2023,
    "Author": "Imgrund, Erik; Ganz, Tom; H\u00e4rterich, Martin; Pirch, Lukas; Risse, Niklas; Rieck, Konrad",
    "Title": "Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery",
    "Publication Title": "Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security",
    "ISBN": "9.80E+12",
    "ISSN": "",
    "DOI": "10.1145/3605764.3623915",
    "Url": "https://doi.org/10.1145/3605764.3623915",
    "Abstract": "Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery.",
    "Date": "2023",
    "Date Added": "4/16/24 18:26",
    "Date Modified": "4/16/24 18:26",
    "Access Date": "",
    "Pages": "149\u2013160",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "AISec '23",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: <conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "causal learning; confounding effect; large language models; overfitting; vulnerability discovery",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "HCMB48VA",
    "Item Type": "conferencePaper",
    "Publication Year": 2020,
    "Author": "Islam, Md Johirul; Pan, Rangeet; Nguyen, Giang; Rajan, Hridesh",
    "Title": "Repairing deep neural networks: fix patterns and challenges",
    "Publication Title": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
    "ISBN": "978-1-4503-7121-6",
    "ISSN": "",
    "DOI": "10.1145/3377811.3380378",
    "Url": "https://doi.org/10.1145/3377811.3380378",
    "Abstract": "Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.",
    "Date": "2020",
    "Date Added": "4/16/24 18:27",
    "Date Modified": "4/16/24 18:27",
    "Access Date": "",
    "Pages": "1135\u20131146",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "ICSE '20",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "Association for Computing Machinery",
    "Place": "New York, NY, USA",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "event-place: Seoul, South Korea",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "bug fix; bug fix patterns; bugs; deep neural networks",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "CTM5I3BF",
    "Item Type": "journalArticle",
    "Publication Year": 2023,
    "Author": "Suneja, Sahil; Zhuang, Yufan; Zheng, Yunhui; Laredo, Jim; Morari, Alessandro; Khurana, Udayan",
    "Title": "Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection",
    "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
    "ISBN": "",
    "ISSN": "1049-331X",
    "DOI": "10.1145/3597202",
    "Url": "https://doi.org/10.1145/3597202",
    "Abstract": "AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models\u2019 signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models\u2019 signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model\u2019s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8\u00d7 improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.",
    "Date": "2023-09",
    "Date Added": "4/16/24 18:25",
    "Date Modified": "4/16/24 18:25",
    "Access Date": "",
    "Pages": "",
    "Num Pages": "",
    "Issue": 6,
    "Volume": 32,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "",
    "Place": "",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "curriculum learning; data augmentation; explainability; Machine learning; neural networks; reliability; signal awareness",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "PZN99XB3",
    "Item Type": "journalArticle",
    "Publication Year": 2024,
    "Author": "Aghababaeyan, Zohreh; Abdellatif, Manel; Dadkhah, Mahboubeh; Briand, Lionel",
    "Title": "DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks",
    "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
    "ISBN": "",
    "ISSN": "1049-331X",
    "DOI": "10.1145/3644388",
    "Url": "https://doi.org/10.1145/3644388",
    "Abstract": "Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability: (1) White-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.",
    "Date": "2024-02",
    "Date Added": "4/16/24 18:25",
    "Date Modified": "4/16/24 18:25",
    "Access Date": "",
    "Pages": "",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "",
    "Place": "",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
    "Notes": "<p>Just Accepted</p>",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "Deep Learning Model Evaluation; Deep Neural Network; Diversity; DNN Fault Detection; Model Retraining Guidance; Multi-Objective Optimization; Test Case Selection; Uncertainty Metrics",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "5T39SIRU",
    "Item Type": "journalArticle",
    "Publication Year": 2024,
    "Author": "Attaoui, Mohammed Oualid; Fahmy, Hazem; Pastore, Fabrizio; Briand, Lionel",
    "Title": "Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches",
    "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
    "ISBN": "",
    "ISSN": "1049-331X",
    "DOI": "10.1145/3643671",
    "Url": "https://doi.org/10.1145/3643671",
    "Abstract": "The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively capturing images of the same failure scenario, thus facilitating root cause analysis. Further, it generates distinct clusters for each root cause of failure, thus enabling engineers to detect all the unsafe scenarios. Interestingly, these results hold even for failure scenarios that are only observed in a small percentage of the failing images.",
    "Date": "2024-02",
    "Date Added": "4/16/24 18:25",
    "Date Modified": "4/16/24 18:25",
    "Access Date": "",
    "Pages": "",
    "Num Pages": "",
    "Issue": null,
    "Volume": null,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "",
    "Place": "",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
    "Notes": "<p>Just Accepted</p>",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "Clustering; DNN Debugging; DNN Explanation; DNN Functional Safety Analysis; Transfer Learning",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  },
  {
    "Key": "3G9X6GVL",
    "Item Type": "journalArticle",
    "Publication Year": 2023,
    "Author": "Huang, Wei; Zhao, Xingyu; Banks, Alec; Cox, Victoria; Huang, Xiaowei",
    "Title": "Hierarchical Distribution-aware Testing of Deep Learning",
    "Publication Title": "ACM Trans. Softw. Eng. Methodol.",
    "ISBN": "",
    "ISSN": "1049-331X",
    "DOI": "10.1145/3625290",
    "Url": "https://doi.org/10.1145/3625290",
    "Abstract": "With its growing use in safety/security-critical applications, Deep Learning (DL) has raised increasing concerns regarding its dependability. In particular, DL has a notorious problem of lacking robustness. Input added with adversarial perturbations, i.e., Adversarial Examples (AEs), are easily mispredicted by the DL model. Despite recent efforts made in detecting AEs via state-of-the-art attack and testing methods, they are normally input distribution\u2013agnostic and/or disregard the perceptual quality of adversarial perturbations. Consequently, the detected AEs are irrelevant inputs in the application context or noticeably unrealistic to humans. This may lead to a limited effect on improving the DL model\u2019s dependability, as the testing budget is likely to be wasted on detecting AEs that are encountered very rarely in its real-life operations. In this article, we propose a new robustness testing approach for detecting AEs that considers both the feature-level distribution and the pixel-level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. First, we select test seeds based on the density of feature-level distribution and the vulnerability of adversarial robustness. The vulnerability of test seeds is indicated by the auxiliary information, which are highly correlated with local robustness. Given a test seed, we then develop a novel genetic algorithm\u2013based local test case generation method, in which two fitness functions work alternatively to control the perceptual quality of detected AEs. Finally, extensive experiments confirm that our holistic approach considering hierarchical distributions is superior to the state-of-the-arts that either disregard any input distribution or only consider a single (non-hierarchical) distribution, in terms of not only detecting imperceptible AEs but also improving the overall robustness of the DL model under testing.",
    "Date": "2023-12",
    "Date Added": "4/16/24 18:25",
    "Date Modified": "4/16/24 18:25",
    "Access Date": "",
    "Pages": "",
    "Num Pages": "",
    "Issue": 2,
    "Volume": 33,
    "Number Of Volumes": "",
    "Journal Abbreviation": "",
    "Short Title": "",
    "Series": "",
    "Series Number": "",
    "Series Text": "",
    "Series Title": "",
    "Publisher": "",
    "Place": "",
    "Language": "",
    "Rights": "",
    "Type": "",
    "Archive": "",
    "Archive Location": "",
    "Library Catalog": "",
    "Call Number": "",
    "Extra": "Place: New York, NY, USA Publisher: Association for Computing Machinery",
    "Notes": "",
    "File Attachments": "",
    "Link Attachments": "",
    "Manual Tags": "adversarial examples detection; Deep learning robustness; distribution-aware testing; natural perturbations; robustness growth; safe AI",
    "Automatic Tags": "",
    "Editor": "",
    "Series Editor": "",
    "Translator": "",
    "Contributor": "",
    "Attorney Agent": "",
    "Book Author": "",
    "Cast Member": "",
    "Commenter": "",
    "Composer": "",
    "Cosponsor": "",
    "Counsel": "",
    "Interviewer": "",
    "Producer": "",
    "Recipient": "",
    "Reviewed Author": "",
    "Scriptwriter": "",
    "Words By": "",
    "Guest": "",
    "Number": "",
    "Edition": "",
    "Running Time": "",
    "Scale": "",
    "Medium": "",
    "Artwork Size": "",
    "Filing Date": "",
    "Application Number": "",
    "Assignee": "",
    "Issuing Authority": "",
    "Country": "",
    "Meeting Name": "",
    "Conference Name": "",
    "Court": "",
    "References": "",
    "Reporter": "",
    "Legal Status": "",
    "Priority Numbers": "",
    "Programming Language": "",
    "Version": "",
    "System": "",
    "Code": "",
    "Code Number": "",
    "Section": "",
    "Session": "",
    "Committee": "",
    "History": "",
    "Legislative Body": "",
    "verdict": "YES"
  }
]