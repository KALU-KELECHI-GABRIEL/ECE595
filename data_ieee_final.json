[
  {
    "Document Title": "SAIL: Analyzing Structural Artifacts of Logic Locking Using Machine Learning",
    "Authors": "P. Chakraborty; J. Cruz; A. Alaql; S. Bhunia",
    "Author Affiliations": "Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; King Abdulaziz City for Science and Technology (KACST), Communication and Information Technology Research Institute, Riyadh, Saudi Arabia; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "13-Aug-21",
    "Publication Year": 2021,
    "Volume": 16,
    "Issue": null,
    "Start Page": 3828,
    "End Page": 3842,
    "Abstract": "Obfuscation or Logic locking (LL) is a technique for protecting hardware intellectual property (IP) blocks against diverse threats, including IP theft, reverse engineering, and malicious modifications. State-of-the-art locking techniques primarily focus on securing a design from unauthorized usage by disabling correct functionality \u201a\u00c4\u00ec they often do not directly address hiding design intent through structural transformations. They rely on the synthesis tool to introduce structural changes. We observe that this process is insufficient as the resulting changes in circuit topology are: (1) local and (2) predictable. In this paper, we analyze the structural transformations introduced by LL and introduce a potential attack, called SAIL, that can exploit structural artifacts introduced by LL. SAIL uses machine learning (ML) guided structural recovery that exposes a critical vulnerability in these techniques. Through this attack, we demonstrate that the gate-level structure of a locked design can be retrieved in most parts through a systematic set of steps. The proposed attack is applicable to most forms of logic locking, and significantly more powerful than existing attacks, e.g., SAT-based attacks, since it does not require the availability of golden functional responses (e.g., an unlocked IC). Evaluation on benchmark circuits shows that we can recover an average of about 92%, up to 97%, transformations (Top-10 R-Metric) introduced by logic locking. We show that this attack is scalable, flexible, and versatile. Additionally, to evaluate the SAIL attack resilience of a locked design, we present the SIVA-Metric that is fast in terms of computation speed and does not require any training. We also propose possible mitigation steps for incorporating SAIL resilience into a locked design.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2021.3096028",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478898",
    "Author Keywords": "Hardware obfuscation;logic locking;hardware security;cybersecurity;machine learning",
    "IEEE Terms": "Logic gates;Resilience;Machine learning;Hardware;Interference;Benchmark testing;Training",
    "Mesh_Terms": "",
    "Article Citation Count": 12,
    "Patent Citation Count": null,
    "Reference Count": 44,
    "License": "IEEE",
    "Online Date": "9-Jul-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Comments on \u201a\u00c4\u00faDropping Activation Outputs with Localized First-Layer Deep Network for Enhancing User Privacy and Data Security\u201a\u00c4\u00f9",
    "Authors": "X. Tan; H. Li; L. Wang; Z. Xu",
    "Author Affiliations": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "17-Jul-20",
    "Publication Year": 2020,
    "Volume": 15,
    "Issue": null,
    "Start Page": 3938,
    "End Page": 3939,
    "Abstract": "Inference based on deep learning models is usually implemented by exposing sensitive user data to the outside models, which of course gives rise to acute privacy concerns. To deal with these concerns, Dong et al. recently proposed an approach, namely the dropping-activation-outputs (DAO) first layer. This approach was claimed to be a non-invertible transformation, such that the privacy of user data could not be compromised. However, In this paper, we prove that the DAO first layer, in fact, can generally be inverted, and hence fails to preserve privacy. We also provide a countermeasure against the privacy vulnerabilities that we examined.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2020.2988156",
    "Funding Information": "National Key Research and Development Program of China(grant numbers:2017YFB0801900); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068271",
    "Author Keywords": "Deep learning;data privacy",
    "IEEE Terms": "Data privacy;Artificial neural networks;Data models;Computational modeling;Data security;Machine learning;Privacy",
    "Mesh_Terms": "",
    "Article Citation Count": 3,
    "Patent Citation Count": null,
    "Reference Count": 5,
    "License": "IEEE",
    "Online Date": "15-Apr-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Analyzing Android Encrypted Network Traffic to Identify User Actions",
    "Authors": "M. Conti; L. V. Mancini; R. Spolaor; N. V. Verde",
    "Author Affiliations": "Dipartimento di Matematica, Universit\u221a\u2020 di Padova, Padua, Italy; Dipartimento di Informatica, Sapienza Universit\u221a\u2020 di Roma, Rome, Italy; Dipartimento di Matematica, Universit\u221a\u2020 di Padova, Padua, Italy; Dipartimento di Informatica, Sapienza Universit\u221a\u2020 di Roma, Rome, Italy",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "19-May-17",
    "Publication Year": 2016,
    "Volume": 11,
    "Issue": 1,
    "Start Page": 114,
    "End Page": 125,
    "Abstract": "Mobile devices can be maliciously exploited to violate the privacy of people. In most attack scenarios, the adversary takes the local or remote control of the mobile device, by leveraging a vulnerability of the system, hence sending back the collected information to some remote web service. In this paper, we consider a different adversary, who does not interact actively with the mobile device, but he is able to eavesdrop the network traffic of the device from the network side (e.g., controlling a Wi-Fi access point). The fact that the network traffic is often encrypted makes the attack even more challenging. In this paper, we investigate to what extent such an external attacker can identify the specific actions that a user is performing on her mobile apps. We design a system that achieves this goal using advanced machine learning techniques. We built a complete implementation of this system, and we also run a thorough set of experiments, which show that our attack can achieve accuracy and precision higher than 95%, for most of the considered actions. We compared our solution with the three state-of-the-art algorithms, and confirming that our system outperforms all these direct competitors.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2015.2478741",
    "Funding Information": "TENACE PRIN Project through the Italian Ministry of Education, University and Research(grant numbers:20103P34XC); European Commission Directorate General Home Affairs through the GAINS Project(grant numbers:HOME/2013/CIPS/AG/4000005057); European Commission through the H2020 SUNFISH Project(grant numbers:644666); EU-India REACH Project(grant numbers:ICI+/2014/342-896); Project entitled Tackling Mobile Malware with Innovative Machine Learning Techniques through the University of Padua; Marie Curie Fellowship through the European Commission(grant numbers:PCIG11-GA-2012-321980); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7265055",
    "Author Keywords": "Cellular phones;information security;privacy",
    "IEEE Terms": "Time series analysis;Cryptography;Privacy;IP networks;Mobile handsets;Machine learning algorithms;Mobile communication",
    "Mesh_Terms": "",
    "Article Citation Count": 191,
    "Patent Citation Count": null,
    "Reference Count": 43,
    "License": "IEEE",
    "Online Date": "14-Sep-15",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Machine-Learning Attacks on PolyPUFs, OB-PUFs, RPUFs, LHS-PUFs, and PUF\u201a\u00c4\u00ecFSMs",
    "Authors": "J. Delvaux",
    "Author Affiliations": "imec-COSIC, KU Leuven, Leuven, Belgium",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "8-May-19",
    "Publication Year": 2019,
    "Volume": 14,
    "Issue": 8,
    "Start Page": 2043,
    "End Page": 2058,
    "Abstract": "A physically unclonable function (PUF) is a circuit of which the input-output behavior is designed to be sensitive to the random variations of its manufacturing process. This building block hence facilitates the authentication of any given device in a population of identically laid-out silicon chips, similar to the biometric authentication of a human. The focus and novelty of this paper is the development of efficient impersonation attacks on the following five Arbiter PUF-based authentication protocols: 1) the so-called Poly PUF protocol of Konigsmark et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2016; 2) the so-called OB-PUF protocol of Gao et al. as presented at the IEEE Conference PerCom 2016; 3) the so-called RPUF protocol of Ye et al. as presented at the IEEE Conference AsianHOST 2016; 4) the so-called LHS-PUF protocol of Idriss and Bayoumi as presented at the IEEE Conference RFID-TA 2017; and 5) the so-called PUF-FSM protocol of Gao et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2018. The common flaw of all five designs is that the use of lightweight obfuscation logic provides insufficient protection against machine-learning attacks.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2019.2891223",
    "Funding Information": "KU Leuven(grant numbers:C16/15/058); European Research Council (ERC)(grant numbers:695305); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603753",
    "Author Keywords": "Physically unclonable function;machine learning;entity authentication",
    "IEEE Terms": "Protocols;Machine learning;Authentication;Delays;Tin;Sociology;Statistics",
    "Mesh_Terms": "",
    "Article Citation Count": 114,
    "Patent Citation Count": null,
    "Reference Count": 38,
    "License": "IEEE",
    "Online Date": "6-Jan-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications",
    "Authors": "B. Flowers; R. M. Buehrer; W. C. Headley",
    "Author Affiliations": "Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "5-Dec-19",
    "Publication Year": 2020,
    "Volume": 15,
    "Issue": null,
    "Start Page": 1102,
    "End Page": 1113,
    "Abstract": "Recent advancements in radio frequency machine learning (RFML) have demonstrated the use of raw in-phase and quadrature (IQ) samples for multiple spectrum sensing tasks. Yet, deep learning techniques have been shown, in other applications, to be vulnerable to adversarial machine learning (ML) techniques, which seek to craft small perturbations that are added to the input to cause a misclassification. The current work differentiates the threats that adversarial ML poses to RFML systems based on where the attack is executed from: direct access to classifier input, synchronously transmitted over the air (OTA), or asynchronously transmitted from a separate device. Additionally, the current work develops a methodology for evaluating adversarial success in the context of wireless communications, where the primary metric of interest is bit error rate and not human perception, as is the case in image recognition. The methodology is demonstrated using the well known Fast Gradient Sign Method to evaluate the vulnerabilities of raw IQ based Automatic Modulation Classification and concludes RFML is vulnerable to adversarial examples, even in OTA attacks. However, RFML domain specific receiver effects, which would be encountered in an OTA attack, can present significant impairments to adversarial evasion.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2019.2934069",
    "Funding Information": "Bradley Masters Fellowship through the Bradley Department of Electrical and Computer Engineering at Virginia Tech; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792120",
    "Author Keywords": "Cognitive radio security;machine learning;modulation classification",
    "IEEE Terms": "Perturbation methods;Receivers;Transmitters;Wireless communication;Modulation",
    "Mesh_Terms": "",
    "Article Citation Count": 77,
    "Patent Citation Count": null,
    "Reference Count": 34,
    "License": "IEEE",
    "Online Date": "8-Aug-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "High Intrinsic Dimensionality Facilitates Adversarial Attack: Theoretical Evidence",
    "Authors": "L. Amsaleg; J. Bailey; A. Barbe; S. M. Erfani; T. Furon; M. E. Houle; M. Radovanovi\u0192\u00e1; X. V. Nguyen",
    "Author Affiliations": "Inria, CNRS, IRISA, Campus de Beaulieu, Univ Rennes, Rennes, France; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Laboratoire de Physique, \u221a\u00e2cole Normale Sup\u221a\u00a9rieure de Lyon, Lyon, France; School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Inria, CNRS, IRISA, Campus de Beaulieu, Univ Rennes, Rennes, France; National Institute of Informatics, Tokyo, Japan; Faculty of Sciences, University of Novi Sad, Novi Sad, Serbia; NVIDIA Corporation, Santa Clara, CA, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "7-Oct-20",
    "Publication Year": 2021,
    "Volume": 16,
    "Issue": null,
    "Start Page": 854,
    "End Page": 865,
    "Abstract": "Machine learning systems are vulnerable to adversarial attack. By applying to the input object a small, carefully-designed perturbation, a classifier can be tricked into making an incorrect prediction. This phenomenon has drawn wide interest, with many attempts made to explain it. However, a complete understanding is yet to emerge. In this paper we adopt a slightly different perspective, still relevant to classification. We consider retrieval, where the output is a set of objects most similar to a user-supplied query object, corresponding to the set of k-nearest neighbors. We investigate the effect of adversarial perturbation on the ranking of objects with respect to a query. Through theoretical analysis, supported by experiments, we demonstrate that as the intrinsic dimensionality of the data domain rises, the amount of perturbation required to subvert neighborhood rankings diminishes, and the vulnerability to adversarial attack rises. We examine two modes of perturbation of the query: either `closer' to the target point, or `farther' from it. We also consider two perspectives: `query-centric', examining the effect of perturbation on the query's own neighborhood ranking, and `target-centric', considering the ranking of the query point in the target's neighborhood set. All four cases correspond to practical scenarios involving classification and retrieval.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2020.3023274",
    "Funding Information": "European(grant numbers:CHIST-ERA ID_IOT); Australian Research Council(grant numbers:DP140101969); ANR-AID Chaire SAIDA; JSPS Kakenhi Kiban (B) Research(grant numbers:18H03296); Serbian National Project(grant numbers:OI174023); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194069",
    "Author Keywords": "Adversarial attack;intrinsic dimensionality;nearest neighbor",
    "IEEE Terms": "Perturbation methods;Feature extraction;Machine learning;Neural networks;Databases;Content-based retrieval;Learning systems",
    "Mesh_Terms": "",
    "Article Citation Count": 15,
    "Patent Citation Count": null,
    "Reference Count": 51,
    "License": "IEEE",
    "Online Date": "10-Sep-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision",
    "Authors": "X. Ji; Y. Cheng; Y. Zhang; K. Wang; C. Yan; W. Xu; K. Fu",
    "Author Affiliations": "Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Ubiquitous System Security Lab (USSLAB), Zhejiang University; Security and Privacy Research Group (SPQR), University of Michigan",
    "Publication Title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "26-Aug-21",
    "Publication Year": 2021,
    "Volume": null,
    "Issue": null,
    "Start Page": 160,
    "End Page": 175,
    "Abstract": "Autonomous vehicles increasingly exploit computer-vision-based object detection systems to perceive environments and make critical driving decisions. To increase the quality of images, image stabilizers with inertial sensors are added to alleviate image blurring caused by camera jitters. However, such a trend opens a new attack surface. This paper identifies a system-level vulnerability resulting from the combination of the emerging image stabilizer hardware susceptible to acoustic manipulation and the object detection algorithms subject to adversarial examples. By emitting deliberately designed acoustic signals, an adversary can control the output of an inertial sensor, which triggers unnecessary motion compensation and results in a blurred image, even if the camera is stable. The blurred images can then induce object misclassification affecting safety-critical decision making. We model the feasibility of such acoustic manipulation and design an attack framework that can accomplish three types of attacks, i.e., hiding, creating, and altering objects. Evaluation results demonstrate the effectiveness of our attacks against four academic object detectors (YOLO V3/V4/V5 and Fast R-CNN), and one commercial detector (Apollo). We further introduce the concept of AMpLe attacks, a new class of system-level security vulnerabilities resulting from a combination of adversarial machine learning and physics-based injection of information-carrying signals into hardware.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-7281-8934-5",
    "DOI": "10.1109/SP40001.2021.00091",
    "Funding Information": "Analog Devices; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519394",
    "Author Keywords": "",
    "IEEE Terms": "Computer vision;Inertial sensors;Object detection;Detectors;Cameras;Acoustics;Hardware",
    "Mesh_Terms": "",
    "Article Citation Count": 14,
    "Patent Citation Count": null,
    "Reference Count": 59,
    "License": "IEEE",
    "Online Date": "26-Aug-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Homogeneous and Heterogeneous Feed-Forward XOR Physical Unclonable Functions",
    "Authors": "S. V. S. Avvaru; Z. Zeng; K. K. Parhi",
    "Author Affiliations": "Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "7-Feb-20",
    "Publication Year": 2020,
    "Volume": 15,
    "Issue": null,
    "Start Page": 2485,
    "End Page": 2498,
    "Abstract": "Physical unclonable functions (PUFs) are hardware security primitives that are used for device authentication and cryptographic key generation. Standard XOR PUFs typically contain multiple standard arbiter PUFs as components, and are more secure than standard arbiter PUFs or feed-forward (FF) arbiter PUFs (FF PUFs). This paper proposes design of feed-forward XOR PUFs (FFXOR PUFs) where each component PUF is a FF PUF. Various homogeneous and heterogeneous FFXOR PUFs are presented and evaluated in terms of four fundamental properties of PUFs: uniqueness, attack-resistance, reliability and randomness. Certain key issues pertaining to XOR PUFs such as their vulnerability to machine learning attacks and instability in responses are investigated. Other important challenges like the lack of uniqueness in FF PUFs and the asymmetry in FPGA arbiter PUFs are addressed and it is shown that FFXOR PUFs can naturally overcome these problems. It is shown that heterogeneous FFXOR PUFs (i.e., FFXOR PUFs with non-identical components) can be resilient to state-of-the-art machine learning attacks. We also present systematic reliability analysis of FFXOR PUFs and demonstrate that soft-response thresholding can be used as an effective countermeasure to overcome the degraded reliability bottleneck. Observations from simulations are further verified through hardware implementation of 64-bit FFXOR PUFs on Xilinx Artix-7 FPGA.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2020.2968113",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963980",
    "Author Keywords": "Hardware security;arbiter PUF;feed-forward PUF;XOR PUF;FPGA PUF;FFXOR PUFs;homogeneous;heterogeneous;reliability;uniqueness;security;attack-resistance;randomness",
    "IEEE Terms": "Reliability;Standards;Delays;Field programmable gate arrays;Physical unclonable function;Machine learning",
    "Mesh_Terms": "",
    "Article Citation Count": 51,
    "Patent Citation Count": null,
    "Reference Count": 53,
    "License": "IEEE",
    "Online Date": "20-Jan-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Optimal Adversarial Policies in the Multiplicative Learning System With a Malicious Expert",
    "Authors": "S. R. Etesami; N. Kiyavash; V. Leon; H. V. Poor",
    "Author Affiliations": "Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana\u201a\u00c4\u00ecChampaign, Urbana, IL, USA; College of Management of Technology, EPFL, Lausanne, Switzerland; Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana\u201a\u00c4\u00ecChampaign, Urbana, IL, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "10-Feb-21",
    "Publication Year": 2021,
    "Volume": 16,
    "Issue": null,
    "Start Page": 2276,
    "End Page": 2287,
    "Abstract": "We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts' advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. The system's loss is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of 1+O\u201a\u00e0\u00f6(ln N)/N, where N is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in O(N3). We also discuss a generalization of our model to multi-expert settings. Our results provide a new direction for vulnerability assessment of commonly-used learning algorithms to internal adversarial attacks.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2021.3052360",
    "Funding Information": "U.S. National Science Foundation(grant numbers:EPCN-1944403,CCF-1908308); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328196",
    "Author Keywords": "Adversarial learning;expert advice;Markov decision process;dynamic programming;approximation ratio",
    "IEEE Terms": "Learning systems;Prediction algorithms;Machine learning algorithms;Motion pictures;Approximation algorithms;Analytical models;Training",
    "Mesh_Terms": "",
    "Article Citation Count": 1,
    "Patent Citation Count": null,
    "Reference Count": 23,
    "License": "IEEE",
    "Online Date": "18-Jan-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems",
    "Authors": "J. Jin; E. McMurtry; B. I. P. Rubinstein; O. Ohrimenko",
    "Author Affiliations": "School of Computing and Information Systems, The University of Melbourne; Department of Computer Science, ETH Zurich; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne",
    "Publication Title": "2022 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "27-Jul-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 473,
    "End Page": 488,
    "Abstract": "Differential privacy is a de facto privacy framework that has seen adoption in practice via a number of mature software platforms. Implementation of differentially private (DP) mechanisms has to be done carefully to ensure end-to-end security guarantees. In this paper we study two implementation flaws in the noise generation commonly used in DP systems. First we examine the Gaussian mechanism\u201a\u00c4\u00f4s susceptibility to a floating-point representation attack. The premise of this first vulnerability is similar to the one carried out by Mironov in 2011 against the Laplace mechanism. Our experiments show the attack\u201a\u00c4\u00f4s success against DP algorithms, including deep learning models trained using differentially-private stochastic gradient descent. In the second part of the paper we study discrete counterparts of the Laplace and Gaussian mechanisms that were previously proposed to alleviate the shortcomings of floating-point representation of real numbers. We show that such implementations unfortunately suffer from another side channel: a novel timing attack. An observer that can measure the time to draw (discrete) Laplace or Gaussian noise can predict the noise magnitude, which can then be used to recover sensitive attributes. This attack invalidates differential privacy guarantees of systems implementing such mechanisms. We demonstrate that several commonly used, state-of-the-art implementations of differential privacy are susceptible to these attacks. We report success rates up to 92.56% for floating point attacks on DP-SGD, and up to 99.65% for end-to-end timing attacks on private sum protected with discrete Laplace. Finally, we evaluate and suggest partial mitigations.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-6654-1316-9",
    "DOI": "10.1109/SP46214.2022.9833672",
    "Funding Information": "University of Melbourne; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833672",
    "Author Keywords": "Differential-Privacy;Timing-Side-Channel;Floating-Point-Representation;Gaussian-Mechanisms;Laplace-Mechanisms",
    "IEEE Terms": "Differential privacy;Privacy;Sensitivity;Stochastic processes;Observers;Libraries;Timing",
    "Mesh_Terms": "",
    "Article Citation Count": 3,
    "Patent Citation Count": null,
    "Reference Count": 55,
    "License": "IEEE",
    "Online Date": "27-Jul-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "RoFL: Robustness of Secure Federated Learning",
    "Authors": "H. Lycklama; L. Burkhalter; A. Viand; N. K\u221a\u00bachler; A. Hithnawi",
    "Author Affiliations": "ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich",
    "Publication Title": "2023 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "21-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 453,
    "End Page": 476,
    "Abstract": "Even though recent years have seen many attacks exposing severe vulnerabilities in Federated Learning (FL), a holistic understanding of what enables these attacks and how they can be mitigated effectively is still lacking. In this work, we demystify the inner workings of existing (targeted) attacks. We provide new insights into why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the need for ML algorithms to memorize tail data has significant implications for FL integrity. This phenomenon has largely been studied in the context of privacy; our analysis sheds light on its implications for ML integrity. We show that certain classes of severe attacks can be mitigated effectively by enforcing constraints such as norm bounds on clients\u201a\u00c4\u00f4 updates. We investigate how to efficiently incorporate these constraints into secure FL protocols in the single-server setting. Based on this, we propose RoFL, a new secure FL system that extends secure aggregation with privacy-preserving input validation. Specifically, RoFL can enforce constraints such as L2 and L\u201a\u00e0\u00fb bounds on high-dimensional encrypted model updates.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-6654-9336-9",
    "DOI": "10.1109/SP46215.2023.10179400",
    "Funding Information": "Semiconductor Research Corporation; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179400",
    "Author Keywords": "federated-learning;secure-aggregation;privacy-preserving-machine-learning",
    "IEEE Terms": "Privacy;Protocols;Federated learning;Scalability;Aggregates;Bandwidth;Tail",
    "Mesh_Terms": "",
    "Article Citation Count": 6,
    "Patent Citation Count": null,
    "Reference Count": 99,
    "License": "IEEE",
    "Online Date": "21-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval",
    "Authors": "X. Yuan; Z. Zhang; X. Wang; L. Wu",
    "Author Affiliations": "School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Swansea University, Swansea, U.K.",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "1-Aug-23",
    "Publication Year": 2023,
    "Volume": 18,
    "Issue": null,
    "Start Page": 4681,
    "End Page": 4694,
    "Abstract": "Deep hashing has been intensively studied and successfully applied in large-scale image retrieval systems due to its efficiency and effectiveness. Recent studies have recognized that the existence of adversarial examples poses a security threat to deep hashing models, that is, adversarial vulnerability. Notably, it is challenging to efficiently distill reliable semantic representatives for deep hashing to guide adversarial learning, and thereby it hinders the enhancement of adversarial robustness of deep hashing-based retrieval models. Moreover, current researches on adversarial training for deep hashing are hard to be formalized into a unified minimax structure. In this paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the adversarial robustness of deep hashing models. Specifically, we conceive a discriminative mainstay features learning (DMFL) scheme to construct semantic representatives for guiding adversarial learning in deep hashing. Particularly, our DMFL with the strict theoretical guarantee is adaptively optimized in a discriminative learning manner, where both discriminative and semantic properties are jointly considered. Moreover, adversarial examples are fabricated by maximizing the Hamming distance between the hash codes of adversarial samples and mainstay features, the efficacy of which is validated in the adversarial attack trials. Further, we, for the first time, formulate the formalized adversarial training of deep hashing into a unified minimax optimization under the guidance of the generated mainstay codes. Extensive experiments on benchmark datasets show superb attack performance against the state-of-the-art algorithms, meanwhile, the proposed adversarial training can effectively eliminate adversarial perturbations for trustworthy deep hashing-based retrieval. Our code is available at https://github.com/xandery-geek/SAAT.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2023.3297791",
    "Funding Information": "Shenzhen Science and Technology Program(grant numbers:RCYX20221008092852077); National Natural Science Foundation of China(grant numbers:62002085); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515010057); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10189878",
    "Author Keywords": "Adversarial attack;adversarial training;trustworthy deep hashing;similarity retrieval",
    "IEEE Terms": "Codes;Training;Semantics;Adversarial machine learning;Task analysis;Robustness;Perturbation methods",
    "Mesh_Terms": "",
    "Article Citation Count": 2,
    "Patent Citation Count": null,
    "Reference Count": 50,
    "License": "IEEE",
    "Online Date": "21-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
    "Authors": "X. Zhang; H. Ye; Z. Huang; X. Ye; Y. Cao; Y. Zhang; M. Yang",
    "Author Affiliations": "Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China; Johns Hopkins University, Baltimore, USA; Fudan University, Shanghai, China; Fudan University, Shanghai, China",
    "Publication Title": "2023 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "21-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 934,
    "End Page": 950,
    "Abstract": "Face Verification Systems (FVSes) are more and more deployed by real-world mobile applications (apps) to verify a human\u201a\u00c4\u00f4s claimed identity. One popular type of FVSes is called cross-side FVS (XFVS), which splits the FVS functionality into two sides: one at a mobile phone to take pictures or videos and the other at a trusted server for verification. Prior works have studied the security of XFVSes from the machine learning perspective, i.e., whether the learning models used by XFVSes are robust to adversarial attacks. However, the security of other parts of XFVSes, especially the design and implementation of the verification procedure used by XFVSes, is not well understood.In this paper, we conduct the first measurement study on the security of real-world XFVSes used by popular mobile apps from a system perspective. More specifically, we design and implement a semi-automated system, called XFVSChecker, to detect XFVSes in mobile apps and then inspect their compliance with four security properties. Our evaluation reveals that most of existing XFVS apps, including those with billions of downloads, are vulnerable to at least one of four types of attacks. These attacks require only easily available attack prerequisites, such as one photo of the victim, to pose significant security risks, including complete account takeover, identity fraud and financial loss. Our findings result in 14 Chinese National Vulnerability Database (CNVD) IDs and one of them, particularly CNVD-2021-86899, is awarded the most valuable vulnerability in 2021 among all the reported vulnerabilities to CNVD.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-6654-9336-9",
    "DOI": "10.1109/SP46215.2023.10179474",
    "Funding Information": "Research and Development; National Natural Science Foundation of China; Shanghai Rising-Star Program; National Science Foundation; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179474",
    "Author Keywords": "Mobile-Security;Face-Verification;System-Perspective",
    "IEEE Terms": "Privacy;Machine learning;Mobile applications;Internet;Fraud;Security;Servers",
    "Mesh_Terms": "",
    "Article Citation Count": 1,
    "Patent Citation Count": null,
    "Reference Count": 87,
    "License": "IEEE",
    "Online Date": "21-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Deep Face Representations for Differential Morphing Attack Detection",
    "Authors": "U. Scherhag; C. Rathgeb; J. Merkle; C. Busch",
    "Author Affiliations": "da/sec\u201a\u00c4\u00eeBiometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; da/sec\u201a\u00c4\u00eeBiometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; Security Networks AG, Essen, Germany; da/sec\u201a\u00c4\u00eeBiometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "3-Jul-20",
    "Publication Year": 2020,
    "Volume": 15,
    "Issue": null,
    "Start Page": 3625,
    "End Page": 3639,
    "Abstract": "The vulnerability of facial recognition systems to face morphing attacks is well known. Many different approaches for morphing attack detection (MAD) have been proposed in the scientific literature. However, the MAD algorithms proposed so far have mostly been trained and tested on datasets whose distributions of image characteristics are either very limited (e.g., only created with a single morphing tool) or rather unrealistic (e.g., no print-scan transformation). As a consequence, these methods easily overfit on certain image types and the results presented cannot be expected to apply to real-world scenarios. For example, the results of the latest NIST FRVT MORPH show that the majority of submitted MAD algorithms lacks robustness and performance when considering unseen and challenging datasets. In this work, subsets of the FERET and FRGCv2 face databases are used to create a realistic database for training and testing of MAD algorithms, containing a large number of ICAO-compliant bona fide facial images, corresponding unconstrained probe images, and morphed images created with four different face morphing tools. Furthermore, multiple post-processings are applied on the reference images, e.g., print-scan and JPEG2000 compression. On this database, previously proposed differential morphing algorithms are evaluated and compared. In addition, the application of deep face representations for differential MAD algorithms is investigated. It is shown that algorithms based on deep face representations can achieve very high detection performance (less than 3% D-EER) and robustness with respect to various post-processings. Finally, the limitations of the developed methods are analyzed.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2020.2994750",
    "Funding Information": "German Federal Ministry of Education and Research; Hessen State Ministry for Higher Education, Research and the Arts within their joint support of the ATHENE (National Research Center for Applied Cybersecurity); Federal Office of Information Security (BSI) through the FACETRUST Project; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093905",
    "Author Keywords": "Biometrics;face recognition;morphing attacks;morphing attack detection;differential attack detection;deep face representation",
    "IEEE Terms": "Face;Databases;Probes;Face recognition;Feature extraction;Neural networks;Forensics",
    "Mesh_Terms": "",
    "Article Citation Count": 75,
    "Patent Citation Count": null,
    "Reference Count": 56,
    "License": "CCBY",
    "Online Date": "14-May-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "SAFELearning: Secure Aggregation in Federated Learning With Backdoor Detectability",
    "Authors": "Z. Zhang; J. Li; S. Yu; C. Makaya",
    "Author Affiliations": "Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; Electrical and Computer Engineering Department, Stevens Institute of Technology, Hoboken, NJ, USA; HP Inc., Palo Alto, CA, USA",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "8-Jun-23",
    "Publication Year": 2023,
    "Volume": 18,
    "Issue": null,
    "Start Page": 3289,
    "End Page": 3304,
    "Abstract": "For model privacy, local model parameters in federated learning shall be obfuscated before sent to the remote aggregator. This technique is referred to as secure aggregation. However, secure aggregation makes model poisoning attacks such as backdooring more convenient given that existing anomaly detection methods mostly require access to plaintext local models. This paper proposes a new federated learning technique SAFELearning to support backdoor detection for secure aggregation. We achieve this through two new primitives -oblivious random grouping (ORG) and partial parameter disclosure (PPD). ORG partitions participants into one-time random subgroups with group configurations oblivious to participants; PPD allows secure partial disclosure of aggregated subgroup models for anomaly detection without leaking individual model privacy. ORG is based on our construction of several new primitives including tree-based random subgroup generation, oblivious secure aggregation, and randomized Diffie-Hellman key exchange. ORG can thwart colluding attackers from knowing each other\u201a\u00c4\u00f4s group membership assignment with non-negligible advantage than random guess. Backdoor attacks are detected based on statistical distributions of the subgroup aggregated parameters of the learning iterations. SAFELearning can significantly reduce backdoor model accuracy without jeopardizing the main task accuracy under common backdoor strategies. Extensive experiments show SAFELearning is robust against malicious and faulty participants, whilst being more efficient than the state-of-art secure aggregation protocol in terms of both communication and computation costs.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2023.3280032",
    "Funding Information": "NSF(grant numbers:ECCS#1923739); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136231",
    "Author Keywords": "Federated learning;secure aggregation;backdoor attack;machine learning",
    "IEEE Terms": "Federated learning;Data models;Computational modeling;Privacy;Servers;Cryptography;Protocols",
    "Mesh_Terms": "",
    "Article Citation Count": 1,
    "Patent Citation Count": null,
    "Reference Count": 51,
    "License": "IEEE",
    "Online Date": "25-May-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Byzantines Can Also Learn From History: Fall of Centered Clipping in Federated Learning",
    "Authors": "K. \u221a\u00f1zfatura; E. \u221a\u00f1zfatura; A. K\u221a\u00bap\u221a\u00df\u221a\u00ba; D. Gunduz",
    "Author Affiliations": "KUIS AI Center, Ko\u221a\u00df University, \u0192\u221eIstanbul, Turkey; IPC Laboratory, Imperial College London, London, U.K; Department of Computer Engineering, Ko\u221a\u00df University, \u0192\u221eIstanbul, Turkey; IPC Laboratory, Imperial College London, London, U.K",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "3-Jan-24",
    "Publication Year": 2024,
    "Volume": 19,
    "Issue": null,
    "Start Page": 2010,
    "End Page": 2022,
    "Abstract": "The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients\u201a\u00c4\u00f4 models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this work, we first expose vulnerabilities of the CC framework, and introduce a novel attack strategy that can circumvent the defences of CC and other robust aggregators and reduce their test accuracy up to %33 on best-case scenarios in image classification tasks. Then, we propose a new robust and fast defence mechanism that is effective against the proposed and other existing Byzantine attacks.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2023.3345171",
    "Funding Information": "T\u221a\u00faB\u0192\u221eITAK; Scientific and Technological Research Council of Turkey(grant numbers:119E088); UK Research and Innovation (UKRI) for the Project \u201a\u00c4\u00faAIR\u201a\u00c4\u00f9 (European Research Council (ERC)-Consolidator Grant)(grant numbers:EP/X030806/1); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366296",
    "Author Keywords": "Federated learning;adversarial machine learning;deep learning",
    "IEEE Terms": "Task analysis;Robustness;Federated learning;Security;Training;Aggregates;Taxonomy",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 58,
    "License": "IEEE",
    "Online Date": "19-Dec-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model",
    "Authors": "X. Ling; S. Ji; J. Zou; J. Wang; C. Wu; B. Li; T. Wang",
    "Author Affiliations": "Zhejiang University; Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies; Zhejiang University; Zhejiang University; Zhejiang University; UIUC; Lehigh University",
    "Publication Title": "2019 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "16-Sep-19",
    "Publication Year": 2019,
    "Volume": null,
    "Issue": null,
    "Start Page": 673,
    "End Page": 690,
    "Abstract": "Deep learning (DL) models are inherently vulnerable to adversarial examples \u201a\u00c4\u00ec maliciously crafted inputs to trigger target DL models to misbehave \u201a\u00c4\u00ec which significantly hinders the application of DL in security-sensitive domains. Intensive research on adversarial learning has led to an arms race between adversaries and defenders. Such plethora of emerging attacks and defenses raise many questions: Which attacks are more evasive, preprocessing-proof, or transferable? Which defenses are more effective, utility-preserving, or general? Are ensembles of multiple defenses more robust than individuals? Yet, due to the lack of platforms for comprehensive evaluation on adversarial attacks and defenses, these critical questions remain largely unsolved. In this paper, we present the design, implementation, and evaluation of DEEPSEC, a uniform platform that aims to bridge this gap. In its current implementation, DEEPSEC incorporates 16 state-of-the-art attacks with 10 attack utility metrics, and 13 state-of-the-art defenses with 5 defensive utility metrics. To our best knowledge, DEEPSEC is the first platform that enables researchers and practitioners to (i) measure the vulnerability of DL models, (ii) evaluate the effectiveness of various attacks/defenses, and (iii) conduct comparative studies on attacks/defenses in a comprehensive and informative manner. Leveraging DEEPSEC, we systematically evaluate the existing adversarial attack and defense methods, and draw a set of key findings, which demonstrate DEEPSEC\u201a\u00c4\u00f4s rich functionality, such as (1) the trade-off between misclassification and imperceptibility is empirically confirmed; (2) most defenses that claim to be universally applicable can only defend against limited types of attacks under restricted settings; (3) it is not necessary that adversarial examples with higher perturbation magnitude are easier to be detected; (4) the ensemble of multiple defenses cannot improve the overall defense capability, but can improve the lower bound of the defense effectiveness of individuals. Extensive analysis on DEEPSEC demonstrates its capabilities and advantages as a benchmark platform which can benefit future adversarial learning research.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-5386-6660-9",
    "DOI": "10.1109/SP.2019.00023",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835375",
    "Author Keywords": "Deep-Learning;Adversarial-machine-learning;benchmark-platform",
    "IEEE Terms": "Measurement;Perturbation methods;Security;Robustness;Terminology;Jacobian matrices;Training",
    "Mesh_Terms": "",
    "Article Citation Count": 66,
    "Patent Citation Count": null,
    "Reference Count": 67,
    "License": "IEEE",
    "Online Date": "16-Sep-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers",
    "Authors": "B. Bichsel; S. Steffen; I. Bogunovic; M. Vechev",
    "Author Affiliations": "ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland",
    "Publication Title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "26-Aug-21",
    "Publication Year": 2021,
    "Volume": null,
    "Issue": null,
    "Start Page": 391,
    "End Page": 409,
    "Abstract": "We present DP-Sniper, a practical black-box method that automatically finds violations of differential privacy.DP-Sniper is based on two key ideas: (i) training a classifier to predict if an observed output was likely generated from one of two possible inputs, and (ii) transforming this classifier into an approximately optimal attack on differential privacy.Our experimental evaluation demonstrates that DP-Sniper obtains up to 12.4 times stronger guarantees than state-of-the-art, while being 15.5 times faster. Further, we show that DP-Sniper is effective in exploiting floating-point vulnerabilities of naively implemented algorithms: it detects that a supposedly 0.1-differentially private implementation of the Laplace mechanism actually does not satisfy even 0.25-differential privacy.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-7281-8934-5",
    "DOI": "10.1109/SP40001.2021.00081",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519405",
    "Author Keywords": "differential privacy;differential distinguishability;inference attacks;machine learning;classifiers",
    "IEEE Terms": "Training;Privacy;Differential privacy;Approximation algorithms;Classification algorithms;Security",
    "Mesh_Terms": "",
    "Article Citation Count": 7,
    "Patent Citation Count": null,
    "Reference Count": 50,
    "License": "IEEE",
    "Online Date": "26-Aug-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Runtime Recovery of Web Applications under Zero-Day ReDoS Attacks",
    "Authors": "Z. Bai; K. Wang; H. Zhu; Y. Cao; X. Jin",
    "Author Affiliations": "Johns Hopkins University; Peking University; Johns Hopkins University; Johns Hopkins University; Peking University",
    "Publication Title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "26-Aug-21",
    "Publication Year": 2021,
    "Volume": null,
    "Issue": null,
    "Start Page": 1575,
    "End Page": 1588,
    "Abstract": "Regular expression denial of service (ReDoS)\u201a\u00c4\u00ee which exploits the super-linear running time of matching regular expressions against carefully crafted inputs\u201a\u00c4\u00eeis an emerging class of DoS attacks to web services. One challenging question for a victim web service under ReDoS attacks is how to quickly recover its normal operation after ReDoS attacks, especially these zero-day ones exploiting previously unknown vulnerabilities.In this paper, we present RegexNet, the first payload-based, automated, reactive ReDoS recovery system for web services. RegexNet adopts a learning model, which is updated constantly in a feedback loop during runtime, to classify payloads of upcoming requests including the request contents and database query responses. If detected as a cause leading to ReDoS, RegexNet migrates those requests to a sandbox and isolates their execution for a fast, first-measure recovery.We have implemented a RegexNet prototype and integrated it with HAProxy and Node.js. Evaluation results show that RegexNet is effective in recovering the performance of web services against zero-day ReDoS attacks, responsive on reacting to attacks in sub-minute, and resilient to different ReDoS attack types including adaptive ones that are designed to evade RegexNet on purpose.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-7281-8934-5",
    "DOI": "10.1109/SP40001.2021.00077",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519496",
    "Author Keywords": "Regular expression Denial of Service (ReDoS);Deep Neural Networks;Adversarial Machine Learning;Online Feedback Loop",
    "IEEE Terms": "Feedback loop;Privacy;Runtime;Databases;Prototypes;Web servers;Data models",
    "Mesh_Terms": "",
    "Article Citation Count": 6,
    "Patent Citation Count": null,
    "Reference Count": 53,
    "License": "IEEE",
    "Online Date": "26-Aug-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "AST-SafeSec: Adaptive Stress Testing for Safety and Security Co-Analysis of Cyber-Physical Systems",
    "Authors": "N. Kaloudi; J. Li",
    "Author Affiliations": "Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway",
    "Publication Title": "IEEE Transactions on Information Forensics and Security",
    "Date Added To Xplore": "11-Sep-23",
    "Publication Year": 2023,
    "Volume": 18,
    "Issue": null,
    "Start Page": 5567,
    "End Page": 5579,
    "Abstract": "Cyber-physical systems are becoming more intelligent with the adoption of heterogeneous sensor networks and machine learning capabilities that deal with an increasing amount of input data. While this complexity aims to solve problems in various domains, it adds new challenges for the system assurance. One issue is the rise in the number of abnormal behaviors that affect system performance due to possible sensor faults and attacks. The combination of safety risks, which are usually caused by random sensor faults and security risks that can happen during any random system state, makes the full coverage testing of the cyber-physical system challenging. Existing techniques are inadequate to deal with complex safety and security co-risks against cyber-physical systems. In this paper, we propose AST-SafeSec, an analysis methodology for both safety and security aspects that utilizes reinforcement learning to identify the most likely adversarial paths at various normal or failure states of a cyber-physical system that can influence system behavior through its sensor data. The methodology is evaluated using an autonomous vehicle scenario by incorporating a security attack into the stochastic sensor elements of a vehicle. Evaluation results show that the methodology analyzes the interaction of malicious attacks with random faults and identifies the incident caused by the interactions and the most likely path that leads to the incident.",
    "ISSN": "1556-6021",
    "ISBNs": "",
    "DOI": "10.1109/TIFS.2023.3309160",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10231138",
    "Author Keywords": "Co-analysis;cyber-physical systems (CPSs);cybersecurity;safety;stress testing;validation;verification",
    "IEEE Terms": "Safety;Security;Testing;Complexity theory;Behavioral sciences;Cyber-physical systems;Cyberattack",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 57,
    "License": "IEEE",
    "Online Date": "28-Aug-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Bookworm Game: Automatic Discovery of LTE Vulnerabilities Through Documentation Analysis",
    "Authors": "Y. Chen; Y. Yao; X. Wang; D. Xu; C. Yue; X. Liu; K. Chen; H. Tang; B. Liu",
    "Author Affiliations": "Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS; Indiana University, Bloomington; CAS-KLONAT Key Laboratory of Network Assessment Technology, CAS, BKLONSPT Beijing Key Laboratory of Network Security and Protection Technology, SKLOIS State Key Laboratory of Information Security, IIE, CAS, Institute of Information Engineering, CAS",
    "Publication Title": "2021 IEEE Symposium on Security and Privacy (SP)",
    "Date Added To Xplore": "26-Aug-21",
    "Publication Year": 2021,
    "Volume": null,
    "Issue": null,
    "Start Page": 1197,
    "End Page": 1214,
    "Abstract": "In the past decade, the security of cellular networks has been increasingly under scrutiny, leading to the discovery of numerous vulnerabilities that expose the network and its users to a wide range of security risks, from denial of service to information leak. However, most of these findings have been made through ad-hoc manual analysis, which is inadequate for fundamentally enhancing the security assurance of a system as complex as the cellular network. An important observation is that the massive amount of technical documentation of cellular network can provide key insights into the protection it puts in place and help identify potential security flaws. Particularly, we found that such documentation often contains hazard indicators (HIs) \u201a\u00c4\u00ec the statement that describes a risky operation (e.g., abort an ongoing procedure) when a certain event happens at a state, which can guide a test on the system to find out whether the operation can indeed be triggered by an unauthorized party to cause harm to the cellular core or legitimate users\u201a\u00c4\u00f4 equipment. Based upon this observation, we present in this paper a new framework that makes the first step toward intelligent and systematic security analysis of cellular networks. Our approach, called Atomic, utilizes natural-language processing and machine learning techniques to scan a large amount of LTE documentation for HIs. The HIs discovered are further parsed and analyzed to recover state and event information for generating test cases. These test cases are further utilized to automatically construct tests in an LTE simulation environment, which runs the tests to detect the vulnerabilities in the LTE that allow the risky operations to happen without proper protection. In our research, we implemented Atomic and ran it on the LTE NAS specification, including 549 pages with 13,598 sentences and 283,850 words. In less than 5 hours, our prototype reported 42 vulnerabilities from 192 HIs discovered, including 10 never reported before, under two threat models. All these vulnerabilities have been confirmed through end-to-end attacks, which lead to unauthorized disruption of the LTE service a legitimate user\u201a\u00c4\u00f4s equipment receives. We reported our findings to authorized parties and received their confirmation that these vulnerabilities indeed exist in major commercial carriers and $2,000 USD reward from Google.",
    "ISSN": "2375-1207",
    "ISBNs": "978-1-7281-8934-5",
    "DOI": "10.1109/SP40001.2021.00104",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519388",
    "Author Keywords": "Cellular Network;4G;LTE;Vulnerability;Attack;Documentation Analysis;NLP",
    "IEEE Terms": "Cellular networks;Protocols;Systematics;Prototypes;Documentation;Manuals;Hazards",
    "Mesh_Terms": "",
    "Article Citation Count": 5,
    "Patent Citation Count": null,
    "Reference Count": 60,
    "License": "IEEE",
    "Online Date": "26-Aug-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "The Ethics of Going Deep: Challenges in Machine Learning for Sensitive Security Domains",
    "Authors": "A. Eusebi; M. Vasek; E. Cockbain; E. Mariconti",
    "Author Affiliations": "UCL, London, UK; UCL, London, UK; UCL, London, UK; UCL, London, UK",
    "Publication Title": "2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)",
    "Date Added To Xplore": "27-Jun-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 533,
    "End Page": 537,
    "Abstract": "Sometimes, machine learning models can determine the trajectory of human life, and a series of cascading ethical failures could be irreversible. Ethical concerns are nevertheless set to increase, in particular when the injection of algorithmic forms of decision-making occurs in highly sensitive security contexts. In cybercrime, there have been cases of algorithms that have not identified racist and hateful speeches, as well as missing the identification of Image Based Sexual Abuse cases. Hence, this paper intends to add a voice of caution on the vulnerabilities pervading the different stages of a machine learning development pipeline and the ethical challenges that these potentially nurture and perpetuate. To highlight both the issues and potential fixes in an adversarial environment, we use Child Sexual Exploitation and its implications on the Internet as a case study, being 2021 its worst year according to the Internet Watch Foundation.",
    "ISSN": "2768-0657",
    "ISBNs": "978-1-6654-9560-8",
    "DOI": "10.1109/EuroSPW55150.2022.00063",
    "Funding Information": "EPSRC(grant numbers:180330); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799317",
    "Author Keywords": "machine learning;ethics;security;online child sexual abuse",
    "IEEE Terms": "Ethics;Machine learning algorithms;Pipelines;Power system protection;Focusing;Machine learning;Internet",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 30,
    "License": "IEEE",
    "Online Date": "27-Jun-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "SoK: Security and Privacy in Machine Learning",
    "Authors": "N. Papernot; P. McDaniel; A. Sinha; M. P. Wellman",
    "Author Affiliations": "Pennsylvania State University; Pennsylvania State University; University of Michigan; University of Michigan",
    "Publication Title": "2018 IEEE European Symposium on Security and Privacy (EuroS&P)",
    "Date Added To Xplore": "9-Jul-18",
    "Publication Year": 2018,
    "Volume": null,
    "Issue": null,
    "Start Page": 399,
    "End Page": 414,
    "Abstract": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive-new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, \u221a\u2020 la PAC theory, will foster a science of security and privacy in ML.",
    "ISSN": "",
    "ISBNs": "978-1-5386-4228-3",
    "DOI": "10.1109/EuroSP.2018.00035",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8406613",
    "Author Keywords": "security;privacy;machine learning",
    "IEEE Terms": "Security;Machine learning;Data models;Training;Privacy;Computational modeling;Analytical models",
    "Mesh_Terms": "",
    "Article Citation Count": 225,
    "Patent Citation Count": 1,
    "Reference Count": 115,
    "License": "IEEE",
    "Online Date": "9-Jul-18",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Effective Machine Learning-based Access Control Administration through Unlearning",
    "Authors": "J. M. Llamas; D. Preuveneers; W. Joosen",
    "Author Affiliations": "imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium",
    "Publication Title": "2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)",
    "Date Added To Xplore": "31-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 50,
    "End Page": 57,
    "Abstract": "With the rapid and increasing complexity of computer systems and software, there is a need for more effective, scalable, and secure access control methods. Machine learning (ML) has gained popularity in complementing manually crafted authorisation policies in such environments. However, given the dynamic and constantly evolving nature of software and access control systems, the administration of the latter presents a significant security challenge. This paper examines the administration problem of Machine Learning-based Access Control (MLBAC) systems through Machine Unlearning as a lightweight and secure method. More specifically, we explore this problem through exact and approximate unlearning and evaluate its impact using real-world data. We demonstrate the effectiveness of Machine Unlearning in both reverting policies and addressing potential vulnerabilities that may emerge during the model\u201a\u00c4\u00f4s lifecycle. Compared to alternative options such as retraining from scratch, our approach reduces deployment and verification costs, making it a promising solution for MLBAC administration.",
    "ISSN": "2768-0657",
    "ISBNs": "979-8-3503-2720-5",
    "DOI": "10.1109/EuroSPW59978.2023.00011",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190682",
    "Author Keywords": "machine learning;machine unlearning;administration;access control;security",
    "IEEE Terms": "Authorization;Costs;Machine learning;Software;Data models;Complexity theory;Behavioral sciences",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 33,
    "License": "IEEE",
    "Online Date": "31-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Comments on \u201a\u00c4\u00faVERSA: Verifiable Secure Aggregation for Cross-Device Federated Learning\u201a\u00c4\u00f9",
    "Authors": "F. Luo; H. Wang; X. Yan",
    "Author Affiliations": "College of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; Department of New Networks, Peng Cheng Laboratory, Shenzhen, China; School of Computer Science, South China Normal University, Guangzhou, China",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "16-Jan-24",
    "Publication Year": 2024,
    "Volume": 21,
    "Issue": 1,
    "Start Page": 499,
    "End Page": 500,
    "Abstract": "Federated learning (FL) allows a large number of users to collaboratively train machine learning (ML) models by sending only their local gradients to a central server for aggregation in each training iteration, without sending their raw training data. The main security issues of FL, that is, the privacy of the gradient vector and the correctness verification of the aggregated gradient, are gaining increasing attention from industry and academia. To protect the privacy of the gradient, a secure aggregation was proposed; to verify the correctness of the aggregated gradient, a verifiable secure aggregation that requires the server to provide a verifiable aggregated gradient was proposed. In 2021, Hahn et al. proposed VERSA, a verifiable secure aggregation. However, in this article, we will point out a flaw in VERSA, which indicates that VERSA does not work. To address the flaw, we present several approaches with different advantages and disadvantages. We hope that by identifying the flaw, similar errors can be avoided in future designs of verifiable secure aggregation.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2023.3253082",
    "Funding Information": "PCL(grant numbers:PCL2022A03); Guangxi Natural Science Foundation(grant numbers:2022GXNSFBA035650); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061268",
    "Author Keywords": "Federated learning;machine learning;privacy-preserving;verifiable secure aggregation",
    "IEEE Terms": "Servers;Protocols;Security;Privacy;Federated learning;Public key;Data models",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 9,
    "License": "IEEE",
    "Online Date": "6-Mar-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Man-in-the-Middle Attacks Against Machine Learning Classifiers Via Malicious Generative Models",
    "Authors": "D. Wang; C. Li; S. Wen; S. Nepal; Y. Xiang",
    "Author Affiliations": "School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; CSIRO's Data 61, North Ryde, NSW, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; CSIRO's Data 61, North Ryde, NSW, Australia",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "31-Aug-21",
    "Publication Year": 2021,
    "Volume": 18,
    "Issue": 5,
    "Start Page": 2074,
    "End Page": 2087,
    "Abstract": "Deep Neural Networks (DNNs) are vulnerable to deliberately crafted adversarial examples. In the past few years, many efforts have been spent on exploring query-optimisation attacks to find adversarial examples of either black-box or white-box DNN models, as well as the defending countermeasures against those attacks. In this article, we explore vulnerabilities of DNN models under the umbrella of Man-in-the-Middle (MitM) attacks, which have not been investigated before. From the perspective of an MitM adversary, the aforementioned adversarial example attacks are not viable anymore. First, such attacks must acquire the outputs from the models multiple times before actually launching attacks, which is difficult for the MitM adversary in practice. Second, such attacks are one-off and cannot be directly generalised onto new data examples, which decreases the rate of return for the attacker. In contrast, using generative models to craft adversarial examples on the fly can mitigate the drawbacks. However, the adversarial capability of the generative models, such as Variational Auto-Encoder (VAE), has not been extensively studied. Therefore, given a classifier, we investigate using a VAE decoder to either transform benign inputs to their adversarial counterparts or decode outputs from benign VAE encoders to be adversarial examples. The proposed method can endue more capability to MitM attackers. Based on our evaluation, the proposed attack can achieve above 95 percent success rates on both MNIST and CIFAR10 datasets, which is better or comparable with state-of-the-art query-optimisation attacks. In the meantime, the attack is 104 times faster than the query-optimisation attacks.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2020.3021008",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9183938",
    "Author Keywords": "Deep neural network;adversarial example;security",
    "IEEE Terms": "Decoding;Optimization;Computational modeling;Gallium nitride;Machine learning;Transforms;Task analysis",
    "Mesh_Terms": "",
    "Article Citation Count": 10,
    "Patent Citation Count": null,
    "Reference Count": 47,
    "License": "IEEE",
    "Online Date": "1-Sep-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection",
    "Authors": "A. Demontis; M. Melis; B. Biggio; D. Maiorca; D. Arp; K. Rieck; I. Corona; G. Giacinto; F. Roli",
    "Author Affiliations": "Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Institute of System Security, Technische Universit\u221a\u00a7t Braunschweig, Rebenring 56, Braunschweig, Germany; Institute of System Security, Technische Universit\u221a\u00a7t Braunschweig, Rebenring 56, Braunschweig, Germany; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy; Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d'Armi, Cagliari, Italy",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "9-Jul-19",
    "Publication Year": 2019,
    "Volume": 16,
    "Issue": 4,
    "Start Page": 711,
    "End Page": 724,
    "Abstract": "To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2017.2700270",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917369",
    "Author Keywords": "Android malware detection;static analysis;secure machine learning;computer security",
    "IEEE Terms": "Androids;Humanoid robots;Malware;Security;Feature extraction;Tools;Algorithm design and analysis",
    "Mesh_Terms": "",
    "Article Citation Count": 139,
    "Patent Citation Count": null,
    "Reference Count": 50,
    "License": "IEEE",
    "Online Date": "2-May-17",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Enhancing Vulnerability Prioritization: Data-Driven Exploit Predictions with Community-Driven Insights",
    "Authors": "J. Jacobs; S. Romanosky; O. Suciu; B. Edwards; A. Sarabi",
    "Author Affiliations": "Cyentia Institute; RAND Corporation; University of Maryland; Cyentia Institute; University of Michigan",
    "Publication Title": "2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)",
    "Date Added To Xplore": "31-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 194,
    "End Page": 206,
    "Abstract": "The number of disclosed vulnerabilities has been steadily increasing over the years. At the same time, organizations face significant challenges patching their systems, leading to a need to prioritize vulnerability remediation in order to reduce the risk of attacks. Unfortunately, existing vulnerability scoring systems are either vendor-specific, proprietary, or are only commercially available. Moreover, these and other prioritization strategies based on vulnerability severity are poor predictors of actual vulnerability exploitation because they do not incorporate new information that might impact the likelihood of exploitation. In this paper we present the efforts behind building a Special Interest Group (SIG) that seeks to develop a completely data-driven exploit scoring system that produces scores for all known vulnerabilities, that is freely available, and which adapts to new information. The Exploit Prediction Scoring System (EPSS) SIG consists of more than 170 experts from around the world and across all industries, providing crowd-sourced expertise and feedback. Based on these collective insights, we describe the design decisions and trade-offs that lead to the development of the next version of EPSS. This new machine learning model provides an 82% performance improvement over past models in distinguishing vulnerabilities that are exploited in the wild and thus may be prioritized for remediation.",
    "ISSN": "2768-0657",
    "ISBNs": "979-8-3503-2720-5",
    "DOI": "10.1109/EuroSPW59978.2023.00027",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190703",
    "Author Keywords": "vulnerability prioritization;exploit prediction;vulnerabilities;exploits;machine learning",
    "IEEE Terms": "Industries;Soft sensors;Current measurement;Machine learning;Organizations;Predictive models;Size measurement",
    "Mesh_Terms": "",
    "Article Citation Count": 3,
    "Patent Citation Count": null,
    "Reference Count": 43,
    "License": "IEEE",
    "Online Date": "31-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "The Limitations of Deep Learning in Adversarial Settings",
    "Authors": "N. Papernot; P. McDaniel; S. Jha; M. Fredrikson; Z. B. Celik; A. Swami",
    "Author Affiliations": "Department of Computer Science and Engineering, Penn State University; Department of Computer Science and Engineering, Penn State University; Department of Computer Science and Engineering, Penn State University; Computer Sciences Department, University of Wisconsin-Madison; Department of Computer Science and Engineering, Penn State University; United States Army Research Laboratory, Adelphi, Maryland",
    "Publication Title": "2016 IEEE European Symposium on Security and Privacy (EuroS&P)",
    "Date Added To Xplore": "12-May-16",
    "Publication Year": 2016,
    "Volume": null,
    "Issue": null,
    "Start Page": 372,
    "End Page": 387,
    "Abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",
    "ISSN": "",
    "ISBNs": "978-1-5090-1752-2",
    "DOI": "10.1109/EuroSP.2016.36",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467366",
    "Author Keywords": "",
    "IEEE Terms": "Neurons;Training;Machine learning;Biological neural networks;Distortion;Force",
    "Mesh_Terms": "",
    "Article Citation Count": 1820,
    "Patent Citation Count": 7,
    "Reference Count": 37,
    "License": "IEEE",
    "Online Date": "12-May-16",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
    "Authors": "H. Yan; X. Li; H. Li; J. Li; W. Sun; F. Li",
    "Author Affiliations": "State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; Department of Computer and Information Technology, Purdue University, West Lafayette, IN, USA; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academic of Sciences, Beijing, China",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "8-Jul-22",
    "Publication Year": 2022,
    "Volume": 19,
    "Issue": 4,
    "Start Page": 2680,
    "End Page": 2694,
    "Abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2021.3069258",
    "Funding Information": "Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University(grant numbers:19151110473); Purdue University; China Scholarship Council(grant numbers:201906960075); National Natural Science Foundation of China(grant numbers:61932015); National Key R&D Project(grant numbers:2017YFB0802203); Shaanxi Innovation Team Project(grant numbers:2018TD-007); Higher Education Discipline Innovation Project(grant numbers:B16037); National Natural Science Foundation of China(grant numbers:U1836203); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389670",
    "Author Keywords": "Machine learning;model extraction attack;extraction status assessment;differential privacy;privacy budget allocation",
    "IEEE Terms": "Adaptation models;Privacy;Monitoring;Data models;Mathematical model;Training;Differential privacy",
    "Mesh_Terms": "",
    "Article Citation Count": 15,
    "Patent Citation Count": null,
    "Reference Count": 45,
    "License": "IEEE",
    "Online Date": "29-Mar-21",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "FairTest: Discovering Unwarranted Associations in Data-Driven Applications",
    "Authors": "F. Tram\u221a\u00aer; V. Atlidakis; R. Geambasu; D. Hsu; J. -P. Hubaux; M. Humbert; A. Juels; H. Lin",
    "Author Affiliations": "Stanford; Columbia University; Columbia University; Columbia University; EPFL; Saarland University; Cornell Tech, Jacobs Institute; EPFL",
    "Publication Title": "2017 IEEE European Symposium on Security and Privacy (EuroS&P)",
    "Date Added To Xplore": "3-Jul-17",
    "Publication Year": 2017,
    "Volume": null,
    "Issue": null,
    "Start Page": 401,
    "End Page": 416,
    "Abstract": "In a world where traditional notions of privacy are increasingly challenged by the myriad companies that collect and analyze our data, it is important that decision-making entities are held accountable for unfair treatments arising from irresponsible data usage. Unfortunately, a lack of appropriate methodologies and tools means that even identifying unfair or discriminatory effects can be a challenge in practice. We introduce the unwarranted associations (UA) framework, a principled methodology for the discovery of unfair, discriminatory, or offensive user treatment in data-driven applications. The UA framework unifies and rationalizes a number of prior attempts at formalizing algorithmic fairness. It uniquely combines multiple investigative primitives and fairness metrics with broad applicability, granular exploration of unfair treatment in user subgroups, and incorporation of natural notions of utility that may account for observed disparities. We instantiate the UA framework in FairTest, the first comprehensive tool that helps developers check data-driven applications for unfair user treatment. It enables scalable and statistically rigorous investigation of associations between application outcomes (such as prices or premiums) and sensitive user attributes (such as race or gender). Furthermore, FairTest provides debugging capabilities that let programmers rule out potential confounders for observed unfair effects. We report on use of FairTest to investigate and in some cases address disparate impact, offensive labeling, and uneven rates of algorithmic error in four data-driven applications. As examples, our results reveal subtle biases against older populations in the distribution of error in a predictive health application and offensive racial labeling in an image tagger.",
    "ISSN": "",
    "ISBNs": "978-1-5090-5762-7",
    "DOI": "10.1109/EuroSP.2017.29",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961993",
    "Author Keywords": "Algorithmic Fairness;Systems;Statistics",
    "IEEE Terms": "Computer bugs;Measurement;Tools;Testing;Medical services;Google;Machine learning algorithms",
    "Mesh_Terms": "",
    "Article Citation Count": 67,
    "Patent Citation Count": 1,
    "Reference Count": 59,
    "License": "IEEE",
    "Online Date": "3-Jul-17",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Sais: Self-Adaptive Identification of Security Bug Reports",
    "Authors": "S. Mostafa; B. Findley; N. Meng; X. Wang",
    "Author Affiliations": "Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Virginia Tech, Blacksburg, VA, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "8-Jul-21",
    "Publication Year": 2021,
    "Volume": 18,
    "Issue": 4,
    "Start Page": 1779,
    "End Page": 1792,
    "Abstract": "Among various bug reports (BRs), security bug reports (SBRs) are unique because they require immediate concealment and fixes. When SBRs are not identified in time, attackers can exploit the vulnerabilities. Prior work identifies SBRs via text mining, which requires a predefined keyword list and trains a classifier with known SBRs and non-security bug reports (NSBRs). The former approach is not reliable, because (1) as the contexts of security vulnerabilities and terminology of SBRs change over time, the predefined list will become out-dated; and (2) users may have insufficient SBRs for training. We introduce a semi-supervised learning-based approach, Sais, to adaptively and reliably identify SBRs. Given a project's BRs containing some labeled SBRs, many more NSBRs, and unlabeled BRs, Sais iteratively mines keywords, trains a classifier based on the keywords from the labeled data, classifies unlabeled BRs, and augments its training data with the newly labeled BRs. Our evaluation shows that Sais is useful for identifying SBRs.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2019.2939132",
    "Funding Information": "National Science Foundation(grant numbers:CNS-1748109,CCF-1846467); HRD(grant numbers:C-SPECC 1736209,DHS-14-ST-062-001); ONR(grant numbers:N00014-17-1-2498); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823031",
    "Author Keywords": "Security bug reports;self learning;bug triaging",
    "IEEE Terms": "Computer bugs;Security;Training;Data models;Databases;Semisupervised learning;Software",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 58,
    "License": "IEEE",
    "Online Date": "3-Sep-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Image Transformation-Based Defense Against Adversarial Perturbation on Deep Learning Models",
    "Authors": "A. Agarwal; R. Singh; M. Vatsa; N. Ratha",
    "Author Affiliations": "Department of Computer Science and Engineering, IIIT-Delhi, New Delhi, Delhi, India; Department of Computer Science and Engineering, IIT Jodhpur, Karwar, Rajasthan, India; Department of Computer Science and Engineering, IIT Jodhpur, Karwar, Rajasthan, India; SUNY-Buffalo, Buffalo, NY, USA",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "27-Aug-21",
    "Publication Year": 2021,
    "Volume": 18,
    "Issue": 5,
    "Start Page": 2106,
    "End Page": 2121,
    "Abstract": "Deep learning algorithms provide state-of-the-art results on a multitude of applications. However, it is also well established that they are highly vulnerable to adversarial perturbations. It is often believed that the solution to this vulnerability of deep learning systems must come from deep networks only. Contrary to this common understanding, in this article, we propose a non-deep learning approach that searches over a set of well-known image transforms such as Discrete Wavelet Transform and Discrete Sine Transform, and classifying the features with a support vector machine-based classifier. Existing deep networks-based defense have been proven ineffective against sophisticated adversaries, whereas image transformation-based solution makes a strong defense because of the non-differential nature, multiscale, and orientation filtering. The proposed approach, which combines the outputs of two transforms, efficiently generalizes across databases as well as different unseen attacks and combinations of both (i.e., cross-database and unseen noise generation CNN model). The proposed algorithm is evaluated on large scale databases, including object database (validation set of ImageNet) and face recognition (MBGC) database. The proposed detection algorithm yields at-least 84.2% and 80.1% detection accuracy under seen and unseen database test settings, respectively. Besides, we also show how the impact of the adversarial perturbation can be neutralized using a wavelet decomposition-based filtering method of denoising. The mitigation results with different perturbation methods on several image databases demonstrate the effectiveness of the proposed method.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2020.3027183",
    "Funding Information": "Department of Science and Technology Ministry of Science and Technology(grant numbers:Swarnajayanti Fellowship); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207872",
    "Author Keywords": "Adversarial attack;detection;mitigation;image transformations;deep learning",
    "IEEE Terms": "Perturbation methods;Databases;Machine learning;Transforms;Detection algorithms;Integrated circuits;Machine learning algorithms",
    "Mesh_Terms": "",
    "Article Citation Count": 23,
    "Patent Citation Count": null,
    "Reference Count": 107,
    "License": "IEEE",
    "Online Date": "28-Sep-20",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Defeating Misclassification Attacks Against Transfer Learning",
    "Authors": "B. Wu; S. Wang; X. Yuan; C. Wang; C. Rudolph; X. Yang",
    "Author Affiliations": "Department of Information Technology, Monash University, Clayton, VIC, Australia; CSIRO Data61, Clayton, VIC, Australia; Department of Information Technology, Monash University, Clayton, VIC, Australia; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Information Technology, Monash University, Clayton, VIC, Australia; Department of Information Technology, Monash University, Clayton, VIC, Australia",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "13-Mar-23",
    "Publication Year": 2023,
    "Volume": 20,
    "Issue": 2,
    "Start Page": 886,
    "End Page": 901,
    "Abstract": "Transfer learning is prevalent as a technique to efficiently generate new models (Student models) based on the knowledge transferred from a pre-trained model (Teacher model). However, Teacher models are often publicly available for sharing and reuse, which inevitably introduces vulnerability to trigger severe attacks against transfer learning systems. In this article, we take a first step towards mitigating one of the most advanced misclassification attacks in transfer learning. We design a distilled differentiator via activation-based network pruning to enervate the attack transferability while retaining accuracy. We adopt an ensemble structure from variant differentiators to improve the defence robustness. To avoid the bloated ensemble size during inference, we propose a two-phase defence, in which inference from the Student model is first performed to narrow down the candidate differentiators to be assembled, and later only a small, fixed number of them can be chosen to validate clean or reject adversarial inputs effectively. Our comprehensive evaluations on both large and small image recognition tasks confirm that the Student models with our defence of only 5 differentiators are immune to over 90% of the adversarial inputs with an accuracy loss of less than 10%. Our comparison also demonstrates that our design outperforms prior problematic defences.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2022.3144988",
    "Funding Information": "Monash-Data61 collaborative research project(grant numbers:Data61 CRP43); Research Grants Council of Hong Kong(grant numbers:CityU 11217819,N_CityU139/21,R6021-20F); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693248",
    "Author Keywords": "Deep neural network;defence against adversarial examples;transfer learning;pre-trained model",
    "IEEE Terms": "Transfer learning;Task analysis;Mathematical models;Training;Computational modeling;Data models;Perturbation methods",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 46,
    "License": "IEEE",
    "Online Date": "25-Jan-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Design, Analysis and Application of Embedded Resistive RAM Based Strong Arbiter PUF",
    "Authors": "R. Govindaraj; S. Ghosh; S. Katkoori",
    "Author Affiliations": "Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Computer Science and Engineering, Pennsylvania State University, Pennsylvania, USA; Computer Science and Engineering, University of South Florida, Tampa, FL, USA",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "6-Nov-20",
    "Publication Year": 2020,
    "Volume": 17,
    "Issue": 6,
    "Start Page": 1232,
    "End Page": 1242,
    "Abstract": "Resistive Random Access Memory (RRAM) based Physical Unclonable Function (PUF) designs exploit either the probabilistic switching or the resistance variability during forming, SET and RESET processes of RRAM. Memory PUFs using RRAM are typically weak PUFs due to fewer number of challenge response pairs. We propose a strong arbiter PUF based on 1T-1R bit cell which is designed from conventional RRAM memory array with minimally invasive changes. Conventional voltage sense amplifier is repurposed to act like an arbiter and generate the response. Similarly, address and data lines are repurposed to act as challenge and response bits respectively. The PUF is simulated using 65 nm predictive technology models for CMOS and Verilog-A model for a hafnium oxide based RRAM. The proposed PUF architecture is evaluated for uniqueness, uniformity and reliability for various number of stages. It demonstrates mean intra-die Hamming Distance (HD) of 0.135 percent and inter-die HD of 51.4 percent, and passes the NIST tests. We study the vulnerability of proposed PUF to machine learning attacks. We also present an application of proposed PUF for data attestation in the internet of things. Proposed PUF-based data attestation consumes 9.88pJ of total energy per data block of 64-bits and offers a speed of 120.7 kbps.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2018.2866425",
    "Funding Information": "Semiconductor Research Corporation(grant numbers:#2442.001); National Science Foundation(grant numbers:CNS-1722557,CNS-1814710,CCF-1718474,DGE-1723687); Defense Advanced Research Projects Agency(grant numbers:#D15AP00089); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443101",
    "Author Keywords": "Physical unclonable function;resistive RAM;hardware security;non-volatile memory;arbiter PUF;ML attacks;data attestation",
    "IEEE Terms": "Resistance;Mathematical model;Computer architecture;Random access memory;Security;Hardware",
    "Mesh_Terms": "",
    "Article Citation Count": 18,
    "Patent Citation Count": null,
    "Reference Count": 44,
    "License": "IEEE",
    "Online Date": "21-Aug-18",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "How About Bug-Triggering Paths? - Understanding and Characterizing Learning-Based Vulnerability Detectors",
    "Authors": "X. Cheng; X. Nie; N. Li; H. Wang; Z. Zheng; Y. Sui",
    "Author Affiliations": "University of Technology Sydney, Ultimo, NSW, Australia; Beijing University of Posts and Telecommunications, Beijing, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Beihang University, Beijing, China; University of Technology Sydney, Ultimo, NSW, Australia",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "13-Mar-24",
    "Publication Year": 2024,
    "Volume": 21,
    "Issue": 2,
    "Start Page": 542,
    "End Page": 558,
    "Abstract": "Machine learning and its promising branch deep learning have proven to be effective in a wide range of application domains. Recently, several efforts have shown success in applying deep learning techniques for automatic vulnerability discovery, as alternatives to traditional static bug detection. In principle, these learning-based approaches are built on top of classification models using supervised learning. Depending on the different granularities to detect vulnerabilities, these approaches rely on learning models which are typically trained with well-labeled source code to predict whether a program method, a program slice, or a particular code line contains a vulnerability or not. The effectiveness of these models is normally evaluated against conventional metrics including precision, recall and F1 score. In this paper, we show that despite yielding promising numbers, the above evaluation strategy can be insufficient and even misleading when evaluating the effectiveness of current learning-based approaches. This is because the underlying learning models only produce the classification results or report individual/isolated program statements, but are unable to pinpoint bug-triggering paths, which is an effective way for bug fixing and the main aim of static bug detection. Our key insight is that a program method or statement can only be stated as vulnerable in the context of a bug-triggering path. In this work, we systematically study the gap between recent learning-based approaches and conventional static bug detectors in terms of fine-grained metrics called BTP metrics using bug-triggering paths. We then characterize and compare the quality of the prediction results of existing learning-based detectors under different granularities. Finally, our comprehensive empirical study reveals several key issues and challenges in developing classification models to pinpoint bug-triggering paths and calls for more advanced learning-based bug detection techniques.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2022.3192419",
    "Funding Information": "Australian Research(grant numbers:DP200101328,DP210101348); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833339",
    "Author Keywords": "Software vulnerabilities;machine learning;bug-triggering paths;empirical study",
    "IEEE Terms": "Computer bugs;Codes;Measurement;Feature extraction;Detectors;Training;Predictive models",
    "Mesh_Terms": "",
    "Article Citation Count": 13,
    "Patent Citation Count": null,
    "Reference Count": 71,
    "License": "IEEE",
    "Online Date": "19-Jul-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "LARGen: Automatic Signature Generation for Malwares Using Latent Dirichlet Allocation",
    "Authors": "S. Lee; S. Kim; S. Lee; J. Choi; H. Yoon; D. Lee; J. -R. Lee",
    "Author Affiliations": "Korea National University of Transportation, Uiwang-si, Korea; National Security Research Institute, Daejeon, Korea; National Security Research Institute, Daejeon, Korea; Gachon University, Seongnam-si, Korea; National Security Research Institute, Daejeon, Korea; National Security Research Institute, Daejeon, Korea; Kangwon National University, Samcheok-si, Korea",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "30-Aug-18",
    "Publication Year": 2018,
    "Volume": 15,
    "Issue": 5,
    "Start Page": 771,
    "End Page": 783,
    "Abstract": "As the quantity and complexity of network threats grow, Intrusion Detection Systems (IDSs) have become critical for securing networks. Achieving computer network intrusion detection with these IDSs requires high-level information technology and security expertise because malicious traffic has to be rigorously analyzed and the appropriate IDS rules written to effectively detect vulnerabilities that may potentially be exploited. However, incorrect IDS rules may produce numerous false positives, thereby degrading the performance of the IDS, and even worse, paralyzing the network. In this paper, we present a novel approach that exploits the Latent Dirichle Allocation (LDA) algorithm to generate IDS rules. Our proposed method, called LDA-based Automatic Rule Generation (LARGen), automatically performs an analysis of the malicious traffic and extracts the appropriate attack signatures that will be used for IDS rules. LARGen first extracts multiple signature strings embedded in network flows. Then, the flows are classified based on the extracted signature strings, and key content strings for malicious traffic are identified through the LDA inferential topic model. Those key content strings are the core of an IDS rule that can detect malicious traffic. We study the effectiveness of LDA in the context of network attack signature generation via extensive experiments with real network trace data, consisting of both benign and malicious traffic. Experimental results confirm that threat rules generated from LARGen accurately detect every cyber attack with high accuracy.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2016.2609907",
    "Funding Information": "Korea National University of Transportation; Basic Science Research Program of the National Research Foundation of Korea (NRF)(grant numbers:NRF-2014R1A1A2053456); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569096",
    "Author Keywords": "Intrusion detection system;automated threat rule generation;latent Dirichlet allocation;system design and implementation",
    "IEEE Terms": "Malware;Security;Resource management;Context;Internet;Protocols;Semantics",
    "Mesh_Terms": "",
    "Article Citation Count": 22,
    "Patent Citation Count": null,
    "Reference Count": 56,
    "License": "IEEE",
    "Online Date": "15-Sep-16",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Optimizing the Numbers of Queries and Replies in Convex Federated Learning With Differential Privacy",
    "Authors": "Y. Zhou; X. Liu; Y. Fu; D. Wu; J. H. Wang; S. Yu",
    "Author Affiliations": "Department of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, NSW, Australia; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; School of Computer Science, University of Technology Sydney, Sydney, NSW, Australia",
    "Publication Title": "IEEE Transactions on Dependable and Secure Computing",
    "Date Added To Xplore": "10-Nov-23",
    "Publication Year": 2023,
    "Volume": 20,
    "Issue": 6,
    "Start Page": 4823,
    "End Page": 4837,
    "Abstract": "Federated learning (FL) empowers distributed clients to collaboratively train a shared machine learning model through exchanging parameter information. Despite the fact that FL can protect clients\u201a\u00c4\u00f4 raw data, malicious users can still crack original data with disclosed parameters. To amend this flaw, differential privacy (DP) is incorporated into FL clients to disturb original parameters, which however can significantly impair the accuracy of the trained model. In this work, we study an imperative question which has been vastly overlooked by existing works: what are the optimal numbers of queries and replies in FL with DP so that the final model accuracy is maximized. In FL, the parameter server (PS) needs to query participating clients for multiple global iterations to complete training. Each client responds a query from the PS by conducting a local iteration. We consider FL that will uniformly and randomly select participating clients to conduct local iterations with the FedSGD algorithm. Our work investigates how many times the PS should query clients and how many times each client should reply the PS by incorporating two most extensively used DP mechanisms (i.e., the Laplace mechanism and Gaussian mechanisms). Through conducting convergence rate analysis, we can determine the optimal numbers of queries and replies in FL with DP so that the final model accuracy can be maximized. Finally, extensive experiments are conducted with publicly available datasets: MNIST and FEMNIST, to verify our analysis and the results demonstrate that properly setting the numbers of queries and replies can significantly improve the final model accuracy in FL with DP.",
    "ISSN": "1941-0018",
    "ISBNs": "",
    "DOI": "10.1109/TDSC.2023.3234599",
    "Funding Information": "National Natural Science Foundation of China(grant numbers:U1911201,U2001209,62072269); Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Shenzhen Fundamental Research Program(grant numbers:20200814105901001); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008087",
    "Author Keywords": "Federated learning;query and reply;differential privacy;convergence rate",
    "IEEE Terms": "Convergence;Differential privacy;Computational modeling;Privacy;Servers;Data models;Training",
    "Mesh_Terms": "",
    "Article Citation Count": 2,
    "Patent Citation Count": null,
    "Reference Count": 45,
    "License": "IEEE",
    "Online Date": "6-Jan-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Why Don\u201a\u00c4\u00f4t XAI Techniques Agree? Characterizing the Disagreements Between Post-hoc Explanations of Defect Predictions",
    "Authors": "S. Roy; G. Laberge; B. Roy; F. Khomh; A. Nikanjam; S. Mondal",
    "Author Affiliations": "University of Saskatchewan, Canada; Polytechnique Montr\u221a\u00a9al, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada; University of Saskatchewan, Canada",
    "Publication Title": "2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "Date Added To Xplore": "19-Dec-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 444,
    "End Page": 448,
    "Abstract": "Machine Learning (ML) based defect prediction models can be used to improve the reliability and overall quality of software systems. However, such defect predictors might not be deployed in real applications due to the lack of transparency. Thus, recently, application of several post-hoc explanation methods (e.g., LIME and SHAP) have gained popularity. These explanation methods can offer insight by ranking features based on their importance in black box decisions. The explainability of ML techniques is reasonably novel in the Software Engineering community. However, it is still unclear whether such explainability methods genuinely help practitioners make better decisions regarding software maintenance. Recent user studies show that data scientists usually utilize multiple post-hoc explainers to understand a single model decision because of the lack of ground truth. Such a scenario causes disagreement between explainability methods and impedes drawing a conclusion. Therefore, our study first investigates three disagreement metrics between LIME and SHAP explanations of 10 defect-predictors, and exposes that disagreements regarding the rankings of feature importance are most frequent. Our findings lead us to propose a method of aggregating LIME and SHAP explanations that puts less emphasis on these disagreements while highlighting the aspect on which explanations agree.",
    "ISSN": "2576-3148",
    "ISBNs": "978-1-6654-7956-1",
    "DOI": "10.1109/ICSME55016.2022.00056",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978217",
    "Author Keywords": "Empirical;Defect Prediction;eXplainable AI;LIME;SHAP;Software Maintenance",
    "IEEE Terms": "Measurement;Software maintenance;Closed box;Machine learning;Predictive models;Maintenance engineering;Software systems",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 23,
    "License": "IEEE",
    "Online Date": "19-Dec-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Automated Characterization of Software Vulnerabilities",
    "Authors": "D. Gonzalez; H. Hastings; M. Mirakhorli",
    "Author Affiliations": "Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA",
    "Publication Title": "2019 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "Date Added To Xplore": "5-Dec-19",
    "Publication Year": 2019,
    "Volume": null,
    "Issue": null,
    "Start Page": 135,
    "End Page": 139,
    "Abstract": "Preventing vulnerability exploits is a critical software maintenance task, and software engineers often rely on Common Vulnerability and Exposure (CVEs) reports for information about vulnerable systems and libraries. These reports include descriptions, disclosure sources, and manually-populated vulnerability characteristics such as root cause from the NIST Vulnerability Description Ontology (VDO). This information needs to be complete and accurate so stakeholders of affected products can prevent and react to exploits of the reported vulnerabilities. In this study, we demonstrate that VDO characteristics can be automatically detected from the textual descriptions included in CVE reports. We evaluated the performance of 6 classification algorithms with a dataset of 365 vulnerability descriptions, each mapped to 1 of 19 characteristics from the VDO. This work demonstrates that it is feasible to train classification techniques to accurately characterize vulnerabilities from their descriptions. All 6 classifiers evaluated produced accurate results, and the Support Vector Machine classifier was the best-performing individual classifier. Automating the vulnerability characterization process is a step towards ensuring stakeholders have the necessary data to effectively maintain their systems.",
    "ISSN": "2576-3148",
    "ISBNs": "978-1-7281-3094-1",
    "DOI": "10.1109/ICSME.2019.00023",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918946",
    "Author Keywords": "software maintainence;vulnerability characterization;text classification;CVE;VDO",
    "IEEE Terms": "Support vector machines;Software;Task analysis;Measurement;NIST;Ontologies;Stakeholders",
    "Mesh_Terms": "",
    "Article Citation Count": 6,
    "Patent Citation Count": null,
    "Reference Count": 17,
    "License": "IEEE",
    "Online Date": "5-Dec-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Test Input Prioritization for Machine Learning Classifiers",
    "Authors": "X. Dang; Y. Li; M. Papadakis; J. Klein; T. F. Bissyand\u221a\u00a9; Y. L. Traon",
    "Author Affiliations": "University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg; University of Luxembourg, Kirchberg, Luxembourg",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "18-Mar-24",
    "Publication Year": 2024,
    "Volume": 50,
    "Issue": 3,
    "Start Page": 413,
    "End Page": 442,
    "Abstract": "Machine learning has achieved remarkable success across diverse domains. Nevertheless, concerns about interpretability in black-box models, especially within Deep Neural Networks (DNNs), have become pronounced in safety-critical fields like healthcare and finance. Classical machine learning (ML) classifiers, known for their higher interpretability, are preferred in these domains. Similar to DNNs, classical ML classifiers can exhibit bugs that could lead to severe consequences in practice. Test input prioritization has emerged as a promising approach to ensure the quality of an ML system, which prioritizes potentially misclassified tests so that such tests can be identified earlier with limited manual labeling costs. However, when applying to classical ML classifiers, existing DNN test prioritization methods are constrained from three perspectives: 1) Coverage-based methods are inefficient and time-consuming; 2) Mutation-based methods cannot be adapted to classical ML models due to mismatched model mutation rules; 3) Confidence-based methods are restricted to a single dimension when applying to binary ML classifiers, solely depending on the model's prediction probability for one class. To overcome the challenges, we propose MLPrior, a test prioritization approach specifically tailored for classical ML models. MLPrior leverages the characteristics of classical ML classifiers (i.e., interpretable models and carefully engineered attribute features) to prioritize test inputs. The foundational principles are: 1) tests more sensitive to mutations are more likely to be misclassified, and 2) tests closer to the model's decision boundary are more likely to be misclassified. Building on the first concept, we design mutation rules to generate two types of mutation features (i.e., model mutation features and input mutation features) for each test. Drawing from the second notion, MLPrior generates attribute features of each test based on its attribute values, which can indirectly reveal the proximity between the test and the decision boundary. For each test, MLPrior combines all three types of features of it into a final vector. Subsequently, MLPrior employs a pre-trained ranking model to predict the misclassification probability of each test based on its final vector and ranks tests accordingly. We conducted an extensive study to evaluate MLPrior based on 185 subjects, encompassing natural datasets, mixed noisy datasets, and fairness datasets. The results demonstrate that MLPrior outperforms all the compared test prioritization approaches, with an average improvement of 14.74%$\\sim$\u201a\u00e0\u00ba66.93% on natural datasets, 18.55%$\\sim$\u201a\u00e0\u00ba67.73% on mixed noisy datasets, and 15.34%$\\sim$\u201a\u00e0\u00ba62.72% on fairness datasets.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2024.3350019",
    "Funding Information": "Luxembourg National Research Fund AFR PhD(grant numbers:17036341); European Research Council (ERC)(grant numbers:949014); Luxembourg National Research Funds (FNR)(grant numbers:C20/IS/14761415/TestFlakes); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10382258",
    "Author Keywords": "Test input prioritization;machine learning;mutation analysis;learning to rank;labelling",
    "IEEE Terms": "Predictive models;Adaptation models;Labeling;Machine learning;Testing;Noise measurement;Manuals",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 120,
    "License": "CCBY",
    "Online Date": "5-Jan-24",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems",
    "Authors": "A. Groce; T. Kulesza; C. Zhang; S. Shamasunder; M. Burnett; W. -K. Wong; S. Stumpf; S. Das; A. Shinsel; F. Bice; K. McIntosh",
    "Author Affiliations": "School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; Centre for HCI Design, School of Informatics, City University London, London, United Kingdom; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "31-Mar-14",
    "Publication Year": 2014,
    "Volume": 40,
    "Issue": 3,
    "Start Page": 307,
    "End Page": 323,
    "Abstract": "How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a \u201a\u00c4\u00fagold standard\u201a\u00c4\u00f9 and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2013.59",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682887",
    "Author Keywords": "Machine learning;end-user testing;test suite size",
    "IEEE Terms": "Testing;Software;Training;Training data;Electronic mail;Software algorithms;Machine learning algorithms",
    "Mesh_Terms": "",
    "Article Citation Count": 42,
    "Patent Citation Count": null,
    "Reference Count": 63,
    "License": "IEEE",
    "Online Date": "12-Dec-13",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Characterizing and Understanding Software Security Vulnerabilities in Machine Learning Libraries",
    "Authors": "N. S. Harzevili; J. Shin; J. Wang; S. Wang; N. Nagappan",
    "Author Affiliations": "Lassonde School of Engineering, York University, Toronto, Canada; Lassonde School of Engineering, York University, Toronto, Canada; Institute of Software Chinese Academy of Sciences, Beijing, China; Lassonde School of Engineering, York University, Toronto, Canada; IIIT Delhi, New Delhi, India",
    "Publication Title": "2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "12-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 27,
    "End Page": 38,
    "Abstract": "The application of machine learning (ML) libraries has tremendously increased in many domains, including autonomous driving systems, medical, and critical industries. Vulnerabilities of such libraries could result in irreparable consequences. However, the characteristics of software security vulnerabilities have not been well studied. In this paper, to bridge this gap, we take the first step toward characterizing and understanding the security vulnerabilities of seven well-known ML libraries, including TensorFlow, PyTorch, Scikit-learn, Mlpack, Pandas, Numpy, and Scipy. To do so, we collected 683 security vulnerabilities to explore four major factors: 1) vulnerability types, 2) root causes, 3) symptoms, and 4) fixing patterns of security vulnerabilities in the studied ML libraries. The findings of this study can help developers and researchers understand the characteristics of security vulnerabilities across the studied ML libraries.",
    "ISSN": "2574-3864",
    "ISBNs": "979-8-3503-1184-6",
    "DOI": "10.1109/MSR59073.2023.00018",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10173858",
    "Author Keywords": "Security vulnerability;machine learning libraries;empirical study",
    "IEEE Terms": "Industries;Machine learning;Debugging;Reliability engineering;Libraries;Software;Software reliability",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 52,
    "License": "IEEE",
    "Online Date": "12-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "An Empirical Study of Challenges in Converting Deep Learning Models",
    "Authors": "M. Openja; A. Nikanjam; A. H. Yahmed; F. Khomh; Z. M. J. Jiang",
    "Author Affiliations": "Polytechnique Montr\u221a\u00a9al, Montreal, Quebec, Canada; Polytechnique Montr\u221a\u00a9al, Montreal, Quebec, Canada; Polytechnique Montr\u221a\u00a9al, Montreal, Quebec, Canada; Polytechnique Montr\u221a\u00a9al, Montreal, Quebec, Canada; York University, Toronto, Ontario, Canada",
    "Publication Title": "2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "Date Added To Xplore": "19-Dec-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 13,
    "End Page": 23,
    "Abstract": "There is an increase in deploying Deep Learning (DL)-based software systems in real-world applications. Usually, DL models are developed and trained using DL frameworks like TensorFlow and PyTorch. Each framework has its own internal mechanisms/formats to represent and train DL models (deep neural networks), and usually those formats cannot be recognized by other frameworks. Moreover, trained models are usually deployed in environments different from where they were developed. To solve the interoperability issue and make DL models compatible with different frameworks/environments, some exchange formats are introduced for DL models, like ONNX and CoreML. However, ONNX and CoreML were never empirically evaluated by the community to reveal their prediction accuracy, performance, and robustness after conversion. Poor accuracy or non-robust behavior of converted models may lead to poor quality of deployed DL-based software systems. We conduct, in this paper, the first empirical study to assess ONNX and CoreML for converting trained DL models. In our systematic approach, two popular DL frameworks, Keras and PyTorch, are used to train five widely used DL models on three popular datasets. The trained models are then converted to ONNX and CoreML and transferred to two runtime environments designated for such formats, to be evaluated. We investigate the prediction accuracy before and after conversion. Our results unveil that the prediction accuracy of converted models are at the same level of originals. The performance (time cost and memory consumption) of converted models are studied as well. The size of models are reduced after conversion, which can result in optimized DL-based software deployment. We also study the adversarial robustness of converted models to make sure about the robustness of deployed DL-based software. Leveraging the state-of-the-art adversarial attack approaches, converted models are generally assessed robust at the same level of originals. However, obtained results show that CoreML models are more vulnerable to adversarial attacks compared to ONNX. The general message of our findings is that DL developers should be cautious on the deployment of converted models that may 1) perform poorly while switching from one framework to another, 2) have challenges in robust deployment, or 3) run slowly, leading to poor quality of deployed DL-based software, including DL-based software maintenance tasks, like bug prediction.",
    "ISSN": "2576-3148",
    "ISBNs": "978-1-6654-7956-1",
    "DOI": "10.1109/ICSME55016.2022.00010",
    "Funding Information": "Natural Sciences and Engineering Research Council of Canada; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978197",
    "Author Keywords": "Empirical;Deep Learning;Converting Trained Models;Deploying ML Models;Robustness",
    "IEEE Terms": "Deep learning;Analytical models;Software maintenance;Runtime environment;Systematics;Switches;Predictive models",
    "Mesh_Terms": "",
    "Article Citation Count": 8,
    "Patent Citation Count": null,
    "Reference Count": 40,
    "License": "IEEE",
    "Online Date": "19-Dec-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "DRLgencert: Deep Learning-Based Automated Testing of Certificate Verification in SSL/TLS Implementations",
    "Authors": "C. Chen; W. Diao; Y. Zeng; S. Guo; C. Hu",
    "Author Affiliations": "Shandong University, Jinan, China; Jinan University, Guangzhou, China; China Mobile (Hangzhou) Information Technology Co., Ltd., Hangzhou, China; Shandong University, Jinan, China; Shandong University, Jinan, China",
    "Publication Title": "2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "Date Added To Xplore": "11-Nov-18",
    "Publication Year": 2018,
    "Volume": null,
    "Issue": null,
    "Start Page": 48,
    "End Page": 58,
    "Abstract": "The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols are the foundation of network security. The certificate verification in SSL/TLS implementations is vital and may become the \"weak link\" in the whole network ecosystem. In previous works, some research focused on the automated testing of certificate verification, and the main approaches rely on generating massive certificates through randomly combining parts of seed certificates for fuzzing. Although the generated certificates could meet the semantic constraints, the cost is quite heavy, and the performance is limited due to the randomness. To fill this gap, in this paper, we propose DRLGENCERT, the first framework of applying deep reinforcement learning to the automated testing of certificate verification in SSL/TLS implementations. DRLGENCERT accepts ordinary certificates as input and outputs newly generated certificates which could trigger discrepancies with high efficiency. Benefited by the deep reinforcement learning, when generating certificates, our framework could choose the best next action according to the result of a previous modification, instead of simple random combinations. At the same time, we developed a set of new techniques to support the overall design, like new feature extraction method for X.509 certificates, fine-grained differential testing, and so forth. Also, we implemented a prototype of DRLGENCERT and carried out a series of real-world experiments. The results show DRLGENCERT is quite efficient, and we obtained 84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say around 46.5% effectiveness. Also, we evaluated six popular SSL/TLS implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification flaws, and most of them were previously unknown.",
    "ISSN": "2576-3148",
    "ISBNs": "978-1-5386-7870-1",
    "DOI": "10.1109/ICSME.2018.00014",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529836",
    "Author Keywords": "Differential Testing;Deep Reinforcement Learning;SSL/TLS Implementations",
    "IEEE Terms": "Testing;Feature extraction;Security;Task analysis;Semantics",
    "Mesh_Terms": "",
    "Article Citation Count": 5,
    "Patent Citation Count": null,
    "Reference Count": 31,
    "License": "IEEE",
    "Online Date": "11-Nov-18",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Impact of Discretization Noise of the Dependent Variable on Machine Learning Classifiers in Software Engineering",
    "Authors": "G. K. Rajbahadur; S. Wang; Y. Kamei; A. E. Hassan",
    "Author Affiliations": "Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada; Principles of Software Languages (POSL) Lab, Graduate School and Faulty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University, Kingston, ON, Canada",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "16-Jul-21",
    "Publication Year": 2021,
    "Volume": 47,
    "Issue": 7,
    "Start Page": 1414,
    "End Page": 1430,
    "Abstract": "Researchers usually discretize a continuous dependent variable into two target classes by introducing an artificial discretization threshold (e.g., median). However, such discretization may introduce noise (i.e., discretization noise) due to ambiguous class loyalty of data points that are close to the artificial threshold. Previous studies do not provide a clear directive on the impact of discretization noise on the classifiers and how to handle such noise. In this paper, we propose a framework to help researchers and practitioners systematically estimate the impact of discretization noise on classifiers in terms of its impact on various performance measures and the interpretation of classifiers. Through a case study of 7 software engineering datasets, we find that: 1) discretization noise affects the different performance measures of a classifier differently for different datasets; 2) Though the interpretation of the classifiers are impacted by the discretization noise on the whole, the top 3 most important features are not affected by the discretization noise. Therefore, we suggest that practitioners and researchers use our framework to understand the impact of discretization noise on the performance of their built classifiers and estimate the exact amount of discretization noise to be discarded from the dataset to avoid the negative impact of such noise.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2019.2924371",
    "Funding Information": "JSPS KAKENHI(grant numbers:JP18H03222); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744330",
    "Author Keywords": "Discretization noise;discretization;classifiers;feature importance analysis;performance;random forest;logistic regression;decision trees;KNN",
    "IEEE Terms": "Software engineering;Computer bugs;Noise measurement;Software;Machine learning;Regression tree analysis;Logistics",
    "Mesh_Terms": "",
    "Article Citation Count": 15,
    "Patent Citation Count": null,
    "Reference Count": 76,
    "License": "IEEE",
    "Online Date": "24-Jun-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents",
    "Authors": "A. Zolfagharian; M. Abdellatif; L. C. Briand; M. Bagherzadeh; R. S",
    "Author Affiliations": "School of Electrical Engineering and Computer Science (EECS), University of Ottawa, Ottawa, ON, Canada; Software and Information Technology Engineering Department, \u221a\u00e2cole de Technologie Sup\u221a\u00a9rieure, Montreal, QC, Canada; School of Electrical Engineering and Computer Science (EECS), University of Ottawa, Ottawa, ON, Canada; Cisco, Ottawa, ON, Canada; Department of Research and Development, General Motors, Warren, MI, USA",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "17-Jul-23",
    "Publication Year": 2023,
    "Volume": 49,
    "Issue": 7,
    "Start Page": 3715,
    "End Page": 3735,
    "Abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agents\u201a\u00c4\u00f4 policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2023.3269804",
    "Funding Information": "Natural Sciences and Engineering Research Council of Canada; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107813",
    "Author Keywords": "Genetic algorithm;machine learning;reinforcement learning;state abstraction;testing",
    "IEEE Terms": "Testing;Reinforcement learning;Safety;Deep learning;Closed box;Training;Genetic algorithms",
    "Mesh_Terms": "",
    "Article Citation Count": 5,
    "Patent Citation Count": null,
    "Reference Count": 85,
    "License": "CCBY",
    "Online Date": "25-Apr-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "VulHunter: Hunting Vulnerable Smart Contracts at EVM Bytecode-Level via Multiple Instance Learning",
    "Authors": "Z. Li; S. Lu; R. Zhang; Z. Zhao; R. Liang; R. Xue; W. Li; F. Zhang; S. Gao",
    "Author Affiliations": "State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Information Engineering University, Zhengzhou, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Information Engineering University, Zhengzhou, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information, Central University of Finance and Economics, Beijing, China",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "16-Nov-23",
    "Publication Year": 2023,
    "Volume": 49,
    "Issue": 11,
    "Start Page": 4886,
    "End Page": 4916,
    "Abstract": "With the economic development of Ethereum, the frequent security incidents involving smart contracts running on this platform have caused billions of dollars in losses. Consequently, there is a pressing need to identify the vulnerabilities in contracts, while the state-of-the-art (SOTA) detection methods have been limited in this regard as they cannot overcome three challenges at the same time. (i) Meet the requirements of detecting the source code, bytecode, and opcode of contracts simultaneously; (ii) reduce the reliance on manual pre-defined rules/patterns and expert involvement; (iii) assist contract developers in completing the contract lifecycle more safely, e.g., vulnerability repair and abnormal monitoring. With the development of machine learning (ML), using it to detect the contract runtime execution sequences (called instances) has made it possible to address these challenges. However, the lack of datasets with fine-grained sequence labels poses a significant obstacle, given the unreadability of bytecode/opcode. To this end, we propose a method named VulHunter that extracts the instances by traversing the Control Flow Graph built from contract opcodes. Based on the hybrid attention and multi-instance learning mechanisms, VulHunter reasons the instance labels and designs an optional classifier to automatically capture the subtle features of both normal and defective contracts, thereby identifying the vulnerable instances. Then, it combines the symbolic execution to construct and solve symbolic constraints to validate their feasibility. Finally, we implement a prototype of VulHunter with 15K lines of code and compare it with 9 SOTA methods on five open source datasets including 52,042 source codes and 184,289 bytecodes. The results indicate that VulHunter can detect contract vulnerabilities more accurately (90.04% accuracy and 85.60% F1 score), efficiently (only 4.4 seconds per contract), and robustly (0% analysis failure rate) than SOTA methods. Also, it can focus on specific metrics such as precision and recall by employing different baseline models and hyperparameters to meet the various user requirements, e.g., vulnerability discovery and misreport mitigation. More importantly, compared with the previous ML-based arts, it can not only provide classification results, defective contract source code statements, key opcode fragments, and vulnerable execution paths, but also eliminate misreports and facilitate more operations such as vulnerability repair and attack simulation during the contract lifecycle.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2023.3317209",
    "Funding Information": "National Key R&D Program of China(grant numbers:2021YFB2700603); National Natural Science Foundation of China(grant numbers:62172405,62072487,62227805,62072398); Major Public Welfare Projects Foundation of Henan(grant numbers:201300210200); Beijing Natural Science Foundation(grant numbers:M21036); Zhejiang Key R&D Plan(grant numbers:2021C01116); Leading Innovative and Entrepreneur Team Introduction Program of Zhejiang(grant numbers:2018R01005); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LD22F020002); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261219",
    "Author Keywords": "Blockchain;smart contract;security analysis;multiple instance learning;symbolic execution",
    "IEEE Terms": "Source coding;Smart contracts;Codes;Pattern matching;Testing;Monitoring;Maintenance engineering",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 76,
    "License": "CCBY",
    "Online Date": "22-Sep-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Astraea: Grammar-Based Fairness Testing",
    "Authors": "E. Soremekun; S. Udeshi; S. Chattopadhyay",
    "Author Affiliations": "Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Esch-sur-Alzette, Luxembourg; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "9-Dec-22",
    "Publication Year": 2022,
    "Volume": 48,
    "Issue": 12,
    "Start Page": 5188,
    "End Page": 5211,
    "Abstract": "Software often produces biased outputs. In particular, machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs. Such unfair program behavior can be caused by societal bias. In the last few years, Amazon, Microsoft and Google have provided software services that produce unfair outputs, mostly due to societal bias (e.g., gender or race). In such events, developers are saddled with the task of conducting fairness testing. Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases. We propose a grammar-based fairness testing approach (called Astraea) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems. Using probabilistic grammars, Astraea also provides fault diagnosis by isolating the cause of observed software bias. Astraea\u201a\u00c4\u00f4s diagnoses facilitate the improvement of ML fairness. Astraea was evaluated on 18 software systems that provide three major natural language processing (NLP) services. In our evaluation, Astraea generated fairness violations at a rate of about 18%. Astraea generated over 573K discriminatory test cases and found over 102K fairness violations. Furthermore, Astraea improves software fairness by about 76% via model-retraining, on average.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2022.3141758",
    "Funding Information": "University of Luxembourg, Ezekiel Soremekun; University of Luxembourg(grant numbers:AUDACITY-2019-Laiwyers); OneConnect Financial(grant numbers:RGOCFT2001); Singapore Ministry of Education; MOE(grant numbers:MOE2018-T2-1-098); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678017",
    "Author Keywords": "software fairness;machine learning;natural language processing;software testing;program debugging",
    "IEEE Terms": "Testing;Grammar;Task analysis;Sentiment analysis;Test pattern generators;Software testing;Software systems",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 88,
    "License": "IEEE",
    "Online Date": "11-Jan-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Researcher Bias: The Use of Machine Learning in Software Defect Prediction",
    "Authors": "M. Shepperd; D. Bowes; T. Hall",
    "Author Affiliations": "Brunel University, Uxbridge, Middlesex, United Kingdom; Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom; Brunel University, Uxbridge, Middlesex, United Kingdom",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "16-Jun-14",
    "Publication Year": 2014,
    "Volume": 40,
    "Issue": 6,
    "Start Page": 603,
    "End Page": 616,
    "Abstract": "Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2014.2322358",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824804",
    "Author Keywords": "Software defect prediction;meta-analysis;researcher bias",
    "IEEE Terms": "Software;Predictive models;Correlation;Data models;Buildings;Software engineering;Measurement",
    "Mesh_Terms": "",
    "Article Citation Count": 245,
    "Patent Citation Count": null,
    "Reference Count": 53,
    "License": "IEEE",
    "Online Date": "3-Jun-14",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Improving Vulnerability Inspection Efficiency Using Active Learning",
    "Authors": "Z. Yu; C. Theisen; L. Williams; T. Menzies",
    "Author Affiliations": "Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Microsoft, Seattle, WA, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "11-Nov-21",
    "Publication Year": 2021,
    "Volume": 47,
    "Issue": 11,
    "Start Page": 2401,
    "End Page": 2420,
    "Abstract": "Software engineers can find vulnerabilities with less effort if they are directed towards code that might contain more vulnerabilities. HARMLESS is an incremental support vector machine tool that builds a vulnerability prediction model from the source code inspected to date, then suggests what source code files should be inspected next. In this way, HARMLESS can reduce the time and effort required to achieve some desired level of recall for finding vulnerabilities. The tool also provides feedback on when to stop (at that desired level of recall) while at the same time, correcting human errors by double-checking suspicious files. This paper evaluates HARMLESS on Mozilla Firefox vulnerability data. HARMLESS found 80, 90, 95, 99 percent of the vulnerabilities by inspecting 10, 16, 20, 34 percent of the source code files. When targeting 90, 95, 99 percent recall, HARMLESS could stop after inspecting 23, 30, 47 percent of the source code files. Even when human reviewers fail to identify half of the vulnerabilities (50 percent false negative rate), HARMLESS could detect 96 percent of the missing vulnerabilities by double-checking half of the inspected files. Our results serve to highlight the very steep cost of protecting software from vulnerabilities (in our case study that cost is, for example, the human effort of inspecting 28,750 \u221a\u00f3 20% = 5,750 source code files to identify 95 percent of the vulnerabilities). While this result could benefit the mission-critical projects where human resources are available for inspecting thousands of source code files, the research challenge for future work is how to further reduce that cost. The conclusion of this paper discusses various ways that goal might be achieved.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2019.2949275",
    "Funding Information": "National Science Foundation(grant numbers:#1506586,#1909516); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883076",
    "Author Keywords": "Active learning;security;vulnerabilities;software engineering;error correction",
    "IEEE Terms": "Inspection;Software;Tools;Security;Predictive models;Error correction;NIST",
    "Mesh_Terms": "",
    "Article Citation Count": 15,
    "Patent Citation Count": null,
    "Reference Count": 50,
    "License": "IEEE",
    "Online Date": "25-Oct-19",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Comments on \u201a\u00c4\u00faUsing $k$k-Core Decomposition on Class Dependency Networks to Improve Bug Prediction Model's Practical Performance\u201a\u00c4\u00f9",
    "Authors": "W. Pan; H. Ming; Z. Yang; T. Wang",
    "Author Affiliations": "School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, Xi'an Jiaotong University and GuardStrike Inc., Shaanxi, China; School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "9-Dec-22",
    "Publication Year": 2022,
    "Volume": 48,
    "Issue": 12,
    "Start Page": 5176,
    "End Page": 5187,
    "Abstract": "In a very recent paper by (Qu et al., 2021), the authors propose an effective equation, top-core, to improve the performance of effort-aware bug prediction models. A distinctive feature of top-core is that it takes into account the coreness of a class in a Class Dependency Network (CDN) when calculating the relative risk of a class to be buggy. In this comment, we show that Qu et al.'s paper contains three shortcomings that may influence the performance of top-core or even have the potential to lead to erroneous results. First, we show that the CDN that they use to calculate the coreness of classes is not very accurate, neglecting many important types of dependency relations between classes such as method call relation, access relation, and instantiates relation. Second, they trained a Logistic Regression model using the scikit-learn framework to predict the probability of a specific class to be buggy. It is actually an L2 regularized Logistic Regression model, which is dependent on the scale of the features. But they neglected to normalize the features, making the obtained results erroneous. Finally, the number of execution times (viz. 10 times in the paper of Qu et al.) they used to reduce the bias caused by the randomness (viz. random split of instances and the process to handle class-imbalance problem) in the experiments is too small to ensure that the obtained results converge to stable values; but they failed to signify the precision level of their results for comparison. In this comment, we provide solutions to the problems by using i) an improved CDN (ICDN) to represent the structure of software systems, ii) the z-score method to normalize the features, and iii) an adaptive mechanism to determine the number of execution times. In the experiments, we find that Qu et al.'s approach based on the Logistic Regression model does not perform significantly better than the state-of-the-art approach Ree, which is inconsistent with the conclusion in Qu et al.'s work. We also observe that replacing CDN with ICDN does improve the performance of Qu et al.'s approach.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2022.3140599",
    "Funding Information": "Natural Science Foundation of Zhejiang Province(grant numbers:LY22F020007); National Natural Science Foundation of China(grant numbers:62032010,61832014); Key R&D Program of Zhejiang Province(grant numbers:2019C01004,2019C03123); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733807",
    "Author Keywords": "Effort-aware bug prediction;coreness;  $k$   k     -core;class dependency network;complex network",
    "IEEE Terms": "Computer bugs;Logistics;Predictive models;Mathematical models;Testing;Software systems;Codes",
    "Mesh_Terms": "",
    "Article Citation Count": 10,
    "Patent Citation Count": null,
    "Reference Count": 17,
    "License": "IEEE",
    "Online Date": "14-Mar-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Learning Approximate Execution Semantics From Traces for Binary Function Similarity",
    "Authors": "K. Pei; Z. Xuan; J. Yang; S. Jana; B. Ray",
    "Author Affiliations": "Columbia University, New York, NY, USA; Purdue University, West Lafayette, IN, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "18-Apr-23",
    "Publication Year": 2023,
    "Volume": 49,
    "Issue": 4,
    "Start Page": 2776,
    "End Page": 2790,
    "Abstract": "Detecting semantically similar binary functions \u201a\u00c4\u00ec a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics \u201a\u00c4\u00ec requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions\u201a\u00c4\u00f4 execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions\u201a\u00c4\u00f4 traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8\u221a\u00f3 faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex \u201a\u00c4\u00ec on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2022.3231621",
    "Funding Information": "National Science Foundation(grant numbers:CCF-18-45893,CCF-18-22965,CCF-16-19123,CNS-18-42456,CNS-18-01426,CNS-16-18771,CNS-16-17670,CNS-15-64055,CNS-15-63843); ONR(grant numbers:N00014-17-1-2010,N00014-16-1-2263,N00014-17-1-2788); NSF CAREER; ARO Young Investigator; Google Faculty Fellowship; JP Morgan Faculty Research Award; DiDi Faculty Research Award; Google Cloud; Capital One Research; Amazon Web Services; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002189",
    "Author Keywords": "Binary analysis;large language models;software security",
    "IEEE Terms": "Semantics;Task analysis;Computer architecture;Optimization;Codes;Behavioral sciences;Computational modeling",
    "Mesh_Terms": "",
    "Article Citation Count": 6,
    "Patent Citation Count": null,
    "Reference Count": 100,
    "License": "IEEE",
    "Online Date": "28-Dec-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Better Pay Attention Whilst Fuzzing",
    "Authors": "S. Zhu; J. Wang; J. Sun; J. Yang; X. Lin; T. Wang; L. Zhang; P. Cheng",
    "Author Affiliations": "College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; School of Computing and Information Systems, Singapore Management University, Singapore; School of Medicine, Zhejiang University, Zhejiang, China; Ant Group, Hangzhou, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China; College of Control Science and Engineering, Zhejiang University, Zhejiang, China",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "12-Feb-24",
    "Publication Year": 2024,
    "Volume": 50,
    "Issue": 2,
    "Start Page": 190,
    "End Page": 208,
    "Abstract": "Fuzzing is one of the prevailing methods for vulnerability detection. However, even state-of-the-art fuzzing methods become ineffective after some period of time, i.e., the coverage hardly improves as existing methods are ineffective to focus the attention of fuzzing on covering the hard-to-trigger program paths. In other words, they cannot generate inputs that can break the bottleneck due to the fundamental difficulty in capturing the complex relations between the test inputs and program coverage. In particular, existing fuzzers suffer from the following main limitations: 1) lacking an overall analysis of the program to identify the most \u201a\u00c4\u00farewarding\u201a\u00c4\u00f9 seeds, and 2) lacking an effective mutation strategy which could continuously select and mutates the more relevant \u201a\u00c4\u00fabytes\u201a\u00c4\u00f9 of the seeds. In this work, we propose an approach called ATTuzz to address these two issues systematically. First, we propose a lightweight dynamic analysis technique that estimates the \u201a\u00c4\u00fareward\u201a\u00c4\u00f9 of covering each basic block and selects the most rewarding seeds accordingly. Second, we mutate the selected seeds according to a neural network model which predicts whether a certain \u201a\u00c4\u00farewarding\u201a\u00c4\u00f9 block will be covered given certain mutations on certain bytes of a seed. The model is a deep learning model equipped with an attention mechanism which is learned and updated periodically whilst fuzzing. Our evaluation shows that ATTuzz significantly outperforms 5 state-of-the-art grey-box fuzzers on 6 popular real-world programs and MAGMA data sets at achieving higher edge coverage and finding new bugs. In particular, ATTuzz achieved 1.2X edge coverage and 1.8X bugs detected than AFL++ over 24-hour runs. In addition, ATTuzz also finds 4 new bugs in the latest version of some popular software including p7zip and openUSD.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2023.3338129",
    "Funding Information": "National Natural Science Foundation of China(grant numbers:61833015,62293511,62102359); Academic Research Fund Tier 3, Ministry of Education, Singapore(grant numbers:MOET32020-0004); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339688",
    "Author Keywords": "Fuzzing;deep learning;program analysis;attention model",
    "IEEE Terms": "Fuzzing;Deep learning;Computer bugs;Codes;Image edge detection;Electronic mail;Recurrent neural networks",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 72,
    "License": "IEEE",
    "Online Date": "4-Dec-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "MANDO-HGT: Heterogeneous Graph Transformers for Smart Contract Vulnerability Detection",
    "Authors": "H. H. Nguyen; N. -M. Nguyen; C. Xie; Z. Ahmadi; D. Kudendo; T. -N. Doan; L. Jiang",
    "Author Affiliations": "L3S Research Center, Leibniz Universit\u221a\u00a7t Hannover, Hannover, Germany; Singapore Management University, Singapore; L3S Research Center, Leibniz Universit\u221a\u00a7t Hannover, Hannover, Germany; L3S Research Center, Leibniz Universit\u221a\u00a7t Hannover, Hannover, Germany; L3S Research Center, Leibniz Universit\u221a\u00a7t Hannover, Hannover, Germany; Independent Researcher, Atlanta, Georgia, USA; Singapore Management University, Singapore",
    "Publication Title": "2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "12-Jul-23",
    "Publication Year": 2023,
    "Volume": null,
    "Issue": null,
    "Start Page": 334,
    "End Page": 346,
    "Abstract": "Smart contracts in blockchains have been increasingly used for high-value business applications. It is essential to check smart contracts' reliability before and after deployment. Although various program analysis and deep learning techniques have been proposed to detect vulnerabilities in either Ethereum smart contract source code or bytecode, their detection accuracy and scalability are still limited. This paper presents a novel framework named MANDO-HGT for detecting smart contract vulnerabilities. Given Ethereum smart contracts, either in source code or bytecode form, and vulnerable or clean, MANDO-HGT custom-builds heterogeneous contract graphs (HCGs) to represent control-flow and/or function-call information of the code. It then adapts heterogeneous graph transformers (HGTs) with customized meta relations for graph nodes and edges to learn their embeddings and train classifiers for detecting various vulnerability types in the nodes and graphs of the contracts more accurately. We have collected more than 55K Ethereum smart contracts from various data sources and verified the labels for 423 buggy and 2,742 clean contracts to evaluate MANDO-HGT. Our empirical results show that MANDO-HGT can significantly improve the detection accuracy of other state-of-the-art vulnerability detection techniques that are based on either machine learning or conventional analysis techniques. The accuracy improvements in terms of F1-score range from 0.7% to more than 76% at either the coarse-grained contract level or the fine-grained line level for various vulnerability types in either source code or bytecode. Our method is general and can be retrained easily for different vulnerability types without the need for manually defined vulnerability patterns.",
    "ISSN": "2574-3864",
    "ISBNs": "979-8-3503-1184-6",
    "DOI": "10.1109/MSR59073.2023.00052",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174104",
    "Author Keywords": "vulnerability detection;smart contracts;source code;bytecode;heterogeneous graph learning;graph transformer",
    "IEEE Terms": "Deep learning;Codes;Source coding;Soft sensors;Scalability;Image edge detection;Smart contracts",
    "Mesh_Terms": "",
    "Article Citation Count": 1,
    "Patent Citation Count": null,
    "Reference Count": 79,
    "License": "IEEE",
    "Online Date": "12-Jul-23",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study",
    "Authors": "T. C. V\u221a\u00a9lez; R. Khatchadourian; M. Bagherzadeh; A. Raja",
    "Author Affiliations": "City University of New York (CUNY) Graduate Center, New York, NY, USA; City University of New York (CUNY) Hunter College, New York, NY, USA; Oakland University, Rochester, MI, USA; City University of New York (CUNY) Hunter College, New York, NY, USA",
    "Publication Title": "2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "21-Jun-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 469,
    "End Page": 481,
    "Abstract": "Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. While hybrid approaches aim for the \u201a\u00c4\u00fabest of both worlds,\u201a\u00c4\u00f9 the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges-and resultant bugs-involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation-the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.",
    "ISSN": "2574-3864",
    "ISBNs": "978-1-4503-9303-4",
    "DOI": "10.1145/3524842.3528455",
    "Funding Information": "City University of New York; ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796227",
    "Author Keywords": "empirical studies;deep learning;imperative programs;hybrid programming paradigms;graph-based execution;software evolution",
    "IEEE Terms": "Deep learning;Codes;Neural networks;Computer bugs;Writing;Reliability;Data mining",
    "Mesh_Terms": "",
    "Article Citation Count": 2,
    "Patent Citation Count": null,
    "Reference Count": 103,
    "License": "",
    "Online Date": "21-Jun-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Beyond Duplicates: Towards Understanding and Predicting Link Types in Issue Tracking Systems",
    "Authors": "C. M. L\u221a\u00baders; A. Bouraffa; W. Maalej",
    "Author Affiliations": "University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany",
    "Publication Title": "2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "21-Jun-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 48,
    "End Page": 60,
    "Abstract": "Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, or Subtask. While previous research has mostly focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. For this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal/Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal/ Causal links. Motivated by the differences between the link types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on the JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice.",
    "ISSN": "2574-3864",
    "ISBNs": "978-1-4503-9303-4",
    "DOI": "10.1145/3524842.3528457",
    "Funding Information": "European Union(grant numbers:732463); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796252",
    "Author Keywords": "Issue Management;Issue Tracking System;Duplicate Detection;Link Type Detection;Dependency Management",
    "IEEE Terms": "Training;Analytical models;Uncertainty;Semantics;Training data;Organizations;Predictive models",
    "Mesh_Terms": "",
    "Article Citation Count": 2,
    "Patent Citation Count": null,
    "Reference Count": 42,
    "License": "",
    "Online Date": "21-Jun-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Cold-Start Software Analytics",
    "Authors": "J. Guo; M. Rahimi; J. Cleland-Huang; A. Rasin; J. H. Hayes; M. Vierhauser",
    "Author Affiliations": "School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; School of Computing, DePaul University, Chicago, IL, USA; Computer Science Department, University of Kentucky, USA; CDL MEVSS, Johannes Kepler University, Linz, Austria",
    "Publication Title": "2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "26-Jan-17",
    "Publication Year": 2016,
    "Volume": null,
    "Issue": null,
    "Start Page": 142,
    "End Page": 153,
    "Abstract": "Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as \"who is the expert?,'' \"which classes are fault prone?,'' or even \"who are the domain experts for these fault-prone classes?'' Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.",
    "ISSN": "",
    "ISBNs": "978-1-4503-4186-8",
    "DOI": "",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832895",
    "Author Keywords": "Cold-start;Software Analytics;Configuration",
    "IEEE Terms": "Software;Measurement;Predictive models;Analytical models;Context;Training;Software engineering",
    "Mesh_Terms": "",
    "Article Citation Count": null,
    "Patent Citation Count": null,
    "Reference Count": 51,
    "License": "",
    "Online Date": "26-Jan-17",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Noisy Label Learning for Security Defects",
    "Authors": "R. Croft; M. A. Babar; H. Chen",
    "Author Affiliations": "CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia; CREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide Cyber Security Cooperative Research Centre, Adelaide, Australia",
    "Publication Title": "2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "21-Jun-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 435,
    "End Page": 447,
    "Abstract": "Data-driven software engineering processes, such as vulnerability prediction heavily rely on the quality of the data used. In this paper, we observe that it is infeasible to obtain a noise-free security defect dataset in practice. Despite the vulnerable class, the non-vulnerable modules are difficult to be verified and determined as truly exploit free given the limited manual efforts available. It results in uncertainty, introduces labeling noise in the datasets and affects conclusion validity. To address this issue, we propose novel learning methods that are robust to label impurities and can leverage the most from limited label data; noisy label learning. We investigate various noisy label learning methods applied to soft-ware vulnerability prediction. Specifically, we propose a two-stage learning method based on noise cleaning to identify and remediate the noisy samples, which improves AUC and recall of baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several hurdles in terms of achieving a performance upper bound with semi-omniscient knowledge of the label noise. Overall, the experimental results show that learning from noisy labels can be effective for data-driven software and security analytics.",
    "ISSN": "2574-3864",
    "ISBNs": "978-1-4503-9303-4",
    "DOI": "10.1145/3524842.3528446",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796240",
    "Author Keywords": "machine learning;noisy label learning;software vulnerabilities",
    "IEEE Terms": "Learning systems;Upper bound;Uncertainty;Manuals;Predictive models;Software;Noise measurement",
    "Mesh_Terms": "",
    "Article Citation Count": 4,
    "Patent Citation Count": null,
    "Reference Count": 76,
    "License": "",
    "Online Date": "21-Jun-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "Evaluating the effectiveness of local explanation methods on source code-based defect prediction models",
    "Authors": "Y. Gao; Y. Zhu; Q. Yu",
    "Author Affiliations": "School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China; School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China; School of Computer Science Jiangsu Normal University, Xuzhou, Jiangsu, China",
    "Publication Title": "2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR)",
    "Date Added To Xplore": "21-Jun-22",
    "Publication Year": 2022,
    "Volume": null,
    "Issue": null,
    "Start Page": 640,
    "End Page": 645,
    "Abstract": "Interpretation has been considered as one of key factors for applying defect prediction in practice. As one way for interpretation, local explanation methods has been widely used for certain predictions on datasets of traditional features. There are also attempts to use local explanation methods on source code-based defect prediction models, but unfortunately, it will get poor results. Since it is unclear how effective those local explanation methods are, we evaluate such methods with automatic metrics which focus on local faithfulness and explanation precision. Based on the results of experiments, we find that the effectiveness of local explanation methods depends on the adopted defect prediction models. They are effective on token frequency-based models, while they may not be effective enough to explain all predictions of deep learning-based models. Besides, we also find that the hyperparameter of local explanation methods should be carefully optimized to get more precise and meaningful explanation.",
    "ISSN": "2574-3864",
    "ISBNs": "978-1-4503-9303-4",
    "DOI": "10.1145/3524842.3528472",
    "Funding Information": "National Natural Science Foundation of China(grant numbers:62077029); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9796250",
    "Author Keywords": "Software Defect Prediction;Local Explanation;Explainable Machine Learning;LIME",
    "IEEE Terms": "Measurement;Codes;Predictive models;Software;Data mining",
    "Mesh_Terms": "",
    "Article Citation Count": 1,
    "Patent Citation Count": null,
    "Reference Count": 31,
    "License": "",
    "Online Date": "21-Jun-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  },
  {
    "Document Title": "A Procedure to Continuously Evaluate Predictive Performance of Just-In-Time Software Defect Prediction Models During Software Development",
    "Authors": "L. Song; L. L. Minku",
    "Author Affiliations": "Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen, Guangdong, China; School of Computer Science, University of Birmingham, Birmingham, U.K.",
    "Publication Title": "IEEE Transactions on Software Engineering",
    "Date Added To Xplore": "14-Feb-23",
    "Publication Year": 2023,
    "Volume": 49,
    "Issue": 2,
    "Start Page": 646,
    "End Page": 666,
    "Abstract": "Just-In-Time Software Defect Prediction (JIT-SDP) uses machine learning to predict whether software changes are defect-inducing or clean. When adopting JIT-SDP, changes in the underlying defect generating process may significantly affect the predictive performance of JIT-SDP models over time. Therefore, being able to continuously track the predictive performance of JIT-SDP models during the software development process is of utmost importance for software companies to decide whether or not to trust the predictions provided by such models over time. However, there has been little discussion on how to continuously evaluate predictive performance in practice, and such evaluation is not straightforward. In particular, labeled software changes that can be used for evaluation arrive over time with a delay, which in part corresponds to the time we have to wait to label software changes as \u201a\u00c4\u00f2clean\u201a\u00c4\u00f4 (waiting time). A clean label assigned based on a given waiting time may not correspond to the true label of the software changes. This can potentially hinder the validity of any continuous predictive performance evaluation procedure for JIT-SDP models. This paper provides the first discussion of how to continuously evaluate predictive performance of JIT-SDP models over time during the software development process, and the first investigation of whether and to what extent waiting time affects the validity of such continuous performance evaluation procedure in JIT-SDP. Based on 13 GitHub projects, we found that waiting time had a significant impact on the validity. Though typically small, the differences in estimated predicted performance were sometimes large, and thus inappropriate choices of waiting time can lead to misleading estimations of predictive performance over time. Such impact did not normally change the ranking between JIT-SDP models, and thus conclusions in terms of which JIT-SDP model performs better are likely reliable independent of the choice of waiting time, especially when considered across projects.",
    "ISSN": "1939-3520",
    "ISBNs": "",
    "DOI": "10.1109/TSE.2022.3158831",
    "Funding Information": "National Natural Science Foundation of China(grant numbers:62002148); Engineering and Physical Sciences Research Council(grant numbers:EP/R006660/2); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X386); Shenzhen Science and Technology Program(grant numbers:KQTD2016112514355531); Research Institute of Trustworthy Autonomous Systems; Southern University of Science and Technology(grant numbers:518055); ",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735354",
    "Author Keywords": "Just-in-time software defect prediction;performance evaluation procedure;concept drift;data stream learning;online learning;verification latency;and label noise",
    "IEEE Terms": "Software;Performance evaluation;Predictive models;Training;Estimation;Software reliability;Delays",
    "Mesh_Terms": "",
    "Article Citation Count": 3,
    "Patent Citation Count": null,
    "Reference Count": 39,
    "License": "IEEE",
    "Online Date": "15-Mar-22",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Journals",
    "verdict": "YES"
  },
  {
    "Document Title": "Where Is the Road for Issue Reports Classification Based on Text Mining?",
    "Authors": "Q. Fan; Y. Yu; G. Yin; T. Wang; H. Wang",
    "Author Affiliations": "College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China; College of Computer, National University of Defence Technology, Changsha, China",
    "Publication Title": "2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)",
    "Date Added To Xplore": "11-Dec-17",
    "Publication Year": 2017,
    "Volume": null,
    "Issue": null,
    "Start Page": 121,
    "End Page": 130,
    "Abstract": "Currently, open source projects receive various kinds of issues daily, because of the extreme openness of Issue Tracking System (ITS) in GitHub. ITS is a labor-intensive and time-consuming task of issue categorization for project managers. However, a contributor is only required a short textual abstract to report an issue in GitHub. Thus, most traditional classification approaches based on detailed and structured data (e.g., priority, severity, software version and so on) are difficult to adopt. In this paper, issue classification approaches on a large-scale dataset, including 80 popular projects and over 252,000 issue reports collected from GitHub, were investigated. First, four traditional text-based classification methods and their performances were discussed. Semantic perplexity (i.e., an issues description confuses bug-related sentences with nonbug-related sentences) is a crucial factor that affects the classification performances based on quantitative and qualitative study. Finally, A two-stage classifier framework based on the novel metrics of semantic perplexity of issue reports was designed. Results show that our two-stage classification can significantly improve issue classification performances.",
    "ISSN": "",
    "ISBNs": "978-1-5090-4039-1",
    "DOI": "10.1109/ESEM.2017.19",
    "Funding Information": "",
    "PDF Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170092",
    "Author Keywords": "issue tracking system;machine learning technique;mining software repositories",
    "IEEE Terms": "Computer bugs;Software;Data mining;Feature extraction;Semantics;Measurement",
    "Mesh_Terms": "",
    "Article Citation Count": 31,
    "Patent Citation Count": null,
    "Reference Count": 34,
    "License": "IEEE",
    "Online Date": "11-Dec-17",
    "Issue Date": "",
    "Meeting Date": "",
    "Publisher": "IEEE",
    "Document Identifier": "IEEE Conferences",
    "verdict": "YES"
  }
]